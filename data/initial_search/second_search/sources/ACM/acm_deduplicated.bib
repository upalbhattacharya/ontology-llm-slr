
@InProceedings{	  10.1145/3397271.3401289,
  author	= {Shi, Yunzhou and Luo, Zhiling and Zhu, Pengcheng and Ji,
		  Feng and Zhou, Wei and Chen, Haiqing and Yang, Yujiu},
  title		= {G2T: Generating Fluent Descriptions for Knowledge Graph},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401289},
  doi		= {10.1145/3397271.3401289},
  abstract	= {Generating natural language descriptions for knowledge
		  graph (KG) is an important category for intelligent
		  writing. Recent models on this task substitute the sequence
		  encoder in a commonly used encoder-decoder framework with a
		  graph encoder. However, these models suffer from entity
		  missing and repetition. In this paper, we propose a novel
		  end-to-end generation model named G2T, which integrates a
		  novel Graph Structure Enhanced Mechanism (GSEM) and a Copy
		  Coverage Loss (CCL). Instead of just considering graph
		  structure in the encoding phase in most existing methods,
		  our GSEM fully utilizes graph structure in the decoding
		  phase and helps to mitigate entity missing problem.
		  Moreover, our CCL can further improve performance by
		  avoiding generating repeated entities. With their help, our
		  model is capable of generating fluent description for KG.
		  The results of automatic and human evaluations show that
		  our model outperforms the state-of-the-art models.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1861–1864},
  numpages	= {4},
  keywords	= {knowledge graph, knowledge representation, natural
		  language generation},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@InProceedings{	  10.1145/3340531.3412028,
  author	= {Leblay, Julien and Chekol, Melisachew Wudage and Liu,
		  Xin},
  title		= {Towards Temporal Knowledge Graph Embeddings with Arbitrary
		  Time Precision},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412028},
  doi		= {10.1145/3340531.3412028},
  abstract	= {Acknowledging the dynamic nature of knowledge graphs, the
		  problem of learning temporal knowledge graph embeddings has
		  recently gained attention. Essentially, the goal is to
		  learn vector representation for the nodes and edges of a
		  knowledge graph taking time into account. These
		  representations must preserve certain properties of the
		  original graph, so as to allow not only classification or
		  clustering tasks, as for classical graph embeddings, but
		  also approximate time-dependent query answering or link
		  predictions over knowledge graphs. For instance, "who was
		  the leader of Germany in 1994?'' or "when was Bonn the
		  capital of Germany?''Several existing work in the area
		  adapt existing knowledge graph embedding models, adding a
		  time dimension, usually restricting to one time
		  granularity, like years or days, or treating time as fixed
		  labels. However, this is not adequate for many facts of
		  life, for instance historical and sensory data. In this
		  work, we introduce and evaluate an approach that gracefully
		  adjusts to time validity of virtually any granularity. Our
		  model is robust to non-contiguous validity periods. It is
		  generic enough to adapt to many existing non-temporal
		  models and its size (number of parameters) does not depend
		  on the size of the graph (number of entities and
		  relations).},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {685–694},
  numpages	= {10},
  keywords	= {learning representations, temporal knowledge graph},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3394486.3403143,
  author	= {Zhou, Kun and Zhao, Wayne Xin and Bian, Shuqing and Zhou,
		  Yuanhang and Wen, Ji-Rong and Yu, Jingsong},
  title		= {Improving Conversational Recommender Systems via Knowledge
		  Graph based Semantic Fusion},
  year		= {2020},
  isbn		= {9781450379984},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394486.3403143},
  doi		= {10.1145/3394486.3403143},
  abstract	= {Conversational recommender systems (CRS) aim to recommend
		  high-quality items to users through interactive
		  conversations. Although several efforts have been made for
		  CRS, two major issues still remain to be solved. First, the
		  conversation data itself lacks of sufficient contextual
		  information for accurately understanding users' preference.
		  Second, there is a semantic gap between natural language
		  expression and item-level user preference.To address these
		  issues, we incorporate both word-oriented and
		  entity-oriented knowledge graphs~(KG) to enhance the data
		  representations in CRSs, and adopt Mutual Information
		  Maximization to align the word-level and entity-level
		  semantic spaces. Based on the aligned semantic
		  representations, we further develop a KG-enhanced
		  recommender component for making accurate recommendations,
		  and a KG-enhanced dialog component that can generate
		  informative keywords or entities in the response text.
		  Extensive experiments have demonstrated the effectiveness
		  of our approach in yielding better performance on both
		  recommendation and conversation tasks.},
  booktitle	= {Proceedings of the 26th ACM SIGKDD International
		  Conference on Knowledge Discovery &amp; Data Mining},
  pages		= {1006–1014},
  numpages	= {9},
  keywords	= {conversational recommender system, knowledge graph, mutual
		  information maximization},
  location	= {Virtual Event, CA, USA},
  series	= {KDD '20}
}

@InProceedings{	  10.1145/3387904.3389281,
  author	= {Zhang, Jinglei and Xie, Rui and Ye, Wei and Zhang, Yuhan
		  and Zhang, Shikun},
  title		= {Exploiting Code Knowledge Graph for Bug Localization via
		  Bi-directional Attention},
  year		= {2020},
  isbn		= {9781450379588},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3387904.3389281},
  doi		= {10.1145/3387904.3389281},
  abstract	= {Bug localization automatic localize relevant source files
		  given a natural language description of bug within a
		  software project. For a large project containing hundreds
		  and thousands of source files, developers need cost lots of
		  time to understand bug reports generated by quality
		  assurance and localize these buggy source files.
		  Traditional methods are heavily depending on the
		  information retrieval technologies which rank the
		  similarity between source files and bug reports in lexical
		  level. Recently, deep learning based models are used to
		  extract semantic information of code with significant
		  improvements for bug localization. However, programming
		  language is a highly structural and logical language, which
		  contains various relations within and cross source files.
		  Thus, we propose KGBugLocator to utilize knowledge graph
		  embeddings to extract these interrelations of code, and a
		  keywords supervised bi-directional attention mechanism
		  regularize model with interactive information between
		  source files and bug reports. With extensive experiments on
		  four different projects, we prove our model can reach the
		  new the-state-of-art(SOTA) for bug localization.},
  booktitle	= {Proceedings of the 28th International Conference on
		  Program Comprehension},
  pages		= {219–229},
  numpages	= {11},
  keywords	= {bug localization, code representation, deep learning,
		  knowledge graph},
  location	= {Seoul, Republic of Korea},
  series	= {ICPC '20}
}

@InProceedings{	  10.1145/3366423.3380107,
  author	= {Zhang, Hongming and Liu, Xin and Pan, Haojie and Song,
		  Yangqiu and Leung, Cane Wing-Ki},
  title		= {ASER: A Large-scale Eventuality Knowledge Graph},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380107},
  doi		= {10.1145/3366423.3380107},
  abstract	= {Understanding human’s language requires complex world
		  knowledge. However, existing large-scale knowledge graphs
		  mainly focus on knowledge about entities while ignoring
		  knowledge about activities, states, or events, which are
		  used to describe how entities or things act in the real
		  world. To fill this gap, we develop ASER (activities,
		  states, events, and their relations), a large-scale
		  eventuality knowledge graph extracted from more than
		  11-billion-token unstructured textual data. ASER contains
		  15 relation types belonging to five categories, 194-million
		  unique eventualities, and 64-million unique edges among
		  them. Both intrinsic and extrinsic evaluations demonstrate
		  the quality and effectiveness of ASER.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {201–211},
  numpages	= {11},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@InProceedings{	  10.1145/3366423.3380005,
  author	= {Lissandrini, Matteo and Mottin, Davide and Palpanas,
		  Themis and Velegrakis, Yannis},
  title		= {Graph-Query Suggestions for Knowledge Graph Exploration},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380005},
  doi		= {10.1145/3366423.3380005},
  abstract	= {We consider the task of exploratory search through graph
		  queries on knowledge graphs. We propose to assist the user
		  by expanding the query with intuitive suggestions to
		  provide a more informative (full) query that can retrieve
		  more detailed and relevant answers. To achieve this result,
		  we propose a model that can bridge graph search paradigms
		  with well-established techniques for information-retrieval.
		  Our approach does not require any additional knowledge from
		  the user and builds on principled language modelling
		  approaches. We empirically show the effectiveness and
		  efficiency of our approach on a large knowledge graph and
		  how our suggestions are able to help build more complete
		  and informative queries.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {2549–2555},
  numpages	= {7},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@InProceedings{	  10.1145/3374587.3374603,
  author	= {Xiaohui, Chen and Yinzhen, Liu and Li, Xu and Lei, Ge and
		  Yiwei, Ma},
  title		= {The Construction Method of Geographic Knowledge Graph
		  Ontology Model Based on GML},
  year		= {2020},
  isbn		= {9781450376273},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3374587.3374603},
  doi		= {10.1145/3374587.3374603},
  abstract	= {Geographic ontology model is the conceptual model of
		  geographic knowledge graph and the logical basis for
		  constructing the pattern layer of geographic knowledge
		  graph. In the classification of geographic ontology
		  research, geographic ontology model is in the category of
		  domain ontology. It is a set of abstract structures to
		  express ontology according to the spatial location,
		  attribute characteristics and relational characteristics of
		  geographic data. This paper discussed the logical
		  components and architecture of geographic ontology,
		  designed the geographic ontology model reference to GML,
		  described the model using OWL language, and constructed the
		  geographic ontology model based on GML. The geographic
		  ontology model comprises three sub-models: element model,
		  geometric model and spatial relation model. Finally, based
		  on Prot\'{e}g\'{e} ontology construction tool, this paper
		  designed the semantic description of geographic entity and
		  realized the construction of geographic ontology system.},
  booktitle	= {Proceedings of the 2019 3rd International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {138–143},
  numpages	= {6},
  keywords	= {GML, Geographic knowledge graph, geographic ontology,
		  logical composition, ontology model construction},
  location	= {Normal, IL, USA},
  series	= {CSAI '19}
}

@InProceedings{	  10.1145/3184558.3191639,
  author	= {Leblay, Julien and Chekol, Melisachew Wudage},
  title		= {Deriving Validity Time in Knowledge Graph},
  year		= {2018},
  isbn		= {9781450356404},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3184558.3191639},
  doi		= {10.1145/3184558.3191639},
  abstract	= {Knowledge Graphs (KGs) are a popular means to represent
		  knowledge on the Web, typically in the form of node/edge
		  labelled directed graphs. We consider temporal KGs, in
		  which edges are further annotated with time intervals,
		  reflecting when the relationship between entities held in
		  time. In this paper, we focus on the task of predicting
		  time validity for unannotated edges. We introduce the
		  problem as a variation of relational embedding. We adapt
		  existing approaches, and explore the importance example
		  selection and the incorporation of side information in the
		  learning process. We present our experimental evaluation in
		  details.},
  booktitle	= {Companion Proceedings of the The Web Conference 2018},
  pages		= {1771–1776},
  numpages	= {6},
  keywords	= {factorization machines, temporal knowledge graph},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3308560.3317708,
  author	= {Mehta, Aman and Singhal, Aashay and Karlapalem,
		  Kamalakar},
  title		= {Scalable Knowledge Graph Construction over Text using Deep
		  Learning based Predicate Mapping},
  year		= {2019},
  isbn		= {9781450366755},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308560.3317708},
  doi		= {10.1145/3308560.3317708},
  abstract	= {Automatic extraction of information from text and its
		  transformation into a structured format is an important
		  goal in both Semantic Web Research and computational
		  linguistics. Knowledge Graphs (KG) serve as an intuitive
		  way to provide structure to unstructured text. A fact in a
		  KG is expressed in the form of a triple which captures
		  entities and their interrelationships (predicates).
		  Multiple triples extracted from text can be semantically
		  identical but they may have a vocabulary gap which could
		  lead to an explosion in the number of redundant triples.
		  Hence, to get rid of this vocabulary gap, there is a need
		  to map triples to a homogeneous namespace. In this work, we
		  present an end-to-end KG construction system, which
		  identifies and extracts entities and relationships from
		  text and maps them to the homogenous DBpedia namespace. For
		  Predicate Mapping, we propose a Deep Learning architecture
		  to model semantic similarity. This mapping step is
		  computation heavy, owing to the large number of triples in
		  DBpedia. We identify and prune unnecessary comparisons to
		  make this step scalable. Our experiments show that the
		  proposed approach is able to construct a richer KG at a
		  significantly lower computation cost with respect to
		  previous work.},
  booktitle	= {Companion Proceedings of The 2019 World Wide Web
		  Conference},
  pages		= {705–713},
  numpages	= {9},
  keywords	= {Deep Learning, Knowledge Graph, Predicate Mapping,
		  Scalability, Sentence Simplification},
  location	= {San Francisco, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3038912.3052558,
  author	= {Xiong, Chenyan and Power, Russell and Callan, Jamie},
  title		= {Explicit Semantic Ranking for Academic Search via
		  Knowledge Graph Embedding},
  year		= {2017},
  isbn		= {9781450349130},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3038912.3052558},
  doi		= {10.1145/3038912.3052558},
  abstract	= {This paper introduces Explicit Semantic Ranking (ESR), a
		  new ranking technique that leverages knowledge graph
		  embedding. Analysis of the query log from our academic
		  search engine, SemanticScholar.org, reveals that a major
		  error source is its inability to understand the meaning of
		  research concepts in queries. To addresses this challenge,
		  ESR represents queries and documents in the entity space
		  and ranks them based on their semantic connections from
		  their knowledge graph embedding. Experiments demonstrate
		  ESR's ability in improving Semantic Scholar's online
		  production system, especially on hard queries where
		  word-based ranking fails.},
  booktitle	= {Proceedings of the 26th International Conference on World
		  Wide Web},
  pages		= {1271–1279},
  numpages	= {9},
  keywords	= {academic search, entity-based ranking, knowledge graph},
  location	= {Perth, Australia},
  series	= {WWW '17}
}

@InProceedings{	  10.1145/3234944.3234963,
  author	= {Lin, Xinshi and Lam, Wai and Lai, Kwun Ping},
  title		= {Entity Retrieval in the Knowledge Graph with Hierarchical
		  Entity Type and Content},
  year		= {2018},
  isbn		= {9781450356565},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3234944.3234963},
  doi		= {10.1145/3234944.3234963},
  abstract	= {We investigate the task of ad-hoc entity retrieval from a
		  knowledge graph with hierarchical entity types and entity
		  descriptions. Our model directly encodes them into a Markov
		  random field based framework via a path aware smoothing
		  method. We conduct experiments on recent benchmark datasets
		  and investigate the incorporation of the Wikipedia type and
		  article information. The results show that our framework
		  achieves improvements over the existing and
		  state-of-the-art models.},
  booktitle	= {Proceedings of the 2018 ACM SIGIR International Conference
		  on Theory of Information Retrieval},
  pages		= {211–214},
  numpages	= {4},
  keywords	= {entity retrieval, structure-aware smoothing, type
		  taxonomy},
  location	= {Tianjin, China},
  series	= {ICTIR '18}
}

@InProceedings{	  10.1145/3340531.3412685,
  author	= {Li, Feng-Lin and Chen, Hehong and Xu, Guohai and Qiu, Tian
		  and Ji, Feng and Zhang, Ji and Chen, Haiqing},
  title		= {AliMeKG: Domain Knowledge Graph Construction and
		  Application in E-commerce},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412685},
  doi		= {10.1145/3340531.3412685},
  abstract	= {Pre-­sales customer service is of importance to
		  E­-commerce plat­forms as it contributes to optimizing
		  customers? buying process. To better serve users, we
		  propose AliMe KG, a domain knowledge graph in E­-commerce
		  that captures user problems, points of inter­est (POI),
		  item information and relations thereof. It helps to under­
		  stand user needs, answer pre­-sales questions and generate
		  explana­tion texts. We applied AliMe KG to several online
		  business scenar­ios such as shopping guide, question
		  answering over properties and selling point generation, and
		  gained positive and beneficial business results. In the
		  paper, we systematically introduce how we construct domain
		  knowledge graph from free text, and demonstrate its
		  busi­ness value with several applications. Our experience
		  shows that min­ ing structured knowledge from free text in
		  vertical domain is prac­ticable, and can be of substantial
		  value in industrial settings.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2581–2588},
  numpages	= {8},
  keywords	= {domain knowledge graph, e-commerce, pre-sales customer
		  service},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3292500.3330942,
  author	= {Jiang, Tianwen and Zhao, Tong and Qin, Bing and Liu, Ting
		  and Chawla, Nitesh V. and Jiang, Meng},
  title		= {The Role of "Condition": A Novel Scientific Knowledge
		  Graph Representation and Construction Model},
  year		= {2019},
  isbn		= {9781450362016},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3292500.3330942},
  doi		= {10.1145/3292500.3330942},
  abstract	= {Conditions play an essential role in scientific
		  observations, hypotheses, and statements. Unfortunately,
		  existing scientific knowledge graphs (SciKGs) represent
		  factual knowledge as a flat relational network of concepts,
		  as same as the KGs in general domain, without considering
		  the conditions of the facts being valid, which loses
		  important contexts for inference and exploration. In this
		  work, we propose a novel representation of SciKG, which has
		  three layers. The first layer has concept nodes, attribute
		  nodes, as well as the attaching links from attribute to
		  concept. The second layer represents both fact tuples and
		  condition tuples. Each tuple is a node of the relation
		  name, connecting to the subject and object that are concept
		  or attribute nodes in the first layer. The third layer has
		  nodes of statement sentences traceable to the original
		  paper and authors. Each statement node connects to a set of
		  fact tuples and/or condition tuples in the second layer. We
		  design a semi-supervised Multi-Input Multi-Output sequence
		  labeling model that learns complex dependencies between the
		  sequence tags from multiple signals and generates output
		  sequences for fact and condition tuples. It has a
		  self-training module of multiple strategies to leverage the
		  massive scientific data for better performance when manual
		  annotation is limited. Experiments on a data set of 141M
		  sentences show that our model outperforms existing methods
		  and the SciKGs we constructed provide a good understanding
		  of the scientific statements.},
  booktitle	= {Proceedings of the 25th ACM SIGKDD International
		  Conference on Knowledge Discovery &amp; Data Mining},
  pages		= {1634–1642},
  numpages	= {9},
  location	= {Anchorage, AK, USA},
  series	= {KDD '19}
}

@InProceedings{	  10.1145/3338533.3366552,
  author	= {Wei, Jiwei and Yang, Yang and Li, Jingjing and Zhu, Lei
		  and Zuo, Lin and Shen, Heng Tao},
  title		= {Residual Graph Convolutional Networks for Zero-Shot
		  Learning},
  year		= {2020},
  isbn		= {9781450368414},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3338533.3366552},
  doi		= {10.1145/3338533.3366552},
  abstract	= {Most existing Zero-Shot Learning (ZSL) approaches adopt
		  the semantic space as a bridge to classify unseen
		  categories. However, it is difficult to transfer knowledge
		  from seen categories to unseen categories through semantic
		  space, since the correlations among categories are
		  uncertain and ambiguous in the semantic space. In this
		  paper, we formulated zero-shot learning as a classifier
		  weight regression problem. Specifically, we propose a novel
		  Residual Graph Convolution Network (ResGCN) which takes
		  word embeddings and knowledge graph as inputs and outputs a
		  visual classifier for each category. ResGCN can effectively
		  alleviate the problem of over-smoothing and over-fitting.
		  During the test, an unseen image can be classified by
		  ranking the inner product of its visual feature and
		  predictive visual classifiers. Moreover, we provide a new
		  method to build a better knowledge graph. Our approach not
		  only further enhances the correlations among categories,
		  but also makes it easy to add new categories to the
		  knowledge graph. Experiments conducted on the large-scale
		  ImageNet 2011 21K dataset demonstrate that our method
		  significantly outperforms existing state-of-the-art
		  approaches.},
  booktitle	= {Proceedings of the 1st ACM International Conference on
		  Multimedia in Asia},
  articleno	= {9},
  numpages	= {6},
  keywords	= {Knowledge Graph, ResGCN, Zero-Shot Learning},
  location	= {Beijing, China},
  series	= {MMAsia '19}
}

@Article{	  10.1145/3361738,
  author	= {Ai, Qingyao and Zhang, Yongfeng and Bi, Keping and Croft,
		  W. Bruce},
  title		= {Explainable Product Search with a Dynamic Relation
		  Embedding Model},
  year		= {2019},
  issue_date	= {January 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {38},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3361738},
  doi		= {10.1145/3361738},
  abstract	= {Product search is one of the most popular methods for
		  customers to discover products online. Most existing
		  studies on product search focus on developing effective
		  retrieval models that rank items by their likelihood to be
		  purchased. However, they ignore the problem that there is a
		  gap between how systems and customers perceive the
		  relevance of items. Without explanations, users may not
		  understand why product search engines retrieve certain
		  items for them, which consequentially leads to imperfect
		  user experience and suboptimal system performance in
		  practice. In this work, we tackle this problem by
		  constructing explainable retrieval models for product
		  search. Specifically, we propose to model the “search and
		  purchase” behavior as a dynamic relation between users
		  and items, and create a dynamic knowledge graph based on
		  both the multi-relational product data and the context of
		  the search session. Ranking is conducted based on the
		  relationship between users and items in the latent space,
		  and explanations are generated with logic inferences and
		  entity soft matching on the knowledge graph. Empirical
		  experiments show that our model, which we refer to as the
		  Dynamic Relation Embedding Model (DREM), significantly
		  outperforms the state-of-the-art baselines and has the
		  ability to produce reasonable explanations for search
		  results.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= oct,
  articleno	= {4},
  numpages	= {29},
  keywords	= {Product search, explainable model, knowledge graph,
		  relation embedding}
}

@InProceedings{	  10.1145/3109859.3109889,
  author	= {Palumbo, Enrico and Rizzo, Giuseppe and Troncy,
		  Rapha\"{e}l},
  title		= {entity2rec: Learning User-Item Relatedness from Knowledge
		  Graphs for Top-N Item Recommendation},
  year		= {2017},
  isbn		= {9781450346528},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3109859.3109889},
  doi		= {10.1145/3109859.3109889},
  abstract	= {Knowledge Graphs have proven to be extremely valuable to
		  recommender systems, as they enable hybrid graph-based
		  recommendation models encompassing both collaborative and
		  content information. Leveraging this wealth of
		  heterogeneous information for top-N item recommendation is
		  a challenging task, as it requires the ability of
		  effectively encoding a diversity of semantic relations and
		  connectivity patterns. In this work, we propose entity2rec,
		  a novel approach to learning user-item relatedness from
		  knowledge graphs for top-N item recommendation. We start
		  from a knowledge graph modeling user-item and item-item
		  relations and we learn property-specific vector
		  representations of users and items applying neural language
		  models on the network. These representations are used to
		  create property-specific user-item relatedness features,
		  which are in turn fed into learning to rank algorithms to
		  learn a global relatedness model that optimizes top-N item
		  recommendations. We evaluate the proposed approach in terms
		  of ranking quality on the MovieLens 1M dataset,
		  outperforming a number of state-of-the-art recommender
		  systems, and we assess the importance of property-specific
		  relatedness scores on the overall ranking quality.},
  booktitle	= {Proceedings of the Eleventh ACM Conference on Recommender
		  Systems},
  pages		= {32–36},
  numpages	= {5},
  keywords	= {hybrid recommender system, knowledge graph, knowledge
		  graph embeddings, learning to rank, linked open data,
		  neural language models, node2vec, word2vec},
  location	= {Como, Italy},
  series	= {RecSys '17}
}

@InProceedings{	  10.1145/3397271.3401067,
  author	= {Huang, Longtao and Yuan, Bo and Zhang, Rong and Lu, Quan},
  title		= {Towards Linking Camouflaged Descriptions to Implicit
		  Products in E-commerce},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401067},
  doi		= {10.1145/3397271.3401067},
  abstract	= {As the emergence of E-commerce services, billions of
		  products are sold online everyday. How to detect illegal
		  products from the large-scale online products has become an
		  important and practical research problem. In order to evade
		  detection, malicious sellers usually utilize camouflaged
		  text to describe their illegal products implicitly. Thus
		  brings great challenges to the current detection systems
		  since newly camouflaged text can hardly be learned from
		  historical data and the distribution of illegal and normal
		  products is extremely unbalanced. Rather than solving this
		  problem as a classification task in most previous efforts,
		  we reformulate the problem from a perspective of implicit
		  entity linking, which targets at linking a camouflaged
		  description to a known product. In this paper, we introduce
		  three types of context that could help to infer implicit
		  entity from camouflaged descriptions and propose an
		  end-to-end contextual representation model to capture the
		  effect of different context. Furthermore, we involve a
		  symmetric metric to model the matching score of the input
		  title to the product by learning the mutual effect among
		  the context. The experimental results on the datasets
		  collected from a real-world E-commerce site demonstrate the
		  advantage of the proposed model against the
		  state-of-the-art methods.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {901–910},
  numpages	= {10},
  keywords	= {implicit entity linking, knowledge graph, neural
		  networks},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@InProceedings{	  10.1145/3343413.3378011,
  author	= {Torbati, Ghazaleh H. and Yates, Andrew and Weikum,
		  Gerhard},
  title		= {Personalized Entity Search by Sparse and Scrutable User
		  Profiles},
  year		= {2020},
  isbn		= {9781450368926},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3343413.3378011},
  doi		= {10.1145/3343413.3378011},
  abstract	= {Prior work on personalizing web search results has focused
		  on considering query-and-click logs to capture users'
		  individual interests. For product search, extensive user
		  histories about purchases and ratings have been exploited.
		  However, for general entity search, such as for books on
		  specific topics or travel destinations with certain
		  features, personalization is largely underexplored. In this
		  paper, we address personalization of book search, as an
		  exemplary case of entity search, by exploiting sparse user
		  profiles obtained through online questionnaires. We devise
		  and compare a variety of re-ranking methods based on
		  language models or neural learning. Our experiments show
		  that even very sparse information about individuals can
		  enhance the effectiveness of the search results.},
  booktitle	= {Proceedings of the 2020 Conference on Human Information
		  Interaction and Retrieval},
  pages		= {427–431},
  numpages	= {5},
  keywords	= {knowledge graph, personalized entity search, sparse user
		  profile},
  location	= {Vancouver BC, Canada},
  series	= {CHIIR '20}
}

@InProceedings{	  10.1145/3433996.3433999,
  author	= {Li, Xiaolian and Zhang, Bo},
  title		= {Discussion on Natural Language Processing and AI Poetry},
  year		= {2020},
  isbn		= {9781450388641},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3433996.3433999},
  doi		= {10.1145/3433996.3433999},
  abstract	= {Letting machines express and create like humans is an
		  important vision of artificial intelligence, and one of the
		  core technical fields to realize this vision is intelligent
		  writing. In recent years, intelligent writing has not only
		  developed rapidly in technology, but also has become more
		  and more important in its application. This article starts
		  with common application forms and examples of smart
		  writing, combined with practical experience, introduces the
		  core technology of smart writing, and discusses the way of
		  human-computer collaboration and the future development
		  direction of smart writing.},
  booktitle	= {Proceedings of the 2020 Conference on Artificial
		  Intelligence and Healthcare},
  pages		= {10–13},
  numpages	= {4},
  keywords	= {AI poetry, Artificial intelligence, Language model,
		  Natural language processing, Word sequence},
  location	= {Taiyuan, China},
  series	= {CAIH2020}
}

@InProceedings{	  10.1145/3331184.3331427,
  author	= {Firsov, Anton and Bugay, Vladimir and Karpenko, Anton},
  title		= {USEing Transfer Learning in Retrieval of Statistical
		  Data},
  year		= {2019},
  isbn		= {9781450361729},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331184.3331427},
  doi		= {10.1145/3331184.3331427},
  abstract	= {DSSM-like models showed good results in retrieval of short
		  documents that semantically match the query. However, these
		  models require large collections of click-through data that
		  are not available in some domains. On the other hand, the
		  recent advances in NLP demonstrated the possibility to
		  fine-tune language models and models trained on one set of
		  tasks to achieve a state of the art results on a multitude
		  of other tasks or to get competitive results using much
		  smaller training sets. Following this trend, we combined
		  DSSM-like architecture with USE (Universal Sentence
		  Encoder) and BERT (Bidirectional Encoder Representations
		  from Transformers) models in order to be able to fine-tune
		  them on a small amount of click-through data and use them
		  for information retrieval. This approach allowed us to
		  significantly improve our search engine for statistical
		  data.},
  booktitle	= {Proceedings of the 42nd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1391–1392},
  numpages	= {2},
  keywords	= {information retrieval, language model, transfer learning},
  location	= {Paris, France},
  series	= {SIGIR'19}
}

@InProceedings{	  10.1145/3341162.3349310,
  author	= {Chen, Fanglin and Hong, Jason I.},
  title		= {Personal bits: mining interaction traces for personalized
		  task intelligence},
  year		= {2019},
  isbn		= {9781450368698},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341162.3349310},
  doi		= {10.1145/3341162.3349310},
  abstract	= {As we work, play, shop, and communicate in digital
		  interfaces, we continuously generate traces of information.
		  To turn such noisy sources of personal data into actual
		  insight, my research introduces Personal Bits, a service
		  that enables personalized task support in various kinds of
		  information tasks such as instant message handling,
		  information retrieval, and text entry. Personal Bits mines
		  a user's interaction traces with web apps and native mobile
		  apps and extracts task-centric entities. I present three
		  example apps for Personal Bits: Deja Wu, MessageOnTap, and
		  ContextBoard, to address inefficiencies presented in these
		  information tasks. Personal Bits acts as the central nexus
		  for intelligence between apps and interaction traces,
		  making it easy for apps to acquire personally relevant task
		  entities in fine granularity.},
  booktitle	= {Adjunct Proceedings of the 2019 ACM International Joint
		  Conference on Pervasive and Ubiquitous Computing and
		  Proceedings of the 2019 ACM International Symposium on
		  Wearable Computers},
  pages		= {358–362},
  numpages	= {5},
  keywords	= {information retrieval, intelligent user interface,
		  interaction traces, language model, messaging, personal
		  data, productivity, task intelligence},
  location	= {London, United Kingdom},
  series	= {UbiComp/ISWC '19 Adjunct}
}

@InProceedings{	  10.1145/3209542.3209548,
  author	= {Bhatia, Sumit and Vishwakarma, Harit},
  title		= {Know Thy Neighbors, and More! Studying the Role of Context
		  in Entity Recommendation},
  year		= {2018},
  isbn		= {9781450354271},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209542.3209548},
  doi		= {10.1145/3209542.3209548},
  abstract	= {Knowledge Graphs capture the semantic relations between
		  real-world entities and can thus, allow end-users to
		  explore different aspects of an entity of interest by
		  traversing through the edges in the graph. Most of the
		  state-of-the-art methods in entity recommendation are
		  limited in the sense that they allow users to search only
		  in the immediate neighborhood of the entity of interest.
		  This is majorly due to efficiency reasons as the search
		  space increases exponentially as we move further away from
		  the entity of interest in the graph. Often, users perform
		  the search task in the context of an information need and
		  we investigate the role this context can play in overcoming
		  the scalability issue and improving knowledge graph
		  exploration. Intuitively, only a small subset of entities
		  in the graph are relevant to a users' interest. We show how
		  can we efficiently select this sub-set by utilizing
		  contextual clues and using graph-theoretic measures to
		  further re-rank this set to offer highly relevant graph
		  exploration capabilities to end-users.},
  booktitle	= {Proceedings of the 29th on Hypertext and Social Media},
  pages		= {87–95},
  numpages	= {9},
  keywords	= {contextual entity recommendation, contextual exploration,
		  entity recommendation, entity retrieval, entity search,
		  information discovery, knowledge graph exploration},
  location	= {Baltimore, MD, USA},
  series	= {HT '18}
}

@InProceedings{	  10.1145/3397271.3401255,
  author	= {Lu, Junyu and Ren, Xiancong and Ren, Yazhou and Liu, Ao
		  and Xu, Zenglin},
  title		= {Improving Contextual Language Models for Response
		  Retrieval in Multi-Turn Conversation},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401255},
  doi		= {10.1145/3397271.3401255},
  abstract	= {As an important branch of current dialogue systems,
		  retrieval-based chatbots leverage information retrieval to
		  select proper predefined responses. Various promising
		  architectures have been designed for boosting response
		  retrieval, however, few researches exploit the
		  effectiveness of the pre-trained contextual language
		  models. In this paper, we propose two approaches to adapt
		  contextual language models in dialogue response selection
		  task. In detail, the Speaker Segmentation approach is
		  designed to discriminate different speakers to fully
		  utilize speaker characteristics. Besides, we propose the
		  Dialogue Augmentation approach, i.e., cutting off real
		  conversations at different time points, to enlarge the
		  training corpora. Compared with previous works which use
		  utterance-level representations, our augmented contextual
		  language models are able to obtain top-hole contextual
		  dialogue representations for deeper semantic understanding.
		  Evaluation on three large-scale datasets has demonstrated
		  that our proposed approaches yield better performance than
		  existing models.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1805–1808},
  numpages	= {4},
  keywords	= {augmentation, pre-trained language model, response
		  retrieval},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@Article{	  10.5555/3455716.3455786,
  author	= {Kazemi, Seyed Mehran and Goel, Rishab and Jain, Kshitij
		  and Kobyzev, Ivan and Sethi, Akshay and Forsyth, Peter and
		  Poupart, Pascal},
  title		= {Representation learning for dynamic graphs: a survey},
  year		= {2020},
  issue_date	= {January 2020},
  publisher	= {JMLR.org},
  volume	= {21},
  number	= {1},
  issn		= {1532-4435},
  abstract	= {Graphs arise naturally in many real-world applications
		  including social networks, recommender systems, ontologies,
		  biology, and computational finance. Traditionally, machine
		  learning models for graphs have been mostly designed for
		  static graphs. However, many applications involve evolving
		  graphs. This introduces important challenges for learning
		  and inference since nodes, attributes, and edges change
		  over time. In this survey, we review the recent advances in
		  representation learning for dynamic graphs, including
		  dynamic knowledge graphs. We describe existing models from
		  an encoder-decoder perspective, categorize these encoders
		  and decoders based on the techniques they employ, and
		  analyze the approaches in each category. We also review
		  several prominent applications and widely used datasets and
		  highlight directions for future research.},
  journal	= {J. Mach. Learn. Res.},
  month		= jan,
  articleno	= {70},
  numpages	= {73},
  keywords	= {graph representation learning, dynamic graphs, knowledge
		  graph embedding, heterogeneous information networks}
}

@InProceedings{	  10.1145/3318464.3386145,
  author	= {Liu, Bang and Guo, Weidong and Niu, Di and Luo, Jinwen and
		  Wang, Chaoyue and Wen, Zhen and Xu, Yu},
  title		= {GIANT: Scalable Creation of a Web-scale Ontology},
  year		= {2020},
  isbn		= {9781450367356},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3318464.3386145},
  doi		= {10.1145/3318464.3386145},
  abstract	= {Understanding what online users may pay attention to on
		  the web is key to content recommendation and search
		  services. These services will benefit from a highly
		  structured and web-scale ontology of entities, concepts,
		  events, topics and categories. While existing knowledge
		  bases and taxonomies embody a large volume of entities and
		  categories, we argue that they fail to discover properly
		  grained concepts, events and topics in the language style
		  of online users. Neither is a logically structured ontology
		  maintained among these notions. In this paper, we present
		  GIANT, a mechanism to construct a user-centered, web-scale,
		  structured ontology, containing a large number of natural
		  language phrases conforming to user attentions at various
		  granularities, mined from the vast volume of web documents
		  and search click logs. Various types of edges are also
		  constructed to maintain a hierarchy in the ontology. We
		  present our detailed techniques used in GIANT, and evaluate
		  the proposed models and methods as compared to a variety of
		  baselines, as well as deploy the resulted Attention
		  Ontology in real-world applications, involving over a
		  billion users, to observe its effect on content
		  recommendation. The online performance of the ontology
		  built by GIANT proves that it can significantly improve the
		  click-through rate in news feeds recommendation.},
  booktitle	= {Proceedings of the 2020 ACM SIGMOD International
		  Conference on Management of Data},
  pages		= {393–409},
  numpages	= {17},
  keywords	= {concept mining, document understanding, event mining,
		  ontology creation, user interest modeling},
  location	= {Portland, OR, USA},
  series	= {SIGMOD '20}
}

@InProceedings{	  10.1145/3106426.3109052,
  author	= {Albukhitan, Saeed and Helmy, Tarek and Alnazer, Ahmed},
  title		= {Arabic ontology learning using deep learning},
  year		= {2017},
  isbn		= {9781450349512},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3106426.3109052},
  doi		= {10.1145/3106426.3109052},
  abstract	= {Ontology, the backbone of Semantic Web, is defined as the
		  formal specification of conceptual hierarchy with
		  relationships between concepts. Ontology Learning (OL) is a
		  process to create an ontology from text automatically or
		  semi-automatically. OL is an important topic in the
		  Semantic Web field in the last two decades but it is still
		  not mature in Arabic not like Latin languages. Currently,
		  there is a limited support for using knowledge from Arabic
		  literature automatically in semantically-enabled systems.
		  Deep Learning (DL), an artificial neural networks learning
		  based application, has proved a good improvement in
		  multiple areas including text mining. By using DL, it is
		  possible to have word embedding as distributed word
		  representations from textual data. The application of DL to
		  aid Arabic ontology development remains largely unexplored.
		  This paper investigates the performance of implementing DL
		  with Arabic ontology learning tasks using major models such
		  as Continuous Bag of Words (CBOW) and Skip-gram. Initial
		  performance results are promising as an effective
		  application of Arabic ontology learning.},
  booktitle	= {Proceedings of the International Conference on Web
		  Intelligence},
  pages		= {1138–1142},
  numpages	= {5},
  keywords	= {arabic ontology, deep learning, ontology learning},
  location	= {Leipzig, Germany},
  series	= {WI '17}
}

@Article{	  10.1145/3399630,
  author	= {Tao, Jie and Zhou, Lina},
  title		= {A Weakly Supervised WordNet-Guided Deep Learning Approach
		  to Extracting Aspect Terms from Online Reviews},
  year		= {2020},
  issue_date	= {September 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {11},
  number	= {3},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3399630},
  doi		= {10.1145/3399630},
  abstract	= {The unstructured nature of online reviews makes it
		  inefficient and inconvenient for prospective consumers to
		  research and use in support of purchase decision making.
		  The aspects of products provide a fine-grained meaningful
		  perspective for understanding and organizing review texts.
		  Traditional aspect term extraction approaches rely on
		  discrete language models that treat words in isolation.
		  Despite that continuous-space language models have
		  demonstrated promise in addressing a wide range of
		  problems, their application in aspect term extraction faces
		  significant challenges. For instance, existing
		  continuous-space language models typically require large
		  collections of labeled data, which remain difficult to
		  obtain in many domains. More importantly, previous methods
		  are largely data driven but overlook the role of human
		  knowledge in guiding model development. To address these
		  limitations, this study designs and develops weakly
		  supervised WordNet-guided deep learning to aspect term
		  extraction. The approach draws on deep-level semantic
		  information from WordNet to guide not only the selection
		  representative seed terms but also the pruning of aspect
		  candidate terms. The weak supervision is provided by a very
		  small set of labeled data. We conduct a comprehensive
		  evaluation of the proposed method using both direct and
		  indirect methods. The evaluation results with Yelp
		  restaurant reviews demonstrate that our proposed method
		  consistently outperforms all baseline methods including
		  discrete models and the state-of-the-art continuous-space
		  language models for aspect term extraction across both
		  direct and indirect evaluations. The research findings have
		  broad research, technical, and practical implications for
		  various stakeholders of online reviews.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= jul,
  articleno	= {13},
  numpages	= {22},
  keywords	= {Aspect term extraction, continuous-space language model,
		  deep learning, semantic knowledge, text analytics}
}

@InProceedings{	  10.1145/3308558.3313511,
  author	= {Vedula, Nikhita and Maneriker, Pranav and Parthasarathy,
		  Srinivasan},
  title		= {BOLT-K: Bootstrapping Ontology Learning via Transfer of
		  Knowledge},
  year		= {2019},
  isbn		= {9781450366748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308558.3313511},
  doi		= {10.1145/3308558.3313511},
  abstract	= {Dynamically extracting and representing continually
		  evolving knowledge entities is an essential scaffold for
		  grounded intelligence and decision making. Creating
		  knowledge schemas for newly emerging, unfamiliar,
		  domain-specific ideas or events poses the following
		  challenges: (i) detecting relevant, often previously
		  unknown concepts associated with the new domain; and (ii)
		  learning ontological, semantically accurate relationships
		  among the new concepts, despite having severely limited
		  annotated data. To this end, we propose a novel LSTM-based
		  framework with attentive pooling, BOLT-K, to learn an
		  ontology for a target subject or domain. We bootstrap our
		  ontology learning approach by adapting and transferring
		  knowledge from an existing, functionally related source
		  domain. We also augment the inadequate labeled data
		  available for the target domain with various strategies to
		  minimize human expertise during model development and
		  training. BOLT-K first employs semantic and graphical
		  features to recognize the entity or concept pairs likely to
		  be related to each other, and filters out spurious concept
		  combinations. It is then jointly trained on knowledge from
		  the target and source domains to learn relationships among
		  the target concepts. The target concepts and their
		  corresponding relationships are subsequently used to
		  construct an ontology. We extensively evaluate our
		  framework on several, real-world bio-medical and commercial
		  product domain ontologies. We obtain significant
		  improvements of 5-25% F1-score points over state-of-the-art
		  baselines. We also examine the potential of BOLT-K in
		  detecting the presence of novel kinds of relationships that
		  were unseen during training.},
  booktitle	= {The World Wide Web Conference},
  pages		= {1897–1908},
  numpages	= {12},
  location	= {San Francisco, CA, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3209219.3213598,
  author	= {Moon, DeKita G.},
  title		= {Modeling Learners' Interest with a Domain-Independent
		  Ontology-Based Framework},
  year		= {2018},
  isbn		= {9781450355896},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209219.3213598},
  doi		= {10.1145/3209219.3213598},
  abstract	= {Ontologies are recognized as a promising approach to
		  support the reusability and interoperability of learners'
		  preferences; which is useful for the optimization and
		  flexibility of data and resources. However, little to no
		  research on adaptive learning systems or semantic
		  technologies explore personalized experiences based on the
		  various out-of-school experiences and activities of the
		  users. This research investigates the design, development,
		  and evaluation of an ontology-based framework for students'
		  interests in a math word problem generator that may be
		  applied to various other learning systems and possibly
		  other domains. The cohesiveness of the problems in addition
		  to the usability, usefulness, and the short-term
		  effectiveness of the derived technology will be
		  investigated by comparing the generated questions to
		  numerical and traditional Algebra I problems. We aim to
		  better understand students' interests to identify the role
		  that their interests can play in semantic technologies,
		  further supporting the recent advances in ontology-based
		  educational technologies and personalized math word problem
		  generators.},
  booktitle	= {Proceedings of the 26th Conference on User Modeling,
		  Adaptation and Personalization},
  pages		= {345–348},
  numpages	= {4},
  keywords	= {domain ontologies, educational technologies,
		  human-centered computing, knowledge graphs, semantic
		  technologies},
  location	= {Singapore, Singapore},
  series	= {UMAP '18}
}

@InProceedings{	  10.1145/3366423.3380156,
  author	= {Zhao, Xiangyu and Wang, Longbiao and He, Ruifang and Yang,
		  Ting and Chang, Jinxin and Wang, Ruifang},
  title		= {Multiple Knowledge Syncretic Transformer for Natural
		  Dialogue Generation},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380156},
  doi		= {10.1145/3366423.3380156},
  abstract	= {Knowledge is essential for intelligent conversation
		  systems to generate informative responses. This knowledge
		  comprises a wide range of diverse modalities such as
		  knowledge graphs (KGs), grounding documents and
		  conversation topics. However, limited abilities in
		  understanding language and utilizing different types of
		  knowledge still challenge existing approaches. Some
		  researchers try to enhance models’ language comprehension
		  ability by employing the pre-trained language models, but
		  they neglect the importance of external knowledge in
		  specific tasks. In this paper, we propose a novel universal
		  transformer-based architecture for dialogue system, the
		  Multiple Knowledge Syncretic Transformer (MKST), which
		  fuses multi-knowledge in open-domain conversation. Firstly,
		  the model is pre-trained on a large-scale corpus to learn
		  commonsense knowledge. Then during fine-tuning, we divide
		  the type of knowledge into two specific categories that are
		  handled in different ways by our model. While the encoder
		  is responsible for encoding dialogue contexts with
		  multifarious knowledge together, the decoder with a
		  knowledge-aware mechanism attentively reads the fusion of
		  multi-knowledge to promote better generation. This is the
		  first attempt that fuses multi-knowledge in one
		  conversation model. The experimental results have been
		  demonstrated that our model achieves significant
		  improvement on knowledge-driven dialogue generation tasks
		  than state-of-the-art baselines. Meanwhile, our new
		  benchmark could facilitate the further study in this
		  research area.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {752–762},
  numpages	= {11},
  keywords	= {Dialogue Generation, Multiple Knowledge, Pre-trained
		  Model, Syncretic Transformer},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@InProceedings{	  10.1145/3308558.3313656,
  author	= {Bhowmik, Rajarshi and de Melo, Gerard},
  title		= {Be Concise and Precise: Synthesizing Open-Domain Entity
		  Descriptions from Facts},
  year		= {2019},
  isbn		= {9781450366748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308558.3313656},
  doi		= {10.1145/3308558.3313656},
  abstract	= {Despite being vast repositories of factual information,
		  cross-domain knowledge graphs, such as Wikidata and the
		  Google Knowledge Graph, only sparsely provide short
		  synoptic descriptions for entities. Such descriptions that
		  briefly identify the most discernible features of an entity
		  provide readers with a near-instantaneous understanding of
		  what kind of entity they are being presented. They can also
		  aid in tasks such as named entity disambiguation,
		  ontological type determination, and answering entity
		  queries. Given the rapidly increasing numbers of entities
		  in knowledge graphs, a fully automated synthesis of
		  succinct textual descriptions from underlying factual
		  information is essential. To this end, we propose a novel
		  fact-to-sequence encoder-decoder model with a suitable copy
		  mechanism to generate concise and precise textual
		  descriptions of entities. In an in-depth evaluation, we
		  demonstrate that our method significantly outperforms
		  state-of-the-art alternatives.},
  booktitle	= {The World Wide Web Conference},
  pages		= {116–126},
  numpages	= {11},
  keywords	= {knowledge graphs, open-domain factual knowledge, synoptic
		  description generation},
  location	= {San Francisco, CA, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3404555.3404635,
  author	= {Gao, Shengxin and Du, Jinlian and Zhang, Xiao},
  title		= {Research on Relation Extraction Method of Chinese
		  Electronic Medical Records Based on BERT},
  year		= {2020},
  isbn		= {9781450377089},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404555.3404635},
  doi		= {10.1145/3404555.3404635},
  abstract	= {Relation extraction is a necessary step in obtaining
		  information from electronic medical records. The deep
		  learning methods for relation extraction are primarily
		  based on word2vec and convolutional or recurrent neural
		  network. However, word vectors generated by word2vec are
		  static and cannot well reflect the different meanings of
		  polysemy in different contexts and the feature extraction
		  ability of RNN (Recurrent Neural Network) is not good
		  enough. At the same time, the BERT (Bidirectional Encoder
		  Representations from Transformers) pre-trained language
		  model has achieved excellent results in many natural
		  language processing tasks. In this paper, we propose a
		  medical relation extraction model based on BERT. We combine
		  the information of the whole sentence obtained from the
		  pre-train language model with the corresponding information
		  of two medical entities to complete relation extraction
		  task. The experimental data were obtained from the Chinese
		  electronic medical records provided by a hospital in
		  Beijing. Experimental results on electronic medical records
		  show that our model's accuracy, precision, recall, and
		  F1-score reach 67.37%, 69.54%, 67.38%, 68.44%, which are
		  higher than other three methods. Because named entity
		  recognition task is the premise of relation extraction, we
		  will combine the model with named entity recognition in the
		  future work.},
  booktitle	= {Proceedings of the 2020 6th International Conference on
		  Computing and Artificial Intelligence},
  pages		= {487–490},
  numpages	= {4},
  keywords	= {BERT, Chinese electronic medical records, Relationship
		  extraction, deep learning},
  location	= {Tianjin, China},
  series	= {ICCAI '20}
}

@InProceedings{	  10.1145/3397271.3401265,
  author	= {Manotumruksa, Jarana and Dalton, Jeff and Meij, Edgar and
		  Yilmaz, Emine},
  title		= {CrossBERT: A Triplet Neural Architecture for Ranking
		  Entity Properties},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401265},
  doi		= {10.1145/3397271.3401265},
  abstract	= {Task-based Virtual Personal Assistants (VPAs) such as the
		  Google Assistant, Alexa, and Siri are increasingly being
		  adopted for a wide variety of tasks. These tasks are
		  grounded in real-world entities and actions (e.g., book a
		  hotel, organise a conference, or requesting funds). In this
		  work we tackle the task of automatically constructing
		  actionable knowledge graphs in response to a user query in
		  order to support a wider variety of increasingly complex
		  assistant tasks. We frame this as an entity property
		  ranking task given a user query with annotated properties.
		  We propose a new method for property ranking, CrossBERT.
		  CrossBERT builds on the Bidirectional Encoder
		  Representations from Transformers (BERT) and creates a new
		  triplet network structure on cross query-property pairs
		  that is used to rank properties. We also study the impact
		  of using external evidence for query entities from textual
		  entity descriptions. We perform experiments on two standard
		  benchmark collections, the NTCIR-13 Actionable Knowledge
		  Graph Generation (AKGG) task and Entity Property
		  Identification (EPI) task. The results demonstrate that
		  CrossBERT significantly outperforms the best performing
		  runs from AKGG and EPI, as well as previous
		  state-of-the-art BERT-based models. In particular,
		  CrossBERT significantly improves Recall and NDCG by
		  approximately 2-12% over the BERT models across the two
		  used datasets.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2049–2052},
  numpages	= {4},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@InProceedings{	  10.1145/3397271.3401089,
  author	= {Lu, Shuqi and Dou, Zhicheng and Xiong, Chenyan and Wang,
		  Xiaojie and Wen, Ji-Rong},
  title		= {Knowledge Enhanced Personalized Search},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401089},
  doi		= {10.1145/3397271.3401089},
  abstract	= {This paper presents a knowledge graph enhanced
		  personalized search model, KEPS. For each user and her
		  queries, KEPS first con- ducts personalized entity linking
		  on the queries and forms better intent representations;
		  then it builds a knowledge enhanced profile for the user,
		  using memory networks to store the predicted search intents
		  and linked entities in her search history. The knowledge
		  enhanced user profile and intent representation are then
		  utilized by KEPS for better, knowledge enhanced,
		  personalized search. Furthermore, after providing
		  personalized search for each query, KEPS leverages user's
		  feedback (click on documents) to post-adjust the entity
		  linking on previous queries. This fixes previous linking
		  errors and improves ranking quality for future queries.
		  Experiments on the public AOL search log demonstrate the
		  advantage of knowledge in personalized search: personalized
		  entity linking better reflects user's search intent, the
		  memory networks better maintain user's subtle preferences,
		  and the post linking adjustment fixes some linking errors
		  with the received feedback signals. The three components
		  together lead to a significantly better ranking accuracy of
		  KEPS.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {709–718},
  numpages	= {10},
  keywords	= {entity-oriented search, knowledge base, personalized
		  search},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@InProceedings{	  10.1145/3144457.3144458,
  author	= {Wu, Yao and Huang, Tao and Zhao, Dan and Chen, Hong and
		  Li, Cuiping},
  title		= {PIN: Potential Wise Crowd From Million Grassroots},
  year		= {2017},
  isbn		= {9781450353687},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3144457.3144458},
  doi		= {10.1145/3144457.3144458},
  abstract	= {Crowdsourcing proves a viable approach to solve certain
		  large-scale problems by posting tasks distributively to
		  humans and harnessing their knowledge to get results
		  effectively and efficiently. Unfortunately, crowdsourcing
		  suffers from lack of available participants with domain
		  knowledge or skills. In this paper, we propose potential
		  wise crowd (i.e., a crowd with similarity and diversity in
		  domain knowledge) find from million grassroots in social
		  networks. We design and implement a distant-supervision
		  framework to find potential crowdsourcers from existing
		  social networks. A knowledge graph is used to assess the
		  domain knowledge in terms of similarity and diversity. The
		  wise crowd formation is a NP-hard problem and we propose
		  greedy algorithms to approach it. Experimental results show
		  the performance of our framework and algorithms in aspects
		  of effectiveness and efficiency.},
  booktitle	= {Proceedings of the 14th EAI International Conference on
		  Mobile and Ubiquitous Systems: Computing, Networking and
		  Services},
  pages		= {186–195},
  numpages	= {10},
  keywords	= {crowd formation, distant supervision, mobile
		  crowdsourcing, mobile recruitment framework},
  location	= {Melbourne, VIC, Australia},
  series	= {MobiQuitous 2017}
}

@InProceedings{	  10.1145/3077136.3080768,
  author	= {Xiong, Chenyan and Callan, Jamie and Liu, Tie-Yan},
  title		= {Word-Entity Duet Representations for Document Ranking},
  year		= {2017},
  isbn		= {9781450350228},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3077136.3080768},
  doi		= {10.1145/3077136.3080768},
  abstract	= {This paper presents a word-entity duet framework for
		  utilizing knowledge bases in ad-hoc retrieval. In this
		  work, the query and documents are modeled by word-based
		  representations and entity-based representations. Ranking
		  features are generated by the interactions between the two
		  representations, incorporating information from the word
		  space, the entity space, and the cross-space connections
		  through the knowledge graph. To handle the uncertainties
		  from the automatically constructed entity representations,
		  an attention-based ranking model AttR-Duet is developed.
		  With back-propagation from ranking labels, the model learns
		  simultaneously how to demote noisy entities and how to rank
		  documents with the word-entity duet. Evaluation results on
		  TREC Web Track ad-hoc task demonstrate that all of the
		  four-way interactions in the duet are useful, the attention
		  mechanism successfully steers the model away from noisy
		  entities, and together they significantly outperform both
		  word-based and entity-based learning to rank systems.},
  booktitle	= {Proceedings of the 40th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {763–772},
  numpages	= {10},
  keywords	= {document ranking, entity-based search, text
		  representation},
  location	= {Shinjuku, Tokyo, Japan},
  series	= {SIGIR '17}
}

@InProceedings{	  10.1145/3178876.3186029,
  author	= {Cannaviccio, Matteo and Barbosa, Denilson and Merialdo,
		  Paolo},
  title		= {Towards Annotating Relational Data on the Web with
		  Language Models},
  year		= {2018},
  isbn		= {9781450356398},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3178876.3186029},
  doi		= {10.1145/3178876.3186029},
  abstract	= {Tables and structured lists on Web pages are a potential
		  source of valuable information, and several methods have
		  been proposed to annotate them with semantics that can be
		  leveraged for search, question answering and information
		  extraction. This paper is concerned with the specific
		  problem of finding and ranking relations from a given
		  Knowledge Graph (KG) that hold over pairs of entities
		  juxtaposed in a table or structured list. The
		  state-of-the-art for this task is to attempt to link the
		  entities mentioned in the table cells to objects in the KG
		  and rank the relations that hold for those linked objects.
		  As a result, these methods are hampered by the
		  incompleteness and uneven coverage in even the best
		  knowledge graphs available today. The alternative described
		  here does not require entity linking, relying instead on
		  ranking relations using generative language models derived
		  from Web-scale corpora. As such, it can produce quality
		  results even when the entities in the table are missing in
		  the KG. The experimental validation, designed to expose the
		  challenges posed by KG incompleteness, shows that our
		  approach is robust and effective in practice.},
  booktitle	= {Proceedings of the 2018 World Wide Web Conference},
  pages		= {1307–1316},
  numpages	= {10},
  keywords	= {generative language models, knowledge graphs, web table
		  understanding},
  location	= {Lyon, France},
  series	= {WWW '18}
}

@InProceedings{	  10.1145/3267851.3267894,
  author	= {Paranjape, Bhargavi and Ge, Yubin and Bai, Zhen and
		  Hammer, Jessica and Cassell, Justine},
  title		= {Towards Automatic Generation of Peer-Targeted Science Talk
		  in Curiosity-Evoking Virtual Agent},
  year		= {2018},
  isbn		= {9781450360135},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3267851.3267894},
  doi		= {10.1145/3267851.3267894},
  abstract	= {Curiosity is a critical skill that spurs learning, but is
		  often found to decline with age and schooling. Recent
		  research has shown that peer interaction may serve a
		  special role in inducing curiosity through increased
		  uncertainty and conceptual conflicts, since peers have
		  similar authority in knowledge. For a virtual agent to
		  stimulate curiosity, it should be able to generate
		  curiosity-eliciting verbal behaviors such as hypothesis
		  verbalization and argumentation, in the manner that
		  simulates peer-like cognitive and behavioral abilities. In
		  this paper, we design and implement a virtual peer that can
		  carry out key curiosity-eliciting science talk during a
		  dialog-based multi-party board game. We propose a
		  child-centered and data-driven approach to simulate the
		  latent reasoning process of young children and
		  age-appropriate language during open-ended game play. In
		  particular, we use a combination of child knowledge-graph
		  construction and child-child interaction driven modeling to
		  generate game appropriate behaviors that are compatible
		  with 9-14 year old children. Encouraging human evaluation
		  of the generated behaviors and generalizability of the
		  generation framework to other tasks opens up new directions
		  in incorporating open-endedness and science talk in virtual
		  agents that will make them truly play a peer role in
		  learning.},
  booktitle	= {Proceedings of the 18th International Conference on
		  Intelligent Virtual Agents},
  pages		= {71–78},
  numpages	= {8},
  keywords	= {Behavior Generation, Board game play, Cognitive
		  Architectures, Curiosity, Knowledge Base, Open-Ended Play,
		  Semantic Memory, Virtual Peer},
  location	= {Sydney, NSW, Australia},
  series	= {IVA '18}
}

@InProceedings{	  10.1145/3292500.3330725,
  author	= {Chen, Qibin and Lin, Junyang and Zhang, Yichang and Yang,
		  Hongxia and Zhou, Jingren and Tang, Jie},
  title		= {Towards Knowledge-Based Personalized Product Description
		  Generation in E-commerce},
  year		= {2019},
  isbn		= {9781450362016},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3292500.3330725},
  doi		= {10.1145/3292500.3330725},
  abstract	= {Quality product descriptions are critical for providing
		  competitive customer experience in an E-commerce platform.
		  An accurate and attractive description not only helps
		  customers make an informed decision but also improves the
		  likelihood of purchase. However, crafting a successful
		  product description is tedious and highly time-consuming.
		  Due to its importance, automating the product description
		  generation has attracted considerable interest from both
		  research and industrial communities. Existing methods
		  mainly use templates or statistical methods, and their
		  performance could be rather limited. In this paper, we
		  explore a new way to generate personalized product
		  descriptions by combining the power of neural networks and
		  knowledge base. Specifically, we propose a KnOwledge Based
		  pErsonalized (or KOBE) product description generation model
		  in the context of E-commerce.In KOBE, we extend the
		  encoder-decoder framework, the Transformer, to a sequence
		  modeling formulation using self-attention. In order to make
		  the description both informative and personalized, KOBE
		  considers a variety of important factors during text
		  generation, including product aspects, user categories, and
		  knowledge base. Experiments on real-world datasets
		  demonstrate that the proposed method outperforms the
		  baseline on various metrics. KOBE can achieve an
		  improvement of 9.7% over state-of-the-arts in terms of
		  BLEU. We also present several case studies as the anecdotal
		  evidence to further prove the effectiveness of the proposed
		  approach. The framework has been deployed in Taobao, the
		  largest online E-commerce platform in China.},
  booktitle	= {Proceedings of the 25th ACM SIGKDD International
		  Conference on Knowledge Discovery &amp; Data Mining},
  pages		= {3040–3050},
  numpages	= {11},
  keywords	= {controllable text generation, knowledge base,
		  personalization, product description generation},
  location	= {Anchorage, AK, USA},
  series	= {KDD '19}
}

@InProceedings{	  10.1145/3214708.3214712,
  author	= {Quamar, Abdul and Ozcan, Fatma and Xirogiannopoulos,
		  Konstantinos},
  title		= {Discovery and Creation of Rich Entities for Knowledge
		  Bases},
  year		= {2018},
  isbn		= {9781450358477},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3214708.3214712},
  doi		= {10.1145/3214708.3214712},
  abstract	= {Businesses and professional organizations from a variety
		  of different domains such as finance, weather, healthcare,
		  social networks, etc., produce massive amounts of
		  unstructured, semi-structured and structured data.
		  Knowledge bases, enable querying and analysis of integrated
		  content derived from such data available as open, third
		  party and propriety data sets. Many knowledge bases today,
		  provide an entity-centric view over the integrated content
		  by using domain-specific ontologies. These entity-centric
		  views enable querying individual real-world entities, as
		  well as exploring exact information (such as address or net
		  revenue of a company) through explicit querying using
		  languages such as SQL or SPARQL. Although very useful for
		  many business and commercial applications, this may not be
		  sufficient for the exploration of relevant and context
		  specific information associated with real-world entities
		  stored in these knowledge bases. Users often need to resort
		  to a manual and tedious process of exploration using ad-hoc
		  queries to gather the required information.To enhance user
		  experience and ameliorate the problem of relevant data
		  exploration, we propose the concept of Rich Entities. These
		  rich entities comprise of all the relevant and context
		  specific information grouped together around real-world
		  entities and served as efficient and meaningful responses
		  to user queries against these entities in a knowledge base.
		  These rich entities are created by grouping together
		  information not only from a single entity represented as an
		  ontology concept, but also related concepts and properties
		  as specified by the domain ontology. In this paper we
		  propose several novel techniques and algorithms to
		  automatically detect, learn, and create domain-specific
		  rich entities. We use inputs from query patterns in
		  existing query workloads against knowledge bases, and
		  leverage the structure and relationships between entities
		  defined in the domain ontology. Our techniques are very
		  effective and can be applied to a wide variety of
		  application domains thus adding great value to data
		  exploration and information extraction from entity-centric
		  real-world knowledge bases.},
  booktitle	= {Proceedings of the 5th International Workshop on
		  Exploratory Search in Databases and the Web},
  articleno	= {4},
  numpages	= {6},
  keywords	= {Knowledge Bases, Ontology, Rich Entities},
  location	= {Houston, TX, USA},
  series	= {ExploreDB 2018}
}

@InProceedings{	  10.1145/3357384.3357889,
  author	= {Zheng, Wen and Zhou, Ke},
  title		= {Enhancing Conversational Dialogue Models with Grounded
		  Knowledge},
  year		= {2019},
  isbn		= {9781450369763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357384.3357889},
  doi		= {10.1145/3357384.3357889},
  abstract	= {Leveraging external knowledge to enhance conversational
		  models has become a popular research area in recent years.
		  Compared to vanilla generative models, the
		  knowledge-grounded models may produce more informative and
		  engaging responses. Although various approaches have been
		  proposed in the past, how to effectively incorporate
		  knowledge remains an open research question. It is unclear
		  how much external knowledge should be retrieved and what is
		  the optimal way to enhance the conversational model,
		  trading off between relevant information and noise.
		  Therefore, in this paper, we aim to bridge the gap by first
		  extensively evaluating various types of state-of-the-art
		  knowledge-grounded conversational models, including
		  recurrent neural network based, memory networks based, and
		  Transformer based models. We demonstrate empirically that
		  those conversational models can only be enhanced with the
		  right amount of external knowledge. To effectively leverage
		  information originated from external knowledge, we propose
		  a novel Transformer with Expanded Decoder (Transformer-ED
		  or TED for short), which can automatically tune the weights
		  for different sources of evidence when generating
		  responses. Our experiments show that our proposed model
		  outperforms state-of-the-art models in terms of both
		  quality and diversity.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {709–718},
  numpages	= {10},
  keywords	= {copy-mechanism, generative model, knowledge-grounded,
		  memory network, multi-task learning, sequence-to-sequence,
		  ted, transformer, transformer-ed},
  location	= {Beijing, China},
  series	= {CIKM '19}
}

@Article{	  10.1145/3345517,
  author	= {Abdulhameed, Tiba Zaki and Zitouni, Imed and Abdel-Qader,
		  Ikhlas},
  title		= {Wasf-Vec: Topology-based Word Embedding for Modern
		  Standard Arabic and Iraqi Dialect Ontology},
  year		= {2019},
  issue_date	= {March 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3345517},
  doi		= {10.1145/3345517},
  abstract	= {Word clustering is a serious challenge in low-resource
		  languages. Since words that share semantics are expected to
		  be clustered together, it is common to use a feature vector
		  representation generated from a distributional theory-based
		  word embedding method. The goal of this work is to utilize
		  Modern Standard Arabic (MSA) for better clustering
		  performance of the low-resource Iraqi vocabulary. We began
		  with a new Dialect Fast Stemming Algorithm (DFSA) that
		  utilizes the MSA data. The proposed algorithm achieved 0.85
		  accuracy measured by the F1 score. Then, the distributional
		  theory-based word embedding method and a new simple, yet
		  effective, feature vector named Wasf-Vec word embedding are
		  tested. Wasf-Vec word representation utilizes a word’s
		  topology features. The difference between Wasf-Vec and
		  distributional theory-based word embedding is that Wasf-Vec
		  captures relations that are not contextually based. The
		  embedding is followed by an analysis of how the dialect
		  words are clustered within other MSA words. The analysis is
		  based on the word semantic relations that are well
		  supported by solid linguistic theories to shed light on the
		  strong and weak word relation representations identified by
		  each embedding method. The analysis is handled by
		  visualizing the feature vector in two-dimensional (2D)
		  space. The feature vectors of the distributional
		  theory-based word embedding method are plotted in 2D space
		  using the t-sne algorithm, while the Wasf-Vec feature
		  vectors are plotted directly in 2D space. A word’s
		  nearest neighbors and the distance-histograms of the
		  plotted words are examined. For validation purpose of the
		  word classification used in this article, the produced
		  classes are employed in Class-based Language Modeling
		  (CBLM). Wasf-Vec CBLM achieved a 7% lower perplexity (pp)
		  than the distributional theory-based word embedding method
		  CBLM. This result is significant when working with
		  low-resource languages.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= dec,
  articleno	= {22},
  numpages	= {27},
  keywords	= {2D visualizing, Arabic language, Topology, Word embedding,
		  class-based language modeling, dialect, morphology,
		  orthographic, phonology, words classification, words
		  features, words ontology}
}

@InProceedings{	  10.1145/3340531.3412777,
  author	= {Sakor, Ahmad and Singh, Kuldeep and Patel, Anery and
		  Vidal, Maria-Esther},
  title		= {Falcon 2.0: An Entity and Relation Linking Tool over
		  Wikidata},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412777},
  doi		= {10.1145/3340531.3412777},
  abstract	= {The Natural Language Processing (NLP) community has
		  significantly contributed to the solutions for entity and
		  relation recognition from a natural language text, and
		  possibly linking them to proper matches in Knowledge Graphs
		  (KGs). Considering Wikidata as the background KG, there are
		  still limited tools to link knowledge within the text to
		  Wikidata. In this paper, we present Falcon 2.0, the first
		  joint entity and relation linking tool over Wikidata. It
		  receives a short natural language text in the English
		  language and outputs a ranked list of entities and
		  relations annotated with the proper candidates in Wikidata.
		  The candidates are represented by their Internationalized
		  Resource Identifier (IRI) in Wikidata. Falcon 2.0 resorts
		  to the English language model for the recognition task
		  (e.g., N-Gram tiling and N-Gram splitting), and then an
		  optimization approach for the linking task. We have
		  empirically studied the performance of Falcon 2.0 on
		  Wikidata and concluded that it outperforms all the existing
		  baselines. Falcon 2.0 is open source and can be reused by
		  the community; all the required instructions of Falcon 2.0
		  are well-documented at our GitHub repository
		  (https://github.com/SDM-TIB/falcon2.0). We also demonstrate
		  an online API, which can be run without any technical
		  expertise. Falcon 2.0 and its background knowledge bases
		  are available as resources at
		  https://labs.tib.eu/falcon/falcon2/.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3141–3148},
  numpages	= {8},
  keywords	= {background knowledge, dbpedia, english morphology, entity
		  linking, nlp, relation linking, wikidata},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3340531.3411895,
  author	= {Yuan, Chenxi and Yuan, Chun and Bai, Yang and Li, Ziran},
  title		= {Logic Enhanced Commonsense Inference with Chain
		  Transformer},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3411895},
  doi		= {10.1145/3340531.3411895},
  abstract	= {We study the commonsense inference task that aims to
		  reason and generate the causes and effects of a given
		  event. Existing neural methods focus more on understanding
		  and representing the event itself, but pay little attention
		  to the relations between different commonsense dimensions
		  (e.g. causes or effects) of the event, making the generated
		  results logically inconsistent and unreasonable. To
		  alleviate this issue, we propose Chain Transformer, a logic
		  enhanced commonsense inference model that combines both
		  direct and indirect inferences to construct a logical chain
		  so as to reason in a more logically consistent way. First,
		  we apply a self-attention based encoder to represent and
		  encode the given event. Then a chain of decoders is
		  implemented to reason and generate for different dimensions
		  following the logical chain, where an attention module is
		  designed to link different decoders and to make each
		  decoder attend to the previous reasoned inferences.
		  Experiments on two real-world datasets show that Chain
		  Transformer outperforms previous methods on both automatic
		  and human evaluation, and demonstrate that Chain
		  Transformer can generate more reasonable and logically
		  consistent inference results.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1763–1772},
  numpages	= {10},
  keywords	= {attention network, commonsense inference, commonsense
		  knowledge, logical chain},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3342558.3345404,
  author	= {Yousefinaghani, Samira and Dara, Rozita and Sharif,
		  Shayan},
  title		= {Impact of In-domain Vector Representations on the
		  Classification of Disease-related Tweets: Avian Influenza
		  Case Study},
  year		= {2019},
  isbn		= {9781450368872},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3342558.3345404},
  doi		= {10.1145/3342558.3345404},
  abstract	= {A number of methods have been proposed for the
		  construction of vector representations for natural language
		  processing (NLP) tasks. These methods have been applied to
		  various domains and each has its own pros and cons. Despite
		  their effectiveness, the proposed approaches usually ignore
		  the sentiment information concerning specific tasks. In
		  this paper, we examined various types of word vectors and
		  their impact on the performance of a sentiment
		  classification problem in the area of infectious diseases.
		  Vectors were used in the embedding layer of a word-based
		  convolutional neural network (CNN) to identify tweets
		  pertaining to avian influenza. We proposed a new approach
		  to build effective word embeddings for the sentiment
		  analysis task. Furthermore, the performance of the language
		  model was compared in terms of using various corpus sizes
		  and vector dimensions. Our experiments indicated that
		  initializing the sentiment learning network with
		  domain-specific word embeddings outperforms general domain
		  embeddings. We found that the proposed method leads to a
		  considerable improvement in the classification
		  performance.},
  booktitle	= {Proceedings of the ACM Symposium on Document Engineering
		  2019},
  articleno	= {34},
  numpages	= {4},
  keywords	= {Convolutional neural network, Twitter, avian influenza,
		  machine learning, sentiment analysis, word embedding},
  location	= {Berlin, Germany},
  series	= {DocEng '19}
}

@InProceedings{	  10.1145/3167020.3167033,
  author	= {Lakhanpal, Shilpa and Gupta, Ajay and Agrawal, Rajeev},
  title		= {Mining Domain Similarity to Enhance Digital Indexing},
  year		= {2017},
  isbn		= {9781450348959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3167020.3167033},
  doi		= {10.1145/3167020.3167033},
  abstract	= {Indexing research articles in scientific publications can
		  be arduous. The authors tag their articles by the topics or
		  domains relevant to their research. A publication's
		  organizers may tag them by the broad topics of the specific
		  publication. A third-party may index or tag these articles
		  based on their subject knowledge. Hence indexing of
		  articles can be uneven due to inconsistencies in area
		  knowledge by third-parties or the niche topic
		  representation by the authors. Publications may have
		  schemes in place for indexing or tagging the articles but
		  such schemes cannot keep up with the continuously changing
		  landscape of research. These schemes may need to be updated
		  with newer topics or domains being churned out by the state
		  of the art research. Our technique endeavors to address
		  this problem. We present a methodology to find similarity
		  among domains extracted from the content of research
		  papers, and cluster related domains. Analysis of these
		  clusters provides insights into how the existing indexing
		  schemes may be enhanced by adding newer domains.},
  booktitle	= {Proceedings of the 9th International Conference on
		  Management of Digital EcoSystems},
  pages		= {88–92},
  numpages	= {5},
  keywords	= {ACM CCS, Domain, K-Means, Ontology, WuP Similarity},
  location	= {Bangkok, Thailand},
  series	= {MEDES '17}
}

@InProceedings{	  10.1145/3018661.3018692,
  author	= {Ensan, Faezeh and Bagheri, Ebrahim},
  title		= {Document Retrieval Model Through Semantic Linking},
  year		= {2017},
  isbn		= {9781450346757},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3018661.3018692},
  doi		= {10.1145/3018661.3018692},
  abstract	= {This paper addresses the task of document retrieval based
		  on the degree of document relatedness to the meanings of a
		  query by presenting a semantic-enabled language model. Our
		  model relies on the use of semantic linking systems for
		  forming a graph representation of documents and queries,
		  where nodes represent concepts extracted from documents and
		  edges represent semantic relatedness between concepts.
		  Based on this graph, our model adopts a probabilistic
		  reasoning model for calculating the conditional probability
		  of a query concept given values assigned to document
		  concepts. We present an integration framework for
		  interpolating other retrieval systems with the presented
		  model in this paper. Our empirical experiments on a number
		  of TREC collections show that the semantic retrieval has a
		  synergetic impact on the results obtained through state of
		  the art keyword-based approaches, and the consideration of
		  semantic information obtained from entity linking on
		  queries and documents can complement and enhance the
		  performance of other retrieval models.},
  booktitle	= {Proceedings of the Tenth ACM International Conference on
		  Web Search and Data Mining},
  pages		= {181–190},
  numpages	= {10},
  keywords	= {information retrieval, language models, semantic linking,
		  semantic relatedness, semantic search},
  location	= {Cambridge, United Kingdom},
  series	= {WSDM '17}
}

@InProceedings{	  10.1145/3366424.3383539,
  author	= {Luo, Feng and Wang, Xiaoli and Wu, Qingfeng and Liang,
		  Jiaying and Qiu, Xueliang and Bao, Zhifeng},
  title		= {HQADeepHelper: A Deep Learning System for Healthcare
		  Question Answering},
  year		= {2020},
  isbn		= {9781450370240},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366424.3383539},
  doi		= {10.1145/3366424.3383539},
  abstract	= {It is challenging to generate high quality answers for
		  healthcare queries in online platforms. Recent studies
		  proposed deep models for healthcare question answering
		  (HQA) tasks. However, these models have not been thoroughly
		  compared, and they were only tested on self-created
		  datasets. This paper demonstrates a novel system, denoted
		  by HQADeepHelper, to facilitate the learning and practicing
		  of deep models for HQA. We have implemented a wide spectrum
		  of state-of-the-art deep models for HQA retrieval. Users
		  can upload self-collected HQA datasets and knowledge
		  graphs, and do simple configurations by selecting datasets,
		  knowledge graphs, neural network models, and evaluation
		  metrics. Based on user’s configuration specified, the
		  system can automatically train and test the model, conduct
		  extensive experimental evaluation of the models selected,
		  and report comprehensive findings. The reports provide new
		  insights about the strengths and weaknesses of deep models
		  that can guide practitioners to select appropriate models
		  for various scenarios. Moreover, users can download the
		  datasets, knowledge graphs, experimental reports and source
		  codes of neural network models for their own practice and
		  evaluations further.},
  booktitle	= {Companion Proceedings of the Web Conference 2020},
  pages		= {194–197},
  numpages	= {4},
  keywords	= {Healthcare question answering, Neural network models},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@InProceedings{	  10.1145/3357384.3357874,
  author	= {Bhutani, Nikita and Jagadish, H V},
  title		= {Online Schemaless Querying of Heterogeneous Open Knowledge
		  Bases},
  year		= {2019},
  isbn		= {9781450369763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357384.3357874},
  doi		= {10.1145/3357384.3357874},
  abstract	= {Applications that depend on a deep understanding of
		  natural language text have led to a renaissance of large
		  knowledge bases (KBs). Some of these are curated manually
		  and conform to an ontology. Many others, called open KBs,
		  are derived automatically from unstructured text without
		  any pre-specified ontology. These open KBs offer broad
		  coverage of information but are far more heterogeneous than
		  curated KBs, which themselves are more heterogeneous than
		  traditional databases with a fixed schema. Due to the
		  heterogeneity of information representation, querying KBs
		  is a challenging task. Traditionally, query expansion is
		  performed to cover all possible transformations and
		  semantically equivalent structures. Such query expansion
		  can be impractical for heterogeneous open KBs, particularly
		  when complex queries lead to a combinatorial explosion of
		  expansion possibilities. Furthermore, learning a query
		  expansion model requires training examples, which is
		  difficult to scale to diverse representations of facts in
		  the KB. In this paper, we introduce an online schemaless
		  querying method that does not require the query to exactly
		  match the facts. Instead of exactly matching a query, it
		  finds matches for individual query components and then
		  identifies an answer by reasoning over the collective
		  evidence. We devise an alignment-based algorithm for
		  extracting answers based on textual and semantic similarity
		  of query components and evidence fields. Thus, any
		  representational mismatches between the query and evidence
		  are handled online at query-time. Experiments show our
		  approach is effective in handling multi-constraint
		  queries.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {699–708},
  numpages	= {10},
  keywords	= {heterogeneity, open knowledge bases, schemaless querying},
  location	= {Beijing, China},
  series	= {CIKM '19}
}

@InProceedings{	  10.1145/3106426.3106465,
  author	= {Ristoski, Petar and Faralli, Stefano and Ponzetto, Simone
		  Paolo and Paulheim, Heiko},
  title		= {Large-scale taxonomy induction using entity and word
		  embeddings},
  year		= {2017},
  isbn		= {9781450349512},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3106426.3106465},
  doi		= {10.1145/3106426.3106465},
  abstract	= {Taxonomies are an important ingredient of knowledge
		  organization, and serve as a backbone for more
		  sophisticated knowledge representations in intelligent
		  systems, such as formal ontologies. However, building
		  taxonomies manually is a costly endeavor, and hence,
		  automatic methods for taxonomy induction are a good
		  alternative to build large-scale taxonomies. In this paper,
		  we propose TIEmb, an approach for automatic unsupervised
		  class subsumption axiom extraction from knowledge bases
		  using entity and text embeddings. We apply the approach on
		  the WebIsA database, a database of subsumption relations
		  extracted from the large portion of the World Wide Web, to
		  extract class hierarchies in the Person and Place domain.},
  booktitle	= {Proceedings of the International Conference on Web
		  Intelligence},
  pages		= {81–87},
  numpages	= {7},
  keywords	= {entity embeddings, ontology induction, text embeddings},
  location	= {Leipzig, Germany},
  series	= {WI '17}
}

@InProceedings{	  10.1145/3422852.3423484,
  author	= {Huang, Bin and Tang, Siao and Shen, Guangyao and Li,
		  Guohao and Wang, Xin and Zhu, Wenwu},
  title		= {Commonsense Learning: An Indispensable Path towards
		  Human-centric Multimedia},
  year		= {2020},
  isbn		= {9781450381512},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3422852.3423484},
  doi		= {10.1145/3422852.3423484},
  abstract	= {Learning commonsense knowledge and conducting commonsense
		  reasoning are basic human ability to make presumptions
		  about the type and essence of ordinary situation in daily
		  life, which serve as very important goals in human-centric
		  Artificial Intelligence (AI). With the increasing number of
		  media types and quantities provided by various Internet
		  services, commonsense learning and reasoning with no doubt
		  are playing key roles in making progresses for
		  human-centric multimedia analysis. Therefore, this paper
		  first introduces the basic concept of commonsense knowledge
		  and commonsense reasoning, then summarizes commonsense
		  resources and benchmarks, gives an overview on recent
		  commonsense learning and reasoning methods, and discusses
		  several popular applications of commonsense knowledge in
		  real-world scenarios. This work distinguishes itself from
		  existing literature that merely pays attention to natural
		  language processing in focusing more on multimedia which
		  include both natural language processing and computer
		  vision. Furthermore, we also present our insights and
		  thinking on future research directions for commonsense.},
  booktitle	= {Proceedings of the 1st International Workshop on
		  Human-Centric Multimedia Analysis},
  pages		= {91–100},
  numpages	= {10},
  keywords	= {commonsense knowledge, reasoning},
  location	= {Seattle, WA, USA},
  series	= {HuMA'20}
}

@InProceedings{	  10.1145/3340531.3417466,
  author	= {Huang, Shanshan and Zhu, Kenny Q. and Liao, Qianzi and
		  Shen, Libin and Zhao, Yinggong},
  title		= {Enhanced Story Representation by ConceptNet for Predicting
		  Story Endings},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3417466},
  doi		= {10.1145/3340531.3417466},
  abstract	= {Predicting endings for narrative stories is a grand
		  challenge for machine commonsense reasoning. The task
		  requires ac- curate representation of the story semantics
		  and structured logic knowledge. Pre-trained language
		  models, such as BERT, made progress recently in this task
		  by exploiting spurious statistical patterns in the test
		  dataset, instead of 'understanding' the stories per se. In
		  this paper, we propose to improve the representation of
		  stories by first simplifying the sentences to some key
		  concepts and second modeling the latent relation- ship
		  between the key ideas within the story. Such enhanced
		  sentence representation, when used with pre-trained
		  language models, makes substantial gains in prediction
		  accuracy on the popular Story Cloze Test without utilizing
		  the biased validation data.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3277–3280},
  numpages	= {4},
  keywords	= {commonsense knowledge, commonsense reasoning, story
		  comprehension},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3340531.3417416,
  author	= {Romero, Julien and Razniewski, Simon},
  title		= {Inside Quasimodo: Exploring Construction and Usage of
		  Commonsense Knowledge},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3417416},
  doi		= {10.1145/3340531.3417416},
  abstract	= {Quasimodo is an open-source commonsense knowledge base
		  that significantly advanced the state of salient
		  commonsense knowledge base construction. It introduced a
		  pipeline that gathers, normalizes, validates and scores
		  statements coming from query log and question answering
		  forums. In this demonstration, we present a companion web
		  portal which allows (i) to explore the data, (ii) to run
		  and analyze the extraction pipeline live, and (iii) inspect
		  the usage of Quasimodo's knowledge in several downstream
		  use cases. The web portal is available at
		  https://quasimodo.r2.enst.fr.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3445–3448},
  numpages	= {4},
  keywords	= {commonsense, datasets, knowledge base, visualisation},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3383972.3384051,
  author	= {Zhao, Lin and Li, Minglei and Kou, Jinqiao and Zhang, Jian
		  and Zhang, Yang},
  title		= {A Framework for Event-oriented Text Retrieval Based on
		  Temporal Aspects: A Recent Review},
  year		= {2020},
  isbn		= {9781450376426},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383972.3384051},
  doi		= {10.1145/3383972.3384051},
  abstract	= {Event, as an important carrier for users to understand the
		  world, has become a special retrieval object. In contrast
		  to traditional text retrieval, Event-oriented Text
		  Retrieval (ETR) can search events by utilizing events
		  knowledge and using events as proxies for information
		  needs. Accordingly, ETR has become the preferred way for
		  users to obtain their interested events from massive web
		  collections. Moreover, it also has already aroused
		  considerable attention from scholars in recent years.
		  However, the retrieval effectiveness of ETR is still
		  subject to the effect of temporal aspects (i.e., temporal
		  dynamics). Thus, in this review, we first analyze three
		  major temporal components in the framework of ETR. After
		  that, we provide a comprehensive overview of
		  state-of-the-art approaches corresponding to such three
		  components. Finally, we summary some ETR-related resources
		  and pinpoint several potential research directions.},
  booktitle	= {Proceedings of the 2020 12th International Conference on
		  Machine Learning and Computing},
  pages		= {39–46},
  numpages	= {8},
  keywords	= {Event-oriented, Review, Temporal dynamics, Text
		  retrieval},
  location	= {Shenzhen, China},
  series	= {ICMLC '20}
}

@InProceedings{	  10.1145/3366423.3380263,
  author	= {Kumar, Ramnath and Yadav, Shweta and Daniulaityte, Raminta
		  and Lamy, Francois and Thirunarayan, Krishnaprasad and
		  Lokala, Usha and Sheth, Amit},
  title		= {eDarkFind: Unsupervised Multi-view Learning for Sybil
		  Account Detection},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380263},
  doi		= {10.1145/3366423.3380263},
  abstract	= {Darknet crypto markets are online marketplaces using
		  crypto currencies (e.g., Bitcoin, Monero) and advanced
		  encryption techniques to offer anonymity to vendors and
		  consumers trading for illegal goods or services. The exact
		  volume of substances advertised and sold through these
		  crypto markets is difficult to assess, at least partially,
		  because vendors tend to maintain multiple accounts (or
		  Sybil accounts) within and across different crypto markets.
		  Linking these different accounts will allow us to
		  accurately evaluate the volume of substances advertised
		  across the different crypto markets by each vendor. In this
		  paper, we present a multi-view unsupervised framework
		  (eDarkFind) that helps modeling vendor characteristics and
		  facilitates Sybil account detection. We employ a multi-view
		  learning paradigm to generalize and improve the performance
		  by exploiting the diverse views from multiple rich sources
		  such as BERT, stylometric, and location representation. Our
		  model is further tailored to take advantage of
		  domain-specific knowledge such as the Drug Abuse Ontology
		  to take into consideration the substance information. We
		  performed extensive experiments and demonstrated that the
		  multiple views obtained from diverse sources can be
		  effective in linking Sybil accounts. Our proposed eDarkFind
		  model achieves an accuracy of 98% on three real-world
		  datasets which shows the generality of the approach.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {1955–1965},
  numpages	= {11},
  keywords	= {Correlation Analysis, Darknet Market, Drug Trafficker
		  Identification, Multi-view Learning, Stylometry, Sybil
		  Detection},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@InProceedings{	  10.1145/3331184.3331396,
  author	= {Nguyen, Vincent and Karimi, Sarvnaz and Jin, Brian},
  title		= {An Experimentation Platform for Precision Medicine},
  year		= {2019},
  isbn		= {9781450361729},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331184.3331396},
  doi		= {10.1145/3331184.3331396},
  abstract	= {Precision medicine - where data from patients, their
		  genes, their lifestyles and the available treatments and
		  their combination are taken into account for finding a
		  suitable treatment - requires searching the biomedical
		  literature and other resources such as clinical trials with
		  the patients' information. The retrieved information could
		  then be used in curating data for clinicians for
		  decision-making. We present information retrieval
		  researchers with an on-line system which enables
		  experimentation in search for precision medicine within the
		  framework provided by the TREC Precision Medicine (PM)
		  track. A number of query and document processing and
		  ranking approaches are provided. These include some ofthe
		  most promising gene mention expansion methods, as well as
		  learning-to-rank using neural networks.},
  booktitle	= {Proceedings of the 42nd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1357–1360},
  numpages	= {4},
  keywords	= {domain-specific search, health informatics, literature
		  search},
  location	= {Paris, France},
  series	= {SIGIR'19}
}

@Article{	  10.1145/3383465,
  author	= {Patil, Charulata and Patwardhan, Manasi},
  title		= {Visual Question Generation: The State of the Art},
  year		= {2020},
  issue_date	= {May 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3383465},
  doi		= {10.1145/3383465},
  abstract	= {Visual question generation (VQG) is an interesting problem
		  that has recently received attention. The task of VQG
		  involves generating meaningful questions based on the input
		  image. It is a multi-modal problem involving image
		  understanding and natural language generation, especially
		  using deep learning methods. VQG can be considered as
		  complementary task of visual question answering. In this
		  article, we review the current state of VQG in terms of
		  methods to understand the problem, existing datasets to
		  train the VQG model, evaluation metrics, and algorithms to
		  handle the problem. Finally, we discuss the challenges that
		  need to be conquered and the possible future directions for
		  an effective VQG.},
  journal	= {ACM Comput. Surv.},
  month		= may,
  articleno	= {47},
  numpages	= {22},
  keywords	= {Image understanding, question generation}
}

@InProceedings{	  10.1145/3394171.3414047,
  author	= {Vu, Xuan-Son and Le, Duc-Trong and Edlund, Christoffer and
		  Jiang, Lili and Nguyen, Hoang D.},
  title		= {Privacy-Preserving Visual Content Tagging using Graph
		  Transformer Networks},
  year		= {2020},
  isbn		= {9781450379885},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394171.3414047},
  doi		= {10.1145/3394171.3414047},
  abstract	= {With the rapid growth of Internet media, content tagging
		  has become an important topic with many multimedia
		  understanding applications, including efficient
		  organisation and search. Nevertheless, existing visual
		  tagging approaches are susceptible to inherent privacy
		  risks in which private information may be exposed
		  unintentionally. The use of anonymisation and
		  privacy-protection methods is desirable, but with the
		  expense of task performance. Therefore, this paper proposes
		  an end-to-end framework (SGTN) using Graph Transformer and
		  Convolutional Networks to significantly improve
		  classification and privacy preservation of visual data.
		  Especially, we employ several mechanisms such as
		  differential privacy based graph construction and
		  noise-induced graph transformation to protect the privacy
		  of knowledge graphs. Our approach unveils new
		  state-of-the-art on MS-COCO dataset in various
		  semi-supervised settings. In addition, we showcase a real
		  experiment in the education domain to address the
		  automation of sensitive document tagging. Experimental
		  results show that our approach achieves an excellent
		  balance of model accuracy and privacy preservation on both
		  public and private datasets.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Multimedia},
  pages		= {2299–2307},
  numpages	= {9},
  keywords	= {dp-adjacency-graph, dp-embedding, graph-transformer,
		  privacy-preservation, visual tagging},
  location	= {Seattle, WA, USA},
  series	= {MM '20}
}

@Article{	  10.1109/tcbb.2020.2979959,
  author	= {Jiang, Tianwen and Zeng, Qingkai and Zhao, Tong and Qin,
		  Bing and Liu, Ting and Chawla, Nitesh V. and Jiang, Meng},
  title		= {Biomedical Knowledge Graphs Construction From Conditional
		  Statements},
  year		= {2020},
  issue_date	= {May-June 2021},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {18},
  number	= {3},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2020.2979959},
  doi		= {10.1109/TCBB.2020.2979959},
  abstract	= {Conditions play an essential role in biomedical
		  statements. However, existing biomedical knowledge graphs
		  (BioKGs) only focus on factual knowledge, organized as a
		  flat relational network of biomedical concepts. These
		  BioKGs ignore the conditions of the facts being valid,
		  which loses essential contexts for knowledge exploration
		  and inference. We consider both facts and their conditions
		  in biomedical statements and proposed a three-layered
		  information-lossless representation of BioKG. The first
		  layer has biomedical concept nodes, attribute nodes. The
		  second layer represents both biomedical fact and condition
		  tuples by nodes of the relation phrases, connecting to the
		  subject and object in the first layer. The third layer has
		  nodes of statements connecting to a set of fact tuples
		  and/or condition tuples in the second layer. We transform
		  the BioKG construction problem into a sequence labeling
		  problem based on a novel designed tag schema. We design a
		  Multi-Input Multi-Output sequence labeling model (MIMO)
		  that learns from &lt;italic&gt;multiple
		  input&lt;/italic&gt; signals and generates proper number of
		  &lt;italic&gt;multiple output&lt;/italic&gt; sequences for
		  tuple extraction. Experiments on a newly constructed
		  dataset show that MIMO outperforms the existing methods.
		  Further case study demonstrates that the BioKGs constructed
		  provide a good understanding of the biomedical
		  statements.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= mar,
  pages		= {823–835},
  numpages	= {13}
}

@InProceedings{	  10.1145/3281375.3281405,
  author	= {Bhattacharya, Sambit and Agrawal, Rajeev and Wagner,
		  Neal},
  title		= {Application of deep learning and geo-knowledge bases to
		  scene understanding},
  year		= {2018},
  isbn		= {9781450356220},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3281375.3281405},
  doi		= {10.1145/3281375.3281405},
  abstract	= {Humans can easily perform tasks that use vision and
		  language jointly, such as describing a scene and answering
		  questions about objects in the scene and how they are
		  related. Image captioning and visual question &amp; answer
		  are two popular research tasks that have emerged from
		  advances in deep learning and the availability of datasets
		  that specifically address these problems. However recent
		  work has shown that deep learning based solutions to these
		  tasks are just as brittle as solutions for only vision or
		  only natural language tasks. Image captioning is vulnerable
		  to adversarial perturbations; novel objects, which are not
		  described in training data, and contextual biases in
		  training data can degrade performance in surprising ways.
		  For these reasons, it is important to find ways in which
		  general-purpose knowledge can guide connectionist models.
		  We investigate challenges to integrate existing ontologies
		  and knowledge bases with deep learning solutions, and
		  possible approaches for overcoming such challenges. We
		  focus on geo-referenced data such as geo-tagged images and
		  videos that capture outdoor scenery. Geo-knowledge bases
		  are domain specific knowledge bases that contain concepts
		  and relations that describe geographic objects. This work
		  proposes to increase the robustness of automatic scene
		  description and inference by leveraging geo-knowledge bases
		  along with the strengths of deep learning for visual object
		  detection and classification.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Management of Digital EcoSystems},
  pages		= {74–79},
  numpages	= {6},
  keywords	= {commonsense knowledge, deep learning, knowledge base,
		  scene understanding},
  location	= {Tokyo, Japan},
  series	= {MEDES '18}
}

@InProceedings{	  10.1145/3340531.3412072,
  author	= {Sahu, Sunil Kumar and Thomas, Derek and Chiu, Billy and
		  Sengupta, Neha and Mahdy, Mohammady},
  title		= {Relation Extraction with Self-determined Graph
		  Convolutional Network},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412072},
  doi		= {10.1145/3340531.3412072},
  abstract	= {Relation Extraction is a way of obtaining the semantic
		  relationship between entities in text. The state-of-the-art
		  methods use linguistic tools to build a graph for the text
		  in which the entities appear and then a Graph Convolutional
		  Network (GCN) is employed to encode the pre-built graphs.
		  Although their performance is promising, the reliance on
		  linguistic tools results in a non end-to-end process. In
		  this work, we propose a novel model, the Self-determined
		  Graph Convolutional Network (SGCN), which determines a
		  weighted graph using a self-attention mechanism, rather
		  using any linguistic tool. Then, the self-determined graph
		  is encoded using a GCN. We test our model on the TACRED
		  dataset and achieve the state-of-the-art result. Our
		  experiments show that SGCN outperforms the traditional GCN,
		  which uses dependency parsing tools to build the graph.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2205–2208},
  numpages	= {4},
  keywords	= {graph neural networks, information extraction, natural
		  language processing, relation extraction},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3360901.3364451,
  author	= {Clark, Peter},
  title		= {Project Aristo: Towards Machines that Capture and Reason
		  with Science Knowledge},
  year		= {2019},
  isbn		= {9781450370080},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3360901.3364451},
  doi		= {10.1145/3360901.3364451},
  abstract	= {AI2's Project Aristo seeks to build a system that has a
		  deep understanding of science, using knowledge captured
		  mainly from large-scale text. Recently, Aristo achieved
		  surprising success on the Grade 8 New York Regents Science
		  Exams, scoring over 90% on the exam's non-diagram, multiple
		  choice (NDMC) questions, where even 3 years ago the best
		  systems scored less than 60%. In this talk, I will describe
		  the journey of Aristo through various knowledge capture
		  technologies that have helped it, including acquiring
		  if/then rules, tables, knowledge graphs, and latent neural
		  representations. I will also discuss the growing tension
		  between capturing structured knowledge vs. capturing
		  knowledge latently using neural models, the latter proving
		  highly effective but hard to interpret. Finally I will
		  speculate on the larger quest towards knowledgable machines
		  that can reason, explain, and discuss, and how structured
		  and latent knowledge can interact to help reach this
		  goal.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Knowledge Capture},
  pages		= {1–2},
  numpages	= {2},
  keywords	= {knowledge acquisition, question answering},
  location	= {Marina Del Rey, CA, USA},
  series	= {K-CAP '19}
}

@InProceedings{	  10.1145/3030024.3038282,
  author	= {Mauro, Noemi},
  title		= {Intelligent and Personalized Community Maps},
  year		= {2017},
  isbn		= {9781450348935},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3030024.3038282},
  doi		= {10.1145/3030024.3038282},
  abstract	= {My PhD project focuses on Participatory GIS (PGIS). In the
		  project I analyze two methodologies to offer personalized
		  search results in community maps and a natural interaction
		  with the system. The first consists of automatically
		  gathering the terms according to which the users express
		  their information needs, in order to enrich the domain
		  conceptualization of a PGIS, giving common definitions for
		  places. The second concerns the creation of ontology-based
		  user models that reflect the interests, lexicon and
		  modality of expression adopted by each person, mapped to
		  the domain ontology adopted by the PGIS. In the project I
		  also analyze how these techniques may be jointly used
		  during the query expansion process to retrieve more
		  accurate and relevant search results.},
  booktitle	= {Companion Proceedings of the 22nd International Conference
		  on Intelligent User Interfaces},
  pages		= {181–184},
  numpages	= {4},
  keywords	= {linked data, ontologies, ontology-based user model,
		  participatory GIS, personalization, semantic search},
  location	= {Limassol, Cyprus},
  series	= {IUI '17 Companion}
}

@InProceedings{	  10.1145/3340531.3412783,
  author	= {Armitage, Jason and Kacupaj, Endri and Tahmasebzadeh,
		  Golsa and Swati and Maleshkova, Maria and Ewerth, Ralph and
		  Lehmann, Jens},
  title		= {MLM: A Benchmark Dataset for Multitask Learning with
		  Multiple Languages and Modalities},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412783},
  doi		= {10.1145/3340531.3412783},
  abstract	= {In this paper, we introduce the MLM (Multiple Languages
		  and Modalities) dataset - a new resource to train and
		  evaluate multitask systems on samples in multiple
		  modalities and three languages. The generation process and
		  inclusion of semantic data provide a resource that further
		  tests the ability for multitask systems to learn
		  relationships between entities. The dataset is designed for
		  researchers and developers who build applications that
		  perform multiple tasks on data encountered on the web and
		  in digital archives. A second version of MLM provides a
		  geo-representative subset of the data with weighted samples
		  for countries of the European Union. We demonstrate the
		  value of the resource in developing novel applications in
		  the digital humanities with a motivating use case and
		  specify a benchmark set of tasks to retrieve modalities and
		  locate entities in the dataset. Evaluation of baseline
		  multitask and single task systems on the full and
		  geo-representative versions of MLM demonstrate the
		  challenges of generalising on diverse data. In addition to
		  the digital humanities, we expect the resource to
		  contribute to research in multimodal representation
		  learning, location estimation, and scene understanding.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2967–2974},
  numpages	= {8},
  keywords	= {machine learning, multilingual data, multimodal data,
		  multitask learning},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InProceedings{	  10.1145/3394171.3413600,
  author	= {Liu, Ye and Yuan, Junsong and Chen, Chang Wen},
  title		= {ConsNet: Learning Consistency Graph for Zero-Shot
		  Human-Object Interaction Detection},
  year		= {2020},
  isbn		= {9781450379885},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394171.3413600},
  doi		= {10.1145/3394171.3413600},
  abstract	= {We consider the problem of Human-Object Interaction (HOI)
		  Detection, which aims to locate and recognize HOI instances
		  in the form of &lt;human, action, object&gt; in images.
		  Most existing works treat HOIs as individual interaction
		  categories, thus can not handle the problem of long-tail
		  distribution and polysemy of action labels. We argue that
		  multi-level consistencies among objects, actions and
		  interactions are strong cues for generating semantic
		  representations of rare or previously unseen HOIs.
		  Leveraging the compositional and relational peculiarities
		  of HOI labels, we propose ConsNet, a knowledge-aware
		  framework that explicitly encodes the relations among
		  objects, actions and interactions into an undirected graph
		  called consistency graph, and exploits Graph Attention
		  Networks (GATs) to propagate knowledge among HOI categories
		  as well as their constituents. Our model takes visual
		  features of candidate human-object pairs and word
		  embeddings of HOI labels as inputs, maps them into
		  visual-semantic joint embedding space and obtains detection
		  results by measuring their similarities. We extensively
		  evaluate our model on the challenging V-COCO and HICO-DET
		  datasets, and results validate that our approach
		  outperforms state-of-the-arts under both fully-supervised
		  and zero-shot settings.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Multimedia},
  pages		= {4235–4243},
  numpages	= {9},
  keywords	= {graph neural networks, human-object interaction detection,
		  zero-shot learning},
  location	= {Seattle, WA, USA},
  series	= {MM '20}
}

@InProceedings{	  10.1145/3019612.3019833,
  author	= {Dragoni, Mauro and Rexha, Andi and Ziak, Hermann and Kern,
		  Roman},
  title		= {A semantic federated search engine for domain-specific
		  document retrieval},
  year		= {2017},
  isbn		= {9781450344869},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3019612.3019833},
  doi		= {10.1145/3019612.3019833},
  abstract	= {Retrieval of domain-specific documents became attractive
		  for the Semantic Web community due to the possibility of
		  integrating classic Information Retrieval (IR) techniques
		  with semantic knowledge. Unfortunately, the gap between the
		  construction of a full semantic search engine and the
		  possibility of exploiting a repository of ontologies
		  covering all possible domains is far from being filled.
		  Recent solutions focused on the aggregation of different
		  domain-specific repositories managed by third-parties. In
		  this paper, we present a semantic federated search engine
		  developed in the context of the EEXCESS EU project. Through
		  the developed platform, users are able to perform federated
		  queries over repositories in a transparent way, i.e.
		  without knowing how their original queries are transformed
		  before being actually submitted. The platform implements a
		  facility for plugging new repositories and for creating,
		  with the support of general purpose knowledge bases,
		  knowledge graphs describing the content of each connected
		  repository. Such knowledge graphs are then exploited for
		  enriching queries performed by users.},
  booktitle	= {Proceedings of the Symposium on Applied Computing},
  pages		= {303–308},
  numpages	= {6},
  keywords	= {federated search, information retrieval, knowledge-based
		  query expansion, semantic web},
  location	= {Marrakech, Morocco},
  series	= {SAC '17}
}

@InProceedings{	  10.1145/3368089.3409731,
  author	= {Xie, Wenkai and Peng, Xin and Liu, Mingwei and Treude,
		  Christoph and Xing, Zhenchang and Zhang, Xiaoxin and Zhao,
		  Wenyun},
  title		= {API method recommendation via explicit matching of
		  functionality verb phrases},
  year		= {2020},
  isbn		= {9781450370431},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3368089.3409731},
  doi		= {10.1145/3368089.3409731},
  abstract	= {Due to the lexical gap between functionality descriptions
		  and user queries, documentation-based API retrieval often
		  produces poor results.Verb phrases and their phrase
		  patterns are essential in both describing API
		  functionalities and interpreting user queries. Thus we
		  hypothesize that API retrieval can be facilitated by
		  explicitly recognizing and matching between the
		  fine-grained structures of functionality descriptions and
		  user queries. To verify this hypothesis, we conducted a
		  large-scale empirical study on the functionality
		  descriptions of 14,733 JDK and Android API methods. We
		  identified 356 different functionality verbs from the
		  descriptions, which were grouped into 87 functionality
		  categories, and we extracted 523 phrase patterns from the
		  verb phrases of the descriptions. Building on these
		  findings, we propose an API method recommendation approach
		  based on explicit matching of functionality verb phrases in
		  functionality descriptions and user queries, called PreMA.
		  Our evaluation shows that PreMA can accurately recognize
		  the functionality categories (92.8%) and phrase patterns
		  (90.4%) of functionality description sentences; and when
		  used for API retrieval tasks, PreMA can help participants
		  complete their tasks more accurately and with fewer retries
		  compared to a baseline approach.},
  booktitle	= {Proceedings of the 28th ACM Joint Meeting on European
		  Software Engineering Conference and Symposium on the
		  Foundations of Software Engineering},
  pages		= {1015–1026},
  numpages	= {12},
  keywords	= {API Documentation, API Retrieval, Functionality
		  Description},
  location	= {Virtual Event, USA},
  series	= {ESEC/FSE 2020}
}

@InProceedings{	  10.1145/3397271.3401442,
  author	= {Wei, Mengxi and He, YIfan and Zhang, Qiong},
  title		= {Robust Layout-aware IE for Visually Rich Documents with
		  Pre-trained Language Models},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401442},
  doi		= {10.1145/3397271.3401442},
  abstract	= {Many business documents processed in modern NLP and IR
		  pipelines are visually rich: in addition to text, their
		  semantics can also be captured by visual traits such as
		  layout, format, and fonts. We study the problem of
		  information extraction from visually rich documents (VRDs)
		  and present a model that combines the power of large
		  pre-trained language models and graph neural networks to
		  efficiently encode both textual and visual information in
		  business documents. We further introduce new fine-tuning
		  objectives to improve in-domain unsupervised fine-tuning to
		  better utilize large amount of unlabeled in-domain data.We
		  experiment on real world invoice and resume data sets and
		  show that the proposed method outperforms strong text-based
		  RoBERTa baselines by 6.3% absolute F1 on invoices and 4.7%
		  absolute F1 on resumes. When evaluated in a few-shot
		  setting, our method requires up to 30x less annotation data
		  than the baseline to achieve the same level of performance
		  at ~90% F1.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2367–2376},
  numpages	= {10},
  keywords	= {graph neural networks, structured information extraction,
		  visually rich document},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@InProceedings{	  10.1145/3132847.3133048,
  author	= {Xiong, Chenyan and Liu, Zhengzhong and Callan, Jamie and
		  Hovy, Eduard},
  title		= {JointSem: Combining Query Entity Linking and Entity based
		  Document Ranking},
  year		= {2017},
  isbn		= {9781450349185},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3132847.3133048},
  doi		= {10.1145/3132847.3133048},
  abstract	= {Entity-based ranking systems often employ entity linking
		  systems to align entities to query and documents.
		  Previously, entity linking systems were not designed
		  specifically for search engines and were mostly used as a
		  preprocessing step. This work presents JointSem, a joint
		  semantic ranking system that combines query entity linking
		  and entity-based document ranking. In JointSem, the
		  spotting and linking signals are used to describe the
		  importance of candidate entities in the query, and the
		  linked entities are utilized to provide additional ranking
		  features for the documents. The linking signals and the
		  ranking signals are combined by a joint learning-to-rank
		  model, and the whole system is fully optimized towards
		  end-to-end ranking performance. Experiments on TREC Web
		  Track datasets demonstrate the effectiveness of joint
		  learning of entity linking and entity-based ranking.},
  booktitle	= {Proceedings of the 2017 ACM on Conference on Information
		  and Knowledge Management},
  pages		= {2391–2394},
  numpages	= {4},
  keywords	= {document ranking, entity linking, entity-based search},
  location	= {Singapore, Singapore},
  series	= {CIKM '17}
}

@InProceedings{	  10.1145/3371158.3371200,
  author	= {Kumar, Sumit and Ramena, Gopi and Goyal, Manoj and
		  Mohanty, Debi and Agarwal, Ankur and Changmai, Benu and
		  Moharana, Sukumar},
  title		= {On-Device Information Extraction from Screenshots in form
		  of tags},
  year		= {2020},
  isbn		= {9781450377386},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3371158.3371200},
  doi		= {10.1145/3371158.3371200},
  abstract	= {We propose a method to make mobile Screenshots easily
		  searchable. In this paper, we present the workflow in which
		  we: 1) pre-processed a collection of screenshots, 2)
		  identified script present in image, 3) extracted
		  unstructured text from images, 4) identified language of
		  the extracted text, 5) extracted keywords from the text, 6)
		  identified tags based on image features, 7) expanded tag
		  set by identifying related keywords, 8) inserted image tags
		  with relevant images after ranking and indexed them to make
		  it searchable on device. We made the pipeline which
		  supports multiple languages and executed it on-device,
		  which addressed privacy concerns. We developed novel
		  architectures for components in the pipeline, optimized
		  performance and memory for on-device computation. We
		  observed from experimentation that the solution developed
		  can reduce overall user effort and improve end user
		  experience while searching, whose results are published.},
  booktitle	= {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
  pages		= {275–281},
  numpages	= {7},
  keywords	= {on-device search, on-device tag extraction, tag expansion,
		  tag recommendation},
  location	= {Hyderabad, India},
  series	= {CoDS COMAD 2020}
}

@Article{	  10.1145/3383123,
  author	= {Huang, Minlie and Zhu, Xiaoyan and Gao, Jianfeng},
  title		= {Challenges in Building Intelligent Open-domain Dialog
		  Systems},
  year		= {2020},
  issue_date	= {July 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {38},
  number	= {3},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3383123},
  doi		= {10.1145/3383123},
  abstract	= {There is a resurgent interest in developing intelligent
		  open-domain dialog systems due to the availability of large
		  amounts of conversational data and the recent progress on
		  neural approaches to conversational AI [33]. Unlike
		  traditional task-oriented bots, an open-domain dialog
		  system aims to establish long-term connections with users
		  by satisfying the human need for communication, affection,
		  and social belonging. This article reviews the recent work
		  on neural approaches that are devoted to addressing three
		  challenges in developing such systems: semantics,
		  consistency, and interactiveness. Semantics requires a
		  dialog system to not only understand the content of the
		  dialog but also identify users’ emotional and social
		  needs during the conversation. Consistency requires the
		  system to demonstrate a consistent personality to win
		  users’ trust and gain their long-term confidence.
		  Interactiveness refers to the system’s ability to
		  generate interpersonal responses to achieve particular
		  social goals such as entertainment and conforming. The
		  studies we select to present in this survey are based on
		  our unique views and are by no means complete.
		  Nevertheless, we hope that the discussion will inspire new
		  research in developing more intelligent open-domain dialog
		  systems.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= apr,
  articleno	= {21},
  numpages	= {32},
  keywords	= {Dialog system, chatbot, conversation generation,
		  conversational AI, response generation, social bot}
}

@InProceedings{	  10.1145/3209978.3209982,
  author	= {Xiong, Chenyan and Liu, Zhengzhong and Callan, Jamie and
		  Liu, Tie-Yan},
  title		= {Towards Better Text Understanding and Retrieval through
		  Kernel Entity Salience Modeling},
  year		= {2018},
  isbn		= {9781450356572},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209978.3209982},
  doi		= {10.1145/3209978.3209982},
  abstract	= {This paper presents a Kernel Entity Salience Model (KESM)
		  that improves text understanding and retrieval by better
		  estimating entity salience (importance) in documents. KESM
		  represents entities by knowledge enriched distributed
		  representations, models the interactions between entities
		  and words by kernels, and combines the kernel scores to
		  estimate entity salience. The whole model is learned
		  end-to-end using entity salience labels. The salience model
		  also improves ad hoc search accuracy, providing effective
		  ranking features by modeling the salience of query entities
		  in candidate documents. Our experiments on two entity
		  salience corpora and two TREC ad hoc search datasets
		  demonstrate the effectiveness of KESM over frequency-based
		  and feature-based methods. We also provide examples showing
		  how KESM conveys its text understanding ability learned
		  from entity salience to search.},
  booktitle	= {The 41st International ACM SIGIR Conference on Research
		  &amp; Development in Information Retrieval},
  pages		= {575–584},
  numpages	= {10},
  keywords	= {entity salience, entity-oriented search, text
		  representation, text understanding},
  location	= {Ann Arbor, MI, USA},
  series	= {SIGIR '18}
}

@InProceedings{	  10.1145/3397271.3401462,
  author	= {Feng, Fuli and Luo, Cheng and He, Xiangnan and Liu, Yiqun
		  and Chua, Tat-Seng},
  title		= {FinIR 2020: The First Workshop on Information Retrieval in
		  Finance},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401462},
  doi		= {10.1145/3397271.3401462},
  abstract	= {This half-day workshop explores challenges and potential
		  research directions about Information Retrieval (IR) in
		  finance. The focus will be on stimulating discussions
		  around the accessing, searching, filtering, and analyzing
		  financial documents in banking, insurance, and investment,
		  such as the financial statements, analyst reports, filling
		  forms, and news articles. We welcome theoretical,
		  experimental, and methodological studies that aim to
		  advance techniques of managing and understanding financial
		  documents, as well as emphasize the applicability in
		  practical applications. The workshop aims to bring together
		  a diverse set of researchers and practitioners interested
		  in investigating relevant topics. Besides, to facilitate
		  developing and testing some relevant techniques, we hold a
		  data challenge on quantifying analyst reports and news
		  articles for the prediction of commodity prices.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2451–2454},
  numpages	= {4},
  keywords	= {finance, information retrieval, unstructured data},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@Article{	  10.1145/3384675,
  author	= {Zhang, Yingying and Fang, Quan and Qian, Shengsheng and
		  Xu, Changsheng},
  title		= {Knowledge-aware Attentive Wasserstein Adversarial Dialogue
		  Response Generation},
  year		= {2020},
  issue_date	= {August 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {11},
  number	= {4},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3384675},
  doi		= {10.1145/3384675},
  abstract	= {Natural language generation has become a fundamental task
		  in dialogue systems. RNN-based natural response generation
		  methods encode the dialogue context and decode it into a
		  response. However, they tend to generate dull and simple
		  responses. In this article, we propose a novel framework,
		  called KAWA-DRG (Knowledge-aware Attentive Wasserstein
		  Adversarial Dialogue Response Generation) to model
		  conversation-specific external knowledge and the importance
		  variances of dialogue context in a unified adversarial
		  encoder-decoder learning framework. In KAWA-DRG, a
		  co-attention mechanism attends to important parts within
		  and among context utterances with word-utterance-level
		  attention. Prior knowledge is integrated into the
		  conditional Wasserstein auto-encoder for learning the
		  latent variable space. The posterior and prior distribution
		  of latent variables are generated and trained through
		  adversarial learning. We evaluate our model on Switchboard,
		  DailyDialog, In-Car Assistant, and Ubuntu Dialogue Corpus.
		  Experimental results show that KAWA-DRG outperforms the
		  existing methods.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= may,
  articleno	= {37},
  numpages	= {20},
  keywords	= {Dialogue system, adversarial learning, co-attention,
		  external knowledge}
}

@InProceedings{	  10.1145/3394171.3413880,
  author	= {Zhang, Shengyu and Tan, Ziqi and Yu, Jin and Zhao, Zhou
		  and Kuang, Kun and Liu, Jie and Zhou, Jingren and Yang,
		  Hongxia and Wu, Fei},
  title		= {Poet: Product-oriented Video Captioner for E-commerce},
  year		= {2020},
  isbn		= {9781450379885},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394171.3413880},
  doi		= {10.1145/3394171.3413880},
  abstract	= {In e-commerce, a growing number of user-generated videos
		  are used for product promotion. How to generate video
		  descriptions that narrate the user-preferred product
		  characteristics depicted in the video is vital for
		  successful promoting. Traditional video captioning methods,
		  which focus on routinely describing what exists and happens
		  in a video, are not amenable for product-oriented video
		  captioning. To address this problem, we propose a
		  product-oriented video captioner framework, abbreviated as
		  Poet. Poet firstly represents the videos as
		  product-oriented spatial-temporal graphs. Then, based on
		  the aspects of the video-associated product, we perform
		  knowledge-enhanced spatial-temporal inference on those
		  graphs for capturing the dynamic change of fine-grained
		  product-part characteristics. The knowledge leveraging
		  module in Poet differs from the traditional design by
		  performing knowledge filtering and dynamic memory modeling.
		  We show that Poet achieves consistent performance
		  improvement over previous methods concerning generation
		  quality, product aspects capturing, and lexical diversity.
		  Experiments are performed on two product-oriented video
		  captioning datasets, buyer-generated fashion video dataset
		  (BFVD) and fan-generated fashion video dataset (FFVD),
		  collected from Mobile Taobao. We will release the
		  desensitized datasets to promote further investigations on
		  both video captioning and general video analysis
		  problems.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Multimedia},
  pages		= {1292–1301},
  numpages	= {10},
  keywords	= {e-commerce, external knowledge, user-generated video
		  analysis, video-to-text generation},
  location	= {Seattle, WA, USA},
  series	= {MM '20}
}

@Article{	  10.1613/jair.1.11259,
  author	= {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
  title		= {From word to sense embeddings: a survey on vector
		  representations of meaning},
  year		= {2018},
  issue_date	= {September 2018},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {63},
  number	= {1},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.11259},
  doi		= {10.1613/jair.1.11259},
  abstract	= {Over the past years, distributed semantic representations
		  have proved to be effective and flexible keepers of prior
		  knowledge to be integrated into downstream applications.
		  This survey focuses on the representation of meaning. We
		  start from the theoretical background behind word vector
		  space models and highlight one of their major limitations:
		  the meaning conflation deficiency, which arises from
		  representing a word with all its possible meanings as a
		  single vector. Then, we explain how this deficiency can be
		  addressed through a transition from the word level to the
		  more fine-grained level of word senses (in its broader
		  acceptation) as a method for modelling unambiguous lexical
		  meaning. We present a comprehensive overview of the wide
		  range of techniques in the two main branches of sense
		  representation, i.e., unsupervised and knowledge-based.
		  Finally, this survey covers the main evaluation procedures
		  and applications for this type of representation, and
		  provides an analysis of four of its important aspects:
		  interpretability, sense granularity, adaptability to
		  different domains and compositionality.},
  journal	= {J. Artif. Int. Res.},
  month		= sep,
  pages		= {743–788},
  numpages	= {46}
}

@InProceedings{	  10.1145/3383583.3398525,
  author	= {Ostendorff, Malte and Ruas, Terry and Schubotz, Moritz and
		  Rehm, Georg and Gipp, Bela},
  title		= {Pairwise Multi-Class Document Classification for Semantic
		  Relations between Wikipedia Articles},
  year		= {2020},
  isbn		= {9781450375856},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383583.3398525},
  doi		= {10.1145/3383583.3398525},
  abstract	= {Many digital libraries recommend literature to their users
		  considering the similarity between a query document and
		  their repository. However, they often fail to distinguish
		  what is the relationship that makes two documents alike. In
		  this paper, we model the problem of finding the
		  relationship between two documents as a pairwise document
		  classification task. To find the semantic relation between
		  documents, we apply a series of techniques, such as GloVe,
		  Paragraph Vectors, BERT, and XLNet under different
		  configurations (e.g., sequence length, vector concatenation
		  scheme), including a Siamese architecture for the
		  Transformer-based systems. We perform our experiments on a
		  newly proposed dataset of 32,168 Wikipedia article pairs
		  and Wikidata properties that define the semantic document
		  relations. Our results show vanilla BERT as the best
		  performing system with an F1-score of 0.93, which we
		  manually examine to better understand its applicability to
		  other domains. Our findings suggest that classifying
		  semantic relations between documents is a solvable task and
		  motivates the development of a recommender system based on
		  the evaluated techniques. The discussions in this paper
		  serve as first steps in the exploration of documents
		  through SPARQL-like queries such that one could find
		  documents that are similar in one aspect but dissimilar in
		  another.},
  booktitle	= {Proceedings of the ACM/IEEE Joint Conference on Digital
		  Libraries in 2020},
  pages		= {127–136},
  numpages	= {10},
  keywords	= {BERT, Siamese networks, Wikipedia, XLNet, document
		  classification, document similarity, recommender systems,
		  transformers},
  location	= {Virtual Event, China},
  series	= {JCDL '20}
}

@InProceedings{	  10.1145/3366423.3380175,
  author	= {Yu, Wenhao and Yu, Mengxia and Zhao, Tong and Jiang,
		  Meng},
  title		= {Identifying Referential Intention with Heterogeneous
		  Contexts},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380175},
  doi		= {10.1145/3366423.3380175},
  abstract	= {Citing, quoting, and forwarding &amp; commenting behaviors
		  are widely seen in academia, news media, and social media.
		  Existing behavior modeling approaches focused on mining
		  content and describing preferences of authors, speakers,
		  and users. However, behavioral intention plays an important
		  role in generating content on the platforms. In this work,
		  we propose to identify the referential intention which
		  motivates the action of using the referred (e.g., cited,
		  quoted, and retweeted) source and content to support their
		  claims. We adopt a theory in sociology to develop a schema
		  of four types of intentions. The challenge lies in the
		  heterogeneity of observed contextual information
		  surrounding the referential behavior, such as referred
		  content (e.g., a cited paper), local context (e.g., the
		  sentence citing the paper), neighboring context (e.g., the
		  former and latter sentences), and network context (e.g.,
		  the academic network of authors, affiliations, and
		  keywords). We propose a new neural framework with
		  Interactive Hierarchical Attention (IHA) to identify the
		  intention of referential behavior by properly aggregating
		  the heterogeneous contexts. Experiments demonstrate that
		  the proposed method can effectively identify the type of
		  intention of citing behaviors (on academic data) and
		  retweeting behaviors (on Twitter). And learning the
		  heterogeneous contexts collectively can improve the
		  performance. This work opens a door for understanding
		  content generation from a fundamental perspective of
		  behavior sciences.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {962–972},
  numpages	= {11},
  keywords	= {Heterogeneous Contexts, Interactive Hierarchical
		  Attention, Referential Intention},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@Article{	  10.1145/3331000,
  author	= {Gon\c{c}alves, Rodrigo and Dorneles, Carina Friedrich},
  title		= {Automated Expertise Retrieval: A Taxonomy-Based Survey and
		  Open Issues},
  year		= {2019},
  issue_date	= {September 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {5},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3331000},
  doi		= {10.1145/3331000},
  abstract	= {Understanding people’s expertise is not a trivial task
		  since it is time-consuming when manually executed.
		  Automated approaches have become a topic of research in
		  recent years in various scientific fields, such as
		  information retrieval, databases, and machine learning.
		  This article carries out a survey on automated expertise
		  retrieval, i.e., finding data linked to a person that
		  describes the person’s expertise, which allows tasks such
		  as profiling or finding people with a certain expertise. A
		  faceted taxonomy is introduced that covers many of the
		  existing approaches and classifies them on the basis of
		  features chosen from studying the state-of-the-art. A list
		  of open issues, with suggestions for future research
		  topics, is introduced as well. It is hoped that our
		  taxonomy and review of related works on expertise retrieval
		  will be useful when analyzing different proposals and will
		  allow a better understanding of existing work and a
		  systematic classification of future work on the topic.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {96},
  numpages	= {30},
  keywords	= {Expertise retrieval, expert finding, expertise profile}
}

@Article{	  10.14778/3415478.3415559,
  author	= {Suri, Sahaana and Chanda, Raghuveer and Bulut, Neslihan
		  and Narayana, Pradyumna and Zeng, Yemao and Bailis, Peter
		  and Basu, Sugato and Narlikar, Girija and R\'{e},
		  Christopher and Sethi, Abishek},
  title		= {Leveraging organizational resources to adapt models to new
		  data modalities},
  year		= {2020},
  issue_date	= {August 2020},
  publisher	= {VLDB Endowment},
  volume	= {13},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3415478.3415559},
  doi		= {10.14778/3415478.3415559},
  abstract	= {As applications in large organizations evolve, the machine
		  learning (ML) models that power them must adapt the same
		  predictive tasks to newly arising data modalities (e.g., a
		  new video content launch in a social media application
		  requires existing text or image models to extend to video).
		  To solve this problem, organizations typically create ML
		  pipelines from scratch. However, this fails to utilize the
		  domain expertise and data they have cultivated from
		  developing tasks for existing modalities. We demonstrate
		  how organizational resources, in the form of aggregate
		  statistics, knowledge bases, and existing services that
		  operate over related tasks, enable teams to construct a
		  common feature space that connects new and existing data
		  modalities. This allows teams to apply methods for data
		  curation (e.g., weak supervision and label propagation) and
		  model training (e.g., forms of multi-modal learning) across
		  these different data modalities. We study how this use of
		  organizational resources composes at production scale in
		  over 5 classification tasks at Google, and demonstrate how
		  it reduces the time needed to develop models for new
		  modalities from months to weeks or days.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {3396–3410},
  numpages	= {15}
}

@InProceedings{	  10.1145/3397271.3401416,
  author	= {van Hulst, Johannes M. and Hasibi, Faegheh and Dercksen,
		  Koen and Balog, Krisztian and de Vries, Arjen P.},
  title		= {REL: An Entity Linker Standing on the Shoulders of
		  Giants},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401416},
  doi		= {10.1145/3397271.3401416},
  abstract	= {Entity linking is a standard component in modern retrieval
		  system that is often performed by third-party toolkits.
		  Despite the plethora of open source options, it is
		  difficult to find a single system that has a modular
		  architecture where certain components may be replaced, does
		  not depend on external sources, can easily be updated to
		  newer Wikipedia versions, and, most important of all, has
		  state-of-the-art performance. The REL system presented in
		  this paper aims to fill that gap. Building on
		  state-of-the-art neural components from natural language
		  processing research, it is provided as a Python package as
		  well as a web API. We also report on an experimental
		  comparison against both well-established systems and the
		  current state-of-the-art on standard entity linking
		  benchmarks.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2197–2200},
  numpages	= {4},
  keywords	= {NER, entity disambiguation, entity linking, toolkit},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@Article{	  10.1109/taslp.2020.3013114,
  author	= {Wang, Yu and Li, Yun and Zhu, Ziye and Tong, Hanghang and
		  Huang, Yue},
  title		= {Adversarial Learning for Multi-Task Sequence Labeling With
		  Attention Mechanism},
  year		= {2020},
  issue_date	= {2020},
  publisher	= {IEEE Press},
  volume	= {28},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2020.3013114},
  doi		= {10.1109/TASLP.2020.3013114},
  abstract	= {With the requirements of natural language applications,
		  multi-task sequence labeling methods have some immediate
		  benefits over the single-task sequence labeling methods.
		  Recently, many state-of-the-art multi-task sequence
		  labeling methods were proposed, while still many issues to
		  be resolved including (C1) exploring a more general
		  relationship between tasks, (C2) extracting the task-shared
		  knowledge purely and (C3) merging the task-shared knowledge
		  for each task appropriately. To address the above
		  challenges, we propose MTAA, a symmetric multi-task
		  sequence labeling model, which performs an arbitrary number
		  of tasks simultaneously. Furthermore, MTAA extracts the
		  shared knowledge among tasks by adversarial learning and
		  integrates the proposed multi-representation fusion
		  attention mechanism for merging feature representations. We
		  evaluate MTAA on two widely used data sets: CoNLL2003 and
		  OntoNotes5.0. Experimental results show that our proposed
		  model outperforms the latest methods on the named entity
		  recognition and the syntactic chunking task by a large
		  margin, and achieves state-of-the-art results on the
		  part-of-speech tagging task.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= sep,
  pages		= {2476–2488},
  numpages	= {13}
}

@Article{	  10.14778/3372716.3372727,
  author	= {Karagiannis, Georgios and Trummer, Immanuel and Jo, Saehan
		  and Khandelwal, Shubham and Wang, Xuezhi and Yu, Cong},
  title		= {Mining an "anti-knowledge base" from Wikipedia updates
		  with applications to fact checking and beyond},
  year		= {2019},
  issue_date	= {December 2019},
  publisher	= {VLDB Endowment},
  volume	= {13},
  number	= {4},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3372716.3372727},
  doi		= {10.14778/3372716.3372727},
  abstract	= {We introduce the problem of anti-knowledge mining. Our
		  goal is to create an "anti-knowledge base" that contains
		  factual mistakes. The resulting data can be used for
		  analysis, training, and benchmarking in the research domain
		  of automated fact checking. Prior data sets feature
		  manually generated fact checks of famous misclaims.
		  Instead, we focus on the long tail of factual mistakes made
		  by Web authors, ranging from erroneous sports results to
		  incorrect capitals.We mine mistakes automatically, by an
		  unsupervised approach, from Wikipedia updates that correct
		  factual mistakes. Identifying such updates (only a small
		  fraction of the total number of updates) is one of the
		  primary challenges. We mine anti-knowledge by a multi-step
		  pipeline. First, we filter out candidate updates via
		  several simple heuristics. Next, we correlate Wikipedia
		  updates with other statements made on the Web. Using claim
		  occurrence frequencies as input to a probabilistic model,
		  we infer the likelihood of corrections via an iterative
		  expectation-maximization approach. Finally, we extract
		  mistakes in the form of subject-predicate-object triples
		  and rank them according to several criteria. Our end result
		  is a data set containing over 110,000 ranked mistakes with
		  a precision of 85% in the top 1% and a precision of over
		  60% in the top 25%. We demonstrate that baselines achieve
		  significantly lower precision. Also, we exploit our data to
		  verify several hypothesis on why users make mistakes. We
		  finally show that the AKB can be used to find mistakes on
		  the entire Web.},
  journal	= {Proc. VLDB Endow.},
  month		= dec,
  pages		= {561–573},
  numpages	= {13}
}

@Article{	  10.1162/coli_a_00363,
  author	= {Laha, Anirban and Jain, Parag and Mishra, Abhijit and
		  Sankaranarayanan, Karthik},
  title		= {Scalable Micro-planned Generation of Discourse from
		  Structured Data},
  year		= {2020},
  issue_date	= {December 2019},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA},
  volume	= {45},
  number	= {4},
  issn		= {0891-2017},
  url		= {https://doi.org/10.1162/coli_a_00363},
  doi		= {10.1162/coli_a_00363},
  abstract	= {We present a framework for generating natural language
		  description from structured data such as tables; the
		  problem comes under the category of data-to-text natural
		  language generation (NLG). Modern data-to-text NLG systems
		  typically use end-to-end statistical and neural
		  architectures that learn from a limited amount of
		  task-specific labeled data, and therefore exhibit limited
		  scalability, domain-adaptability, and interpretability.
		  Unlike these systems, ours is a modular, pipeline-based
		  approach, and does not require task-specific parallel data.
		  Rather, it relies on monolingual corpora and basic
		  off-the-shelf NLP tools. This makes our system more
		  scalable and easily adaptable to newer domains.Our system
		  utilizes a three-staged pipeline that: (i) converts entries
		  in the structured data to canonical form, (ii) generates
		  simple sentences for each atomic entry in the canonicalized
		  representation, and (iii) combines the sentences to produce
		  a coherent, fluent, and adequate paragraph description
		  through sentence compounding and co-reference replacement
		  modules. Experiments on a benchmark mixed-domain data set
		  curated for paragraph description from tables reveals the
		  superiority of our system over existing data-to-text
		  approaches. We also demonstrate the robustness of our
		  system in accepting other popular data sets covering
		  diverse data types such as knowledge graphs and key-value
		  maps.},
  journal	= {Comput. Linguist.},
  month		= jan,
  pages		= {737–763},
  numpages	= {27}
}

@InProceedings{	  10.1145/3340531.3412736,
  author	= {Moerchen, Fabian and Ernst, Patrick and Zappella,
		  Giovanni},
  title		= {Personalizing Natural Language Understanding using
		  Multi-armed Bandits and Implicit Feedback},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3412736},
  doi		= {10.1145/3340531.3412736},
  abstract	= {Natural Language Understanding (NLU) models on
		  voice-controlled speakers face several challenges. In
		  particular, music streaming services have large catalogs,
		  often containing millions of songs, artists, and albums and
		  several thousands of custom playlists and stations. In many
		  cases there is ambiguity and little structural difference
		  between carrier phrases and entity names. In this work, we
		  describe how we leveraged multi-armed bandits in
		  combination with implicit customer feedback to improve
		  accuracy and personalization of responses to voice request
		  in the music domain. Our models are tested in a large-scale
		  industrial system containing several other components. In
		  particular, we focused on using this technology to correct
		  errors made by upstream NLU models and personalize
		  responses based on customer preferences and music provider
		  functionality. The models resulted in significant
		  improvement of playback rate for Amazon Music and are
		  deployed in systems serving several countries and
		  languages. We further used the implicit feedback of the
		  customers to generate weakly labeled training data for the
		  NLU models. This improved the experience for customers
		  using other music providers on all Alexa devices.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2661–2668},
  numpages	= {8},
  keywords	= {multi-armed bandits, music, natural language
		  understanding, personalization},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@Article{	  10.1109/taslp.2020.3012060,
  author	= {Qin, Yujia and Qi, Fanchao and Ouyang, Sicong and Liu,
		  Zhiyuan and Yang, Cheng and Wang, Yasheng and Liu, Qun and
		  Sun, Maosong},
  title		= {Improving Sequence Modeling Ability of Recurrent Neural
		  Networks via Sememes},
  year		= {2020},
  issue_date	= {2020},
  publisher	= {IEEE Press},
  volume	= {28},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2020.3012060},
  doi		= {10.1109/TASLP.2020.3012060},
  abstract	= {Sememes, the minimum semantic units of human languages,
		  have been successfully utilized in various natural language
		  processing applications. However, most existing studies
		  exploit sememes in specific tasks and few efforts are made
		  to utilize sememes more fundamentally. In this paper, we
		  propose to incorporate sememes into recurrent neural
		  networks (RNNs) to improve their sequence modeling ability,
		  which is beneficial to all kinds of downstream tasks. We
		  design three different sememe incorporation methods and
		  employ them in typical RNNs including LSTM, GRU and their
		  bidirectional variants. In evaluation, we use several
		  benchmark datasets involving PTB and WikiText-2 for
		  language modeling, SNLI for natural language inference and
		  another two datasets for sentiment analysis and paraphrase
		  detection. Experimental results show evident and consistent
		  improvement of our sememe-incorporated models compared with
		  vanilla RNNs, which proves the effectiveness of our sememe
		  incorporation methods. Moreover, we find the
		  sememe-incorporated models have higher robustness and
		  outperform adversarial training in defending adversarial
		  attack. All the code and data of this work can be obtained
		  at https://github.com/thunlp/SememeRNN.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {2364–2373},
  numpages	= {10}
}

@InProceedings{	  10.1145/3372020.3391564,
  author	= {Chondamrongkul, Nacha and Sun, Jing and Warren, Ian and
		  Lee, Scott Uk-Jin},
  title		= {Semantic-based Architecture Smell Analysis},
  year		= {2020},
  isbn		= {9781450370714},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3372020.3391564},
  doi		= {10.1145/3372020.3391564},
  abstract	= {Software smells have negative impacts on the reliability
		  and modifiability of software systems. The smells in
		  architecture design can be cascaded down to the
		  implementation level and cause issues that require much
		  effort to fix. Therefore, early detection of the
		  architecture smells can benefit the overall quality of the
		  software system. This paper presents an integration of
		  methods that formally define the software architecture
		  design towards architecture smell detection. Our approach
		  serves as a framework that allows the architectural
		  structures and behaviours to be formally analysed based on
		  a coherent technique. We evaluated the accuracy and
		  performance of our approach with the models generated from
		  open source projects. The results show that our approach is
		  effective and functions well.},
  booktitle	= {Proceedings of the 8th International Conference on Formal
		  Methods in Software Engineering},
  pages		= {109–118},
  numpages	= {10},
  keywords	= {Architecture Smells, Model Checking, Ontology Web
		  Language, Smell Detection, Software Architecture},
  location	= {Seoul, Republic of Korea},
  series	= {FormaliSE '20}
}

@InProceedings{	  10.1145/3102254.3102279,
  author	= {Cochez, Michael and Ristoski, Petar and Ponzetto, Simone
		  Paolo and Paulheim, Heiko},
  title		= {Biased graph walks for RDF graph embeddings},
  year		= {2017},
  isbn		= {9781450352253},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3102254.3102279},
  doi		= {10.1145/3102254.3102279},
  abstract	= {Knowledge Graphs have been recognized as a valuable source
		  for background information in many data mining, information
		  retrieval, natural language processing, and knowledge
		  extraction tasks. However, obtaining a suitable feature
		  vector representation from RDF graphs is a challenging
		  task. In this paper, we extend the RDF2Vec approach, which
		  leverages language modeling techniques for unsupervised
		  feature extraction from sequences of entities. We generate
		  sequences by exploiting local information from graph
		  substructures, harvested by graph walks, and learn latent
		  numerical representations of entities in RDF graphs. We
		  extend the way we compute feature vector representations by
		  comparing twelve different edge weighting functions for
		  performing biased walks on the RDF graph, in order to
		  generate higher quality graph embeddings. We evaluate our
		  approach using different machine learning, as well as
		  entity and document modeling benchmark data sets, and show
		  that the naive RDF2Vec approach can be improved by
		  exploiting Biased Graph Walks.},
  booktitle	= {Proceedings of the 7th International Conference on Web
		  Intelligence, Mining and Semantics},
  articleno	= {21},
  numpages	= {12},
  keywords	= {data mining, graph embeddings, linked open data},
  location	= {Amantea, Italy},
  series	= {WIMS '17}
}

@InProceedings{	  10.1145/3289600.3291030,
  author	= {Zhu, Qi and Ren, Xiang and Shang, Jingbo and Zhang, Yu and
		  El-Kishky, Ahmed and Han, Jiawei},
  title		= {Integrating Local Context and Global Cohesiveness for Open
		  Information Extraction},
  year		= {2019},
  isbn		= {9781450359405},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3289600.3291030},
  doi		= {10.1145/3289600.3291030},
  abstract	= {Extracting entities and their relations from text is an
		  important task for understanding massive text corpora. Open
		  information extraction (IE) systems mine relation tuples
		  (i.e., entity arguments and a predicate string to describe
		  their relation) from sentences. These relation tuples are
		  not confined to a predefined schema for the relations of
		  interests. However, current Open IE systems focus on
		  modeling local context information in a sentence to extract
		  relation tuples, while ignoring the fact that global
		  statistics in a large corpus can be collectively leveraged
		  to identify high-quality sentence-level extractions. In
		  this paper, we propose a novel Open IE system, called
		  ReMine, which integrates local context signals and global
		  structural signals in a unified, distant-supervision
		  framework. Leveraging facts from external knowledge bases
		  as supervision, the new system can be applied to many
		  different domains to facilitate sentence-level tuple
		  extractions using corpus-level statistics. Our system
		  operates by solving a joint optimization problem to unify
		  (1) segmenting entity/relation phrases in individual
		  sentences based on local context; and (2) measuring the
		  quality of tuples extracted from individual sentences with
		  a translating-based objective. Learning the two subtasks
		  jointly helps correct errors produced in each subtask so
		  that they can mutually enhance each other. Experiments on
		  two real-world corpora from different domains demonstrate
		  the effectiveness, generality, and robustness of ReMine
		  when compared to state-of-the-art open IE systems.},
  booktitle	= {Proceedings of the Twelfth ACM International Conference on
		  Web Search and Data Mining},
  pages		= {42–50},
  numpages	= {9},
  keywords	= {distant supervision, entity recognition, open information
		  extraction, relation extraction, weakly-supervised
		  learning},
  location	= {Melbourne VIC, Australia},
  series	= {WSDM '19}
}

@InProceedings{	  10.1145/3077136.3080803,
  author	= {Jameel, Shoaib and Bouraoui, Zied and Schockaert, Steven},
  title		= {MEmbER: Max-Margin Based Embeddings for Entity Retrieval},
  year		= {2017},
  isbn		= {9781450350228},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3077136.3080803},
  doi		= {10.1145/3077136.3080803},
  abstract	= {We propose a new class of methods for learning vector
		  space embeddings of entities. While most existing methods
		  focus on modelling similarity, our primary aim is to learn
		  embeddings that are interpretable, in the sense that query
		  terms have a direct geometric representation in the vector
		  space. Intuitively, we want all entities that have some
		  property (i.e. for which a given term is relevant) to be
		  located in some well-defined region of the space. This is
		  achieved by imposing max-margin constraints that are
		  derived from a bag-of-words representation of the entities.
		  The resulting vector spaces provide us with a natural
		  vehicle for identifying entities that have a given property
		  (or ranking them according to how much they have the
		  property), and conversely, to describe what a given set of
		  entities have in common. As we show in our experiments, our
		  models lead to a substantially better performance in a
		  range of entity-oriented search tasks, such as list
		  completion and entity ranking.},
  booktitle	= {Proceedings of the 40th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {783–792},
  numpages	= {10},
  keywords	= {entity embedding, entity ranking, list completion, maximum
		  margin},
  location	= {Shinjuku, Tokyo, Japan},
  series	= {SIGIR '17}
}

@InBook{	  10.1145/2915031.2915035,
  title		= {Text Data Understanding},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915035},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@Article{	  10.1145/3185663,
  author	= {Huang, Jizhou and Ding, Shiqiang and Wang, Haifeng and
		  Liu, Ting},
  title		= {Learning to Recommend Related Entities With Serendipity
		  for Web Search Users},
  year		= {2018},
  issue_date	= {September 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3185663},
  doi		= {10.1145/3185663},
  abstract	= {Entity recommendation, providing entity suggestions to
		  assist users in discovering interesting information, has
		  become an indispensable feature of today’s Web search
		  engine. However, the majority of existing entity
		  recommendation methods are not designed to boost the
		  performance in terms of serendipity, which also plays an
		  important role in the appreciation of users for a
		  recommendation system. To keep users engaged, it is
		  important to take into account serendipity when building an
		  entity recommendation system. In this article, we propose a
		  learning to recommend framework that consists of two
		  components: related entity finding and candidate entity
		  ranking. To boost serendipity performance, three different
		  sets of features that correlate with the three aspects of
		  serendipity are employed in the proposed framework.
		  Extensive experiments are conducted on large-scale,
		  real-world datasets collected from a widely used commercial
		  Web search engine. The experiments show that our method
		  significantly outperforms several strong baseline methods.
		  An analysis on the impact of features reveals that the set
		  of interestingness features is the most powerful feature
		  set, and the set of unexpectedness features can
		  significantly contribute to recommendation effectiveness.
		  In addition, online controlled experiments conducted on a
		  commercial Web search engine demonstrate that our method
		  can significantly improve user engagement against multiple
		  baseline methods. This further confirms the effectiveness
		  of the proposed framework.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {25},
  numpages	= {22},
  keywords	= {Serendipity, Web search, entity recommendation,
		  recommender system, serendipitous entities, serendipitous
		  recommendations}
}

@InProceedings{	  10.1145/3394486.3403244,
  author	= {Huang, Jiaxin and Xie, Yiqing and Meng, Yu and Zhang,
		  Yunyi and Han, Jiawei},
  title		= {CoRel: Seed-Guided Topical Taxonomy Construction by
		  Concept Learning and Relation Transferring},
  year		= {2020},
  isbn		= {9781450379984},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394486.3403244},
  doi		= {10.1145/3394486.3403244},
  abstract	= {Taxonomy is not only a fundamental form of knowledge
		  representation, but also crucial to vast knowledge-rich
		  applications, such as question answering and web search.
		  Most existing taxonomy construction methods extract
		  hypernym-hyponym entity pairs to organize a "universal"
		  taxonomy. However, these generic taxonomies cannot satisfy
		  user's specific interest in certain areas and relations.
		  Moreover, the nature of instance taxonomy treats each node
		  as a single word, which has low semantic coverage for
		  people to fully understand. In this paper, we propose a
		  method for seed-guided topical taxonomy construction, which
		  takes a corpus and a seed taxonomy described by concept
		  names as input, and constructs a more complete taxonomy
		  based on user's interest, wherein each node is represented
		  by a cluster of coherent terms. Our framework, CoRel, has
		  two modules to fulfill this goal. A relation transferring
		  module learns and transfers the user's interested relation
		  along multiple paths to expand the seed taxonomy structure
		  in width and depth. A concept learning module enriches the
		  semantics of each concept node by jointly embedding the
		  taxonomy and text. Comprehensive experiments conducted on
		  real-world datasets show that CoRel generates high-quality
		  topical taxonomies and outperforms all the baselines
		  significantly.},
  booktitle	= {Proceedings of the 26th ACM SIGKDD International
		  Conference on Knowledge Discovery &amp; Data Mining},
  pages		= {1928–1936},
  numpages	= {9},
  keywords	= {relation extraction, semantic computing, taxonomy
		  construction, topic discovery},
  location	= {Virtual Event, CA, USA},
  series	= {KDD '20}
}

@Article{	  10.1145/3313873,
  author	= {Yang, Xiaoshan and Xu, Changsheng},
  title		= {Image Captioning by Asking Questions},
  year		= {2019},
  issue_date	= {April 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2s},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3313873},
  doi		= {10.1145/3313873},
  abstract	= {Image captioning and visual question answering are typical
		  tasks that connect computer vision and natural language
		  processing. Both of them need to effectively represent the
		  visual content using computer vision methods and smoothly
		  process the text sentence using natural language processing
		  skills. The key problem of these two tasks is to infer the
		  target result based on the interactive understanding of the
		  word sequence and the image. Though they practically use
		  similar algorithms, they are studied independently in the
		  past few years. In this article, we attempt to exploit the
		  mutual correlation between these two tasks. We propose the
		  first VQA-improved image-captioning method that transfers
		  the knowledge learned from the VQA corpora to the
		  image-captioning task. A VQA model is first pretrained on
		  image--question--answer instances. Then, the pretrained VQA
		  model is used to extract VQA-grounded semantic
		  representations according to selected free-form open-ended
		  visual question--answer pairs. The VQA-grounded features
		  are complementary to the visual features, because they
		  interpret images from a different perspective. We
		  incorporate the VQA model into the image-captioning model
		  by adaptively fusing the VQA-grounded feature and the
		  attended visual feature. We show that such simple
		  VQA-improved image-captioning (VQA-IIC) models perform
		  better than conventional image-captioning methods on
		  large-scale public datasets.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= jul,
  articleno	= {55},
  numpages	= {19},
  keywords	= {Image captioning, attention networks, visual question
		  answering}
}

@InProceedings{	  10.1145/3331184.3331257,
  author	= {Dietz, Laura},
  title		= {ENT Rank: Retrieving Entities for Topical Information
		  Needs through Entity-Neighbor-Text Relations},
  year		= {2019},
  isbn		= {9781450361729},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331184.3331257},
  doi		= {10.1145/3331184.3331257},
  abstract	= {Related work has demonstrated the helpfulness of utilizing
		  information about entities in text retrieval; here we
		  explore the converse: Utilizing information about text in
		  entity retrieval. We model the relevance of
		  Entity-Neighbor-Text (ENT) relations to derive a
		  learning-to-rank-entities model.We focus on the task of
		  retrieving (multiple) relevant entities in response to a
		  topical information need such as "Zika fever". The ENT Rank
		  model is designed to exploit semi-structured knowledge
		  resources such as Wikipedia for entity retrieval. The ENT
		  Rank model combines (1) established features of
		  entity-relevance, with (2) information from neighboring
		  entities (co-mentioned or mentioned-on-page) through (3)
		  relevance scores of textual contexts through traditional
		  retrieval models such as BM25 and RM3.},
  booktitle	= {Proceedings of the 42nd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {215–224},
  numpages	= {10},
  keywords	= {context, edge weight prediction, entity links, entity
		  retrieval, neighbor-relations},
  location	= {Paris, France},
  series	= {SIGIR'19}
}

@InProceedings{	  10.1145/3196321.3196335,
  author	= {Zhou, Cheng and Li, Bin and Sun, Xiaobing and Guo,
		  Hongjing},
  title		= {Recognizing software bug-specific named entity in software
		  bug repository},
  year		= {2018},
  isbn		= {9781450357142},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3196321.3196335},
  doi		= {10.1145/3196321.3196335},
  abstract	= {Software bug issues are unavoidable in software
		  development and maintenance. In order to manage bugs
		  effectively, bug tracking systems are developed to help to
		  record, manage and track the bugs of each project. The rich
		  information in the bug repository provides the possibility
		  of establishment of entity-centric knowledge bases to help
		  understand and fix the bugs. However, existing named entity
		  recognition (NER) systems deal with text that is
		  structured, formal, well written, with a good grammatical
		  structure and few spelling errors, which cannot be directly
		  used for bug-specific named entity recognition. For bug
		  data, they are free-form texts, which include a mixed
		  language studded with code, abbreviations and
		  software-specific vocabularies. In this paper, we summarize
		  the characteristics of bug entities, propose a
		  classification method for bug entities, and build a
		  baseline corpus on two open source projects (Mozilla and
		  Eclipse). On this basis, we propose an approach for
		  bug-specific entity recognition called BNER with the
		  Conditional Random Fields (CRF) model and word embedding
		  technique. An empirical study is conducted to evaluate the
		  accuracy of our BNER technique, and the results show that
		  the two designed baseline corpus are suitable for
		  bug-specific named entity recognition, and our BNER
		  approach is effective on cross-projects NER.},
  booktitle	= {Proceedings of the 26th Conference on Program
		  Comprehension},
  pages		= {108–119},
  numpages	= {12},
  keywords	= {CRF model, named entity recognition, software bug,
		  software bug corpus, word embedding},
  location	= {Gothenburg, Sweden},
  series	= {ICPC '18}
}

@InBook{	  10.1145/2915031.2915055,
  title		= {Index},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915055},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InBook{	  10.1145/2915031.2915037,
  title		= {Overview of Text Data Access},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915037},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3302425.3302484,
  author	= {Cao, Juan and Gong, Junpeng and Zhang, Pengzhou},
  title		= {Open-Domain Table-to-Text Generation based on Seq2seq},
  year		= {2018},
  isbn		= {9781450366250},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3302425.3302484},
  doi		= {10.1145/3302425.3302484},
  abstract	= {Table-to-text generation involves using natural language
		  to describe a table which has formal structure and valuable
		  information. Open-domain table-to-text especially refers to
		  table-to-text generation for open domain. This paper
		  introduces a theme model based on seq2seq for open-domain
		  table-to-text generation. To deal with the problem of
		  out-of-vocabulary and make the most of the internal
		  correlation within table and the relevance between table
		  and text, this study adopts an improved encoder-decoder
		  approach and a method associating table and text. In
		  addition, this paper improves the beam search method for
		  the inference of the model. The model is experimented on
		  WIKITABLETEXT, and improves the current state-of-the-art
		  BLEU-4 score from 38.23 to 38.71.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  articleno	= {72},
  numpages	= {5},
  keywords	= {Beam search, Open-domain, Seq2seq, Table-to-text
		  generation},
  location	= {Sanya, China},
  series	= {ACAI '18}
}

@InProceedings{	  10.1145/3269206.3271668,
  author	= {Van Gysel, Christophe and de Rijke, Maarten and Kanoulas,
		  Evangelos},
  title		= {Mix 'n Match: Integrating Text Matching and Product
		  Substitutability within Product Search},
  year		= {2018},
  isbn		= {9781450360142},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3269206.3271668},
  doi		= {10.1145/3269206.3271668},
  abstract	= {Two products are substitutes if both can satisfy the same
		  consumer need. Intrinsic incorporation of product
		  substitutability - where substitutability is integrated
		  within latent vector space models - is in contrast to the
		  extrinsic re-ranking of result lists. The fusion of text
		  matching and product substitutability objectives allows
		  latent vector space models to mix and match regularities
		  contained within text descriptions and substitution
		  relations. We introduce a method for intrinsically
		  incorporating product substitutability within latent vector
		  space models for product search that are estimated using
		  gradient descent; it integrates flawlessly with
		  state-of-the-art vector space models. We compare our method
		  to existing methods for incorporating structural entity
		  relations, where product substitutability is incorporated
		  extrinsically by re-ranking. Our method outperforms the
		  best extrinsic method on four benchmarks. We investigate
		  the effect of different levels of text matching and product
		  similarity objectives, and provide an analysis of the
		  effect of incorporating product substitutability on product
		  search ranking diversity. Incorporating product
		  substitutability information improves search relevance at
		  the cost of diversity.},
  booktitle	= {Proceedings of the 27th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1373–1382},
  numpages	= {10},
  keywords	= {entity similarity, latent vector space models, product
		  search},
  location	= {Torino, Italy},
  series	= {CIKM '18}
}

@Article{	  10.1145/3368960,
  author	= {Sun, Xiao and Li, Jia and Wei, Xing and Li, Changliang and
		  Tao, Jianhua},
  title		= {Emotional Conversation Generation Based on a Bayesian Deep
		  Neural Network},
  year		= {2019},
  issue_date	= {January 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {38},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3368960},
  doi		= {10.1145/3368960},
  abstract	= {The field of conversation generation using neural networks
		  has attracted increasing attention from researchers for
		  several years. However, traditional neural language models
		  tend to generate a generic reply with poor semantic logic
		  and no emotion. This article proposes an emotional
		  conversation generation model based on a Bayesian deep
		  neural network that can generate replies with rich
		  emotions, clear themes, and diverse sentences. The topic
		  and emotional keywords of the replies are pregenerated by
		  introducing commonsense knowledge in the model. The reply
		  is divided into multiple clauses, and then a
		  multidimensional generator based on the transformer
		  mechanism proposed in this article is used to iteratively
		  generate clauses from two dimensions: sentence granularity
		  and sentence structure. Subjective and objective
		  experiments prove that compared with existing models, the
		  proposed model effectively improves the semantic logic and
		  emotional accuracy of replies. This model also
		  significantly enhances the diversity of replies, largely
		  overcoming the shortcomings of traditional models that
		  generate safe replies.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= dec,
  articleno	= {8},
  numpages	= {24},
  keywords	= {Bayesian neural network, Emotional conversation
		  generation, affective computing, deep learning, natural
		  language processing}
}

@InBook{	  10.1145/2915031.2915033,
  title		= {Introduction},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915033},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3097983.3098200,
  author	= {Ahmed, Amr and Long, James and Silva, Daniel and Wang,
		  Yuan},
  title		= {A Practical Algorithm for Solving the Incoherence Problem
		  of Topic Models In Industrial Applications},
  year		= {2017},
  isbn		= {9781450348874},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3097983.3098200},
  doi		= {10.1145/3097983.3098200},
  abstract	= {Topic models are often applied in industrial settings to
		  discover user profiles from activity logs where documents
		  correspond to users and words to complex objects such as
		  web sites and installed apps. Standard topic models ignore
		  the content-based similarity structure between these
		  objects largely because of the inability of the Dirichlet
		  prior to capture such side information of word-word
		  correlation. Several approaches were proposed to replace
		  the Dirichlet prior with more expressive alternatives.
		  However, this added expressivity comes with a heavy
		  premium: inference becomes intractable and sparsity is lost
		  which renders these alternatives not suitable for
		  industrial scale applications. In this paper we take a
		  radically different approach to incorporating word-word
		  correlation in topic models by applying this side
		  information at the posterior level rather than at the prior
		  level. We show that this choice preserves sparsity and
		  results in a graph-based sampler for LDA whose
		  computational complexity is asymptotically on bar with the
		  state of the art Alias base sampler for LDA cite{aliasLDA}.
		  We illustrate the efficacy of our approach over real
		  industrial datasets that span up to billion of users, tens
		  of millions of words and thousands of topics. To the best
		  of our knowledge, our approach provides the first practical
		  and scalable solution to this important problem.},
  booktitle	= {Proceedings of the 23rd ACM SIGKDD International
		  Conference on Knowledge Discovery and Data Mining},
  pages		= {1713–1721},
  numpages	= {9},
  keywords	= {big data, interpretable models, knowledge representation,
		  latent variable models, topic models, user modeling},
  location	= {Halifax, NS, Canada},
  series	= {KDD '17}
}

@InBook{	  10.1145/2915031.2915054,
  title		= {References},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915054},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3340555.3356100,
  author	= {Li, Hao and Liu, Chen and Zhu, Su and Yu, Kai},
  title		= {Robust Spoken Language Understanding with Acoustic and
		  Domain Knowledge},
  year		= {2019},
  isbn		= {9781450368605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340555.3356100},
  doi		= {10.1145/3340555.3356100},
  abstract	= {Spoken language understanding (SLU) converts user
		  utterances into structured semantic forms. There are still
		  two main issues for SLU: robustness to ASR-errors and the
		  data sparsity of new and extended domains. In this paper,
		  we propose a robust SLU system by leveraging both acoustic
		  and domain knowledge. We extract audio features by training
		  ASR models on a large number of utterances without semantic
		  annotations. For exploiting domain knowledge, we design
		  lexicon features from the domain ontology and propose an
		  error elimination algorithm to help predicted values
		  recovered from ASR-errors. The results of CATSLU challenge
		  show that our systems can outperform all of the other teams
		  across four domains.},
  booktitle	= {2019 International Conference on Multimodal Interaction},
  pages		= {531–535},
  numpages	= {5},
  keywords	= {Robustness, Spoken Language Understanding},
  location	= {Suzhou, China},
  series	= {ICMI '19}
}

@InProceedings{	  10.5555/3291291.3291311,
  author	= {Boyer, John M.},
  title		= {Natural language question answering in the financial
		  domain},
  year		= {2018},
  publisher	= {IBM Corp.},
  address	= {USA},
  abstract	= {This paper describes a natural language question answering
		  system focused on answering financial domain questions
		  using a daily updated corpus of financial reports.
		  Financial entity types of interest included company stocks,
		  country bonds, currencies, industries, commodities, and
		  diversified assets. Financial questions of interest
		  included explanatory and factual questions about entities
		  as well as financial outlook for entities.An important
		  architectural divergence emerged between the approach
		  required for answering financial outlook questions versus
		  the approach for answering other financial information
		  questions. The financial domain focus also introduced
		  additional challenges to open domain natural language
		  processing that were addressed in the areas of document
		  ingestion, question classification accuracy, question
		  analysis techniques, speed of machine learning, answer
		  ranking by linguistic confidence versus temporality, and
		  system accuracy assessment.},
  booktitle	= {Proceedings of the 28th Annual International Conference on
		  Computer Science and Software Engineering},
  pages		= {189–200},
  numpages	= {12},
  keywords	= {financial domain, financial information retrieval,
		  financial sentiment analysis, question analysis, question
		  answering systems, question classification, text
		  analytics},
  location	= {Markham, Ontario, Canada},
  series	= {CASCON '18}
}

@InProceedings{	  10.1145/3366423.3380158,
  author	= {Huang, Yen-Hao and Liu, Ting-Wei and Lee, Ssu-Rui and
		  Calderon Alvarado, Fernando Henrique and Chen, Yi-Shin},
  title		= {Conquering Cross-source Failure for News Credibility:
		  Learning Generalizable Representations beyond Content
		  Embedding},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380158},
  doi		= {10.1145/3366423.3380158},
  abstract	= {False information on the Internet has caused severe damage
		  to society. Researchers have proposed methods to determine
		  the credibility of news and have obtained good results. As
		  different media sources (publishers) have different content
		  generators (writers) and may focus on different topics or
		  aspects, the word/topic distribution for each media source
		  is divergent from others. We expose a challenge in the
		  generalizability of existing content-based methods to
		  perform consistently when applied to news from media
		  sources non-existing in the training set, namely the
		  cross-source failure. A cross-source setting can cause a
		  decrease beyond in accuracy for current methods;
		  content-sensitive features are considered one of the major
		  causes of cross-source failure for a content-based
		  approach. To overcome this challenge, we propose a
		  syntactic network for news credibility (SYNC), which
		  focuses on function words and syntactic structure to learn
		  generalizable representations for news credibility and
		  further reinforce the cross-source robustness for different
		  media. Experiments with cross-validation on 194 real-world
		  media sources showed that the proposed method could learn
		  the generalizable features and outperformed the
		  state-of-the-art methods on unseen media sources. Extensive
		  analysis on the embedding feature representation represents
		  a strength of the proposed method compared to current
		  content embedding feature approaches. We envision that the
		  proposed method is more robust for real-life application
		  with SYNC on account of its good generalizability.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {774–784},
  numpages	= {11},
  keywords	= {cross-source failure, fake news, generalizability, neural
		  network},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@Article{	  10.1145/3374217,
  author	= {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and
		  Li, Chenliang},
  title		= {Adversarial Attacks on Deep-learning Models in Natural
		  Language Processing: A Survey},
  year		= {2020},
  issue_date	= {June 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {11},
  number	= {3},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3374217},
  doi		= {10.1145/3374217},
  abstract	= {With the development of high computational devices, deep
		  neural networks (DNNs), in recent years, have gained
		  significant popularity in many Artificial Intelligence (AI)
		  applications. However, previous efforts have shown that
		  DNNs are vulnerable to strategically modified samples,
		  named adversarial examples. These samples are generated
		  with some imperceptible perturbations, but can fool the
		  DNNs to give false predictions. Inspired by the popularity
		  of generating adversarial examples against DNNs in Computer
		  Vision (CV), research efforts on attacking DNNs for Natural
		  Language Processing (NLP) applications have emerged in
		  recent years. However, the intrinsic difference between
		  image (CV) and text (NLP) renders challenges to directly
		  apply attacking methods in CV to NLP. Various methods are
		  proposed addressing this difference and attack a wide range
		  of NLP applications. In this article, we present a
		  systematic survey on these works. We collect all related
		  academic works since the first appearance in 2017. We then
		  select, summarize, discuss, and analyze 40 representative
		  works in a comprehensive way. To make the article
		  self-contained, we cover preliminary knowledge of NLP and
		  discuss related seminal works in computer vision. We
		  conclude our survey with a discussion on open issues to
		  bridge the gap between the existing progress and more
		  robust adversarial attacks on NLP DNNs.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= apr,
  articleno	= {24},
  numpages	= {41},
  keywords	= {Deep neural networks, adversarial examples, natural
		  language processing, textual data}
}

@InProceedings{	  10.1145/3383583.3398521,
  author	= {Zhang, Jinsong and Guo, Chun and Liu, Xiaozhong},
  title		= {Characterize and Evaluate Scientific Domain and Domain
		  Context Knowledge Map},
  year		= {2020},
  isbn		= {9781450375856},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3383583.3398521},
  doi		= {10.1145/3383583.3398521},
  abstract	= {Domain knowledge map, a.k.a., scholarly network,
		  construction as an important method can describe the
		  significant characters of a selected domain. In this
		  research, we will address three fundamental problems for
		  scholarly network generation. Firstly, two different
		  methods will be investigated to associate keywords on the
		  graph: Co-occur Domain Distance and Citation Probability
		  Distribution Distance. Secondly, this paper will construct
		  domain (core journals and conference proceedings) knowledge
		  and domain referral (domain citation) scholarly networks,
		  and propose a novel method to integrate those graphs by
		  optimizing the nodes and their linkage. Finally, the paper
		  will propose an innovative method to evaluate the accuracy
		  and coverage of scholarly networks based on training
		  keyword oriented Labeled-LDA model and validate different
		  domain or domain referral graphs.},
  booktitle	= {Proceedings of the ACM/IEEE Joint Conference on Digital
		  Libraries in 2020},
  pages		= {187–196},
  numpages	= {10},
  keywords	= {domain characterization, domain context, knowledge map,
		  scientific domain},
  location	= {Virtual Event, China},
  series	= {JCDL '20}
}

@Article{	  10.1145/3345317,
  author	= {Folt\'{y}nek, Tom\'{a}\v{s} and Meuschke, Norman and Gipp,
		  Bela},
  title		= {Academic Plagiarism Detection: A Systematic Literature
		  Review},
  year		= {2019},
  issue_date	= {November 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3345317},
  doi		= {10.1145/3345317},
  abstract	= {This article summarizes the research on computational
		  methods to detect academic plagiarism by systematically
		  reviewing 239 research papers published between 2013 and
		  2018. To structure the presentation of the research
		  contributions, we propose novel technically oriented
		  typologies for plagiarism prevention and detection efforts,
		  the forms of academic plagiarism, and computational
		  plagiarism detection methods. We show that academic
		  plagiarism detection is a highly active research field.
		  Over the period we review, the field has seen major
		  advances regarding the automated detection of strongly
		  obfuscated and thus hard-to-identify forms of academic
		  plagiarism. These improvements mainly originate from better
		  semantic text analysis methods, the investigation of
		  non-textual content features, and the application of
		  machine learning. We identify a research gap in the lack of
		  methodologically thorough performance evaluations of
		  plagiarism detection systems. Concluding from our analysis,
		  we see the integration of heterogeneous analysis methods
		  for textual and non-textual content features using machine
		  learning as the most promising area for future research
		  contributions to improve the detection of academic
		  plagiarism further.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {112},
  numpages	= {42},
  keywords	= {Plagiarism detection, literature review, machine learning,
		  semantic analysis, text-matching software}
}

@InProceedings{	  10.1145/3131704.3131713,
  author	= {Lin, Zeqi and Zhao, Junfeng and Zou, Yanzhen and Xie,
		  Bing},
  title		= {Document Distance Estimation via Code Graph Embedding},
  year		= {2017},
  isbn		= {9781450353137},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3131704.3131713},
  doi		= {10.1145/3131704.3131713},
  abstract	= {Accurately representing the distance between two documents
		  (i.e. pieces of textual information extracted from various
		  software artifacts) has far-reaching applications in many
		  automated software engineering approaches, such as concept
		  location, bug location and traceability link recovery. This
		  is a challenging task, since documents containing different
		  words may have similar semantic meanings. In this paper, we
		  propose a novel document distance estimation approach. This
		  approach captures latent semantic associations between
		  documents through analyzing structural information in
		  software source code: first, we embed code elements as
		  points in a shared representation space according to
		  structural dependencies between them; then, we represent
		  documents as weighted point clouds of code elements in the
		  representation space and reduce the distance between two
		  documents to an earth mover's distance transportation
		  problem. We define a document classification task in
		  StackOverflow dataset to evaluate the effectiveness of our
		  approach. The empirical evaluation results show that our
		  approach outperforms several state-of-the-art approaches.},
  booktitle	= {Proceedings of the 9th Asia-Pacific Symposium on
		  Internetware},
  articleno	= {11},
  numpages	= {10},
  keywords	= {code graph, document distance, multi-relational data
		  embedding},
  location	= {Shanghai, China},
  series	= {Internetware '17}
}

@InProceedings{	  10.1145/3394486.3403237,
  author	= {Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Chang,
		  Kai-Wei and Sun, Yizhou},
  title		= {GPT-GNN: Generative Pre-Training of Graph Neural
		  Networks},
  year		= {2020},
  isbn		= {9781450379984},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394486.3403237},
  doi		= {10.1145/3394486.3403237},
  abstract	= {Graph neural networks (GNNs) have been demonstrated to be
		  powerful in modeling graph-structured data. However,
		  training GNNs requires abundant task-specific labeled data,
		  which is often arduously expensive to obtain. One effective
		  way to reduce the labeling effort is to pre-train an
		  expressive GNN model on unlabelled data with
		  self-supervision and then transfer the learned model to
		  downstream tasks with only a few labels. In this paper, we
		  present the GPT-GNN framework to initialize GNNs by
		  generative pre-training. GPT-GNN introduces a
		  self-supervised attributed graph generation task to
		  pre-train a GNN so that it can capture the structural and
		  semantic properties of the graph. We factorize the
		  likelihood of graph generation into two components: 1)
		  attribute generation and 2) edge generation. By modeling
		  both components, GPT-GNN captures the inherent dependency
		  between node attributes and graph structure during the
		  generative process. Comprehensive experiments on the
		  billion-scale open academic graph and Amazon recommendation
		  data demonstrate that GPT-GNN significantly outperforms
		  state-of-the-art GNN models without pre-training by up to
		  9.1% across various downstream tasks?},
  booktitle	= {Proceedings of the 26th ACM SIGKDD International
		  Conference on Knowledge Discovery &amp; Data Mining},
  pages		= {1857–1867},
  numpages	= {11},
  keywords	= {generative pre-training, gnn pre-training, graph neural
		  networks, graph representation learning, network
		  embedding},
  location	= {Virtual Event, CA, USA},
  series	= {KDD '20}
}

@InProceedings{	  10.1145/3308558.3313578,
  author	= {Zhao, Chen and He, Yeye},
  title		= {Auto-EM: End-to-end Fuzzy Entity-Matching using
		  Pre-trained Deep Models and Transfer Learning},
  year		= {2019},
  isbn		= {9781450366748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308558.3313578},
  doi		= {10.1145/3308558.3313578},
  abstract	= {Entity matching (EM), also known as entity resolution,
		  fuzzy join, and record linkage, refers to the process of
		  identifying records corresponding to the same real-world
		  entities from different data sources. It is an important
		  and long-standing problem in data integration and data
		  mining. So far progresses have been made mainly in the form
		  of model improvements, where models with better accuracy
		  are developed when large amounts of training data is
		  available. In real-world applications we find that advanced
		  approaches can often require too many labeled examples that
		  is expensive to obtain, which has become a key obstacle to
		  wider adoption. We in this work take a different tack,
		  proposing a transfer-learning approach to EM, leveraging
		  pre-trained EM models from large-scale, production
		  knowledge bases (KB). Specifically, for each entity-type in
		  KB, (e.g., location, organization, people, etc.), we use
		  rich synonymous names of known entities in the KB as
		  training data, to pre-train type-detection and EM models
		  for each type, using a novel hierarchical neural network
		  architecture we develop. Given a new EM task, with little
		  or no training data, we can either fine-tune or directly
		  leverage pre-trained EM models, to build end-to-end,
		  high-quality EM systems. Experiments on a variety of real
		  EM tasks suggest that the pre-trained approach is effective
		  and outperforms existing EM methods.1.},
  booktitle	= {The World Wide Web Conference},
  pages		= {2413–2424},
  numpages	= {12},
  location	= {San Francisco, CA, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3269206.3269245,
  author	= {Dargahi Nobari, Arash and Askari, Arian and Hasibi,
		  Faegheh and Neshati, Mahmood},
  title		= {Query Understanding via Entity Attribute Identification},
  year		= {2018},
  isbn		= {9781450360142},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3269206.3269245},
  doi		= {10.1145/3269206.3269245},
  abstract	= {Understanding searchers' queries is an essential component
		  of semantic search systems. In many cases, search queries
		  involve specific attributes of an entity in a knowledge
		  base (KB), which can be further used to find query answers.
		  In this study, we aim to move forward the understanding of
		  queries by identifying their related entity attributes from
		  a knowledge base. To this end, we introduce the task of
		  entity attribute identification and propose two methods to
		  address it: (i) a model based on Markov Random Field, and
		  (ii) a learning to rank model. We develop a human annotated
		  test collection and show that our proposed methods can
		  bring significant improvements over the baseline methods.},
  booktitle	= {Proceedings of the 27th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1759–1762},
  numpages	= {4},
  keywords	= {entity attributes, entity search, query understanding},
  location	= {Torino, Italy},
  series	= {CIKM '18}
}

@InProceedings{	  10.1145/3299869.3314043,
  author	= {Korn, Flip and Wang, Xuezhi and Wu, You and Yu, Cong},
  title		= {Automatically Generating Interesting Facts from Wikipedia
		  Tables},
  year		= {2019},
  isbn		= {9781450356435},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3299869.3314043},
  doi		= {10.1145/3299869.3314043},
  abstract	= {Modern search engines provide contextual information
		  surrounding query entities beyond ten blue links in the
		  form of information cards. Among the various attributes
		  displayed about entities there has been recent interest in
		  providing fun facts. Obtaining such trivia at a large scale
		  is, however, non-trivial: hiring professional content
		  creators is expensive and extracting statements from the
		  Web is prone to uninteresting, out-of-context and/or
		  unreliable facts.In this paper we show how fun facts can be
		  mined from superlative tables in Wikipedia, whose rows are
		  ranked according to some statistics, to provide a large
		  volume of reliable and interesting content. We employ a
		  template-based approach to semi-automatically generate
		  natural language statements as fun facts. We show how to
		  bootstrap and streamline the process for faster and cheaper
		  task completion. However, the content contained in these
		  tables is dynamic. Therefore, we address the problem of
		  automatically maintaining the pairing of templates to
		  tables as the tables are updated over time. Fun facts
		  produced by our work is now part of Google's production
		  search results.},
  booktitle	= {Proceedings of the 2019 International Conference on
		  Management of Data},
  pages		= {349–361},
  numpages	= {13},
  keywords	= {dynamic maintenance, fun facts generation, superlative
		  tables},
  location	= {Amsterdam, Netherlands},
  series	= {SIGMOD '19}
}

@InBook{	  10.1145/2915031.2915032,
  title		= {Preface},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915032},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3397271.3401173,
  author	= {Zheng, Jianming and Cai, Fei and Chen, Honghui},
  title		= {Incorporating Scenario Knowledge into A Unified
		  Fine-tuning Architecture for Event Representation},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401173},
  doi		= {10.1145/3397271.3401173},
  abstract	= {Given an occurred event, human can easily predict the next
		  event or reason the preceding event, yet which is difficult
		  for machine to perform such event reasoning. Event
		  representation bridges the connection and targets to model
		  the process of event reasoning as a machine-readable
		  format, which then can support a wide range of applications
		  in information retrieval, e.g., question answering and
		  information extraction. Existing work mainly resorts to a
		  joint training to integrate all levels of training loss in
		  event chains by a simple loss summation, which is easily
		  trapped into a local optimum. In addition, the scenario
		  knowledge in event chains is not well investigated for
		  event representation. In this paper, we propose a unified
		  fine-tuning architecture, incorporated with scenario
		  knowledge for event representation, i.e., UniFA-S, which
		  mainly consists of a unified fine-tuning architecture
		  (UniFA) and a scenario-level variational auto-encoder
		  (S-VAE). In detail, UniFA employs a multi-step fine-tuning
		  to integrate all levels of training and S-VAE applies a
		  stochastic variable to implicitly represent the
		  scenario-level knowledge. We evaluate our proposal from two
		  aspects, i.e., the representation and inference abilities.
		  For the representation ability, our ensemble model UniFA-S
		  can beat state-of-the-art baselines for two similarity
		  tasks. For the inference ability, UniFA-S can outperform
		  the best baseline, achieving 4.1%-8.2% improvements in
		  terms of accuracy for various inference tasks.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {249–258},
  numpages	= {10},
  keywords	= {event representation, fine-tuning, pre-training, scenario
		  knowledge},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@InBook{	  10.1145/2915031.2915052,
  title		= {Toward A Unified System for Text Management and Analysis},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915052},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3331184.3331254,
  author	= {Chen, Xu and Chen, Hanxiong and Xu, Hongteng and Zhang,
		  Yongfeng and Cao, Yixin and Qin, Zheng and Zha, Hongyuan},
  title		= {Personalized Fashion Recommendation with Visual
		  Explanations based on Multimodal Attention Network: Towards
		  Visually Explainable Recommendation},
  year		= {2019},
  isbn		= {9781450361729},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331184.3331254},
  doi		= {10.1145/3331184.3331254},
  abstract	= {Fashion recommendation has attracted increasing attention
		  from both industry and academic communities. This paper
		  proposes a novel neural architecture for fashion
		  recommendation based on both image region-level features
		  and user review information. Our basic intuition is that:
		  for a fashion image, not all the regions are equally
		  important for the users, i.e., people usually care about a
		  few parts of the fashion image. To model such human sense,
		  we learn an attention model over many pre-segmented image
		  regions, based on which we can understand where a user is
		  really interested in on the image, and correspondingly,
		  represent the image in a more accurate manner. In addition,
		  by discovering such fine-grained visual preference, we can
		  visually explain a recommendation by highlighting some
		  regions of its image. For better learning the attention
		  model, we also introduce user review information as a weak
		  supervision signal to collect more comprehensive user
		  preference. In our final framework, the visual and textual
		  features are seamlessly coupled by a multimodal attention
		  network. Based on this architecture, we can not only
		  provide accurate recommendation, but also can accompany
		  each recommended item with novel visual explanations. We
		  conduct extensive experiments to demonstrate the
		  superiority of our proposed model in terms of Top-N
		  recommendation, and also we build a collectively labeled
		  dataset for evaluating our provided visual explanations in
		  a quantitative manner.},
  booktitle	= {Proceedings of the 42nd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {765–774},
  numpages	= {10},
  keywords	= {collaborative filtering, fashion recommendation,
		  recommender system},
  location	= {Paris, France},
  series	= {SIGIR'19}
}

@InProceedings{	  10.1145/3290688.3290710,
  author	= {Huang, Haojie and Wong, Raymond},
  title		= {Web Service based Intelligent Search on Legal Documents},
  year		= {2019},
  isbn		= {9781450366038},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3290688.3290710},
  doi		= {10.1145/3290688.3290710},
  abstract	= {Web services such as RESTful APIs provide high flexibility
		  for knowledge base systems in different domains. To apply
		  it to the legal aspect, we can obtain much data of the case
		  law efficiently enabling us to relate one case to another
		  easily and even to compare the details of a plenty of the
		  case laws simultaneously. Having said that, to ensure the
		  performance of the web services and the accuracy of the
		  data sourcing from them is onerous in the consideration of
		  the backend system for the web services and relevant search
		  engine.In this paper, we introduce a web service for the
		  legal knowledge, LegalKB, enhancing a concept search, and
		  implement a method, WMD ranking method, to speed up the
		  data search through the enhanced system queries.On top of
		  this, we propose a method to automate the optimisation of
		  the system parameters to ensure that the system queries run
		  in the most optimal manner - integration of multiple
		  machine learning methods into our web service to facilitate
		  third-party applications to interface with our web service
		  enlarging the knowledge base to a great extent.},
  booktitle	= {Proceedings of the Australasian Computer Science Week
		  Multiconference},
  articleno	= {50},
  numpages	= {9},
  keywords	= {Information extraction, Knowledge base, Legal domain
		  knowledge base, Web service},
  location	= {Sydney, NSW, Australia},
  series	= {ACSW '19}
}

@InBook{	  10.1145/2915031.2915042,
  title		= {Web Search},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915042},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InBook{	  10.1145/2915031.2915048,
  title		= {Text Summarization},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915048},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3041048.3041054,
  author	= {Turner, Ronald C.},
  title		= {Proposed Model for Natural Language ABAC Authoring},
  year		= {2017},
  isbn		= {9781450349109},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3041048.3041054},
  doi		= {10.1145/3041048.3041054},
  abstract	= {Authorization policy authoring has required tools from the
		  start. With access policy governance now an executive-level
		  responsibility, it is imperative that such a tool expose
		  the policy to business users' with little or no IT
		  intervention-as natural language. NIST SP 800-162 [1] first
		  prescribes natural language policies (NLPs) as the
		  preferred expression of policy and then implicitly calls
		  for automated translation of NLP to machine-executable
		  code. This paper therefore proposes an interoperable model
		  for the NLP's human expression. It furthermore documents
		  the research and development of a tool set for end-to-end
		  authoring and translation. This R&amp;D journey-focusing
		  constantly on end users' has debunked certain myths, has
		  responded to steadily increasing market sophistication, has
		  applied formal disciplines (e.g. ontologies, grammars and
		  compiler design) and has motivated an informal
		  demonstration of autonomic code generation. The lessons
		  learned should be of practical value to the entire ABAC
		  community. The research in progress' increasingly complex
		  policies, proactive rule analytics, and expanded NLP
		  authoring language support will require collaboration with
		  an ever-expanding technical community from industry and
		  academia.},
  booktitle	= {Proceedings of the 2nd ACM Workshop on Attribute-Based
		  Access Control},
  pages		= {61–72},
  numpages	= {12},
  keywords	= {ABAC, RDF analytics, SPARQL queries, XACML authoring tool,
		  business rules, natural language policies, policy
		  semantics},
  location	= {Scottsdale, Arizona, USA},
  series	= {ABAC '17}
}

@InBook{	  10.1145/2915031.2915039,
  title		= {Feedback},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915039},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InBook{	  10.1145/2915031.2915044,
  title		= {Overview of Text Data Analysis},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915044},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3340531.3411937,
  author	= {Xu, Feifei and Wang, Xinpeng and Ma, Yunpu and Tresp,
		  Volker and Wang, Yuyi and Zhou, Shanlin and Du, Haizhou},
  title		= {Controllable Multi-Character Psychology-Oriented Story
		  Generation},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3411937},
  doi		= {10.1145/3340531.3411937},
  abstract	= {Story generation, which aims to generate a long and
		  coherent story automatically based on the title or an input
		  sentence, is an important research area in the field of
		  natural language generation. There is relatively little
		  work on story generation with appointed emotions. Most
		  existing works focus on using only one specific emotion to
		  control the generation of a whole story and ignore the
		  emotional changes in the characters in the course of the
		  story. In our work, we aim to design an emotional line for
		  each character that considers multiple emotions common in
		  psychological theories, with the goal of generating stories
		  with richer emotional changes in the characters. To the
		  best of our knowledge, this work is first to focuses on
		  characters' emotional lines in story generation. We present
		  a novel model-based attention mechanism that we call SoCP
		  (Storytelling of multi-Character Psychology). We show that
		  the proposed model can generate stories considering the
		  changes in the psychological state of different characters.
		  To take into account the particularity of the model, in
		  addition to commonly used evaluation indicators(BLEU,
		  ROUGE, etc.), we introduce the accuracy rate of
		  psychological state control as a novel evaluation metric.
		  The new indicator reflects the effect of the model on the
		  psychological state control of story characters.
		  Experiments show that with SoCP, the generated stories
		  follow the psychological state for each character according
		  to both automatic and human evaluations.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1675–1684},
  numpages	= {10},
  keywords	= {attention, character, psychology, story generation},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@InBook{	  10.1145/2915031.2915036,
  title		= {MeTA : A Unified Toolkit for Text Data Management and
		  Analysis},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915036},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3357384.3358057,
  author	= {Xiao, Teng and Ren, Jiaxin and Meng, Zaiqiao and Sun, Huan
		  and Liang, Shangsong},
  title		= {Dynamic Bayesian Metric Learning for Personalized Product
		  Search},
  year		= {2019},
  isbn		= {9781450369763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357384.3358057},
  doi		= {10.1145/3357384.3358057},
  abstract	= {In this paper, we study the problem of personalized
		  product search under streaming scenarios. We address the
		  problem by proposing a Dynamic Bayesian Metric Learning
		  model, abbreviated as DBML, which can collaboratively track
		  the evolutions of latent semantic representations of
		  different categories of entities (i.e., users, products and
		  words) over time in a joint metric space. In particular,
		  unlike previous work using inner-product metric to model
		  the affinities between entities, our DBML is a novel
		  probabilistic metric learning approach that is able to
		  avoid the contradicts, keep the triangle inequality in the
		  latent space, and correctly utilize implicit feedbacks. For
		  inferring dynamic embeddings of the entities, we propose a
		  scalable online inference algorithm, which can jointly
		  learn the latent representations of entities and smooth
		  their changes across time, based on amortized inference.
		  The inferred dynamic semantic representations of entities
		  collaboratively inferred in a unified form by our DBML can
		  benefit not only for improving personalized product search,
		  but also for capturing the affinities between users,
		  products and words. Experimental results on large datasets
		  over a number of applications demonstrate that our DBML
		  outperforms the state-of-the-art algorithms, and can
		  effectively capture the evolutions of semantic
		  representations of different categories of entities over
		  time.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1693–1702},
  numpages	= {10},
  keywords	= {metric learning, online learning, probabilistic model,
		  product search},
  location	= {Beijing, China},
  series	= {CIKM '19}
}

@InBook{	  10.1145/2915031.2915053,
  title		= {Appendixes},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915053},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3162957.3162984,
  author	= {Zhang, Chao and Song, Hui and Liu, Zhenyu},
  title		= {MiSAS: a multi-domain feature-level sentiment analysis
		  system on micro-blog},
  year		= {2017},
  isbn		= {9781450353656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3162957.3162984},
  doi		= {10.1145/3162957.3162984},
  abstract	= {Big data from micro-blog has been an important access to
		  social groups' psychology, market feedback and so on.
		  Unlike the review corpus which is usually related to the
		  specific object (e.g. a product), the micro-blog content
		  covers the opinion of many domains. It is less useful to
		  extract the fine-grained feature-level opinion target
		  without detect the domain. This paper proposed a systematic
		  feature-level sentiment analysis approach on Micro-blog
		  that recognize data related to the interesting topic
		  automatically. Working with the big micro-blog data we
		  figure out valuable text features to train the opinion
		  targets extraction and sentimental polarity detection
		  models that achieve better multi-domain adaption. We
		  implement the MiSAS system, which crawls micro-blog raw
		  data, outputs opinion targets and orientation summarization
		  on the giving domains, offering valuable analytical tool
		  for practical applications.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Communication and Information Processing},
  pages		= {14–18},
  numpages	= {5},
  keywords	= {MiSAS, micro-blog, opinion target extraction, sentiment
		  analysis, sentimental polarity detection},
  location	= {Tokyo, Japan},
  series	= {ICCIP '17}
}

@InProceedings{	  10.1145/3366423.3380193,
  author	= {Rosset, Corbin and Xiong, Chenyan and Song, Xia and
		  Campos, Daniel and Craswell, Nick and Tiwary, Saurabh and
		  Bennett, Paul},
  title		= {Leading Conversational Search by Suggesting Useful
		  Questions},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380193},
  doi		= {10.1145/3366423.3380193},
  abstract	= {This paper studies a new scenario in conversational
		  search, conversational question suggestion, which leads
		  search engine users to more engaging experiences by
		  suggesting interesting, informative, and useful follow-up
		  questions. We first establish a novel evaluation metric,
		  usefulness, which goes beyond relevance and measures
		  whether the suggestions provide valuable information for
		  the next step of a user’s journey, and construct a public
		  benchmark for useful question suggestion. Then we develop
		  two suggestion systems, a BERT based ranker and a GPT-2
		  based generator, both trained with novel weak supervision
		  signals that convey past users’ search behaviors in
		  search sessions. The weak supervision signals help ground
		  the suggestions to users’ information-seeking
		  trajectories: we identify more coherent and informative
		  sessions using encodings, and then weakly supervise our
		  models to imitate how users transition to the next state of
		  search. Our offline experiments demonstrate the crucial
		  role our “next-turn” inductive training plays in
		  improving usefulness over a strong online system. Our
		  online A/B test in Bing shows that our more useful question
		  suggestions receive 8% more user clicks than the previous
		  system.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {1160–1170},
  numpages	= {11},
  keywords	= {Conversational Search, Question Suggestion, Usefulness},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@InBook{	  10.1145/2915031.2915040,
  title		= {Search Engine Implementation},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915040},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InBook{	  10.1145/2915031.2915047,
  title		= {Text Categorization},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915047},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InBook{	  10.1145/2915031.2915043,
  title		= {Recommender Systems},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915043},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InBook{	  10.1145/2915031.2915034,
  title		= {Background},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915034},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3018661.3018687,
  author	= {Wang, Pengwei and Zhang, Yong and Ji, Lei and Yan, Jun and
		  Jin, Lianwen},
  title		= {Concept Embedded Convolutional Semantic Model for Question
		  Retrieval},
  year		= {2017},
  isbn		= {9781450346757},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3018661.3018687},
  doi		= {10.1145/3018661.3018687},
  abstract	= {The question retrieval, which aims to find similar
		  questions of a given question, is playing pivotal role in
		  various question answering (QA) systems. This task is quite
		  challenging mainly on three aspects: lexical gap, polysemy
		  and word order. In this paper, we propose a unified
		  framework to simultaneously handle these three problems. We
		  use word combined with corresponding concept information to
		  handle the polysemous problem. The concept embedding and
		  word embedding are learned at the same time from both
		  context-dependent and context-independent view. The lexical
		  gap problem is handled since the semantic information has
		  been encoded into the embedding. Then, we propose to use a
		  high-level feature embedded convolutional semantic model to
		  learn the question embedding by inputting the concept
		  embedding and word embedding without manually labeling
		  training data. The proposed framework nicely represent the
		  hierarchical structures of word information and concept
		  information in sentences with their layer-by-layer
		  composition and pooling. Finally, the framework is trained
		  in a weakly-supervised manner on question answer pairs,
		  which can be directly obtained without manually labeling.
		  Experiments on two real question answering datasets show
		  that the proposed framework can significantly outperform
		  the state-of-the-art solutions.},
  booktitle	= {Proceedings of the Tenth ACM International Conference on
		  Web Search and Data Mining},
  pages		= {395–403},
  numpages	= {9},
  keywords	= {concept embedding, question embedding, question
		  retrieval},
  location	= {Cambridge, United Kingdom},
  series	= {WSDM '17}
}

@InBook{	  10.1145/2915031.2915046,
  title		= {Text Clustering},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915046},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@Proceedings{	  10.1145/3308560,
  title		= {WWW '19: Companion Proceedings of The 2019 World Wide Web
		  Conference},
  year		= {2019},
  isbn		= {9781450366755},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to &lt;I&gt;The
		  Web Conference 2019&lt;/I&gt;. The Web Conference is the
		  premier venue focused on understanding the current state
		  and the evolution of the Web through the lens of computer
		  science, computational social science, economics, policy,
		  and many other disciplines. The 2019 edition of the
		  conference is a reflection point as we celebrate the 30th
		  anniversary of the Web.},
  location	= {San Francisco, USA}
}

@Article{	  10.1613/jair.1.11640,
  author	= {Ruder, Sebastian and Vuli\'{c}, Ivan and S\o{}gaard,
		  Anders},
  title		= {A survey of cross-lingual word embedding models},
  year		= {2019},
  issue_date	= {May 2019},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {65},
  number	= {1},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.11640},
  doi		= {10.1613/jair.1.11640},
  abstract	= {Cross-lingual representations of words enable us to reason
		  about word meaning in multilingual contexts and are a key
		  facilitator of cross-lingual transfer when developing
		  natural language processing models for low-resource
		  languages. In this survey, we provide a comprehensive
		  typology of cross-lingual word embedding models. We compare
		  their data requirements and objective functions. The
		  recurring theme of the survey is that many of the models
		  presented in the literature optimize for the same
		  objectives, and that seemingly different models are often
		  equivalent, modulo optimization strategies,
		  hyper-parameters, and such. We also discuss the different
		  ways cross-lingual word embeddings are evaluated, as well
		  as future challenges and research horizons.},
  journal	= {J. Artif. Int. Res.},
  month		= may,
  pages		= {569–630},
  numpages	= {62}
}

@Article{	  10.1145/3106745,
  author	= {Goodwin, Travis R. and Harabagiu, Sanda M.},
  title		= {Knowledge Representations and Inference Techniques for
		  Medical Question Answering},
  year		= {2017},
  issue_date	= {March 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3106745},
  doi		= {10.1145/3106745},
  abstract	= {Answering medical questions related to complex medical
		  cases, as required in modern Clinical Decision Support
		  (CDS) systems, imposes (1) access to vast medical knowledge
		  and (2) sophisticated inference techniques. In this
		  article, we examine the representation and role of
		  combining medical knowledge automatically derived from (a)
		  clinical practice and (b) research findings for inferring
		  answers to medical questions. Knowledge from medical
		  practice was distilled from a vast Electronic Medical
		  Record (EMR) system, while research knowledge was processed
		  from biomedical articles available in PubMed Central. The
		  knowledge automatically acquired from the EMR system took
		  into account the clinical picture and therapy recognized
		  from each medical record to generate a probabilistic Markov
		  network denoted as a Clinical Picture and Therapy Graph
		  (CPTG). Moreover, we represented the background of medical
		  questions available from the description of each complex
		  medical case as a medical knowledge sketch. We considered
		  three possible representations of medical knowledge
		  sketches that were used by four different probabilistic
		  inference methods to pinpoint the answers from the CPTG. In
		  addition, several answer-informed relevance models were
		  developed to provide a ranked list of biomedical articles
		  containing the answers. Evaluations on the TREC-CDS data
		  show which of the medical knowledge representations and
		  inference methods perform optimally. The experiments
		  indicate an improvement of biomedical article ranking by
		  49% over state-of-the-art results.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  articleno	= {14},
  numpages	= {26},
  keywords	= {Clinical decision support, medical information retrieval,
		  medical knowledge representation, medical question
		  answering, probabilistic inference}
}

@InBook{	  10.1145/2915031.2915050,
  title		= {Opinion Mining and Sentiment Analysis},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915050},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@Article{	  10.1145/3299986.3299990,
  author	= {Karmaker Santu, Shubhra Kanti and Geigle, Chase and
		  Ferguson, Duncan and Cope, William and Kalantzis, Mary and
		  Searsmith, Duane and Zhai, Chengxiang},
  title		= {SOFSAT: Towards a Setlike Operator based Framework for
		  Semantic Analysis of Text},
  year		= {2018},
  issue_date	= {December 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {2},
  issn		= {1931-0145},
  url		= {https://doi.org/10.1145/3299986.3299990},
  doi		= {10.1145/3299986.3299990},
  abstract	= {As data reported by humans about our world, text data play
		  a very important role in all data mining applications, yet
		  how to develop a general text analysis system to sup- port
		  all text mining applications is a difficult challenge. In
		  this position paper, we introduce SOFSAT, a new frame- work
		  that can support set-like operators for semantic analy- sis
		  of natural text data with variable text representations. It
		  includes three basic set-like operators|TextIntersect, Tex-
		  tUnion, and TextDi erence|that are analogous to the cor-
		  responding set operators intersection, union, and di
		  erence, respectively, which can be applied to any
		  representation of text data, and di erent representations
		  can be combined via transformation functions that map text
		  to and from any rep- resentation. Just as the set operators
		  can be exibly com- bined iteratively to construct arbitrary
		  subsets or supersets based on some given sets, we show that
		  the correspond- ing text analysis operators can also be
		  combined exibly to support a wide range of analysis tasks
		  that may require di erent work ows, thus enabling an
		  application developer to program" a text mining application
		  by using SOFSAT as an application programming language for
		  text analysis. We discuss instantiations and implementation
		  strategies of the framework with some speci c examples,
		  present ideas about how the framework can be implemented by
		  exploit- ing/extending existing techniques, and provide a
		  roadmap for future research in this new direction.},
  journal	= {SIGKDD Explor. Newsl.},
  month		= dec,
  pages		= {21–30},
  numpages	= {10},
  keywords	= {Intelligent Text Analysis, Semantic Analysis, Semantic
		  Operator for Text, Text Mining}
}

@InBook{	  10.1145/2915031.2915045,
  title		= {Word Association Mining},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915045},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InBook{	  10.1145/2915031.2915051,
  title		= {Joint Analysis of Text and Structured Data},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915051},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3361242.3361251,
  author	= {Sun, Zhiyu and Peng, Fang and Guan, Junrui and Sun,
		  Yanchun},
  title		= {An approach to helping developers learn open source
		  projects based on machine learning},
  year		= {2019},
  isbn		= {9781450377010},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3361242.3361251},
  doi		= {10.1145/3361242.3361251},
  abstract	= {Developers usually learn excellent coding methods and
		  design patterns by reading the code from well-known
		  open-source projects, and participate in the development of
		  open-source projects to enhance their programming
		  capabilities. When developers have just joined an existing
		  open-source project development, the first thing to do is
		  to read and understand the project code. However, almost no
		  project will maintain design documentations. Developers can
		  only understand code according to user guide (mainly focus
		  on how to use code but not on how to develop code) or brief
		  code comments, which is relatively difficult for new
		  developers. To help developers learn open-source projects
		  more quickly, we propose an approach to helping developers
		  learn open-source projects based on machine learning.
		  First, we build a code structure graph for the project code
		  by static analysis. Second, we implement a project entries
		  recommendation approach based on clustering and machine
		  learning to recommend project entries suitable for
		  developers to read. Third, we implement a learning path
		  recommendation algorithm. The algorithm recommends learning
		  paths based on function nodes in the code structure graph
		  selected by the developers, helps developers understand
		  open-source projects better. In experiments, we select two
		  famous c++ open-source projects, Lua and Memcache, as
		  examples to perform project learning path recommendation.
		  The experimental results show that our approach save a lot
		  of time for developers to learn open-source projects while
		  maintaining the accuracy of the recommendations.},
  booktitle	= {Proceedings of the 11th Asia-Pacific Symposium on
		  Internetware},
  articleno	= {13},
  numpages	= {10},
  keywords	= {Code structure graph, Learning path recommendation,
		  Machine Learning, Software reverse engineering},
  location	= {Fukuoka, Japan},
  series	= {Internetware '19}
}

@InBook{	  10.1145/2915031.2915041,
  title		= {Search Engine Evaluation},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915041},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@Proceedings{	  10.1145/3308558,
  title		= {WWW '19: The World Wide Web Conference},
  year		= {2019},
  isbn		= {9781450366748},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to The Web
		  Conference 2019. The Web Conference is the premier venue
		  focused on understanding the current state and the
		  evolution of the Web through the lens of computer science,
		  computational social science, economics, policy, and many
		  other disciplines. The 2019 edition of the conference is a
		  reflection point as we celebrate the 30th anniversary of
		  the Web.},
  location	= {San Francisco, CA, USA}
}

@InProceedings{	  10.1145/3397271.3401130,
  author	= {Voskarides, Nikos and Li, Dan and Ren, Pengjie and
		  Kanoulas, Evangelos and de Rijke, Maarten},
  title		= {Query Resolution for Conversational Search with Limited
		  Supervision},
  year		= {2020},
  isbn		= {9781450380164},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397271.3401130},
  doi		= {10.1145/3397271.3401130},
  abstract	= {In this work we focus on multi-turn passage retrieval as a
		  crucial component of conversational search. One of the key
		  challenges in multi-turn passage retrieval comes from the
		  fact that the current turn query is often underspecified
		  due to zero anaphora, topic change, or topic return.
		  Context from the conversational history can be used to
		  arrive at a better expression of the current turn query,
		  defined as the task of query resolution. In this paper, we
		  model the query resolution task as a binary term
		  classification problem: for each term appearing in the
		  previous turns of the conversation decide whether to add it
		  to the current turn query or not. We propose QuReTeC (Query
		  Resolution by Term Classification), a neural query
		  resolution model based on bidirectional transformers. We
		  propose a distant supervision method to automatically
		  generate training data by using query-passage relevance
		  labels. Such labels are often readily available in a
		  collection either as human annotations or inferred from
		  user interactions. We show that QuReTeC outperforms
		  state-of-the-art models, and furthermore, that our distant
		  supervision method can be used to substantially reduce the
		  amount of human-curated data required to train QuReTeC. We
		  incorporate QuReTeC in a multi-turn, multi-stage passage
		  retrieval architecture and demonstrate its effectiveness on
		  the TREC CAsT dataset.},
  booktitle	= {Proceedings of the 43rd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {921–930},
  numpages	= {10},
  keywords	= {conversational search, query resolution},
  location	= {Virtual Event, China},
  series	= {SIGIR '20}
}

@Article{	  10.1145/3402179,
  author	= {Uprety, Sagar and Gkoumas, Dimitris and Song, Dawei},
  title		= {A Survey of Quantum Theory Inspired Approaches to
		  Information Retrieval},
  year		= {2020},
  issue_date	= {September 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {5},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3402179},
  doi		= {10.1145/3402179},
  abstract	= {Since 2004, researchers have been using the mathematical
		  framework of quantum theory in information retrieval (IR).
		  Quantum theory offers a generalized probability and logic
		  framework. Such a framework has been shown to be capable of
		  unifying the representation, ranking, and user cognitive
		  aspects of IR, and helpful in developing more dynamic,
		  adaptive, and context-aware IR systems. Although
		  quantum-inspired IR is still a growing area, a wide array
		  of work in different aspects of IR has been done and
		  produced promising results. This article presents a survey
		  of the research done in this area, aiming to show the
		  landscape of the field and draw a road map of future
		  directions.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {98},
  numpages	= {39},
  keywords	= {Information retrieval, quantum theory, quantum-inspired
		  models}
}

@InProceedings{	  10.1145/3340555.3356099,
  author	= {Tan, Chaohong and Ling, Zhenhua},
  title		= {Multi-Classification Model for Spoken Language
		  Understanding},
  year		= {2019},
  isbn		= {9781450368605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340555.3356099},
  doi		= {10.1145/3340555.3356099},
  abstract	= {The spoken language understanding (SLU) is an important
		  part of spoken dialogue system (SDS). In the paper, we
		  focus on how to extract a set of act-slot-value tuples from
		  users’ utterances in the 1st Chinese Audio-Textual Spoken
		  Language Understanding Challenge (CATSLU). This paper
		  adopts the pretrained BERT model to encode users’
		  utterances and builds multiple classifiers to get the
		  required tuples. In our framework, finding acts and values
		  of slots are recognized as classification tasks
		  respectively. Such multi-task training is expected to help
		  the encoder to get better understanding of the utterance.
		  Since the system is built on the transcriptions given by
		  automatic speech recognition (ASR), some tricks are applied
		  to correct the errors of the tuples. We also found that
		  using the minimum edit distance (MED) between results and
		  candidates to rebuild the tuples was beneficial in our
		  experiments.},
  booktitle	= {2019 International Conference on Multimodal Interaction},
  pages		= {526–530},
  numpages	= {5},
  keywords	= {BERT, Spoken language understanding, classification,
		  multi-task learning, text tagging},
  location	= {Suzhou, China},
  series	= {ICMI '19}
}

@Article{	  10.1145/3274784.3274788,
  author	= {Culpepper, J. Shane and Diaz, Fernando and Smucker, Mark
		  D.},
  title		= {Research Frontiers in Information Retrieval: Report from
		  the Third Strategic Workshop on Information Retrieval in
		  Lorne (SWIRL 2018)},
  year		= {2018},
  issue_date	= {June 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {1},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3274784.3274788},
  doi		= {10.1145/3274784.3274788},
  abstract	= {The purpose of the Strategic Workshop in Information
		  Retrieval in Lorne is to explore the long-range issues of
		  the Information Retrieval field, to recognize challenges
		  that are on - or even over - the horizon, to build
		  consensus on some of the key challenges, and to disseminate
		  the resulting information to the research community. The
		  intent is that this description of open problems will help
		  to inspire researchers and graduate students to address the
		  questions, and will provide funding agencies data to focus
		  and coordinate support for information retrieval
		  research.},
  journal	= {SIGIR Forum},
  month		= aug,
  pages		= {34–90},
  numpages	= {57}
}

@InBook{	  10.1145/2915031.2915038,
  title		= {Retrieval Models},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915038},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1145/3325291.3325357,
  author	= {Alibadi, Zaid and Du, Mingzhe and Vidal, Jose M.},
  title		= {Using Pre-trained Embeddings to Detect the Intent of an
		  Email},
  year		= {2019},
  isbn		= {9781450371735},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3325291.3325357},
  doi		= {10.1145/3325291.3325357},
  abstract	= {This research studies the problem of email overload and
		  proposes a system that automatically detects whether the
		  email is "to read" or "to do". The goal of our research is
		  to test if we could automate both the features extraction
		  and sentence classification phases by using word embedding.
		  We propose to achieve this goal using a simple
		  feature-based adaptation approach, where email's sentences
		  are represented as dense numeric vectors of reduced
		  dimensionality using either word embeddings or sentence
		  encodings. Given that several types of word embeddings and
		  sentence encodings exist, we compare email's sentence
		  representations corresponding to different word embeddings
		  and sentence encodings with the goal of understanding what
		  embeddings/encodings are more suitable for use in the task
		  of detecting the intent of an email. Our experimental
		  results using three different types of embeddings:
		  context-free word embeddings (word2vec and GloVe),
		  contextual word embeddings (ELMo and BERT), and sentence
		  embeddings (DAN-based Universal Sentence Encoder and
		  Transformer-based Universal Sentence Encoder) suggest that
		  the email's sentences representations based on ELMo
		  embeddings produce better results than the representations
		  that use other embeddings. We achieved an accuracy of
		  90.10%, comparing with word2vec (82.02%), BERT (58.08%),
		  DAN-based USE (86.66%), and Transformer-based USE
		  (88.16%).},
  booktitle	= {Proceedings of the 7th ACIS International Conference on
		  Applied Computing and Information Technology},
  articleno	= {2},
  numpages	= {7},
  keywords	= {email intent, email overload, email speech acts,
		  embeddings, transfer learning},
  location	= {Honolulu, HI, USA},
  series	= {ACIT '19}
}

@InProceedings{	  10.1145/3269206.3271732,
  author	= {Gaur, Manas and Kursuncu, Ugur and Alambo, Amanuel and
		  Sheth, Amit and Daniulaityte, Raminta and Thirunarayan,
		  Krishnaprasad and Pathak, Jyotishman},
  title		= {"Let Me Tell You About Your Mental Health!":
		  Contextualized Classification of Reddit Posts to DSM-5 for
		  Web-based Intervention},
  year		= {2018},
  isbn		= {9781450360142},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3269206.3271732},
  doi		= {10.1145/3269206.3271732},
  abstract	= {Social media platforms are increasingly being used to
		  share and seek advice on mental health issues. In
		  particular, Reddit users freely discuss such issues on
		  various subreddits, whose structure and content can be
		  leveraged to formally interpret and relate subreddits and
		  their posts in terms of mental health diagnostic
		  categories. There is prior research on the extraction of
		  mental health-related information, including symptoms,
		  diagnosis, and treatments from social media; however, our
		  approach can additionally provide actionable information to
		  clinicians about the mental health of a patient in
		  diagnostic terms for web-based intervention. Specifically,
		  we provide a detailed analysis of the nature of subreddit
		  content from domain expert's perspective and introduce a
		  novel approach to map each subreddit to the best matching
		  DSM-5 (Diagnostic and Statistical Manual of Mental
		  Disorders - 5th Edition) category using multi-class
		  classifier. Our classification algorithm analyzes all the
		  posts of a subreddit by adapting topic modeling and
		  word-embedding techniques, and utilizing curated medical
		  knowledge bases to quantify relationship to DSM-5
		  categories. Our semantic encoding-decoding optimization
		  approach reduces the false-alarm-rate from 30% to 2.5% over
		  a comparable heuristic baseline, and our mapping results
		  have been verified by domain experts achieving a kappa
		  score of 0.84.},
  booktitle	= {Proceedings of the 27th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {753–762},
  numpages	= {10},
  keywords	= {drug abuse ontology, dsm-5, medical knowledge bases,
		  mental health, reddit, semantic encoding and decoding,
		  semantic social computing},
  location	= {Torino, Italy},
  series	= {CIKM '18}
}

@InProceedings{	  10.1145/3394486.3403357,
  author	= {Han, Fred X. and Niu, Di and Chen, Haolan and Guo, Weidong
		  and Yan, Shengli and Long, Bowei},
  title		= {Meta-Learning for Query Conceptualization at Web Scale},
  year		= {2020},
  isbn		= {9781450379984},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3394486.3403357},
  doi		= {10.1145/3394486.3403357},
  abstract	= {Concepts naturally constitute an abstraction for
		  fine-grained entities and knowledge in the open domain.
		  They enable search engines and recommendation systems to
		  enhance user experience by discovering high-level
		  abstraction of a search query and the user intent behind
		  it. In this paper, we study the problem of query
		  conceptualization, which is to find the most appropriate
		  matching concepts for any given search query from a large
		  pool of pre-defined concepts. We propose a coarse-to-fine
		  approach to first reduce the search space for each query
		  through a shortlisting scheme and then identify the
		  matching concepts using pre-trained language models, which
		  are meta-tuned to our query-concept matching task. Our
		  shortlisting scheme involves using a GRU-based Relevant
		  Words Generator (RWG) to first expand and complete the
		  context of the given query and then shortlisting the
		  candidate concepts through a scoring mechanism based on
		  word overlaps. To accurately identify the most appropriate
		  matching concepts for a query, even when the concepts may
		  have zero verbatim overlaps with the query, we
		  meta-fine-tune a BERT pairwise text-matching model under
		  the Reptile meta-learning algorithm, which achieves
		  zero-shot transfer learning on the conceptualization
		  problem. Our two-stage framework can be trained with data
		  completely derived from a search click graph, without
		  requiring any human labelling efforts. For evaluation, we
		  have constructed a large click graph based on more than $7$
		  million instances of the click history recorded in Tencent
		  QQ browser and performed the query conceptualization task
		  based on a large ontology with $159,148$ unique concepts.
		  Results from a range of evaluation methods, including an
		  offline evaluation procedure on the click graph, human
		  evaluation, online A/B testing and case studies, have
		  demonstrated the superiority of our approach over a number
		  of competitive pre-trained language models and fine-tuned
		  neural network baselines.},
  booktitle	= {Proceedings of the 26th ACM SIGKDD International
		  Conference on Knowledge Discovery &amp; Data Mining},
  pages		= {3064–3073},
  numpages	= {10},
  keywords	= {conceptualization, information retrieval, meta-learning,
		  query analysis},
  location	= {Virtual Event, CA, USA},
  series	= {KDD '20}
}

@Article{	  10.1145/3410569,
  author	= {Laatar, Rim and Aloulou, Chafik and Belguith, Lamia
		  Hadrich},
  title		= {Disambiguating Arabic Words According to Their Historical
		  Appearance in the Document Based on Recurrent Neural
		  Networks},
  year		= {2020},
  issue_date	= {November 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3410569},
  doi		= {10.1145/3410569},
  abstract	= {How can we determine the semantic meaning of a word in
		  relation to its context of appearance? We eventually have
		  to grabble with this difficult question, as one of the
		  paramount problems of Natural Language Processing (NLP). In
		  other words, this issue is commonly defined as Word Sense
		  Disambiguation (WSD). The latter is one of the crucial
		  difficulties within the NLP field. In this respect, word
		  vectors extracted from a neural network model have been
		  successfully applied for resolving the WSD problem.
		  Accordingly, this article presents an unprecedented method
		  to disambiguate Arabic words according to both their
		  contextual appearance in a source text and the era in which
		  they emerged. In fact, in the few previous decades, many
		  researchers have been grabbling with Arabic Word Sense
		  Disambiguation.It should be noted that the Arabic language
		  can be divided into three major historical periods: old
		  Arabic, middle-age Arabic, and contemporary Arabic.
		  Actually, contemporary Arabic has proved to be the greatest
		  concern of many researchers. The main gist of our work is
		  to disambiguate Arabic words according to the historical
		  period in which they appeared. To perform such a task, we
		  suggest a method that deploys contextualized word
		  embeddings to better gather valid syntactic and semantic
		  information of the same word by taking into account its
		  contextual uses. The preponderant thing is to convert both
		  the senses and the contextual uses of an ambiguous item to
		  vectors, then determine which of the possible conceptual
		  meanings of the target word is closer to the given
		  context.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  articleno	= {86},
  numpages	= {16},
  keywords	= {Natural language processing, contemporary arabic,
		  contextualized word embeddings, historical dictionary,
		  middle-age arabic, old arabic, recurrent neural networks,
		  word sense disambiguation}
}

@InProceedings{	  10.1145/3038912.3052642,
  author	= {Kejriwal, Mayank and Szekely, Pedro},
  title		= {Information Extraction in Illicit Web Domains},
  year		= {2017},
  isbn		= {9781450349130},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3038912.3052642},
  doi		= {10.1145/3038912.3052642},
  abstract	= {Extracting useful entities and attribute values from
		  illicit domains such as human trafficking is a challenging
		  problem with the potential for widespread social impact.
		  Such domains employ atypical language models, have 'long
		  tails' and suffer from the problem of concept drift. In
		  this paper, we propose a lightweight, feature-agnostic
		  Information Extraction (IE) paradigm specifically designed
		  for such domains. Our approach uses raw, unlabeled text
		  from an initial corpus, and a few (12-120) seed annotations
		  per domain-specific attribute, to learn robust IE models
		  for unobserved pages and websites. Empirically, we
		  demonstrate that our approach can outperform
		  feature-centric Conditional Random Field baselines by over
		  18% F-Measure on five annotated sets of real-world human
		  trafficking datasets in both low-supervision and
		  high-supervision settings. We also show that our approach
		  is demonstrably robust to concept drift, and can be
		  efficiently bootstrapped even in a serial computing
		  environment.},
  booktitle	= {Proceedings of the 26th International Conference on World
		  Wide Web},
  pages		= {997–1006},
  numpages	= {10},
  keywords	= {distributional semantics, feature-agnostic, illicit
		  domains, information extraction, named entity recognition},
  location	= {Perth, Australia},
  series	= {WWW '17}
}

@InProceedings{	  10.1145/3438872.3439101,
  author	= {Yang, JinXiong and Bai, Liang and Guo, Yanming},
  title		= {A survey of text classification models},
  year		= {2020},
  isbn		= {9781450388306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3438872.3439101},
  doi		= {10.1145/3438872.3439101},
  abstract	= {With the rapid development of artificial intelligence,
		  text classification method based on deep learning model has
		  surpassed traditional machine learning method in various
		  aspects. This paper introduces dozens of deep learning
		  models for text classification according to the different
		  network structures of the models. In addition, this paper
		  briefly introduces the evaluation indicators and
		  application scenarios of text classification, summarizes
		  and forecasts the current challenges and future development
		  trend of text classification.},
  booktitle	= {Proceedings of the 2020 2nd International Conference on
		  Robotics, Intelligent Control and Artificial Intelligence},
  pages		= {327–334},
  numpages	= {8},
  keywords	= {Deep learning, Model, Text classification},
  location	= {Shanghai, China},
  series	= {RICAI '20}
}

@InProceedings{	  10.1145/3314221.3314594,
  author	= {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad
		  and Socher, Richard and Lam, Monica S.},
  title		= {Genie: a generator of natural language semantic parsers
		  for virtual assistant commands},
  year		= {2019},
  isbn		= {9781450367127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3314221.3314594},
  doi		= {10.1145/3314221.3314594},
  abstract	= {To understand diverse natural language commands, virtual
		  assistants today are trained with numerous labor-intensive,
		  manually annotated sentences. This paper presents a
		  methodology and the Genie toolkit that can handle new
		  compound commands with significantly less manual effort. We
		  advocate formalizing the capability of virtual assistants
		  with a Virtual Assistant Programming Language (VAPL) and
		  using a neural semantic parser to translate natural
		  language into VAPL code. Genie needs only a small realistic
		  set of input sentences for validating the neural model.
		  Developers write templates to synthesize data; Genie uses
		  crowdsourced paraphrases and data augmentation, along with
		  the synthesized data, to train a semantic parser. We also
		  propose design principles that make VAPL languages amenable
		  to natural language translation. We apply these principles
		  to revise ThingTalk, the language used by the Almond
		  virtual assistant. We use Genie to build the first semantic
		  parser that can support compound virtual assistants
		  commands with unquoted free-form parameters. Genie achieves
		  a 62% accuracy on realistic user inputs. We demonstrate
		  Genie’s generality by showing a 19% and 31% improvement
		  over the previous state of the art on a music skill,
		  aggregate functions, and access control.},
  booktitle	= {Proceedings of the 40th ACM SIGPLAN Conference on
		  Programming Language Design and Implementation},
  pages		= {394–410},
  numpages	= {17},
  keywords	= {data augmentation, data engineering, semantic parsing,
		  training data generation, virtual assistants},
  location	= {Phoenix, AZ, USA},
  series	= {PLDI 2019}
}

@Article{	  10.1145/3201407,
  author	= {Li, Peipei and Wang, Haixun and Li, Hongsong and Wu,
		  Xindong},
  title		= {Employing Semantic Context for Sparse Information
		  Extraction Assessment},
  year		= {2018},
  issue_date	= {October 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {12},
  number	= {5},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3201407},
  doi		= {10.1145/3201407},
  abstract	= {A huge amount of texts available on the World Wide Web
		  presents an unprecedented opportunity for information
		  extraction (IE). One important assumption in IE is that
		  frequent extractions are more likely to be correct. Sparse
		  IE is hence a challenging task because no matter how big a
		  corpus is, there are extractions supported by only a small
		  amount of evidence in the corpus. However, there is limited
		  research on sparse IE, especially in the assessment of the
		  validity of sparse IEs. Motivated by this, we introduce a
		  lightweight, explicit semantic approach for assessing
		  sparse IE.1 We first use a large semantic network
		  consisting of millions of concepts, entities, and
		  attributes to explicitly model the context of any semantic
		  relationship. Second, we learn from three semantic contexts
		  using different base classifiers to select an optimal
		  classification model for assessing sparse extractions.
		  Finally, experiments show that as compared with several
		  state-of-the-art approaches, our approach can significantly
		  improve the F-score in the assessment of sparse extractions
		  while maintaining the efficiency.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= jun,
  articleno	= {54},
  numpages	= {36},
  keywords	= {Sparse information extraction, classification, isA
		  relationship, semantic network}
}

@InProceedings{	  10.1145/3264996.3265002,
  author	= {Anbarasan and Lee, Jeannie S.A.},
  title		= {Speech and Gestures for Smart-Home Control and Interaction
		  for Older Adults},
  year		= {2018},
  isbn		= {9781450359825},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3264996.3265002},
  doi		= {10.1145/3264996.3265002},
  abstract	= {Older adults have been encountering difficulties in using
		  modern technological devices to control home appliances as
		  they are lacking in technology literacy and mobility. This
		  led to the usage of remote controllers or requiring
		  assistance from family members, which is not beneficial for
		  older adults since there is less independence. To alleviate
		  this problem, this project aims to develop a prototype
		  system named "Genie" which caters for older adults ranging
		  from 65 to 80 years old, allowing for easy control of smart
		  home appliances through combination of speech and gesture
		  interactions. An experiment was carried out with a total of
		  20 older adults on the prototype system where the initial
		  results demonstrate a significant increase in usability.
		  Based on the evaluation, such interaction methods show
		  promise to be effective in replacing manual operations of
		  home appliances through the use of simple speech or gesture
		  commands.},
  booktitle	= {Proceedings of the 3rd International Workshop on
		  Multimedia for Personal Health and Health Care},
  pages		= {49–57},
  numpages	= {9},
  keywords	= {gestures, human interactions, older adults, smart-home,
		  speech},
  location	= {Seoul, Republic of Korea},
  series	= {HealthMedia'18}
}

@InBook{	  10.1145/2915031.2915049,
  title		= {Topic Analysis},
  year		= {2018},
  isbn		= {9781970001174},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/2915031.2915049},
  abstract	= {Recent years have seen a dramatic growth of natural
		  language text data, including web pages, news articles,
		  scientific literature, emails, enterprise documents, and
		  social media (such as blog articles, forum posts, product
		  reviews, and tweets). This has led to an increasing demand
		  for powerful software tools to help people manage and
		  analyze vast amounts of text data effectively and
		  efficiently. Unlike data generated by a computer system or
		  sensors, text data are usually generated directly by
		  humans, and capture semantically rich content. As such,
		  text data are especially valuable for discovering knowledge
		  about human opinions and preferences, in addition to many
		  other kinds of knowledge that we encode in text. In
		  contrast to structured data, which conform to well-defined
		  schemas (thus are relatively easy for computers to handle),
		  text has less explicit structure, requiring computer
		  processing toward understanding of the content encoded in
		  text. The current technology of natural language processing
		  has not yet reached a point to enable a computer to
		  precisely understand natural language text, but a wide
		  range of statistical and heuristic approaches to management
		  and analysis of text data have been developed over the past
		  few decades. They are usually very robust and can be
		  applied to analyze and manage text data in any natural
		  language, and about any topic.This book provides a
		  systematic introduction to many of these approaches, with
		  an emphasis on covering the most useful knowledge and
		  skills required to build a variety of practically useful
		  text information systems. Because humans can understand
		  natural languages far better than computers can, effective
		  involvement of humans in a text information system is
		  generally needed and text information systems often serve
		  as intelligent assistants for humans. Depending on how a
		  text information system collaborates with humans, we
		  distinguish two kinds of text information systems. The
		  first is information retrieval systems which include search
		  engines and recommender systems; they assist users in
		  finding from a large collection of text data the most
		  relevant text data that are actually needed for solving a
		  specific application problem, thus effecively turning big
		  raw text data into much smaller relevant text data that can
		  be more easily processed by humans. The second is text
		  mining application systems; they can assist users in
		  analyzing patterns in text data to extract and discover
		  useful actionable knowledge directly useful for task
		  completion or decision making, thus providing more direct
		  task support for users. This book covers the major
		  concepts, techniques, and ideas in information retrieval
		  and text data mining from a practical viewpoint, and
		  includes many hands-on exercises designed with a companion
		  software toolkit (i.e., MeTA) to help readers learn how to
		  apply techniques of information retrieval and text mining
		  to real-world text data and how to experiment with and
		  improve some of the algorithms for interesting application
		  tasks. This book can be used as a textbook for computer
		  science undergraduates and graduates, library and
		  information scientists, or as a reference book for
		  practitioners working on relevant problems in managing and
		  analyzing text data.},
  booktitle	= {Text Data Management and Analysis: A Practical
		  Introduction to Information Retrieval and Text Mining}
}

@InProceedings{	  10.1109/icse.2019.00057,
  author	= {Abad, Zahra Shakeri Hossein and Gervasi, Vincenzo and
		  Zowghi, Didar and Far, Behrouz H.},
  title		= {Supporting analysts by dynamic extraction and
		  classification of requirements-related knowledge},
  year		= {2019},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ICSE.2019.00057},
  doi		= {10.1109/ICSE.2019.00057},
  abstract	= {In many software development projects, analysts are
		  required to deal with systems' requirements from unfamiliar
		  domains. Familiarity with the domain is necessary in order
		  to get full leverage from interaction with stakeholders and
		  for extracting relevant information from the existing
		  project documents. Accurate and timely extraction and
		  classification of requirements knowledge support analysts
		  in this challenging scenario. Our approach is to mine
		  real-time interaction records and project documents for the
		  relevant phrasal units about the requirements related
		  topics being discussed during elicitation. We propose to
		  use both generative and discriminating methods. To extract
		  the relevant terms, we leverage the flexibility and power
		  of Weighted Finite State Transducers (WFSTs) in dynamic
		  modelling of natural language processing tasks. We used an
		  extended version of Support Vector Machines (SVMs) with
		  variable-sized feature vectors to efficiently and
		  dynamically extract and classify requirements-related
		  knowledge from the existing documents. To evaluate the
		  performance of our approach intuitively and quantitatively,
		  we used edit distance and precision/recall metrics. We show
		  in three case studies that the snippets extracted by our
		  method are intuitively relevant and reasonably accurate.
		  Furthermore, we found that statistical and linguistic
		  parameters such as smoothing methods, and words contiguity
		  and order features can impact the performance of both
		  extraction and classification tasks.},
  booktitle	= {Proceedings of the 41st International Conference on
		  Software Engineering},
  pages		= {442–453},
  numpages	= {12},
  keywords	= {dynamic language models, natural language processing,
		  requirements classification, requirements elicitation,
		  weighted finite state transducers},
  location	= {Montreal, Quebec, Canada},
  series	= {ICSE '19}
}

@InProceedings{	  10.1145/3377325.3377486,
  author	= {Yaghoub-Zadeh-Fard, Mohammad-Ali and Benatallah, Boualem
		  and Casati, Fabio and Barukh, Moshe Chai and Zamanirad,
		  Shayan},
  title		= {Dynamic word recommendation to obtain diverse crowdsourced
		  paraphrases of user utterances},
  year		= {2020},
  isbn		= {9781450371186},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3377325.3377486},
  doi		= {10.1145/3377325.3377486},
  abstract	= {Building task-oriented bots requires mapping a user
		  utterance to an intent with its associated entities to
		  serve the request. Doing so is not easy since it requires
		  large quantities of high-quality and diverse training data
		  to learn how to map all possible variations of utterances
		  with the same intent. Crowdsourcing may be an effective,
		  inexpensive, and scalable technique for collecting such
		  large datasets. However, the diversity of the results
		  suffers from the priming effect (i.e. workers are more
		  likely to use the words in the sentence we are asking to
		  paraphrase). In this paper, we leverage priming as an
		  opportunity rather than a threat: we dynamically generate
		  word suggestions to motivate crowd workers towards
		  producing diverse utterances. The key challenge is to make
		  suggestions that can improve diversity without resulting in
		  semantically invalid paraphrases. To achieve this, we
		  propose a probabilistic model that generates continuously
		  improved versions of word suggestions that balance
		  diversity and semantic relevance. Our experiments show that
		  the proposed approach improves the diversity of
		  crowdsourced paraphrases.},
  booktitle	= {Proceedings of the 25th International Conference on
		  Intelligent User Interfaces},
  pages		= {55–66},
  numpages	= {12},
  keywords	= {bots, crowdsourcing, paraphrasing},
  location	= {Cagliari, Italy},
  series	= {IUI '20}
}

@InProceedings{	  10.1145/3292500.3330710,
  author	= {Weng, Wei-Hung and Chung, Yu-An and Szolovits, Peter},
  title		= {Unsupervised Clinical Language Translation},
  year		= {2019},
  isbn		= {9781450362016},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3292500.3330710},
  doi		= {10.1145/3292500.3330710},
  abstract	= {As patients' access to their doctors' clinical notes
		  becomes common, translating professional, clinical jargon
		  to layperson-understandable language is essential to
		  improve patient-clinician communication. Such translation
		  yields better clinical outcomes by enhancing patients'
		  understanding of their own health conditions, and thus
		  improving patients' involvement in their own care. Existing
		  research has used dictionary-based word replacement or
		  definition insertion to approach the need. However, these
		  methods are limited by expert curation, which is hard to
		  scale and has trouble generalizing to unseen datasets that
		  do not share an overlapping vocabulary. In contrast, we
		  approach the clinical word and sentence translation problem
		  in a completely unsupervised manner. We show that a
		  framework using representation learning, bilingual
		  dictionary induction and statistical machine translation
		  yields the best precision at 10 of 0.827 on
		  professional-to-consumer word translation, and mean opinion
		  scores of 4.10 and 4.28 out of 5 for clinical correctness
		  and layperson readability, respectively, on sentence
		  translation. Our fully-unsupervised strategy overcomes the
		  curation problem, and the clinically meaningful evaluation
		  reduces biases from inappropriate evaluators, which are
		  critical in clinical machine learning.},
  booktitle	= {Proceedings of the 25th ACM SIGKDD International
		  Conference on Knowledge Discovery &amp; Data Mining},
  pages		= {3121–3131},
  numpages	= {11},
  keywords	= {consumer health, machine translation, representation
		  learning, unsupervised learning},
  location	= {Anchorage, AK, USA},
  series	= {KDD '19}
}

@InProceedings{	  10.1145/3352411.3352452,
  author	= {Shuoqiu, Yang and Chaojun, Xu},
  title		= {Research on Constructing Sentiment Dictionary of Online
		  Course Reviews based on Multi-source Combination},
  year		= {2019},
  isbn		= {9781450371414},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3352411.3352452},
  doi		= {10.1145/3352411.3352452},
  abstract	= {The construction of sentiment dictionary is an important
		  task in text sentiment analysis. Using the sentiment words
		  of specific fields to establish a domain-oriented sentiment
		  dictionary can significantly improve the effect on
		  sentiment recognition and classification in the specific
		  field. A method of constructing a sentiment dictionary for
		  online course reviews was proposed, which based on
		  multi-source combination. In the first place, the sentiment
		  words were identified and extracted using TextRank and the
		  word2vec model, which combined with the general sentiment
		  dictionary and the online course reviews corpus. Then, the
		  label propagation algorithm was applied to discriminate the
		  sentiment polarity of sentiment words, thus constructing a
		  sentiment dictionary for online course reviews. Through the
		  accuracy experiment on the determination of the sentiment
		  polarity of words and the experiment on sentiment
		  classification based on different dictionaries, the
		  accuracy rate, the recall rate and the F value are
		  calculated. The experimental results show that the proposed
		  method was an accuracy and effective way to achieve the
		  sentiment classification of online course reviews.},
  booktitle	= {Proceedings of the 2019 2nd International Conference on
		  Data Science and Information Technology},
  pages		= {71–76},
  numpages	= {6},
  keywords	= {Label propagation, Online course reviews, Sentiment
		  dictionary, TextRank, Word vector},
  location	= {Seoul, Republic of Korea},
  series	= {DSIT 2019}
}

@Article{	  10.1109/taslp.2018.2819941,
  author	= {Yu, Kai and Zhao, Zijian and Wu, Xueyang and Lin, Hongtao
		  and Liu, Xuan},
  title		= {Rich Short Text Conversation Using Semantic-Key-Controlled
		  Sequence Generation},
  year		= {2018},
  issue_date	= {August 2018},
  publisher	= {IEEE Press},
  volume	= {26},
  number	= {8},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2018.2819941},
  doi		= {10.1109/TASLP.2018.2819941},
  abstract	= {With the recent advances of the sequence-to-sequence
		  framework, generation approaches for the short text
		  conversation STC become attractive. The traditional
		  sequence-to-sequence approaches for the STC often suffer
		  from poor diversity and general reply without
		  substantiality. It is also hard to control the topic or
		  semantics of the selected reply from multiple generated
		  candidates. In this paper, a novel external-memory-driven
		  sequence-to-sequence learning approach is proposed to
		  address these problems. A tensor of the external memory is
		  constructed to represent interpretable topics or semantics.
		  During generation, a controllable memory trigger is
		  extracted given the input sequence, and a reply is then
		  generated using the memory trigger as well as the
		  sequence-to-sequence model. Experiments show that the
		  proposed approach can generate much richer diversity than
		  the traditional sequence-to-sequence training with
		  attention. Meanwhile, it achieves better quality score in
		  human evaluation. It is also observed that by manually
		  manipulating the memory trigger, it is possible to
		  interpretably guide the topics or semantics of the reply.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {1359–1368},
  numpages	= {10}
}

@InProceedings{	  10.1145/3331184.3331198,
  author	= {Zhang, Hongfei and Song, Xia and Xiong, Chenyan and
		  Rosset, Corby and Bennett, Paul N. and Craswell, Nick and
		  Tiwary, Saurabh},
  title		= {Generic Intent Representation in Web Search},
  year		= {2019},
  isbn		= {9781450361729},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331184.3331198},
  doi		= {10.1145/3331184.3331198},
  abstract	= {This paper presents GEneric iNtent Encoder (GEN Encoder)
		  which learns a distributed representation space for user
		  intent in search. Leveraging large scale user clicks from
		  Bing search logs as weak supervision of user intent, GEN
		  Encoder learns to map queries with shared clicks into
		  similar embeddings end-to-end and then fine-tunes on
		  multiple paraphrase tasks. Experimental results on an
		  intrinsic evaluation task - query intent similarity
		  modeling - demonstrate GEN Encoder's robust and significant
		  advantages over previous representation methods. Ablation
		  studies reveal the crucial role of learning from implicit
		  user feedback in representing user intent and the
		  contributions of multi-task learning in representation
		  generality. We also demonstrate that GEN Encoder alleviates
		  the sparsity of tail search traffic and cuts down half of
		  the unseen queries by using an efficient approximate
		  nearest neighbor search to effectively identify previous
		  queries with the same search intent. Finally, we
		  demonstrate distances between GEN encodings reflect certain
		  information seeking behaviors in search sessions.},
  booktitle	= {Proceedings of the 42nd International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {65–74},
  numpages	= {10},
  keywords	= {generic intent representation, query embedding, user
		  intent},
  location	= {Paris, France},
  series	= {SIGIR'19}
}

@InProceedings{	  10.1145/3038912.3052630,
  author	= {Chen, Long and Jose, Joemon M. and Yu, Haitao and Yuan,
		  Fajie},
  title		= {A Semantic Graph-Based Approach for Mining Common Topics
		  from Multiple Asynchronous Text Streams},
  year		= {2017},
  isbn		= {9781450349130},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3038912.3052630},
  doi		= {10.1145/3038912.3052630},
  abstract	= {In the age of Web 2.0, a substantial amount of
		  unstructured content are distributed through multiple text
		  streams in an asynchronous fashion, which makes it
		  increasingly difficult to glean and distill useful
		  information. An effective way to explore the information in
		  text streams is topic modelling, which can further
		  facilitate other applications such as search, information
		  browsing, and pattern mining. In this paper, we propose a
		  semantic graph based topic modelling approach for
		  structuring asynchronous text streams. Our model integrates
		  topic mining and time synchronization, two core modules for
		  addressing the problem, into a unified model. Specifically,
		  for handling the lexical gap issues, we use global semantic
		  graphs of each timestamp for capturing the hidden
		  interaction among entities from all the text streams. For
		  dealing with the sources asynchronism problem, local
		  semantic graphs are employed to discover similar topics of
		  different entities that can be potentially separated by
		  time gaps. Our experiment on two real-world datasets shows
		  that the proposed model significantly outperforms the
		  existing ones.},
  booktitle	= {Proceedings of the 26th International Conference on World
		  Wide Web},
  pages		= {1201–1209},
  numpages	= {9},
  keywords	= {knowledge repository, language modelling, topic
		  modelling},
  location	= {Perth, Australia},
  series	= {WWW '17}
}

@Article{	  10.1145/3426723,
  author	= {Fang, Hui and Zhang, Danning and Shu, Yiheng and Guo,
		  Guibing},
  title		= {Deep Learning for Sequential Recommendation: Algorithms,
		  Influential Factors, and Evaluations},
  year		= {2020},
  issue_date	= {January 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {39},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3426723},
  doi		= {10.1145/3426723},
  abstract	= {In the field of sequential recommendation, deep
		  learning--(DL) based methods have received a lot of
		  attention in the past few years and surpassed traditional
		  models such as Markov chain-based and factorization-based
		  ones. However, there is little systematic study on DL-based
		  methods, especially regarding how to design an effective DL
		  model for sequential recommendation. In this view, this
		  survey focuses on DL-based sequential recommender systems
		  by taking the aforementioned issues into consideration.
		  Specifically, we illustrate the concept of sequential
		  recommendation, propose a categorization of existing
		  algorithms in terms of three types of behavioral sequences,
		  summarize the key factors affecting the performance of
		  DL-based models, and conduct corresponding evaluations to
		  showcase and demonstrate the effects of these factors. We
		  conclude this survey by systematically outlining future
		  directions and challenges in this field.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= nov,
  articleno	= {10},
  numpages	= {42},
  keywords	= {Sequential recommendation, deep learning, evaluations,
		  influential factors, session-based recommendation, survey}
}

@Article{	  10.1109/tcbb.2018.2801303,
  author	= {Xu, Bo and Lin, Hongfei and Lin, Yuan},
  title		= {Learning to Refine Expansion Terms for Biomedical
		  Information Retrieval Using Semantic Resources},
  year		= {2019},
  issue_date	= {May 2019},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {16},
  number	= {3},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2018.2801303},
  doi		= {10.1109/TCBB.2018.2801303},
  abstract	= {With the rapid development of biomedicine, the number of
		  biomedical articles has increased accordingly, which
		  presents a great challenge for biologists trying to keep up
		  with the latest research. Information retrieval seeks to
		  meet this challenge by searching among a large number of
		  articles based on given queries and providing the most
		  relevant ones to fulfill information needs. As an effective
		  information retrieval technique, query expansion has some
		  room for improvement to achieve the desired performance
		  when directly applied for biomedical information retrieval
		  because there exist many domain-related terms both in
		  users' queries and in related articles. To solve this
		  problem, we propose a biomedical query expansion framework
		  based on learning-to-rank methods, in which we refine
		  candidate expansion terms by training term-ranking models
		  to select the most relevant terms. To train the
		  term-ranking models, we first propose a pseudo-relevance
		  feedback method based on MeSH to select candidate expansion
		  terms and then represent the candidate terms as feature
		  vectors by defining both the corpus-based term features and
		  the resource-based term features. Experimental results
		  obtained for TREC genomics datasets show that our method
		  can capture more relevant terms to expand the original
		  query and effectively improve biomedical information
		  retrieval performance.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= may,
  pages		= {954–966},
  numpages	= {13}
}

@Article{	  10.1145/3378414,
  author	= {Zarnoufi, Randa and Jaafar, Hamid and Abik, Mounia},
  title		= {Machine Normalization: Bringing Social Media Text from
		  Non-Standard to Standard Form},
  year		= {2020},
  issue_date	= {July 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3378414},
  doi		= {10.1145/3378414},
  abstract	= {User-generated text in social media communication (SMC) is
		  mainly characterized by non-standard form. It may contain
		  code switching (CS) text, a widespread phenomenon in SMC,
		  in addition to noisy elements used, especially in written
		  conversations (use of abbreviations, symbols, emoticons) or
		  misspelled words. All of these factors constitute a wall in
		  front of text mining applications. Common text mining tools
		  are dedicated to standard use of standard languages but
		  cannot deal with other forms, especially written text in
		  social media. To overcome these problems, in this work we
		  present our solution for the normalization of non-standard
		  use of standard and non-standard languages (dialects) in
		  SMC text with the use of existent resources and tools. The
		  main processing in our solution consists of CS
		  normalization from multiple to one language by the use of a
		  machine translation--like approach. This processing relies
		  on a linguistic approach of CS, which aims at identifying
		  automatically the translation source and target languages
		  (without human intervention). The remaining processing
		  operations concern the normalization of SMC special
		  expressions and spelling correction of out-of-vocabulary
		  words. To preserve the coded-switched sentence meaning
		  across translation, we adopt a knowledge-based approach for
		  word sense translation disambiguation reinforced with a
		  multi-lingual vertical context. All of these processes are
		  embedded in what we refer to as the machine normalization
		  system. Our solution can be used as a front-end of text
		  mining processing, enabling the analysis of SMC noisy text.
		  The conducted experiments show that our system performs
		  better than considered baselines.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {49},
  numpages	= {30},
  keywords	= {Text normalization, automatic language identification,
		  code switching normalization, dialects, matrix language,
		  multilingual vertical context, social media, standard
		  languages, word sense disambiguation}
}

@InProceedings{	  10.1145/3408066.3408101,
  author	= {Chew, Lit-Jie and Haw, Su-Cheng and Subramaniam, Samini},
  title		= {Recommender System for Retail Domain: An Insight on
		  Techniques and Evaluations},
  year		= {2020},
  isbn		= {9781450377034},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3408066.3408101},
  doi		= {10.1145/3408066.3408101},
  abstract	= {Recommender system has been developed as a useful tool
		  especially when we reached the era of big data and in the
		  meanwhile the internet has been overwhelming with lots of
		  choices. There is a need for people to filter the
		  information to search for their needs and wants
		  efficiently. E-commerce website such as Amazon and Netflix
		  have been using recommender system to build and boost their
		  sales through the personalization recommendation. With the
		  success in the e-commerce area, researchers are keen on
		  finding a method to boost traditional offline retailer
		  sales thru the recommender system. Therefore, in this
		  paper, we introduced the existing recommender system and
		  discuss the method of filtering of each method. Then, we
		  provide the overview of the recent paper in retailer and
		  e-commerce domain to provide the insight and trends such as
		  the filtering techniques and evaluation metric used.
		  Several possible research direction has been discussed
		  based on the current trends and problems.},
  booktitle	= {Proceedings of the 12th International Conference on
		  Computer Modeling and Simulation},
  pages		= {9–13},
  numpages	= {5},
  keywords	= {Collaborative filtering, Content-based filtering, Hybrid
		  filtering, Recommender system},
  location	= {Brisbane, QLD, Australia},
  series	= {ICCMS '20}
}

@Article{	  10.1145/3394592,
  author	= {Ren, Xuhui and Yin, Hongzhi and Chen, Tong and Wang, Hao
		  and Hung, Nguyen Quoc Viet and Huang, Zi and Zhang,
		  Xiangliang},
  title		= {CRSAL: Conversational Recommender Systems with Adversarial
		  Learning},
  year		= {2020},
  issue_date	= {October 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {38},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3394592},
  doi		= {10.1145/3394592},
  abstract	= {Recommender systems have been attracting much attention
		  from both academia and industry because of their ability to
		  capture user interests and generate personalized item
		  recommendations. As the life pace in contemporary society
		  speeds up, traditional recommender systems are inevitably
		  limited by their disconnected interaction styles and low
		  adaptivity to users’ evolving demands. Consequently,
		  conversational recommender systems emerge as a prospective
		  research area, where an intelligent dialogue agent is
		  integrated with a recommender system. Conversational
		  recommender systems possess the ability to accurately
		  understand end-users’ intent or request and generate
		  human-like dialogue responses when performing
		  recommendations. However, existing conversational
		  recommender systems only allow the systems to ask users for
		  more preference information, while users’ further
		  questions and concerns about the recommended items (e.g.,
		  enquiring the location of a recommended restaurant) can
		  hardly be addressed. Though the recent task-oriented
		  dialogue systems allow for two-way communications, they are
		  not easy to train because of their high dependence on human
		  guidance in terms of user intent recognition and system
		  response generation. Hence, to enable two-way human-machine
		  communications and tackle the challenges brought by
		  manually crafted rules, we propose Conversational
		  Recommender System with Adversarial Learning (CRSAL), a
		  novel end-to-end system to tackle the task of
		  conversational recommendation. In CRSAL, we innovatively
		  design a fully statistical dialogue state tracker coupled
		  with a neural policy agent to precisely capture each
		  user’s intent from limited dialogue data and generate
		  conversational recommendation actions. We further develop
		  an adversarial Actor-Critic reinforcement learning approach
		  to adaptively refine the quality of generated system
		  actions, thus ensuring coherent human-like dialogue
		  responses. Extensive experiments on two benchmark datasets
		  fully demonstrate the superiority of CRSAL on
		  conversational recommendation tasks.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jun,
  articleno	= {34},
  numpages	= {40},
  keywords	= {Conversational recommender systems, adversarial learning,
		  deep neural networks, dialogue systems}
}

@InProceedings{	  10.1145/3290605.3300805,
  author	= {Chen, Fanglin and Xia, Kewei and Dhabalia, Karan and Hong,
		  Jason I.},
  title		= {MessageOnTap: A Suggestive Interface to Facilitate
		  Messaging-related Tasks},
  year		= {2019},
  isbn		= {9781450359702},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3290605.3300805},
  doi		= {10.1145/3290605.3300805},
  abstract	= {Text messages are sometimes prompts that lead to
		  information related tasks, e.g. checking one's schedule,
		  creating reminders, or sharing content. We introduce
		  MessageOnTap, a suggestive inter-face for smartphones that
		  uses the text in a conversation to suggest task shortcuts
		  that can streamline likely next actions. When activated,
		  MessageOnTap uses word embeddings to rank relevant external
		  apps, and parameterizes associated task shortcuts using key
		  phrases mentioned in the conversation, such as times,
		  persons, or events. MessageOnTap also tailors the
		  auto-complete dictionary based on text in the conversation,
		  to streamline any text input.We first conducted a
		  month-long study of messaging behaviors(N=22) that informed
		  our design. We then conducted a lab study to evaluate the
		  effectiveness of MessageOnTap's suggestive interface, and
		  found that participants can complete tasks 3.1x faster
		  withMessageOnTap than their typical task flow.},
  booktitle	= {Proceedings of the 2019 CHI Conference on Human Factors in
		  Computing Systems},
  pages		= {1–14},
  numpages	= {14},
  keywords	= {contextual computing, information seeking &amp; search,
		  messaging/communication, personal data/tracking,
		  productivity, text/speech/language, user experience
		  design},
  location	= {Glasgow, Scotland Uk},
  series	= {CHI '19}
}

@InProceedings{	  10.1145/3219819.3219956,
  author	= {Li, Yan and Ye, Jieping},
  title		= {Learning Adversarial Networks for Semi-Supervised Text
		  Classification via Policy Gradient},
  year		= {2018},
  isbn		= {9781450355520},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3219819.3219956},
  doi		= {10.1145/3219819.3219956},
  abstract	= {Semi-supervised learning is a branch of machine learning
		  techniques that aims to make fully use of both labeled and
		  unlabeled instances to improve the prediction performance.
		  The size of modern real world datasets is ever-growing so
		  that acquiring label information for them is
		  extraordinarily difficult and costly. Therefore, deep
		  semi-supervised learning is becoming more and more popular.
		  Most of the existing deep semi-supervised learning methods
		  are built under the generative model based scheme, where
		  the data distribution is approximated via input data
		  reconstruction. However, this scheme does not naturally
		  work on discrete data, e.g., text; in addition, learning a
		  good data representation is sometimes directly opposed to
		  the goal of learning a high performance prediction model.
		  To address the issues of this type of methods, we
		  reformulate the semi-supervised learning as a model-based
		  reinforcement learning problem and propose an adversarial
		  networks based framework. The proposed framework contains
		  two networks: a predictor network for target estimation and
		  a judge network for evaluation. The judge network
		  iteratively generates proper reward to guide the training
		  of predictor network, and the predictor network is trained
		  via policy gradient. Based on the aforementioned framework,
		  we propose a recurrent neural network based model for
		  semi-supervised text classification. We conduct
		  comprehensive experimental analysis on several real world
		  benchmark text datasets, and the results from our
		  evaluations show that our method outperforms other
		  competing state-of-the-art methods.},
  booktitle	= {Proceedings of the 24th ACM SIGKDD International
		  Conference on Knowledge Discovery &amp; Data Mining},
  pages		= {1715–1723},
  numpages	= {9},
  keywords	= {adversarial networks, policy gradients, semi-supervised
		  learning, text classification},
  location	= {London, United Kingdom},
  series	= {KDD '18}
}

@InProceedings{	  10.1145/3077136.3080796,
  author	= {Zhang, Shuo and Balog, Krisztian},
  title		= {EntiTables: Smart Assistance for Entity-Focused Tables},
  year		= {2017},
  isbn		= {9781450350228},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3077136.3080796},
  doi		= {10.1145/3077136.3080796},
  abstract	= {Tables are among the most powerful and practical tools for
		  organizing and working with data. Our motivation is to
		  equip spreadsheet programs with smart assistance
		  capabilities. We concentrate on one particular family of
		  tables, namely, tables with an entity focus. We introduce
		  and focus on two specifc tasks: populating rows with
		  additional instances (entities) and populating columns with
		  new headings. We develop generative probabilistic models
		  for both tasks. For estimating the components of these
		  models, we consider a knowledge base as well as a large
		  table corpus. Our experimental evaluation simulates the
		  various stages of the user entering content into an actual
		  table. A detailed analysis of the results shows that the
		  models' components are complimentary and that our methods
		  outperform existing approaches from the literature.},
  booktitle	= {Proceedings of the 40th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {255–264},
  numpages	= {10},
  keywords	= {intelligent table assistance, semantic search, table
		  completion},
  location	= {Shinjuku, Tokyo, Japan},
  series	= {SIGIR '17}
}

@InProceedings{	  10.1145/3423334.3431450,
  author	= {Wang, Zhangyu and Moosavi, Vahid},
  title		= {From PIace2Vec to Multi-Scale Built-Environment
		  Representation: A General-Purpose Distributional Embedding
		  for Urban Data Analysis},
  year		= {2020},
  isbn		= {9781450381604},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3423334.3431450},
  doi		= {10.1145/3423334.3431450},
  abstract	= {Built environments like cities, roads, communities are
		  rich sources of urban data. Many downstream applications
		  require comprehensive analysis like geographic information
		  retrieval, recommender systems, geographic knowledge
		  graphs, and in general, understanding urban spaces [28].
		  Points of Interests (POI), as one of the most researched
		  aspects of urban data, has been successfully modeled using
		  concepts borrowed from Machine Learning (ML) and Natural
		  Language Processing (NLP). In the work of Place2Vec [28], a
		  Word2Vec-like statistical model is proposed to represent
		  spatial adjacency with a continuous embedding space. This
		  method successfully models the functional semantics of POIs
		  with regard to several human-assessment based evaluations.
		  However, though the Place2Vec model addresses the
		  distributional heterogeneity within a given spatial context
		  with ITDL augmentation, it does not address the spatial
		  heterogeneity among different regions. To solve this
		  problem, we propose to introduce a hierarchical,
		  density-based, self-adjusting clustering mechanism. The
		  boundary of relatedness and unrelatedness is learned from
		  the given context, where denser areas have tighter bounds
		  while sparser areas have looser ones. We train our model on
		  both the baseline Yelp hierarchical dataset [28] and our
		  OpenStreetMap dataset. We demonstrate that 1) our model
		  significantly improves the performance on 2 of the 3
		  baseline tasks and the stability of training, and 2) our
		  model generalizes excellently across 112 cities of
		  radically different scales (minimum 1725 POIs, maximum
		  2694070 POIs), regions (North America, Europe, Asia,
		  Africa) and types (commercial, touristy, industrial, etc.)
		  without the need of adjusting or tuning any
		  hyperparameters. We also demonstrate that our model can be
		  used to discover interesting facts about cities like
		  inter-city semantic analogy and intra-city connectivity,
		  which can be very useful in urban planning, social
		  computing and public policy making.},
  booktitle	= {Proceedings of the 4th ACM SIGSPATIAL Workshop on
		  Location-Based Recommendations, Geosocial Networks, and
		  Geoadvertising},
  articleno	= {1},
  numpages	= {12},
  keywords	= {Geo-Semantics, Machine Learning, Points of Interest,
		  Similarity},
  location	= {Seattle, WA, USA},
  series	= {LocalRec'20}
}

@Article{	  10.1145/3379443,
  author	= {Li, Guangjie and Liu, Hui and Nyamawe, Ally S.},
  title		= {A Survey on Renamings of Software Entities},
  year		= {2020},
  issue_date	= {March 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3379443},
  doi		= {10.1145/3379443},
  abstract	= {More than 70% of characters in the source code are used to
		  label identifiers. Consequently, identifiers are one of the
		  most important source for program comprehension. Meaningful
		  identifiers are crucial to understand and maintain
		  programs. However, for reasons like constrained schedule,
		  inexperience, and unplanned evolution, identifiers may fail
		  to convey the semantics of the entities associated with
		  them. As a result, such entities should be renamed to
		  improve software quality. However, manual renaming and
		  recommendation are fastidious, time consuming, and error
		  prone, whereas automating the process of renamings is
		  challenging: (1) It involves complex natural language
		  processing to understand the meaning of identifers; (2) It
		  also involves difficult semantic analysis to determine the
		  role of software entities. Researchers proposed a number of
		  approaches and tools to facilitate renamings. We present a
		  survey on existing approaches and classify them into
		  identification of renaming opportunities, execution of
		  renamings, and detection of renamings. We find that there
		  is an imbalance between the three type of approaches, and
		  most of implementation of approaches and evaluation dataset
		  are not publicly available. We also discuss the challenges
		  and present potential research directions. To the best of
		  our knowledge, this survey is the first comprehensive study
		  on renamings of software entities.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {41},
  numpages	= {38},
  keywords	= {Rename refactoring, identifier, software quality}
}

@InProceedings{	  10.1145/3173574.3173851,
  author	= {Huber, Bernd and McDuff, Daniel and Brockett, Chris and
		  Galley, Michel and Dolan, Bill},
  title		= {Emotional Dialogue Generation using Image-Grounded
		  Language Models},
  year		= {2018},
  isbn		= {9781450356206},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3173574.3173851},
  doi		= {10.1145/3173574.3173851},
  abstract	= {Computer-based conversational agents are becoming
		  ubiquitous. However, for these systems to be engaging and
		  valuable to the user, they must be able to express emotion,
		  in addition to providing informative responses. Humans rely
		  on much more than language during conversations; visual
		  information is key to providing context. We present the
		  first example of an image-grounded conversational agent
		  using visual sentiment, facial expression and scene
		  features. We show that key qualities of the generated
		  dialogue can be manipulated by the features used for
		  training the agent. We evaluate our model on a large and
		  very challenging real-world dataset of conversations from
		  social media (Twitter). The image-grounding leads to
		  significantly more informative, emotional and specific
		  responses, and the exact qualities can be tuned depending
		  on the image features used. Furthermore, our model improves
		  the objective quality of dialogue responses when evaluated
		  on standard natural language metrics.},
  booktitle	= {Proceedings of the 2018 CHI Conference on Human Factors in
		  Computing Systems},
  pages		= {1–12},
  numpages	= {12},
  keywords	= {computer vision, conversation, conversational agents,
		  dialogue, emotion},
  location	= {Montreal QC, Canada},
  series	= {CHI '18}
}

@InProceedings{	  10.1145/3331453.3361319,
  author	= {Deng, Chunhui and Deng, Huifang and Liu, Yuxin},
  title		= {Online Hot Topic Discovery and Hotness Evaluation},
  year		= {2019},
  isbn		= {9781450362948},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3331453.3361319},
  doi		= {10.1145/3331453.3361319},
  abstract	= {In this paper, by analyzing the inadequacies of
		  traditional TF-IDF(Term Frequency-Inverse Document
		  Frequency) method and taking into account the factors of
		  the location information, named entity and feature term
		  burstiness, we put forward an improved weight calculation
		  formula i.e., a new TF-IDF to update the feature term
		  weight in real time. In this way, the accuracy of news
		  representation model can be improved to some extent.
		  Incremental k-means clustering based on time window and
		  multi-center topic model is proposed to tackle topic center
		  drift problem, reduce the error caused by inadequate topic
		  model, and therefore, improve the clustering accuracy. At
		  last, we defined an improved energy accumulation formula.
		  And based on media attention, topic competition, topic
		  burstiness magnitude and topic cohesiveness, we constructed
		  a topic hotness evaluation model to quantify the topic
		  hotness and therefore to better distinguish the hot topics
		  from the cold topics. The experimental results demonstrated
		  the effectiveness of our approaches and models.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Computer Science and Application Engineering},
  articleno	= {43},
  numpages	= {8},
  keywords	= {Hotness evaluation, Improved TF-IDF, Incremental k-means
		  clustering, Multi-center topic model, Online hot topic
		  discovery, Single pass, Vector space model},
  location	= {Sanya, China},
  series	= {CSAE '19}
}

@InProceedings{	  10.1145/3278681.3278704,
  author	= {Ramunyisi, Ndivhuwo Stan and Badenhorst, Jaco and Moors,
		  Carmen and Gumede, Tebogo},
  title		= {Rapid development of a command and control interface for
		  smart office environments},
  year		= {2018},
  isbn		= {9781450366472},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3278681.3278704},
  doi		= {10.1145/3278681.3278704},
  abstract	= {Speech-driven command and control is increasingly gaining
		  momentum in the world around us. Using human speech to
		  control appliances in trendy living spaces allows a user to
		  control many appliances remotely. This paper discusses the
		  rapid development of a Speech Node device as a solution to
		  an internet-of-things requirement, creating a smart office
		  environment. An intuitive user interface design, and the
		  development and evaluation of an accented command and
		  control model is described. User evaluation indicates good
		  acceptance of the UI and highlights specific future
		  improvement strategies.},
  booktitle	= {Proceedings of the Annual Conference of the South African
		  Institute of Computer Scientists and Information
		  Technologists},
  pages		= {188–194},
  numpages	= {7},
  keywords	= {accented speech corpus, command and control, pocketsphinx,
		  speech recognition},
  location	= {Port Elizabeth, South Africa},
  series	= {SAICSIT '18}
}

@InProceedings{	  10.1145/3209978.3209983,
  author	= {Ahmad, Wasi Uddin and Chang, Kai-Wei and Wang, Hongning},
  title		= {Intent-aware Query Obfuscation for Privacy Protection in
		  Personalized Web Search},
  year		= {2018},
  isbn		= {9781450356572},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209978.3209983},
  doi		= {10.1145/3209978.3209983},
  abstract	= {Modern web search engines exploit users' search history to
		  personalize search results, with a goal of improving their
		  service utility on a per-user basis. But it is this very
		  dimension that leads to the risk of privacy infringement
		  and raises serious public concerns. In this work, we
		  propose a client-centered intent-aware query obfuscation
		  solution for protecting user privacy in a personalized web
		  search scenario. In our solution, each user query is
		  submitted with l additional cover queries and corresponding
		  clicks, which act as decoys to mask users' genuine search
		  intent from a search engine. The cover queries are
		  sequentially sampled from a set of hierarchically organized
		  language models to ensure the coherency of fake search
		  intents in a cover search task. Our approach emphasizes the
		  plausibility of generated cover queries, not only to the
		  current genuine query but also to previous queries in the
		  same task, to increase the complexity for a search engine
		  to identify a user's true intent. We also develop two new
		  metrics from an information theoretic perspective to
		  evaluate the effectiveness of provided privacy protection.
		  Comprehensive experiment comparisons with state-of-the-art
		  query obfuscation techniques are performed on the public
		  AOL search log, and the propitious results substantiate the
		  effectiveness of our solution.},
  booktitle	= {The 41st International ACM SIGIR Conference on Research
		  &amp; Development in Information Retrieval},
  pages		= {285–294},
  numpages	= {10},
  keywords	= {query obfuscation, search privacy, search tasks},
  location	= {Ann Arbor, MI, USA},
  series	= {SIGIR '18}
}

@Article{	  10.1145/3418062,
  author	= {Guangce, Ruan and Lei, Xia},
  title		= {Knowledge Discovery of News Text Based on Artificial
		  Intelligence},
  year		= {2020},
  issue_date	= {January 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3418062},
  doi		= {10.1145/3418062},
  abstract	= {The explosion of news text and the development of
		  artificial intelligence provide a new opportunity and
		  challenge to provide high-quality media monitoring service.
		  In this article, we propose a semantic analysis approach
		  based on the Latent Dirichlet Allocation (LDA) and Apriori
		  algorithm, and we realize application to improve media
		  monitoring reports by mining large-scale news text. First,
		  we propose to use LDA model to mine news text topic words
		  and reducing news dimensionality. Then, we propose to use
		  Apriori algorithm to discovering the relationship of topic
		  words. Finally, we discovery the relevance of news text
		  topic words and show the intensity and dependency among
		  topic words through drawing. This application can realize
		  to extract the news topics and discover the correlation and
		  dependency among news topics in mass news text. The results
		  show that the method based on LDA and Apriori can help the
		  media monitoring staff to better understand the hidden
		  knowledge in the news text and improve the media analysis
		  report.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {6},
  numpages	= {18},
  keywords	= {LDA, association rules, knowledge discovery, news text}
}

@Proceedings{	  10.5555/3041021,
  title		= {WWW '17 Companion: Proceedings of the 26th International
		  Conference on World Wide Web Companion},
  year		= {2017},
  isbn		= {9781450349147},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  abstract	= {We welcome you to the WWW2017 conference, the 26th of the
		  series, and only the second one to be held in Australia.The
		  annual World Wide Web Conference is the premier
		  international forum to present and discuss progress in
		  research, development, standards, and applications related
		  to the Web. This conference is organised under the aegis of
		  the International World Wide Web Conference Committee
		  (IW3C2) in collaboration with local conference organisers
		  in the host city, in this case, the four public
		  universities in Western Australia: Curtin University,
		  Murdoch University, the University of Western Australia and
		  Edith Cowan University. This year, WWW 2017 is offered as
		  the centerpiece of the inaugural Festival of the Web in
		  Perth, a week-long celebration.WWW 2017 provides an
		  opportunity to hear from the leaders of the web including
		  three distinguished keynote speeches by world-class
		  experts: Mark Pesce, Yoelle Maarek, and Melanie
		  Johnston-Hollitt. There is a rich environment of technical
		  activities, including 164 high quality papers in the
		  Research Tracks, 54 papers in the four alternate tracks,
		  over 100 papers in 15 workshops, 13 tutorial sessions, a
		  Ph.D. Symposium track comprising presentations by seven
		  doctoral students, an Industry track consisting of 20
		  papers focused on applied research, 20 demonstrations, a
		  W3C track examining the latest Web standards and emerging
		  technologies and 64 posters with, for the first time, a
		  number of these offered as e-posters to augment the static
		  poster panels. Overall, WWW2017 provides more than 400 high
		  quality presentations on the key research and development
		  issues of the World Wide Web.Co-located events in the
		  Festival of the Web 2017 include the 4th Big Data
		  Innovators Gathering (BIG 2017), the Web for All conference
		  (W4A2017), and the 5th Serious Games and Applications for
		  Health conference (SeGaH'17). In addition, several new
		  events include Collaboration-Innovation, a one day
		  conference focusing on building smart business innovation
		  through collaboration; the Trust Factory, a curated forum
		  exploring issues of trust and privacy on the web; and Bytes
		  and Rights, a conference focused on issues of web
		  governance, copyright, digital rights, privacy and security
		  on the web. Finally, the Big Day In is a one-day IT careers
		  conference designed by students for students, including
		  tips and advice for secondary school students interested in
		  IT and the web.Given Perth's location in one of the world's
		  richest areas of natural resources, DeepSensor, a world
		  class gathering of industry professionals, is being
		  conducted as part of the Festival as an opportunity for
		  professionals to share their real-world insights into the
		  continuing development of the internet of things in the
		  mining, oil and gas industries.},
  location	= {Perth, Australia}
}

@InProceedings{	  10.1145/3411564.3411628,
  author	= {Mane, Babacar and Rocha, Wita dos Santos and Ribeiro,
		  Elivaldo Lozer Fracalossi and Jesus, Luis Emanuel Neves de
		  and Motta, Israel Cerqueira and Lima, Edmilson and Claro,
		  Daniela Barreiro},
  title		= {Enhancing Semantic Interoperability on MIDAS with Similar
		  DaaS Parameters},
  year		= {2020},
  isbn		= {9781450388733},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411564.3411628},
  doi		= {10.1145/3411564.3411628},
  abstract	= {The vast amount of social data and the extensive use of
		  smart devices have enabled different formats of DaaS (Data
		  as a Service). Cloud consumers are encouraged to access
		  such DaaS from a SaaS (Software as a Service) directly.
		  However, DaaS can evolve and modify some of its parameters,
		  thus avoiding a SaaS to catch all such adjustments. Thus,
		  it is important to have a middleware to bind the request
		  from SaaS applications to data from DaaS. One such
		  middleware is MIDAS (Middleware for DaaS and SaaS), which
		  provides a way from SaaS to DaaS seamlessly. Although there
		  are many advantages to publishing data as a DaaS format,
		  the major challenge is the dynamicity of DaaS parameters,
		  which can change and evolve. Such changes can affect SaaS
		  applications, even providing runtime errors. Thus, our
		  approach overcomes this problem by ensuring a transparent
		  mechanism to DaaS parameters when modifications have
		  occurred. Our work aims to recognize each similar attribute
		  seamlessly, increasing availability to interoperate SaaS
		  and DaaS through MIDAS. We evaluate the distance measures
		  thought 22 parameters from 11 DaaS providers into five
		  scenarios on parameters change. Two of them were the most
		  outstanding measures (Cosine and Jaccard), and we included
		  them in our approach to better obtain the similarity of two
		  parameters. Each parameter was carried out toward WordNet
		  thesaurus, and a second evaluation was performed analyzing
		  our approach into MIDAS through three criteria: overhead,
		  execution time, and correctness. Our experiments have shown
		  that we are in a good direction to provide interoperability
		  into SaaS and DaaS.},
  booktitle	= {Proceedings of the XVI Brazilian Symposium on Information
		  Systems},
  articleno	= {19},
  numpages	= {8},
  keywords	= {DaaS, Interoperability, Middleware, SaaS, Semantic
		  similarity},
  location	= {S\~{a}o Bernardo do Campo, Brazil},
  series	= {SBSI '20}
}

@InProceedings{	  10.1145/3127526.3127527,
  author	= {Singh, Mayank and Dan, Soham and Agarwal, Sanyam and
		  Goyal, Pawan and Mukherjee, Animesh},
  title		= {AppTechMiner: Mining Applications and Techniques from
		  Scientific Articles},
  year		= {2017},
  isbn		= {9781450353885},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3127526.3127527},
  doi		= {10.1145/3127526.3127527},
  abstract	= {This paper presents AppTechMiner, a rule-based information
		  extraction framework that automatically constructs a
		  knowledge base of all application areas and problem solving
		  techniques. Techniques include tools, methods, datasets or
		  evaluation metrics. We also categorize individual research
		  articles based on their application areas and the
		  techniques proposed/improved in the article. Our system
		  achieves high average precision (~82%) and recall (~84%) in
		  knowledge base creation. It also performs well in
		  application and technique assignment to an individual
		  article (average accuracy ~66%). In the end, we further
		  present two use cases presenting a trivial information
		  retrieval system and an extensive temporal analysis of the
		  usage of techniques and application areas. At present, we
		  demonstrate the framework for the domain of computational
		  linguistics but this can be easily generalized to any other
		  field of research. We plan to make the codes publicly
		  available.},
  booktitle	= {Proceedings of the 6th International Workshop on Mining
		  Scientific Publications},
  pages		= {1–8},
  numpages	= {8},
  keywords	= {Information extraction, application area, computational
		  linguistic, techniques},
  location	= {Toronto, ON, Canada},
  series	= {WOSP 2017}
}

@InProceedings{	  10.1145/3022227.3022299,
  author	= {Ryu, Seonghan and Yu, Hwanjo and Lee, Gary Geunbae},
  title		= {Two-stage approach to named entity recognition using
		  Wikipedia and DBpedia},
  year		= {2017},
  isbn		= {9781450348881},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3022227.3022299},
  doi		= {10.1145/3022227.3022299},
  abstract	= {In natural language understanding, extraction of named
		  entity (NE) mentions in given text and classification of
		  the mentions into pre-defined NE types are important
		  processes. Most NE recognition (NER) relies on resources
		  such as a training corpus or NE dictionary, but collecting
		  them manually is laborious and time-consuming. This paper
		  proposes a two-stage approach based on nothing but
		  Wikipedia and DBpedia to implement NER. This paper also
		  addresses technical problems in developing Korean NER. In
		  experiments, the proposed method can recognize NEs in short
		  question sentences with 14.2% errors.},
  booktitle	= {Proceedings of the 11th International Conference on
		  Ubiquitous Information Management and Communication},
  articleno	= {73},
  numpages	= {4},
  keywords	= {DBpedia, Wikipedia, information extraction, named entity
		  recognition, question answering},
  location	= {Beppu, Japan},
  series	= {IMCOM '17}
}

@Article{	  10.1109/tcbb.2019.2897769,
  author	= {Yao, Heng and Shi, Yunjia and Guan, Jihong and Zhou,
		  Shuigeng},
  title		= {Accurately Detecting Protein Complexes by Graph Embedding
		  and Combining Functions with Interactions},
  year		= {2020},
  issue_date	= {May-June 2020},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {17},
  number	= {3},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2019.2897769},
  doi		= {10.1109/TCBB.2019.2897769},
  abstract	= {Identifying protein complexes is helpful for understanding
		  cellular functions and designing drugs. In the last
		  decades, many computational methods have been proposed
		  based on detecting dense subgraphs or subnetworks in
		  Protein-Protein Interaction Networks (PINs). However, the
		  high rate of false positive/negative interactions in PINs
		  prevents from the achievement of satisfactory detection
		  results directly from PINs, because most of such existing
		  methods exploit mainly topological information to do
		  network partitioning. In this paper, we propose a new
		  approach for protein complex detection by merging
		  topological information of PINs and functional information
		  of proteins. We first split proteins to a number of protein
		  groups from the perspective of protein functions by using
		  FunCat data. Then, for each of the resulting protein
		  groups, we calculate two protein-protein similarity
		  matrices: one is computed by using graph embedding over a
		  PIN, the other is by using GO terms, and combine these two
		  matrices to get an integrated similarity matrix. Following
		  that, we cluster the proteins in each group based on the
		  corresponding integrated similarity matrix, and obtain a
		  number of small protein clusters. We map these clusters of
		  proteins onto the PIN, and get a number of connected
		  subgraphs. After a round of merging of overlapping
		  subgraphs, finally we get the detected complexes. We
		  conduct empirical evaluation on four PPI datasets (Collins,
		  Gavin, Krogan, and Wiphi) with two complex benchmarks
		  (CYC2008 and MIPS). Experimental results show that our
		  method performs better than the state-of-the-art methods.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jun,
  pages		= {777–787},
  numpages	= {11}
}

@Article{	  10.1145/3178315.3178331,
  author	= {Dwivedi, Ashish Kumar and Rath, Santanu Kumar},
  title		= {Transformation of Alloy Notation into a Semantic
		  Notation},
  year		= {2018},
  issue_date	= {January 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {1},
  issn		= {0163-5948},
  url		= {https://doi.org/10.1145/3178315.3178331},
  doi		= {10.1145/3178315.3178331},
  abstract	= {Transformation of a model based on first-order logic to a
		  model that provides semantic notations is helpful necessary
		  during the analysis phase of any proposed software. The
		  semantic notations often guide the designer to develop
		  pseudocode correctly. This study focuses on facilitation of
		  transformation of one formal model, i.e., Alloy into
		  another, i.e., OWL. The proposed approach extends the
		  concept of existing techniques i.e., UML2Alloy and TwoUse
		  to transform Alloy model into OWL. UML2Alloy transforms UML
		  model into Alloy model, whereas TwoUse approach bridges the
		  gap between UML model and OWL model. Alloy2OWL is based on
		  metamodel-based transformation techniques, which help to
		  map source model, i.e., Alloy into target model, i.e., OWL.
		  For the proper explanation of this study, a model
		  transformation framework is presented, which can be applied
		  to other transformation languages. The proposed approach
		  utilizes the Model-Driven Development techniques to deal
		  with the analysis of Alloy model and determines design
		  problems within a specification. In this paper, various
		  challenges are also presented which occur during the
		  transformation of Alloy to OWL.},
  journal	= {SIGSOFT Softw. Eng. Notes},
  month		= mar,
  pages		= {1–6},
  numpages	= {6},
  keywords	= {alloy, formal methods, model-driven development, owl,
		  uml}
}

@InBook{	  10.1145/3233795.3233811,
  author	= {Waibel, Alexander},
  title		= {Multimodal dialogue processing for machine translation},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233811},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {577–620},
  numpages	= {44}
}

@InProceedings{	  10.1145/3176653.3176665,
  author	= {Basha, Syed Muzamil and Rajput, Dharmendra Singh},
  title		= {Evaluating the Impact of Feature Selection on Overall
		  Performance of Sentiment Analysis},
  year		= {2017},
  isbn		= {9781450363518},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3176653.3176665},
  doi		= {10.1145/3176653.3176665},
  abstract	= {Now a days the importance of analyzing the hidden
		  sentiments from user reviews playing a prominent role
		  towards increasing profitability in any organization. To
		  address the challenges being faced in analyzing the text
		  information and transforming the same in to polarities
		  values with an objective of saving time in understanding
		  the public opinion on particular product or service.
		  Traditionally, there are different approaches carried out
		  in transforming text data in to values based on different
		  features of Text. In our research we make use of Stanford
		  CoreNLP, Alias-i's Lingpipe (uses Logistic regression for
		  document classification), Senti WordNet and synthesize
		  libraries from different sources to include several other
		  techniques that are used for text mining to evaluate the
		  impact of feature selection on overall sentiment analysis
		  by scoring a sentences in a review using different scoring
		  Techniques. we also included NTU Lib Linear to make use of
		  linear SVM for document classification. The Features
		  considered on our experiments are Term Frequency and N-Gram
		  (1Gram &amp; 2Gram) with Decision Tree as Prediction model
		  to evaluate the Accuracy, Area under ROC Curve and Kappa
		  value. Finally, Compared the polarities of the reviews
		  obtained using three different sentiment scoring
		  approaches. The findings in our research is, Term Frequency
		  have good impact of (0.932) on classifying the sentiment,
		  In contrast, 2Gram have an impact of (0.8505).},
  booktitle	= {Proceedings of the 2017 International Conference on
		  Information Technology},
  pages		= {96–102},
  numpages	= {7},
  keywords	= {Alias-i's Lingpipe, SentiWordNet, Stanford CoreNLP,
		  YTextMiner, sentiment analysis},
  location	= {Singapore, Singapore},
  series	= {ICIT '17}
}

@Article{	  10.1145/3212695,
  author	= {Allamanis, Miltiadis and Barr, Earl T. and Devanbu,
		  Premkumar and Sutton, Charles},
  title		= {A Survey of Machine Learning for Big Code and
		  Naturalness},
  year		= {2018},
  issue_date	= {July 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {51},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3212695},
  doi		= {10.1145/3212695},
  abstract	= {Research at the intersection of machine learning,
		  programming languages, and software engineering has
		  recently taken important steps in proposing learnable
		  probabilistic models of source code that exploit the
		  abundance of patterns of code. In this article, we survey
		  this work. We contrast programming languages against
		  natural languages and discuss how these similarities and
		  differences drive the design of probabilistic models. We
		  present a taxonomy based on the underlying design
		  principles of each model and use it to navigate the
		  literature. Then, we review how researchers have adapted
		  these models to application areas and discuss cross-cutting
		  and application-specific challenges and opportunities.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {81},
  numpages	= {37},
  keywords	= {Big code, code naturalness, machine learning, software
		  engineering tools}
}

@Article{	  10.1145/3094786,
  author	= {Gao, Yang and Li, Yuefeng and Lau, Raymond Y. K. and Xu,
		  Yue and Bashar, Md Abul},
  title		= {Finding Semantically Valid and Relevant Topics by
		  Association-Based Topic Selection Model},
  year		= {2017},
  issue_date	= {January 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {1},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3094786},
  doi		= {10.1145/3094786},
  abstract	= {Topic modelling methods such as Latent Dirichlet
		  Allocation (LDA) have been successfully applied to various
		  fields, since these methods can effectively characterize
		  document collections by using a mixture of semantically
		  rich topics. So far, many models have been proposed.
		  However, the existing models typically outperform on full
		  analysis on the whole collection to find all topics but
		  difficult to capture coherent and specifically meaningful
		  topic representations. Furthermore, it is very challenging
		  to incorporate user preferences into existing topic
		  modelling methods to extract relevant topics. To address
		  these problems, we develop a novel personalized
		  Association-based Topic Selection (ATS) model, which can
		  identify semantically valid and relevant topics from a set
		  of raw topics based on the semantical relatedness between
		  users’ preferences and the structured patterns captured
		  in topics. The advantage of the proposed ATS model is that
		  it enables an interactive topic modelling process driven by
		  users’ specific interests. Based on three benchmark
		  datasets, namely, RCV1, R8, and WT10G under the context of
		  information filtering (IF) and information retrieval (IR),
		  our rigorous experiments show that the proposed ATS model
		  can effectively identify relevant topics with respect to
		  users’ specific interests, and hence to improve the
		  performance of IF and IR.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= aug,
  articleno	= {3},
  numpages	= {22},
  keywords	= {Topic selection, information filtering, topic components,
		  topic evaluation}
}

@InProceedings{	  10.1145/3377713.3377803,
  author	= {Wei, Chao and Zhu, Lijun and Wang, Juncheng and Shi,
		  Jiaoxiang and Wang, Zheng and Chen, Liang},
  title		= {Extracting Word Embeddings via Joint Learning of
		  Syntagmatic and Paradigmatic Structure},
  year		= {2020},
  isbn		= {9781450372619},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3377713.3377803},
  doi		= {10.1145/3377713.3377803},
  abstract	= {In this work, we explore the latent distributional
		  semantic between each given word and its context in a way
		  of reconstruction, and propose an Auto-encoder architecture
		  to encourage the model pay attention to the interactions of
		  Syntagmatic and Paradigmatic structure of context (SPC). In
		  particular, SPC represent the syntagmatic words of the
		  context through a vocabulary co-occurrence matrix, and
		  extract a likely embedding for a better joint predicting of
		  all paradigmatic words to fill in the input context. During
		  the error back propagation, our model focus on the salient
		  statistical structure by allowing a sub-network to
		  approximate the nonzero sub-matrix. Finally, we trained our
		  model on a public Wikipedia corpus and evaluated on word
		  analogy, word similarity tasks and documents clustering and
		  classification. The results show that our method yields
		  almost 4% improvement on word analogy and nearly 3%
		  improvement on word similarity tasks compared to
		  state-of-the-art methods. The evidences demonstrate that
		  SPC outperform baseline and the state-of-the-art methods on
		  all tasks.},
  booktitle	= {Proceedings of the 2019 2nd International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  pages		= {526–532},
  numpages	= {7},
  keywords	= {Auto-encoder, Syntagmatic and Paradigmatic structure, Word
		  Analogy and Similarity, Word embeddings},
  location	= {Sanya, China},
  series	= {ACAI '19}
}

@InProceedings{	  10.1145/3340531.3411974,
  author	= {Xu, Silei and Campagna, Giovanni and Li, Jian and Lam,
		  Monica S.},
  title		= {Schema2QA: High-Quality and Low-Cost Q&amp;A Agents for
		  the Structured Web},
  year		= {2020},
  isbn		= {9781450368599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340531.3411974},
  doi		= {10.1145/3340531.3411974},
  abstract	= {Building a question-answering agent currently requires
		  large annotated datasets, which are prohibitively
		  expensive. This paper proposes Schema2QA, an open-source
		  toolkit that can generate a Q&amp;A system from a database
		  schema augmented with a few annotations for each field. The
		  key concept is to cover the space of possible compound
		  queries on the database with a large number of in-domain
		  questions synthesized with the help of a corpus of generic
		  query templates. The synthesized data and a small
		  paraphrase set are used to train a novel neural network
		  based on the BERT pretrained model. We use Schema2QA to
		  generate Q&amp;A systems for five Schema.org domains,
		  restaurants, people, movies, books and music, and obtain an
		  overall accuracy between 64% and 75% on crowdsourced
		  questions for these domains. Once annotations and
		  paraphrases are obtained for a Schema.org schema, no
		  additional manual effort is needed to create a Q&amp;A
		  agent for any website that uses the same schema.
		  Furthermore, we demonstrate that learning can be
		  transferred from the restaurant to the hotel domain,
		  obtaining a 64% accuracy on crowdsourced questions with no
		  manual effort. Schema2QA achieves an accuracy of 60% on
		  popular restaurant questions that can be answered using
		  Schema.org. Its performance is comparable to Google
		  Assistant, 7% lower than Siri, and 15% higher than Alexa.
		  It outperforms all these assistants by at least 18% on more
		  complex, long-tail questions.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1685–1694},
  numpages	= {10},
  keywords	= {KBQA, NLIDB, data augmentation, data synthesis, linked
		  data, question answering, schema.org, semantic parsing,
		  semantic web},
  location	= {Virtual Event, Ireland},
  series	= {CIKM '20}
}

@Article{	  10.1109/tcbb.2020.3010975,
  author	= {Nguyen, Trinh-Trung-Duong and Ho, Quang-Thai and Le,
		  Nguyen-Quoc-Khanh and Phan, Van-Dinh and Ou, Yu-Yen},
  title		= {Use Chou's 5-Steps Rule With Different Word Embedding
		  Types to Boost Performance of Electron Transport Protein
		  Prediction Model},
  year		= {2020},
  issue_date	= {March-April 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {2},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2020.3010975},
  doi		= {10.1109/TCBB.2020.3010975},
  abstract	= {Living organisms receive necessary energy substances
		  directly from cellular respiration. The completion of
		  electron storage and transportation requires the process of
		  cellular respiration with the aid of electron transport
		  chains. Therefore, the work of deciphering electron
		  transport proteins is inevitably needed. The identification
		  of these proteins with high performance has a prompt
		  dependence on the choice of methods for feature extraction
		  and machine learning algorithm. In this study, protein
		  sequences served as natural language sentences comprising
		  words. The nominated word embedding-based feature sets,
		  hinged on the word embedding modulation and protein motif
		  frequencies, were useful for feature choosing. Five word
		  embedding types and a variety of conjoint features were
		  examined for such feature selection. The support vector
		  machine algorithm consequentially was employed to perform
		  classification. The performance statistics within the
		  5-fold cross-validation including average accuracy,
		  specificity, sensitivity, as well as MCC rates surpass
		  0.95. Such metrics in the independent test are 96.82,
		  97.16, 95.76 percent, and 0.9, respectively. Compared to
		  state-of-the-art predictors, the proposed method can
		  generate more preferable performance above all metrics
		  indicating the effectiveness of the proposed method in
		  determining electron transport proteins. Furthermore, this
		  study reveals insights about the applicability of various
		  word embeddings for understanding surveyed sequences.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jul,
  pages		= {1235–1244},
  numpages	= {10}
}

@InProceedings{	  10.1145/3077136.3080740,
  author	= {Cohan, Arman and Goharian, Nazli},
  title		= {Contextualizing Citations for Scientific Summarization
		  using Word Embeddings and Domain Knowledge},
  year		= {2017},
  isbn		= {9781450350228},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3077136.3080740},
  doi		= {10.1145/3077136.3080740},
  abstract	= {Citation texts are sometimes not very informative or in
		  some cases inaccurate by themselves; they need the
		  appropriate context from the referenced paper to reflect
		  its exact contributions. To address this problem, we
		  propose an unsupervised model that uses distributed
		  representation of words as well as domain knowledge to
		  extract the appropriate context from the reference paper.
		  Evaluation results show the effectiveness of our model by
		  significantly outperforming the state-of-the-art. We
		  furthermore demonstrate how an effective contextualization
		  method results in improving citation-based summarization of
		  the scientific articles.},
  booktitle	= {Proceedings of the 40th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1133–1136},
  numpages	= {4},
  keywords	= {information retrieval, scientific text, text
		  summarization},
  location	= {Shinjuku, Tokyo, Japan},
  series	= {SIGIR '17}
}

@InProceedings{	  10.1145/3041021.3054163,
  author	= {Huang, Chieh-Yang and Chen, Mei-Hua and Ku, Lun-Wei},
  title		= {Towards a Better Learning of Near-Synonyms: Automatically
		  Suggesting Example Sentences via Fill in the Blank},
  year		= {2017},
  isbn		= {9781450349147},
  publisher	= {International World Wide Web Conferences Steering
		  Committee},
  address	= {Republic and Canton of Geneva, CHE},
  url		= {https://doi.org/10.1145/3041021.3054163},
  doi		= {10.1145/3041021.3054163},
  abstract	= {Language learners are confused by near-synonyms and often
		  look for answers from the Web. However, there is little to
		  aid them in sorting through the overwhelming load of
		  information that is offered. In this paper, we propose a
		  new research problem: suggesting example sentences for
		  learning word distinctions. We focus on near-synonyms as
		  the first step. Two kinds of one-class classifiers, the GMM
		  and BiLSTM models, are used to solve fill-in-the-blank
		  (FITB) questions and further to select example sentences
		  which best differentiate groups of near-synonyms.
		  Experiments are conducted on both an open benchmark and a
		  private dataset for the FITB task. Experiments show that
		  the proposed approach yields an accuracy of 73.05% and
		  83.59% respectively, comparable to state-of-the-art
		  multi-class classifiers. Learner study further shows the
		  results of the example sentence suggestion by the learning
		  effectiveness and demonstrates the proposed model indeed is
		  more effective in learning near-synonyms compared to the
		  resource-based models.},
  booktitle	= {Proceedings of the 26th International Conference on World
		  Wide Web Companion},
  pages		= {293–302},
  numpages	= {10},
  keywords	= {bilstm, computer-assisted language learning, data-driven
		  language learning, gmm, natural language processing},
  location	= {Perth, Australia},
  series	= {WWW '17 Companion}
}

@InProceedings{	  10.1145/3080546.3080547,
  author	= {Kapoor, Rahul and Kejriwal, Mayank and Szekely, Pedro},
  title		= {Using contexts and constraints for improved geotagging of
		  human trafficking webpages},
  year		= {2017},
  isbn		= {9781450350471},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3080546.3080547},
  doi		= {10.1145/3080546.3080547},
  abstract	= {Extracting geographical tags from webpages is a
		  well-motiva-ted application in many domains. In illicit
		  domains with unusual language models, like human
		  trafficking, extracting geotags with both high precision
		  and recall is a challenging problem. In this paper, we
		  describe a geotag extraction framework in which context,
		  constraints and the openly available Geonames knowledge
		  base work in tandem in an Integer Linear Programming (ILP)
		  model to achieve good performance. In preliminary empirical
		  investigations, the framework improves precision by 28.57%
		  and F-measure by 36.9% on a difficult human trafficking
		  geotagging task compared to a machine learning-based
		  baseline. The method is already being integrated into an
		  existing knowledge base construction system widely used by
		  US law enforcement agencies to combat human trafficking.},
  booktitle	= {Proceedings of the Fourth International ACM Workshop on
		  Managing and Mining Enriched Geo-Spatial Data},
  articleno	= {3},
  numpages	= {6},
  keywords	= {distributional semantics, feature-agnostic, human
		  trafficking, information extraction, integer linear
		  programming, named entity recognition},
  location	= {Chicago, Illinois},
  series	= {GeoRich '17}
}

@Article{	  10.1145/3295822,
  author	= {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and
		  Wang, Yinglong and Ma, Jun and Kankanhalli, Mohan},
  title		= {Attentive Long Short-Term Preference Modeling for
		  Personalized Product Search},
  year		= {2019},
  issue_date	= {April 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {37},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3295822},
  doi		= {10.1145/3295822},
  abstract	= {E-commerce users may expect different products even for
		  the same query, due to their diverse personal preferences.
		  It is well known that there are two types of preferences:
		  long-term ones and short-term ones. The former refers to
		  users’ inherent purchasing bias and evolves slowly. By
		  contrast, the latter reflects users’ purchasing
		  inclination in a relatively short period. They both affect
		  users’ current purchasing intentions. However, few
		  research efforts have been dedicated to jointly model them
		  for the personalized product search. To this end, we
		  propose a novel Attentive Long Short-Term Preference model,
		  dubbed as ALSTP, for personalized product search. Our model
		  adopts the neural networks approach to learn and integrate
		  the long- and short-term user preferences with the current
		  query for the personalized product search. In particular,
		  two attention networks are designed to distinguish which
		  factors in the short-term as well as long-term user
		  preferences are more relevant to the current query. This
		  unique design enables our model to capture users’ current
		  search intentions more accurately. Our work is the first to
		  apply attention mechanisms to integrate both long- and
		  short-term user preferences with the given query for the
		  personalized search. Extensive experiments over four Amazon
		  product datasets show that our model significantly
		  outperforms several state-of-the-art product search methods
		  in terms of different evaluation metrics.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {19},
  numpages	= {27},
  keywords	= {Personalized product search, attention mechanism, long
		  short-term preference}
}

@InProceedings{	  10.1145/3382507.3418813,
  author	= {Yin, Yufeng and Huang, Baiyu and Wu, Yizhen and Soleymani,
		  Mohammad},
  title		= {Speaker-Invariant Adversarial Domain Adaptation for
		  Emotion Recognition},
  year		= {2020},
  isbn		= {9781450375818},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3382507.3418813},
  doi		= {10.1145/3382507.3418813},
  abstract	= {Automatic emotion recognition methods are sensitive to the
		  variations across different datasets and their performance
		  drops when evaluated across corpora. We can apply domain
		  adaptation techniques e.g., Domain-Adversarial Neural
		  Network (DANN) to mitigate this problem. Though the DANN
		  can detect and remove the bias between corpora, the bias
		  between speakers still remains which results in reduced
		  performance. In this paper, we propose Speaker-Invariant
		  Domain-Adversarial Neural Network (SIDANN) to reduce both
		  the domain bias and the speaker bias. Specifically, based
		  on the DANN, we add a speaker discriminator to unlearn
		  information representing speakers' individual
		  characteristics with a gradient reversal layer (GRL). Our
		  experiments with multimodal data (speech, vision, and text)
		  and the cross-domain evaluation indicate that the proposed
		  SIDANN outperforms (+5.6% and +2.8% on average for
		  detecting arousal and valence) the DANN model, suggesting
		  that the SIDANN has a better domain adaptation ability than
		  the DANN. Besides, the modality contribution analysis shows
		  that the acoustic features are the most informative for
		  arousal detection while the lexical features perform the
		  best for valence detection.},
  booktitle	= {Proceedings of the 2020 International Conference on
		  Multimodal Interaction},
  pages		= {481–490},
  numpages	= {10},
  keywords	= {domain adaptation, emotion recognition, multimodal
		  learning, neural networks},
  location	= {Virtual Event, Netherlands},
  series	= {ICMI '20}
}

@InProceedings{	  10.1145/3243082.3243096,
  author	= {Soares, Eduardo R. and Barr\'{e}re, Eduardo},
  title		= {Automatic Topic Segmentation for Video Lectures Using Low
		  and High-Level Audio Features},
  year		= {2018},
  isbn		= {9781450358675},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3243082.3243096},
  doi		= {10.1145/3243082.3243096},
  abstract	= {Nowadays, video lectures are a very popular way to
		  transmit knowledge, and because of that, there are many
		  repositories with a large catalog of those videos on web.
		  Despite all benefits that this high availability of video
		  lectures brings, some problems also emerge from this
		  scenario. One of these problems is that, it is very
		  difficult find relevant content associate with those
		  videos. Many times, students must to watch the entire video
		  lecture to find the point of interest and, sometimes, these
		  points are not found. For that reason, in this work we
		  propose a novel method based on early fusion of low and
		  high-level audio features for automatic topic segmentation
		  in video lectures. We have performed experiments in two
		  sets of video lectures where we obtained very satisfactory
		  results that evidence the applicability of our method on
		  improving content search in those videos.},
  booktitle	= {Proceedings of the 24th Brazilian Symposium on Multimedia
		  and the Web},
  pages		= {189–196},
  numpages	= {8},
  keywords	= {Audio processing, Automatic Speech Recognition, Knowledge
		  base, Semantic annotation, Topic segmentation, Video
		  lectures},
  location	= {Salvador, BA, Brazil},
  series	= {WebMedia '18}
}

@Article{	  10.5555/3207692.3207704,
  author	= {Paetzold, Gustavo H. and Specia, Lucia},
  title		= {A survey on lexical simplification},
  year		= {2017},
  issue_date	= {September 2017},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {60},
  number	= {1},
  issn		= {1076-9757},
  abstract	= {Lexical Simplification is the process of replacing complex
		  words in a given sentence with simpler alternatives of
		  equivalent meaning. This task has wide applicability both
		  as an assistive technology for readers with cognitive
		  impairments or disabilities, such as Dyslexia and Aphasia,
		  and as a pre-processing tool for other Natural Language
		  Processing tasks, such as machine translation and
		  summarisation. The problem is commonly framed as a pipeline
		  of four steps: the identification of complex words, the
		  generation of substitution candidates, the selection of
		  those candidates that fit the context, and the ranking of
		  the selected substitutes according to their simplicity. In
		  this survey we review the literature for each step in this
		  typical Lexical Simplification pipeline and provide a
		  benchmarking of existing approaches for these steps on
		  publicly available datasets. We also provide pointers for
		  datasets and resources available for the task.},
  journal	= {J. Artif. Int. Res.},
  month		= sep,
  pages		= {549–593},
  numpages	= {45}
}

@InProceedings{	  10.1145/3341161.3343510,
  author	= {M\"{u}ngen, Ahmet An\i{}l and Do\u{G}an, Emre and Kaya,
		  Mehmet},
  title		= {Text generation with diversified source literature
		  review},
  year		= {2020},
  isbn		= {9781450368681},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3341161.3343510},
  doi		= {10.1145/3341161.3343510},
  abstract	= {Almost all academic studies include a literature review
		  section. This section is of significance in terms of
		  presenting the value of the suggested method of the
		  researcher and making comparisons. Due to the increasing
		  number of academic papers and the emergence of various
		  directories and indices, the time spent for finding the
		  related previous studies is an important period for the
		  researcher, which consumes a significant amount of time. By
		  means of the suggested method, researchers can access
		  various types of featured publications related to the
		  keyword from different years from a single address. The
		  system also helps to reveal an exemplary and guiding
		  literature review among the found publications by
		  conducting a text generation. The system uses the TF-IDF
		  method for keyword-based publication search and
		  "Template-Based Text Generation" method for the text
		  generation algorithm. In the study, the largest open-access
		  journal platform, T\"{U}BundefinedTAK Dergipark and SOBIAD
		  Citation Index were used as the data set. As a result of
		  the conducted tests, a method that supports the literature
		  review process, even helping to the writing of literature
		  review, was suggested. Along with the fact that there has
		  not been an equivalent of the suggested study, the
		  comparisons for success, "Text Generation" and "Literature
		  Review" were independently calculated and presented.},
  booktitle	= {Proceedings of the 2019 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {765–770},
  numpages	= {6},
  keywords	= {TF-IDF, academic data, literature generation, text
		  generation, text mining},
  location	= {Vancouver, British Columbia, Canada},
  series	= {ASONAM '19}
}

@InProceedings{	  10.1145/3358501.3361235,
  author	= {Roy Chaudhuri, Subhrojyoti and Natarajan, Swaminathan and
		  Banerjee, Amar and Choppella, Venkatesh},
  title		= {Methodology to develop domain specific modeling
		  languages},
  year		= {2019},
  isbn		= {9781450369848},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3358501.3361235},
  doi		= {10.1145/3358501.3361235},
  abstract	= {Domain Specific Modeling Languages (DSML) significantly
		  improve productivity in designing Computer Based System
		  (CBS), by enabling them to be modeled at higher levels of
		  abstraction. It is common for large and complex systems
		  with distributed teams, to use DSMLs, to express and
		  communicate designs of such systems uniformly, using a
		  common language. DSMLs enable domain experts, with no or
		  minimal software development background, to model
		  solutions, using the language and terminologies used in
		  their respective domains. Although, there are already a
		  number of DSMLs available for modeling CBSs, their need is
		  felt strongly across multiple domains, which still are not
		  well supported with DSMLs. Developing a new DSML, however,
		  is non trivial, as it requires (a) significant knowledge
		  about the domain for which the DSML needs to be developed,
		  as well as (b) skills to create new languages. In the
		  current practice, DSMLs are developed by experts, who have
		  substantial understanding of the domain of interest and
		  strong background in computer science. One of the many
		  challenges in the development of DSMLs, is the collection
		  of domain knowledge and its utilization, based on which the
		  abstract syntax, the backbone of the DSML is defined. There
		  is a clear gap in the current state of art and practice,
		  with respect to overcoming this challenge. We propose a
		  methodology, which makes it easier for people with
		  different backgrounds such as domain experts, solution
		  architects, to contribute towards defining the abstract
		  syntax of the DSML. The methodology outlines a set of steps
		  to systematically capture knowledge about the domain of
		  interest, and use that to arrive at the abstract syntax of
		  the DSML. The key contribution of our work is in
		  abstracting a CBS from a domain into a Domain Specific
		  Machine, embodied in domain specific concepts. The
		  methodology outlines, how the Domain Specific Machine, when
		  coupled with guidelines from current practices of
		  developing DSMLs, results in the definition of the abstract
		  syntax of the intended DSML. We discuss our methodology in
		  detail, in this paper.},
  booktitle	= {Proceedings of the 17th ACM SIGPLAN International Workshop
		  on Domain-Specific Modeling},
  pages		= {1–10},
  numpages	= {10},
  keywords	= {domain specific language, domain specific modeling
		  language, language engineering, modeling},
  location	= {Athens, Greece},
  series	= {DSM 2019}
}

@InProceedings{	  10.1145/3183713.3193562,
  author	= {Basik, Fuat and H\"{a}ttasch, Benjamin and Ilkhechi, Amir
		  and Usta, Arif and Ramaswamy, Shekar and Utama, Prasetya
		  and Weir, Nathaniel and Binnig, Carsten and Cetintemel,
		  Ugur},
  title		= {DBPal: A Learned NL-Interface for Databases},
  year		= {2018},
  isbn		= {9781450347037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3183713.3193562},
  doi		= {10.1145/3183713.3193562},
  abstract	= {In this demo, we present DBPal, a novel data exploration
		  tool with a natural language interface. DBPal leverages
		  recent advances in deep models to make query understanding
		  more robust in the following ways: First, DBPal uses novel
		  machine translation models to translate natural language
		  statements to SQL, making the translation process more
		  robust to paraphrasing and linguistic variations. Second,
		  to support the users in phrasing questions without knowing
		  the database schema and the query features, DBPal provides
		  a learned auto-completion model that suggests to users
		  partial query extensions during query formulation and thus
		  helps to write complex queries.},
  booktitle	= {Proceedings of the 2018 International Conference on
		  Management of Data},
  pages		= {1765–1768},
  numpages	= {4},
  keywords	= {natural language to sql, nlidb, relational database,
		  robust natural language interface},
  location	= {Houston, TX, USA},
  series	= {SIGMOD '18}
}

@InProceedings{	  10.1145/3323771.3323824,
  author	= {Maroengsit, Wari and Piyakulpinyo, Thanarath and Phonyiam,
		  Korawat and Pongnumkul, Suporn and Chaovalit, Pimwadee and
		  Theeramunkong, Thanaruk},
  title		= {A Survey on Evaluation Methods for Chatbots},
  year		= {2019},
  isbn		= {9781450366397},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3323771.3323824},
  doi		= {10.1145/3323771.3323824},
  abstract	= {Nowadays chatbots have been widely adopted in many
		  industries to automatically answer users' questions and
		  requests via chat interfaces. While it has become much
		  easier to develop a chatbot system, the system itself is a
		  complex system in nature. It is a challenge to evaluate and
		  compare various chatbot systems in terms of effectiveness,
		  efficiency, goal achievability, and the ability to satisfy
		  users. This paper presents a survey, starting from
		  literature review, chatbot architecture, evaluation
		  methods/criteria, and comparison of evaluation methods.
		  Focused on the three subprocesses in the chatbot
		  architecture: text processing, semantic understanding, and
		  response generation. Moreover, the survey is conducted with
		  classification of chatbot evaluation methods and their
		  analysis according to chatbot types and three main
		  evaluation schemes; content evaluation, user satisfaction,
		  and chat function.},
  booktitle	= {Proceedings of the 2019 7th International Conference on
		  Information and Education Technology},
  pages		= {111–119},
  numpages	= {9},
  keywords	= {Chatbot Evaluation, Information Systems, Natural Language
		  Processing, Natural Language Understanding},
  location	= {Aizu-Wakamatsu, Japan},
  series	= {ICIET 2019}
}

@InProceedings{	  10.1145/3209978.3210046,
  author	= {Yang, Xiao and Awadallah, Ahmed Hassan and Khabsa, Madian
		  and Wang, Wei and Wang, Miaosen},
  title		= {Characterizing and Supporting Question Answering in
		  Human-to-Human Communication},
  year		= {2018},
  isbn		= {9781450356572},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3209978.3210046},
  doi		= {10.1145/3209978.3210046},
  abstract	= {Email continues to be one of the most important means of
		  online communication. People spend a significant amount of
		  time sending, reading, searching and responding to email in
		  order to manage tasks, exchange information, etc. In this
		  paper, we focus on information exchange over enterprise
		  email in the form of questions and answers. We study a
		  large scale publicly available email dataset to
		  characterize information exchange via questions and answers
		  in enterprise email. We augment our analysis with a survey
		  to gain insights on the types of questions exchanged, when
		  and how do people get back to them and whether this
		  behavior is adequately supported by existing email
		  management and search functionality. We leverage this
		  understanding to define the task of extracting
		  question/answer pairs from threaded email conversations. We
		  propose a neural network based approach that matches the
		  question to the answer considering comparisons at different
		  levels of granularity. We also show that we can improve the
		  performance by leveraging external data of question and
		  answer pairs. We test our approach using a manually labeled
		  email data collected using a crowd-sourcing annotation
		  study. Our findings have implications for designing email
		  clients and intelligent agents that support question
		  answering and information lookup in email.},
  booktitle	= {The 41st International ACM SIGIR Conference on Research
		  &amp; Development in Information Retrieval},
  pages		= {345–354},
  numpages	= {10},
  keywords	= {email, information retrieval, question answering},
  location	= {Ann Arbor, MI, USA},
  series	= {SIGIR '18}
}

@InProceedings{	  10.1145/3278721.3278778,
  author	= {Zhao, Jianxin and Mortier, Richard and Crowcroft, Jon and
		  Wang, Liang},
  title		= {Privacy-Preserving Machine Learning Based Data Analytics
		  on Edge Devices},
  year		= {2018},
  isbn		= {9781450360128},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3278721.3278778},
  doi		= {10.1145/3278721.3278778},
  abstract	= {Emerging Machine Learning (ML) techniques, such as Deep
		  Neural Network, are widely used in today's applications and
		  services. However, with social awareness of privacy and
		  personal data rapidly rising, it becomes a pressing and
		  challenging societal issue to both keep personal data
		  private and benefit from the data analytics power of ML
		  techniques at the same time. In this paper, we argue that
		  to avoid those costs, reduce latency in data processing,
		  and minimise the raw data revealed to service providers,
		  many future AI and ML services could be deployed on users'
		  devices at the Internet edge rather than putting everything
		  on the cloud. Moving ML-based data analytics from cloud to
		  edge devices brings a series of challenges. We make three
		  contributions in this paper. First, besides the widely
		  discussed resource limitation on edge devices, we further
		  identify two other challenges that are not yet recognised
		  in existing literature: lack of suitable models for users,
		  and difficulties in deploying services for users. Second,
		  we present preliminary work of the first systematic
		  solution, i.e. Zoo, to fully support the construction,
		  composing, and deployment of ML models on edge and local
		  devices. Third, in the deployment example, ML service are
		  proved to be easy to compose and deploy with Zoo.
		  Evaluation shows its superior performance compared with
		  state-of-art deep learning platforms and Google ML
		  services.},
  booktitle	= {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {341–346},
  numpages	= {6},
  keywords	= {edge computing, machine learning, privacy},
  location	= {New Orleans, LA, USA},
  series	= {AIES '18}
}

@InProceedings{	  10.1145/3216122.3216173,
  author	= {Vaira, Lucia and Bochicchio, Mario A. and Conte, Matteo
		  and Casaluci, Francesco Margiotta and Melpignano, Antonio},
  title		= {MamaBot: a System based on ML and NLP for supporting Women
		  and Families during Pregnancy},
  year		= {2018},
  isbn		= {9781450365277},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3216122.3216173},
  doi		= {10.1145/3216122.3216173},
  abstract	= {Artificial intelligence is transforming healthcare with a
		  profound paradigm shift impacting diagnostic techniques,
		  drug discovery, health analytics, interventions and much
		  more. In this paper we focus on exploiting AI-based chatbot
		  systems, mainly based on machine learning algorithms and
		  Natural Language Processing, to understand and respond to
		  needs of patients and their families. In particular, we
		  describe an application scenario for an AI-chatbot
		  delivering support to pregnant women, mothers, and families
		  with young children, by giving them help and instructions
		  in relevant situations.},
  booktitle	= {Proceedings of the 22nd International Database Engineering
		  &amp; Applications Symposium},
  pages		= {273–277},
  numpages	= {5},
  keywords	= {Artificial Intelligence, Chatbot, Machine Learning,
		  Natural Language Processing, eHealth, mHealth},
  location	= {Villa San Giovanni, Italy},
  series	= {IDEAS '18}
}

@InProceedings{	  10.1145/3366423.3380226,
  author	= {Chen, Jiaoyan and Chen, Xi and Horrocks, Ian and B.
		  Myklebust, Erik and Jimenez-Ruiz, Ernesto},
  title		= {Correcting Knowledge Base Assertions},
  year		= {2020},
  isbn		= {9781450370233},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3366423.3380226},
  doi		= {10.1145/3366423.3380226},
  abstract	= {The usefulness and usability of knowledge bases (KBs) is
		  often limited by quality issues. One common issue is the
		  presence of erroneous assertions, often caused by lexical
		  or semantic confusion. We study the problem of correcting
		  such assertions, and present a general correction framework
		  which combines lexical matching, semantic embedding, soft
		  constraint mining and semantic consistency checking. The
		  framework is evaluated using DBpedia and an enterprise
		  medical KB.},
  booktitle	= {Proceedings of The Web Conference 2020},
  pages		= {1537–1547},
  numpages	= {11},
  keywords	= {Assertion Correction, Consistency Checking, Constraint
		  Mining, Knowledge Base Quality, Semantic Embedding},
  location	= {Taipei, Taiwan},
  series	= {WWW '20}
}

@InProceedings{	  10.1145/3106426.3109420,
  author	= {Franzoni, Valentina and Li, Yuanxi and Mengoni, Paolo},
  title		= {A path-based model for emotion abstraction on facebook
		  using sentiment analysis and taxonomy knowledge},
  year		= {2017},
  isbn		= {9781450349512},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3106426.3109420},
  doi		= {10.1145/3106426.3109420},
  abstract	= {Each term in a short text can potentially convey emotional
		  meaning. Facebook comments and shared posts often convey
		  human biases, which play a pivotal role in information
		  spreading and content consumption. Such bias is at the
		  basis of human-generated content, and capable of conveying
		  contexts which shape the opinion of users through the
		  social media flow of information. Starting from the
		  observation that a separation in topic clusters, i.e.
		  sub-contexts, spontaneously occur if evaluated by human
		  common sense, this work introduces a process for automated
		  extraction of sub-context in Facebook. Basing on emotional
		  abstraction and valence, the automated extraction is
		  exploited through a class of path-based semantic similarity
		  measures and sentiment analysis. Experimental results are
		  obtained using validated clustering techniques on such
		  features, on the domain of information security, over a
		  sample of over 9 million page users. An additional expert
		  evaluation of clusters in tag clouds confirms that the
		  proposed automated algorithm for emotional abstraction
		  clusters Facebook comments compatibly with human common
		  sense. The baseline methods rely on the robust notion of
		  collective concept similarity.},
  booktitle	= {Proceedings of the International Conference on Web
		  Intelligence},
  pages		= {947–952},
  numpages	= {6},
  keywords	= {artificial intelligence, collective knowledge, data
		  mining, emotional abstraction, knowledge discovery,
		  semantic distance, sentiment analysis, word similarity},
  location	= {Leipzig, Germany},
  series	= {WI '17}
}

@Article{	  10.1145/3310254,
  author	= {Jha, Kishlay and Xun, Guangxu and Gopalakrishnan,
		  Vishrawas and Zhang, Aidong},
  title		= {DWE-Med: Dynamic Word Embeddings for Medical Domain},
  year		= {2019},
  issue_date	= {April 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {2},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3310254},
  doi		= {10.1145/3310254},
  abstract	= {Recent advances in unsupervised language processing
		  methods have created an opportunity to exploit massive text
		  corpora for developing high-quality vector space
		  representation (also known as word embeddings) of words.
		  Towards this direction, practitioners have developed and
		  applied several data driven embedding models with quite
		  good rate of success. However, a drawback of these models
		  lies in their premise of static context; wherein, the
		  meaning of a word is assumed to remain the same over the
		  period of time. This is limiting because it is known that
		  the semantic meaning of a concept evolves over time. While
		  such semantic drifts are routinely observed in almost all
		  the domains; their effect is acute in domain such as
		  biomedicine, where the semantic meaning of a concept
		  changes relatively fast. To address this, in this study, we
		  aim to learn temporally aware vector representation of
		  medical concepts from the timestamped text data, and in
		  doing so provide a systematic approach to formalize the
		  problem. More specifically, a dynamic word embedding based
		  model that jointly learns the temporal characteristics of
		  medical concepts and performs across time-alignment is
		  proposed. Apart from capturing the evolutionary
		  characteristics in an optimal manner, the model also
		  factors in the implicit medical properties useful for a
		  variety of bio-medical applications. Empirical studies
		  conducted on two important bio-medical use cases validates
		  the effectiveness of the proposed approach and suggests
		  that the model not only learns quality embeddings but also
		  facilitates intuitive trajectory visualizations.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= mar,
  articleno	= {19},
  numpages	= {21},
  keywords	= {Biomedical domain, temporal dynamics, word embeddings}
}

@InProceedings{	  10.1145/3340555.3353737,
  author	= {Soleymani, Mohammad and Stefanov, Kalin and Kang, Sin-Hwa
		  and Ondras, Jan and Gratch, Jonathan},
  title		= {Multimodal Analysis and Estimation of Intimate
		  Self-Disclosure},
  year		= {2019},
  isbn		= {9781450368605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3340555.3353737},
  doi		= {10.1145/3340555.3353737},
  abstract	= {Self-disclosure to others has a proven benefit for one’s
		  mental health. It is shown that disclosure to computers can
		  be similarly beneficial for emotional and psychological
		  well-being. In this paper, we analyzed verbal and nonverbal
		  behavior associated with self-disclosure in two datasets
		  containing structured human-human and human-agent
		  interviews from more than 200 participants. Correlation
		  analysis of verbal and nonverbal behavior revealed that
		  linguistic features such as affective and cognitive content
		  in verbal behavior, and nonverbal behavior such as head
		  gestures are associated with intimate self-disclosure. A
		  multimodal deep neural network was developed to
		  automatically estimate the level of intimate
		  self-disclosure from verbal and nonverbal behavior. Between
		  modalities, verbal behavior was the best modality for
		  estimating self-disclosure within-corpora achieving r =
		  0.66. However, the cross-corpus evaluation demonstrated
		  that nonverbal behavior can outperform language modality in
		  cross-corpus evaluation. Such automatic models can be
		  deployed in interactive virtual agents or social robots to
		  evaluate rapport and guide their conversational strategy.},
  booktitle	= {2019 International Conference on Multimodal Interaction},
  pages		= {59–68},
  numpages	= {10},
  keywords	= {natural language understanding, neural networks, nonverbal
		  behavior, self-disclosure},
  location	= {Suzhou, China},
  series	= {ICMI '19}
}

@InProceedings{	  10.1145/3018896.3036392,
  author	= {Xiao, Bin and Rahmani, Rahim},
  title		= {A deep relation learning method for IoT interoperability
		  enhancement within semantic formalization framework},
  year		= {2017},
  isbn		= {9781450347747},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3018896.3036392},
  doi		= {10.1145/3018896.3036392},
  abstract	= {Internet of Things (IoT) is facing with the
		  interoperability issue due to the massive amount of
		  heterogeneous entities (both physical and virtual entities)
		  constantly generating heterogeneous data objects; semantic
		  formalization has been widely recognized as a basis for the
		  IoT interoperability, by which IoT can acquire the ability
		  to comprehend data and further recognize the logic
		  relations among heterogeneous IoT entities and
		  heterogeneous data objects, thus to establish mutual
		  understanding between each other to support with
		  interoperability. Even semantic-driven track has emphasizes
		  a lot on the logic relations in connection to the service
		  rules and policies for interoperability, it is important
		  that the quantity-driven relations should be also explored
		  with adhering to the framework of semantic formalization.
		  This paper explores a Deep Recursive Auto-encoders formed
		  data relation learner in line with the semantic framework,
		  which supports the data interoperability enhancement in a
		  quantity-driven way based on the logic-driven framework.
		  The learner starts with representing the virtual IoT
		  entities via feature extraction; based on that, learner is
		  trained in a manner of considering the surrounding
		  relations of the targeted entity. As a baseline, a contrast
		  learner with "regular" structure has been proposed which
		  cannot functionally support semantic framework, even though
		  the semantic formalization is indispensable; regardless the
		  limitations in lab environment, the conducted experiments
		  show that the proposed learner performs a bit better than
		  the contrast learner under the same conditions. So that,
		  the proposed method can synergistically enhances the
		  interoperability within a semantic formalization
		  framework.},
  booktitle	= {Proceedings of the Second International Conference on
		  Internet of Things, Data and Cloud Computing},
  articleno	= {149},
  numpages	= {8},
  keywords	= {big data, deep recursive neural network, internet of
		  things, interoperability, semantic formalization},
  location	= {Cambridge, United Kingdom},
  series	= {ICC '17}
}

@InProceedings{	  10.1145/3308560.3316584,
  author	= {Wang, Dongsheng and Li, Qiuchi and Chaves Lima, Lucas and
		  Grue Simonsen, Jakob and Lioma, Christina},
  title		= {Contextual Compositionality Detection with External
		  Knowledge Bases and Word Embeddings},
  year		= {2019},
  isbn		= {9781450366755},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3308560.3316584},
  doi		= {10.1145/3308560.3316584},
  abstract	= {When the meaning of a phrase cannot be inferred from the
		  individual meanings of its words (e.g., hot dog), that
		  phrase is said to be non-compositional. Automatic
		  compositionality detection in multi-word phrases is
		  critical in any application of semantic processing, such as
		  search engines [9]; failing to detect non-compositional
		  phrases can hurt system effectiveness notably. Existing
		  research treats phrases as either compositional or
		  non-compositional in a deterministic manner. In this paper,
		  we operationalize the viewpoint that compositionality is
		  contextual rather than deterministic, i.e., that whether a
		  phrase is compositional or non-compositional depends on its
		  context. For example, the phrase “green card” is
		  compositional when referring to a green colored card,
		  whereas it is non-compositional when meaning permanent
		  residence authorization. We address the challenge of
		  detecting this type of contextual compositionality as
		  follows: given a multi-word phrase, we enrich the word
		  embedding representing its semantics with evidence about
		  its global context (terms it often collocates with) as well
		  as its local context (narratives where that phrase is used,
		  which we call usage scenarios). We further extend this
		  representation with information extracted from external
		  knowledge bases. The resulting representation incorporates
		  both localized context and more general usage of the phrase
		  and allows to detect its compositionality in a
		  non-deterministic and contextual way. Empirical evaluation
		  of our model on a dataset of phrase compositionality1,
		  manually collected by crowdsourcing contextual
		  compositionality assessments, shows that our model
		  outperforms state-of-the-art baselines notably on detecting
		  phrase compositionality.},
  booktitle	= {Companion Proceedings of The 2019 World Wide Web
		  Conference},
  pages		= {317–323},
  numpages	= {7},
  keywords	= {Compositionality detection, Knowledge base, Word
		  embedding},
  location	= {San Francisco, USA},
  series	= {WWW '19}
}

@InProceedings{	  10.1145/3090354.3090380,
  author	= {Mifrah, Sara and Ben Lahmar, El Habib},
  title		= {Semantico-automatic Evaluation of Scientific Papers: State
		  of the Art},
  year		= {2017},
  isbn		= {9781450348522},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3090354.3090380},
  doi		= {10.1145/3090354.3090380},
  abstract	= {Natural Language Processing (NLP) and Automatic Language
		  Processing (ALP) are fields that becomes more attractive
		  for researchers in the study of potential and emerging
		  applications. This article presents a state of the art,
		  which aims to classify scientific documents automatically;
		  Based on the semantic aspect defining the semantic links
		  between the document and its references, determining its
		  relevant subjects, and then comparing them with other cited
		  documents. Citations considered as a bridge between a
		  scientific paper and its references. The realization of
		  this work requires the analysis of citations and the
		  detection of all subjects "Topics" covered in the
		  document.},
  booktitle	= {Proceedings of the 2nd International Conference on Big
		  Data, Cloud and Applications},
  articleno	= {25},
  numpages	= {6},
  keywords	= {Semantic evaluation, automatic analysis, citation
		  analysis, detection of subjects, scientific paper},
  location	= {Tetouan, Morocco},
  series	= {BDCA'17}
}

@InProceedings{	  10.1145/3343031.3350972,
  author	= {Sheng, Shurong and Moens, Marie-Francine},
  title		= {Generating Captions for Images of Ancient Artworks},
  year		= {2019},
  isbn		= {9781450368896},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3343031.3350972},
  doi		= {10.1145/3343031.3350972},
  abstract	= {The neural encoder-decoder framework is widely adopted for
		  image captioning of natural images. However, few works have
		  contributed to generating captions for cultural images
		  using this scheme. In this paper, we propose an artwork
		  type enriched image captioning model where the encoder
		  represents an input artwork image as a 512-dimensional
		  vector and the decoder generates a corresponding caption
		  based on the input image vector. The artwork type is first
		  predicted by a convolutional neural network classifier and
		  then merged into the decoder. We investigate multiple
		  approaches to integrate the artwork type into the
		  captioning model among which is one that applies a
		  step-wise weighted sum of the artwork type vector and the
		  hidden representation vector of the decoder. This model
		  outperforms three baseline image captioning models for a
		  Chinese art image captioning dataset on all evaluation
		  metrics. One of the baselines is a state-of-the-art
		  approach fusing textual image attributes into the
		  captioning model for natural images. The proposed model
		  also obtains promising results for another Egyptian art
		  image captioning dataset.},
  booktitle	= {Proceedings of the 27th ACM International Conference on
		  Multimedia},
  pages		= {2478–2486},
  numpages	= {9},
  keywords	= {artwork type, image captioning, neural encoder-decoder},
  location	= {Nice, France},
  series	= {MM '19}
}

@Article{	  10.1145/3380954,
  author	= {Nie, Liqiang and Li, Yongqi and Feng, Fuli and Song,
		  Xuemeng and Wang, Meng and Wang, Yinglong},
  title		= {Large-Scale Question Tagging via Joint Question-Topic
		  Embedding Learning},
  year		= {2020},
  issue_date	= {April 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {38},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3380954},
  doi		= {10.1145/3380954},
  abstract	= {Recent years have witnessed a flourishing of
		  community-driven question answering (cQA), like Yahoo!
		  Answers and AnswerBag, where people can seek precise
		  information. After 2010, some novel cQA systems, including
		  Quora and Zhihu, gained momentum. Besides interactions, the
		  latter enables users to label the questions with topic tags
		  that highlight the key points conveyed in the questions. In
		  this article, we shed light on automatically annotating a
		  newly posted question with topic tags that are predefined
		  and preorganized into a directed acyclic graph. To
		  accomplish this task, we present an end-to-end deep
		  interactive embedding model to jointly learn the embeddings
		  of questions and topics by projecting them into the same
		  space for a similarity measure. In particular, we first
		  learn the embeddings of questions and topic tags by two
		  deep parallel models. Thereinto, we regularize the
		  embeddings of topic tags via fully exploring their
		  hierarchical structures, which is able to alleviate the
		  problem of imbalanced topic distribution. Thereafter, we
		  interact each question embedding with the topic tag matrix,
		  i.e., all the topic tag embeddings. Following that, a
		  sigmoid cross-entropy loss is appended to reward the
		  positive question-topic pairs and penalize the negative
		  ones. To justify our model, we have conducted extensive
		  experiments on an unprecedented large-scale social QA
		  dataset obtained from Zhihu.com, and the experimental
		  results demonstrate that our model achieves superior
		  performance to several state-of-the-art baselines.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= feb,
  articleno	= {20},
  numpages	= {23},
  keywords	= {CQA, Question tagging, embedding learning, topic
		  hierarchy}
}

@InBook{	  10.1145/3233795.3233798,
  author	= {Johnston, Michael},
  title		= {Multimodal integration for interactive conversational
		  systems},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233798},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {21–76},
  numpages	= {56}
}

@InProceedings{	  10.1145/3077136.3080825,
  author	= {Glater, Rafael and Santos, Rodrygo L.T. and Ziviani,
		  Nivio},
  title		= {Intent-Aware Semantic Query Annotation},
  year		= {2017},
  isbn		= {9781450350228},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3077136.3080825},
  doi		= {10.1145/3077136.3080825},
  abstract	= {Query understanding is a challenging task primarily due to
		  the inherent ambiguity of natural language. A common
		  strategy for improving the understanding of natural
		  language queries is to annotate them with semantic
		  information mined from a knowledge base. Nevertheless,
		  queries with different intents may arguably benefit from
		  specialized annotation strategies. For instance, some
		  queries could be effectively annotated with a single entity
		  or an entity attribute, others could be better represented
		  by a list of entities of a single type or by entities of
		  multiple distinct types, and others may be simply
		  ambiguous. In this paper, we propose a framework for
		  learning semantic query annotations suitable to the target
		  intent of each individual query. Thorough experiments on a
		  publicly available benchmark show that our proposed
		  approach can significantly improve state-of-the-art
		  intent-agnostic approaches based on Markov random fields
		  and learning to rank. Our results further demonstrate the
		  consistent effectiveness of our approach for queries of
		  various target intents, lengths, and difficulty levels, as
		  well as its robustness to noise in intent detection.},
  booktitle	= {Proceedings of the 40th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {485–494},
  numpages	= {10},
  keywords	= {intent-aware, learning to rank, semantic query
		  annotation},
  location	= {Shinjuku, Tokyo, Japan},
  series	= {SIGIR '17}
}

@InProceedings{	  10.1109/icse.2017.9,
  author	= {Guo, Jin and Cheng, Jinghui and Cleland-Huang, Jane},
  title		= {Semantically enhanced software traceability using deep
		  learning techniques},
  year		= {2017},
  isbn		= {9781538638682},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ICSE.2017.9},
  doi		= {10.1109/ICSE.2017.9},
  abstract	= {In most safety-critical domains the need for trace-ability
		  is prescribed by certifying bodies. Trace links are
		  generally created among requirements, design, source code,
		  test cases and other artifacts; however, creating such
		  links manually is time consuming and error prone. Automated
		  solutions use information retrieval and machine learning
		  techniques to generate trace links; however, current
		  techniques fail to understand semantics of the software
		  artifacts or to integrate domain knowledge into the tracing
		  process and therefore tend to deliver imprecise and
		  inaccurate results. In this paper, we present a solution
		  that uses deep learning to incorporate requirements
		  artifact semantics and domain knowledge into the tracing
		  solution. We propose a tracing network architecture that
		  utilizes Word Embedding and Recurrent Neural Network (RNN)
		  models to generate trace links. Word embedding learns word
		  vectors that represent knowledge of the domain corpus and
		  RNN uses these word vectors to learn the sentence semantics
		  of requirements artifacts. We trained 360 different
		  configurations of the tracing network using existing trace
		  links in the Positive Train Control domain and identified
		  the Bidirectional Gated Recurrent Unit (BI-GRU) as the best
		  model for the tracing task. BI-GRU significantly
		  out-performed state-of-the-art tracing methods including
		  the Vector Space Model and Latent Semantic Indexing.},
  booktitle	= {Proceedings of the 39th International Conference on
		  Software Engineering},
  pages		= {3–14},
  numpages	= {12},
  keywords	= {deep learning, recurrent neural network, semantic
		  representation, traceability},
  location	= {Buenos Aires, Argentina},
  series	= {ICSE '17}
}

@Article{	  10.1145/3349527,
  author	= {Tamine, Lynda and Soulier, Laure and Nguyen, Gia-Hung and
		  Souf, Nathalie},
  title		= {Offline versus Online Representation Learning of Documents
		  Using External Knowledge},
  year		= {2019},
  issue_date	= {October 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {37},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3349527},
  doi		= {10.1145/3349527},
  abstract	= {An intensive recent research work investigated the
		  combined use of hand-curated knowledge resources and
		  corpus-driven resources to learn effective text
		  representations. The overall learning process could be run
		  by online revising the learning objective or by offline
		  refining an original learned representation. The
		  differentiated impact of each of the learning approaches on
		  the quality of the learned representations has not been
		  studied so far in the literature. This article focuses on
		  the design of comparable offline vs. online
		  knowledge-enhanced document representation learning models
		  and the comparison of their effectiveness using a set of
		  standard IR and NLP downstream tasks. The results of
		  quantitative and qualitative analyses show that (1) offline
		  vs. online learning approaches have dissimilar result
		  trends regarding the task as well as the dataset
		  distribution counts with regard to domain application; (2)
		  while considering external knowledge resources is
		  undoubtedly beneficial, the way used to express relational
		  constraints could affect semantic inference effectiveness.
		  The findings of this work present opportunities for the
		  design of future representation learning models, but also
		  for providing insights about the evaluation of such
		  models.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= sep,
  articleno	= {42},
  numpages	= {34},
  keywords	= {Representation learning, information retrieval, knowledge
		  resources, natural language processing}
}

@InBook{	  10.1145/3122865.3122867,
  author	= {Wu, Zuxuan and Yao, Ting and Fu, Yanwei and Jiang,
		  Yu-Gang},
  title		= {Deep learning for video classification and captioning},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122867},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {3–29},
  numpages	= {27}
}

@Article{	  10.1145/3290768.3290775,
  author	= {Zhao, Pengfei and Ma, Jian and Hua, Zhongsheng and Fang,
		  Shijian},
  title		= {Academic Social Network-Based Recommendation Approach for
		  Knowledge Sharing},
  year		= {2018},
  issue_date	= {November 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {49},
  number	= {4},
  issn		= {0095-0033},
  url		= {https://doi.org/10.1145/3290768.3290775},
  doi		= {10.1145/3290768.3290775},
  abstract	= {Academic information overload has brought researchers
		  great difficulty due to the rapid growth of scientific
		  articles. Methods have been proposed to help professional
		  readers find relevant articles on the basis of their
		  publications. Although effectively sharing publications is
		  essential to spreading knowledge and ideas, few studies
		  have focused on knowledge sharing from an author
		  perspective. This study leverages the online academic
		  social network to propose a recommendation approach for
		  knowledge sharing. In our approach, we integrate
		  researcher-level and document-level analyses in the same
		  model. Our model works in two stages: 1) researcher-level
		  analysis and 2) document-level analysis. The former
		  combines research topic relevance, social relations, and
		  research quality dimension, and the latter uses the machine
		  learning method to learn the vector representation for each
		  word. Online social behavior information is also leveraged
		  to enhance readers' short-term interests. Our approach is
		  deployed in ScholarMate, a prevalent academic social
		  network. Compared with other baseline methods (CB, LDA, and
		  part of the proposed approach), our approach significantly
		  improves the accuracy of recommendations. Moreover, our
		  method can disseminate papers efficiently to readers who
		  have no publications.},
  journal	= {SIGMIS Database},
  month		= nov,
  pages		= {78–91},
  numpages	= {14},
  keywords	= {academic social network, knowledge sharing, recommender
		  systems}
}

@InProceedings{	  10.1145/3357384.3357967,
  author	= {Zou, Jie and Kanoulas, Evangelos},
  title		= {Learning to Ask: Question-based Sequential Bayesian
		  Product Search},
  year		= {2019},
  isbn		= {9781450369763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3357384.3357967},
  doi		= {10.1145/3357384.3357967},
  abstract	= {Product search is generally recognized as the first and
		  foremost stage of online shopping and thus significant for
		  users and retailers of e-commerce. Most of the traditional
		  retrieval methods use some similarity functions to match
		  the user's query and the document that describes a product,
		  either directly or in a latent vector space. However, user
		  queries are often too general to capture the minute details
		  of the specific product that a user is looking for. In this
		  paper, we propose a novel interactive method to effectively
		  locate the best matching product. The method is based on
		  the assumption that there is a set of candidate questions
		  for each product to be asked. In this work, we instantiate
		  this candidate set by making the hypothesis that products
		  can be discriminated by the entities that appear in the
		  documents associated with them. We propose a Question-based
		  Sequential Bayesian Product Search method, QSBPS, which
		  directly queries users on the expected presence of entities
		  in the relevant product documents. The method learns the
		  product relevance as well as the reward of the potential
		  questions to be asked to the user by being trained on the
		  search history and purchase behavior of a specific user
		  together with that of other users. The experimental results
		  show that the proposed method can greatly improve the
		  performance of product search compared to the
		  state-of-the-art baselines.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Information and Knowledge Management},
  pages		= {369–378},
  numpages	= {10},
  keywords	= {bayesian search, learning to asking, product search,
		  question-based search},
  location	= {Beijing, China},
  series	= {CIKM '19}
}

@InProceedings{	  10.1145/3106426.3109416,
  author	= {Ohbe, Tatsuya and Ozono, Tadachika and Shintani,
		  Toramatsu},
  title		= {A sentiment polarity classifier for regional event
		  reputation analysis},
  year		= {2017},
  isbn		= {9781450349512},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3106426.3109416},
  doi		= {10.1145/3106426.3109416},
  abstract	= {It is important to analyze the reputation or demands for a
		  regional event, such as a school festival. In our work, we
		  use sentiment polarity classification in order to
		  coordinate regional event reputation. We proposed sentiment
		  polarity classification based on bag-of-words models in the
		  previous works. To get over the traditional models, we
		  proposed several classifier models based on deep learning
		  models. As the application, we also described the overview
		  of a system supports to analyze regional event reputation
		  and an example of regional event analysis using our system.
		  In this paper, we described how to improve the performance
		  of the sentiment polarity classification using deep
		  learning models. We compared the performance of four models
		  in terms of the classification accuracy and the training
		  speed. We found the Convolutional Neural Networks based
		  model, three words convolutions, was the best model among
		  the four models.},
  booktitle	= {Proceedings of the International Conference on Web
		  Intelligence},
  pages		= {1207–1213},
  numpages	= {7},
  keywords	= {convolutional neural networks, recurrent neural networks,
		  sentiment polarity classification, sentiment
		  visualization},
  location	= {Leipzig, Germany},
  series	= {WI '17}
}

@Article{	  10.1109/taslp.2017.2788182,
  author	= {Yu, Liang-Chih and Wang, Jin and Lai, K. Robert and Zhang,
		  Xuejie},
  title		= {Refining Word Embeddings Using Intensity Scores for
		  Sentiment Analysis},
  year		= {2018},
  issue_date	= {March 2018},
  publisher	= {IEEE Press},
  volume	= {26},
  number	= {3},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2017.2788182},
  doi		= {10.1109/TASLP.2017.2788182},
  abstract	= {Word embeddings that provide continuous low-dimensional
		  vector representations of words have been extensively used
		  for various natural language processing tasks. However,
		  existing context-based word embeddings such as Word2vec and
		  GloVe typically fail to capture sufficient sentiment
		  information, which may result in words with similar vector
		  representations having an opposite sentiment polarity e.g.,
		  good and bad, thus degrading sentiment analysis
		  performance. To tackle this problem, recent studies have
		  suggested learning sentiment embeddings to incorporate the
		  sentiment polarity positive and negative information from
		  labeled corpora. This study adopts another strategy to
		  learn sentiment embeddings. Instead of creating a new word
		  embedding from labeled corpora, we propose a word vector
		  refinement model to refine existing pretrained word vectors
		  using real-valued sentiment intensity scores provided by
		  sentiment lexicons. The idea of the refinement model is to
		  improve each word vector such that it can be closer in the
		  lexicon to both semantically and sentimentally similar
		  words i.e., those with similar intensity scores and further
		  away from sentimentally dissimilar words i.e., those with
		  dissimilar intensity scores. An obvious advantage of the
		  proposed method is that it can be applied to any pretrained
		  word embeddings. In addition, the intensity scores can
		  provide more fine-grained real-valued sentiment information
		  than binary polarity labels to guide the refinement
		  process. Experimental results show that the proposed
		  refinement model can improve both conventional word
		  embeddings and previously proposed sentiment embeddings for
		  binary, ternary, and fine-grained sentiment classification
		  on the SemEval and Stanford Sentiment Treebank datasets.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {671–681},
  numpages	= {11}
}

@InProceedings{	  10.1145/3132847.3132947,
  author	= {Huang, Zhengjie and Ye, Zi and Li, Shuangyin and Pan,
		  Rong},
  title		= {Length Adaptive Recurrent Model for Text Classification},
  year		= {2017},
  isbn		= {9781450349185},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3132847.3132947},
  doi		= {10.1145/3132847.3132947},
  abstract	= {In recent years, recurrent neural networks have been
		  widely used for various text classification tasks. However,
		  most of the recurrent architectures will not assign a class
		  label to a text until they read the last word, while human
		  beings are able to determine the text class before reading
		  the whole text. In this paper, we propose a Length Adaptive
		  Recurrent Model (LARM) which can automatically determine
		  the minimum text length that is necessary to perform the
		  classification. With three parts includingReader, Predictor
		  andAgent, our model is designed to read a text word by
		  word, and terminate the process when the adequate
		  information has been caught for the text classification
		  task. The experimental results show that our model has
		  comparable or even better performance compared to the
		  vanilla LSTM when both are fed with partial text input.
		  Besides, we can speed up text classification by truncating
		  the text when sufficient evidence is found for
		  classification. Furthermore, we also visualize our model
		  and show that our model works like human beings, who can
		  gradually come up with the general idea of a text while
		  reading texts sequentially.},
  booktitle	= {Proceedings of the 2017 ACM on Conference on Information
		  and Knowledge Management},
  pages		= {1019–1027},
  numpages	= {9},
  keywords	= {recurrent neural network, text classification},
  location	= {Singapore, Singapore},
  series	= {CIKM '17}
}

@Article{	  10.5555/3241691.3241693,
  author	= {Gatt, Albert and Krahmer, Emiel},
  title		= {Survey of the state of the art in natural language
		  generation: core tasks, applications and evaluation},
  year		= {2018},
  issue_date	= {January 2018},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {61},
  number	= {1},
  issn		= {1076-9757},
  abstract	= {This paper surveys the current state of the art in Natural
		  Language Generation (NLG), defined as the task of
		  generating text or speech from non-linguistic input. A
		  survey of NLG is timely in view of the changes that the
		  field has undergone over the past two decades, especially
		  in relation to new (usually data-driven) methods, as well
		  as new applications of NLG technology. This survey
		  therefore aims to (a) give an up-to-date synthesis of
		  research on the core tasks in NLG and the architectures
		  adopted in which such tasks are organised; (b) highlight a
		  number of recent research topics that have arisen partly as
		  a result of growing synergies between NLG and other areas
		  of artifical intelligence; (c) draw attention to the
		  challenges in NLG evaluation, relating them to similar
		  challenges faced in other areas of nlp, with an emphasis on
		  different evaluation methods and the relationships between
		  them.},
  journal	= {J. Artif. Int. Res.},
  month		= jan,
  pages		= {65–170},
  numpages	= {106}
}

@InBook{	  10.1145/3122865.3122878,
  title		= {Bibliography},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122878},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {315–377},
  numpages	= {63}
}

@InProceedings{	  10.1145/3127526.3127529,
  author	= {Accuosto, Pablo and Ronzano, Francesco and Ferr\'{e}s,
		  Daniel and Saggion, Horacio},
  title		= {Multi-level mining and visualization of scientific text
		  collections: Exploring a bi-lingual scientific repository},
  year		= {2017},
  isbn		= {9781450353885},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3127526.3127529},
  doi		= {10.1145/3127526.3127529},
  abstract	= {We present a system to mine and visualize collections of
		  scientific documents by semantically browsing information
		  extracted from single publications or aggregated throughout
		  corpora of articles. The text mining tool performs deep
		  analysis of document collections allowing the extraction
		  and interpretation of research paper's contents. In
		  addition to the extraction and enrichment of documents with
		  metadata (titles, authors, affiliations, etc), the deep
		  analysis performed comprises semantic interpretation,
		  rhetorical analysis of sentences, triple-based information
		  extraction, and text summarization. The visualization
		  components allow geographical-based exploration of
		  collections, topic-evolution interpretation, and
		  collaborative network analysis among others. The paper
		  presents a case study of a bi-lingual collection in the
		  field of Natural Language Processing (NLP).},
  booktitle	= {Proceedings of the 6th International Workshop on Mining
		  Scientific Publications},
  pages		= {9–16},
  numpages	= {8},
  keywords	= {Big Scientific Data, Data Visualization, Information
		  Extraction, Language Resources, PDF Conversion, Scientific
		  Text Mining},
  location	= {Toronto, ON, Canada},
  series	= {WOSP 2017}
}

@Article{	  10.1145/3237189,
  author	= {Marge, Matthew and Rudnicky, Alexander I.},
  title		= {Miscommunication Detection and Recovery in Situated
		  Human–Robot Dialogue},
  year		= {2019},
  issue_date	= {March 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {1},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3237189},
  doi		= {10.1145/3237189},
  abstract	= {Even without speech recognition errors, robots may face
		  difficulties interpreting natural-language instructions. We
		  present a method for robustly handling miscommunication
		  between people and robots in task-oriented spoken dialogue.
		  This capability is implemented in TeamTalk, a
		  conversational interface to robots that supports detection
		  and recovery from the situated grounding problems of
		  referential ambiguity and impossible actions. We introduce
		  a representation that detects these problems and a
		  nearest-neighbor learning algorithm that selects recovery
		  strategies for a virtual robot. When the robot encounters a
		  grounding problem, it looks back on its interaction history
		  to consider how it resolved similar situations. The
		  learning method is trained initially on crowdsourced data
		  but is then supplemented by interactions from a
		  longitudinal user study in which six participants performed
		  navigation tasks with the robot. We compare results
		  collected using a general model to user-specific models and
		  find that user-specific models perform best on measures of
		  dialogue efficiency, while the general model yields the
		  highest agreement with human judges. Our overall
		  contribution is a novel approach to detecting and
		  recovering from miscommunication in dialogue by including
		  situated context, namely, information from a robot’s path
		  planner and surroundings.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= feb,
  articleno	= {3},
  numpages	= {40},
  keywords	= {Human–robot communication, human–robot interaction,
		  language grounding, physically situated dialogue,
		  spoken-dialogue systems}
}

@InBook{	  10.1145/3233795.3233801,
  author	= {Feld, Michael and Neβelrath, Robert and Schwartz, Tim},
  title		= {Software platforms and toolkits for building multimodal
		  systems and applications},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233801},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {145–190},
  numpages	= {46}
}

@InBook{	  10.1145/3233795.3233814,
  title		= {Index},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233814},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {705–745},
  numpages	= {41}
}

@Article{	  10.1145/3078841,
  author	= {Mills, Chris and Bavota, Gabriele and Haiduc, Sonia and
		  Oliveto, Rocco and Marcus, Andrian and Lucia, Andrea De},
  title		= {Predicting Query Quality for Applications of Text
		  Retrieval to Software Engineering Tasks},
  year		= {2017},
  issue_date	= {January 2017},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {26},
  number	= {1},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3078841},
  doi		= {10.1145/3078841},
  abstract	= {Context: Since the mid-2000s, numerous recommendation
		  systems based on text retrieval (TR) have been proposed to
		  support software engineering (SE) tasks such as concept
		  location, traceability link recovery, code reuse, impact
		  analysis, and so on. The success of TR-based solutions
		  highly depends on the query submitted, which is either
		  formulated by the developer or automatically extracted from
		  software artifacts.Aim: We aim at predicting the quality of
		  queries submitted to TR-based approaches in SE. This can
		  lead to benefits for developers and for the quality of
		  software systems alike. For example, knowing when a query
		  is poorly formulated can save developers the time and
		  frustration of analyzing irrelevant search results.
		  Instead, they could focus on reformulating the query. Also,
		  knowing if an artifact used as a query leads to irrelevant
		  search results may uncover underlying problems in the query
		  artifact itself.Method: We introduce an automatic query
		  quality prediction approach for software artifact retrieval
		  by adapting NL-inspired solutions to their use on software
		  data. We present two applications and evaluations of the
		  approach in the context of concept location and
		  traceability link recovery, where TR has been applied most
		  often in SE. For concept location, we use the approach to
		  determine if the list of retrieved code elements is likely
		  to contain code relevant to a particular change request or
		  not, in which case, the queries are good candidates for
		  reformulation. For traceability link recovery, the queries
		  represent software artifacts. In this case, we use the
		  query quality prediction approach to identify artifacts
		  that are hard to trace to other artifacts and may therefore
		  have a low intrinsic quality for TR-based traceability link
		  recovery.Results: For concept location, the evaluation
		  shows that our approach is able to correctly predict the
		  quality of queries in 82% of the cases, on average, using
		  very little training data. In the case of traceability
		  recovery, the proposed approach is able to detect hard to
		  trace artifacts in 74% of the cases, on
		  average.Conclusions: The results of our evaluation on
		  applications for concept location and traceability link
		  recovery indicate that our approach can be used to predict
		  the results of a TR-based approach by assessing the quality
		  of the text query. This can lead to saved effort and time,
		  as well as the identification of software artifacts that
		  may be difficult to trace using TR.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= may,
  articleno	= {3},
  numpages	= {45},
  keywords	= {Text retrieval, artifact traceability, concept location}
}

@InBook{	  10.1145/3233795.3233797,
  title		= {Introduction: toward the design, construction, and
		  deployment of multimodal-multisensor interfaces},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233797},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {1–20},
  numpages	= {20}
}

@InBook{	  10.1145/3233795.3233796,
  title		= {Preface},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233796},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {xvii–xix}
}

@InBook{	  10.1145/3233795.3233808,
  author	= {Sonntag, Daniel},
  title		= {Medical and health systems},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233808},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {423–476},
  numpages	= {54}
}

@Article{	  10.1145/3092698,
  author	= {Kaur, Kiranbir and Sharma, DR. Sandeep and Kahlon, DR.
		  Karanjeet Singh},
  title		= {Interoperability and Portability Approaches in
		  Inter-Connected Clouds: A Review},
  year		= {2017},
  issue_date	= {July 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {50},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3092698},
  doi		= {10.1145/3092698},
  abstract	= {Inter-connected cloud computing is an inherent evolution
		  of Cloud Computing. Numerous benefits provided by
		  connecting clouds have garnered attraction from the
		  academic as well as the industry sector. Just as every new
		  evolution faces challenges, inter-connected clouds have
		  their own set of challenges such as security, monitoring,
		  authorization and identity management, vendor lock-in, and
		  so forth. This article considers the vendor lock-in
		  problem, which is a direct consequence of the lack of
		  interoperability and portability. An extensive literature
		  review by surveying more than 120 papers has been done to
		  analyze and categorize various solutions suggested in
		  literature for solving the interoperability and portability
		  issues of inter-connected clouds. After categorizing the
		  solutions, the literature has been mapped to a specific
		  solution and a comparative analysis of the papers under the
		  same solution has been done. The term “inter-connected
		  clouds” has been used generically in this article to
		  refer to any collaboration of clouds which may be from the
		  user side (Multi-clouds or Aggregated service by Broker) or
		  the provider side (Federated clouds or Hybrid clouds).
		  Lastly, two closely related issues (Brokers and
		  Meta-scheduling) and the remaining challenges of
		  inter-connected clouds are discussed.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {49},
  numpages	= {40},
  keywords	= {Cloud computing, inter-connected clouds, model-based
		  techniques, open solutions, semantic-based techniques,
		  standards}
}

@Article{	  10.1145/3105906,
  author	= {Monperrus, Martin},
  title		= {Automatic Software Repair: A Bibliography},
  year		= {2018},
  issue_date	= {January 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {51},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3105906},
  doi		= {10.1145/3105906},
  abstract	= {This article presents a survey on automatic software
		  repair. Automatic software repair consists of automatically
		  finding a solution to software bugs without human
		  intervention. This article considers all kinds of repairs.
		  First, it discusses behavioral repair where test suites,
		  contracts, models, and crashing inputs are taken as oracle.
		  Second, it discusses state repair, also known as runtime
		  repair or runtime recovery, with techniques such as
		  checkpoint and restart, reconfiguration, and invariant
		  restoration. The uniqueness of this article is that it
		  spans the research communities that contribute to this body
		  of knowledge: software engineering, dependability,
		  operating systems, programming languages, and security. It
		  provides a novel and structured overview of the diversity
		  of bug oracles and repair operators used in the
		  literature.},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  articleno	= {17},
  numpages	= {24},
  keywords	= {Program repair, self-healing software}
}

@Article{	  10.1162/evco_a_00266,
  author	= {Wever, Marcel and van Rooijen, Lorijn and Hamann, Heiko},
  title		= {Multioracle Coevolutionary Learning of Requirements
		  Specifications from Examples in On-The-Fly Markets},
  year		= {2020},
  issue_date	= {Summer 2020},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA},
  volume	= {28},
  number	= {2},
  issn		= {1063-6560},
  url		= {https://doi.org/10.1162/evco_a_00266},
  doi		= {10.1162/evco_a_00266},
  abstract	= {In software engineering, the imprecise requirements of a
		  user are transformed to a formal requirements specification
		  during the requirements elicitation process. This process
		  is usually guided by requirements engineers interviewing
		  the user. We want to partially automate this first step of
		  the software engineering process in order to enable users
		  to specify a desired software system on their own. With our
		  approach, users are only asked to provide exemplary
		  behavioral descriptions. The problem of synthesizing a
		  requirements specification from examples can partially be
		  reduced to the problem of grammatical inference, to which
		  we apply an active coevolutionary learning approach.
		  However, this approach would usually require many feedback
		  queries to be sent to the user. In this work, we extend and
		  generalize our active learning approach to receive
		  knowledge from multiple oracles, also known as proactive
		  learning. The ``user oracle'' represents input received
		  from the user and the “knowledge oracle” represents
		  available, formalized domain knowledge. We call our
		  two-oracle approach the “first apply knowledge then
		  query” (FAKT/Q) algorithm. We compare FAKT/Q to the
		  active learning approach and provide an extensive benchmark
		  evaluation. As result we find that the number of required
		  user queries is reduced and the inference process is sped
		  up significantly. Finally, with so-called On-The-Fly
		  Markets, we present a motivation and an application of our
		  approach where such knowledge is available.},
  journal	= {Evol. Comput.},
  month		= jun,
  pages		= {165–193},
  numpages	= {29},
  keywords	= {Multiobjective optimization, proactive learning,
		  multioracle, coevolution, search-based software
		  engineering, requirements specification, grammatical
		  inference.}
}

@InBook{	  10.1145/3233795.3233802,
  author	= {Allen, James and Andr\'{e}, Elisabeth and Cohen, Philip R.
		  and Hakkani-T\"{u}r, Dilek and Kaplan, Ronald and Lemon,
		  Oliver and Traum, David},
  title		= {Challenge discussion: advancing multimodal dialogue},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233802},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {191–217},
  numpages	= {27}
}

@Article{	  10.1145/3242177,
  author	= {Jarrar, Mustafa and Zaraket, Fadi and Asia, Rami and
		  Amayreh, Hamzeh},
  title		= {Diacritic-Based Matching of Arabic Words},
  year		= {2018},
  issue_date	= {June 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3242177},
  doi		= {10.1145/3242177},
  abstract	= {Words in Arabic consist of letters and short vowel symbols
		  called diacritics inscribed atop regular letters. Changing
		  diacritics may change the syntax and semantics of a word;
		  turning it into another. This results in difficulties when
		  comparing words based solely on string matching. Typically,
		  Arabic NLP applications resort to morphological analysis to
		  battle ambiguity originating from this and other
		  challenges. In this article, we introduce three alternative
		  algorithms to compare two words with possibly different
		  diacritics. We propose the Subsume knowledge-based
		  algorithm, the Imply rule-based algorithm, and the Alike
		  machine-learning-based algorithm. We evaluated the
		  soundness, completeness, and accuracy of the algorithms
		  against a large dataset of 86,886 word pairs. Our
		  evaluation shows that the accuracy of Subsume (100%), Imply
		  (99.32%), and Alike (99.53%). Although accurate, Subsume
		  was able to judge only 75% of the data. Both Subsume and
		  Imply are sound, while Alike is not. We demonstrate the
		  utility of the algorithms using a real-life use case -- in
		  lemma disambiguation and in linking hundreds of Arabic
		  dictionaries.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= dec,
  articleno	= {10},
  numpages	= {21},
  keywords	= {Arabic, diacritics, disambiguation}
}

@InBook{	  10.1145/3233795.3233807,
  author	= {Valstar, Michel},
  title		= {Multimodal databases},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233807},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {393–421},
  numpages	= {29}
}

@InBook{	  10.1145/3233795.3233809,
  author	= {Schnelle-Walka, Dirk and Radomski, Stefan},
  title		= {Automotive multimodal human-machine interface},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233809},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {477–522},
  numpages	= {46}
}

@InBook{	  10.1145/3233795.3233804,
  author	= {Heloir, Alexis and Nunnari, Fabrizio and Bachynskyi,
		  Myroslav},
  title		= {Ergonomics for the design of multimodal interfaces},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233804},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {263–304},
  numpages	= {42}
}

@InBook{	  10.1145/3233795.3233812,
  author	= {Cohen, Philip R. and Tumuluri, Raj},
  title		= {Commercialization of multimodal systems},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233812},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {621–658},
  numpages	= {38}
}

@InBook{	  10.1145/3233795.3233806,
  author	= {Tumuluri, Raj and Dahl, Deborah and Patern\`{o}, Fabio and
		  Zancanaro, Massimo},
  title		= {Standardized representations and markup languages for
		  multimodal interaction},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233806},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {347–392},
  numpages	= {46}
}

@InBook{	  10.1145/3233795.3233799,
  author	= {Skantze, Gabriel and Gustafson, Joakim and Beskow, Jonas},
  title		= {Multimodal conversational interaction with robots},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233799},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {77–104},
  numpages	= {28}
}

@Article{	  10.1145/3363560,
  author	= {Zhao, Sicheng and Wang, Shangfei and Soleymani, Mohammad
		  and Joshi, Dhiraj and Ji, Qiang},
  title		= {Affective Computing for Large-scale Heterogeneous
		  Multimedia Data: A Survey},
  year		= {2019},
  issue_date	= {November 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {3s},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3363560},
  doi		= {10.1145/3363560},
  abstract	= {The wide popularity of digital photography and social
		  networks has generated a rapidly growing volume of
		  multimedia data (i.e., images, music, and videos),
		  resulting in a great demand for managing, retrieving, and
		  understanding these data. Affective computing (AC) of these
		  data can help to understand human behaviors and enable wide
		  applications. In this article, we survey the
		  state-of-the-art AC technologies comprehensively for
		  large-scale heterogeneous multimedia data. We begin this
		  survey by introducing the typical emotion representation
		  models from psychology that are widely employed in AC. We
		  briefly describe the available datasets for evaluating AC
		  algorithms. We then summarize and compare the
		  representative methods on AC of different multimedia types,
		  i.e., images, music, videos, and multimodal data, with the
		  focus on both handcrafted features-based methods and deep
		  learning methods. Finally, we discuss some challenges and
		  future directions for multimedia affective computing.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= dec,
  articleno	= {93},
  numpages	= {32},
  keywords	= {Affective computing, emotion recognition, large-scale
		  multimedia, sentiment analysis}
}

@Book{		  10.1145/3233795,
  editor	= {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip
		  R. and Sonntag, Daniel and Potamianos, Gerasimos and
		  Kr\"{u}ger, Antonio},
  title		= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  abstract	= {The Handbook of Multimodal-Multisensor Interfaces provides
		  the first authoritative resource on what has become the
		  dominant paradigm for new computer interfaces---user input
		  involving new media (speech, multi-touch, hand and body
		  gestures, facial expressions, writing) embedded in
		  multimodal-multisensor interfaces.This three-volume
		  handbook is written by international experts and pioneers
		  in the field. It provides a textbook, reference, and
		  technology roadmap for professionals working in this and
		  related areas.This third volume focuses on state-of-the-art
		  multimodal language and dialogue processing, including
		  semantic integration of modalities. The development of
		  increasingly expressive embodied agents and robots has
		  become an active test-bed for coordinating multimodal
		  dialogue input and output, including processing of language
		  and nonverbal communication. In addition, major application
		  areas are featured for commercializing
		  multimodal-multisensor systems, including automotive,
		  robotic, manufacturing, machine translation, banking,
		  communications, and others. These systems rely heavily on
		  software tools, data resources, and international standards
		  to facilitate their development. For insights into the
		  future, emerging multimodal-multisensor technology trends
		  are highlighted for medicine, robotics, interaction with
		  smart spaces, and similar topics. Finally, this volume
		  discusses the societal impact of more widespread adoption
		  of these systems, such as privacy risks and how to mitigate
		  them. The handbook chapters provide a number of
		  walk-through examples of system design and processing,
		  information on practical resources for developing and
		  evaluating new systems, and terminology and tutorial
		  support for mastering this emerging field. In the final
		  section of this volume, experts exchange views on a timely
		  and controversial challenge topic, and how they believe
		  multimodal-multisensor interfaces need to be equipped to
		  most effectively advance human performance during the next
		  decade.}
}

@Article{	  10.1162/coli_a_00342,
  author	= {Cheng, Jianpeng and Reddy, Siva and Saraswat, Vijay and
		  Lapata, Mirella},
  title		= {Learning an executable neural semantic parser},
  year		= {2019},
  issue_date	= {March 2019},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA},
  volume	= {45},
  number	= {1},
  issn		= {0891-2017},
  url		= {https://doi.org/10.1162/coli_a_00342},
  doi		= {10.1162/coli_a_00342},
  abstract	= {This article describes a neural semantic parser that maps
		  natural language utterances onto logical forms that can be
		  executed against a task-specific environment, such as a
		  knowledge base or a database, to produce a response. The
		  parser generates tree-structured logical forms with a
		  transition-based approach, combining a generic
		  tree-generation algorithm with domain-general grammar
		  defined by the logical language. The generation process is
		  modeled by structured recurrent neural networks, which
		  provide a rich encoding of the sentential context and
		  generation history for making predictions. To tackle
		  mismatches between natural language and logical form
		  tokens, various attention mechanisms are explored. Finally,
		  we consider different training settings for the neural
		  semantic parser, including fully supervised training where
		  annotated logical forms are given, weakly supervised
		  training where denotations are provided, and distant
		  supervision where only unlabeled sentences and a knowledge
		  base are available. Experiments across a wide range of data
		  sets demonstrate the effectiveness of our parser.},
  journal	= {Comput. Linguist.},
  month		= mar,
  pages		= {59–94},
  numpages	= {36}
}

@Article{	  10.1145/3291124,
  author	= {Zhang, Jing and Li, Wanqing and Ogunbona, Philip and Xu,
		  Dong},
  title		= {Recent Advances in Transfer Learning for Cross-Dataset
		  Visual Recognition: A Problem-Oriented Perspective},
  year		= {2019},
  issue_date	= {January 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3291124},
  doi		= {10.1145/3291124},
  abstract	= {This article takes a problem-oriented perspective and
		  presents a comprehensive review of transfer-learning
		  methods, both shallow and deep, for cross-dataset visual
		  recognition. Specifically, it categorises the cross-dataset
		  recognition into 17 problems based on a set of carefully
		  chosen data and label attributes. Such a problem-oriented
		  taxonomy has allowed us to examine how different
		  transfer-learning approaches tackle each problem and how
		  well each problem has been researched to date. The
		  comprehensive problem-oriented review of the advances in
		  transfer learning with respect to the problem has not only
		  revealed the challenges in transfer learning for visual
		  recognition but also the problems (e.g., 8 of the 17
		  problems) that have been scarcely studied. This survey not
		  only presents an up-to-date technical review for
		  researchers but also a systematic approach and a reference
		  for a machine-learning practitioner to categorise a real
		  problem and to look up for a possible solution
		  accordingly.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {7},
  numpages	= {38},
  keywords	= {Cross-dataset recognition, domain adaptation}
}

@InBook{	  10.1145/3233795.3233803,
  author	= {Cafaro, Angelo and Pelachaud, Catherine and Marsella,
		  Stacy C.},
  title		= {Nonverbal behavior in multimodal performances},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233803},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {219–262},
  numpages	= {44}
}

@InBook{	  10.1145/3233795.3233813,
  author	= {Friedland, Gerald and Tschantz, Michael Carl},
  title		= {Privacy concerns of multimodal sensor systems},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233813},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {659–704},
  numpages	= {46}
}

@InBook{	  10.1145/3233795.3233805,
  author	= {Hornung, Rachel and Chen, Nutan and van der Smagt,
		  Patrick},
  title		= {Early integration for movement modeling in latent spaces},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233805},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {305–345},
  numpages	= {41}
}

@InBook{	  10.1145/3233795.3233800,
  author	= {Bohus, Dan and Horvitz, Eric},
  title		= {Situated interaction},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233800},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {105–143},
  numpages	= {39}
}

@InBook{	  10.1145/3233795.3233810,
  author	= {Kirchner, Elsa A. and Fairclough, Stephen H. and Kirchner,
		  Frank},
  title		= {Embedded multimodal interfaces in robotics: applications,
		  future trends, and societal implications},
  year		= {2019},
  isbn		= {9781970001754},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3233795.3233810},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Language Processing, Software, Commercialization, and
		  Emerging Directions},
  pages		= {523–576},
  numpages	= {54}
}

@Article{	  10.1145/3313801,
  author	= {Gon\c{c}ales, Lucian Jos\'{e} and Farias, Kleinner and
		  Oliveira, Toacy Cavalcante De and Scholl, Murilo},
  title		= {Comparison of Software Design Models: An Extended
		  Systematic Mapping Study},
  year		= {2019},
  issue_date	= {May 2020},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3313801},
  doi		= {10.1145/3313801},
  abstract	= {Model comparison has been widely used to support many
		  tasks in model-driven software development. For this
		  reason, many techniques of comparing them have been
		  proposed in the last few decades. However, academia and
		  industry have overlooked a classification of currently
		  available approaches to the comparison of design models.
		  Hence, a thorough understanding of state-of-the-art
		  techniques remains limited and inconclusive. This article,
		  therefore, focuses on providing a classification and a
		  thematic analysis of studies on the comparison of software
		  design models. We carried out a systematic mapping study
		  following well-established guidelines to answer nine
		  research questions. In total, 56 primary studies (out of
		  4,132) were selected from 10 widely recognized electronic
		  databases after a careful filtering process. The main
		  results are that a majority of the primary studies (1)
		  provide coarse-grained techniques of the comparison of
		  general-purpose diagrams, (2) adopt graphs as principal
		  data structure and compare software design models
		  considering structural properties only, (3) pinpoint
		  commonalities and differences between software design
		  models rather than assess their similarity, and (4) propose
		  new techniques while neglecting the production of empirical
		  knowledge from experimental studies. Finally, this article
		  highlights some challenges and directions that can be
		  explored in upcoming studies.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {48},
  numpages	= {41},
  keywords	= {UML, model comparison, model similarity, software design
		  models}
}

@Article{	  10.1145/3037755,
  author	= {Muram, Faiz ul and Tran, Huy and Zdun, Uwe},
  title		= {Systematic Review of Software Behavioral Model Consistency
		  Checking},
  year		= {2017},
  issue_date	= {March 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {50},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3037755},
  doi		= {10.1145/3037755},
  abstract	= {In software development, models are often used to
		  represent multiple views of the same system. Such models
		  need to be properly related to each other in order to
		  provide a consistent description of the developed system.
		  Models may contain contradictory system specifications, for
		  instance, when they evolve independently. Therefore, it is
		  very crucial to ensure that models conform to each other.
		  In this context, we focus on consistency checking of
		  behavior models. Several techniques and approaches have
		  been proposed in the existing literature to support
		  behavioral model consistency checking. This article
		  presents a Systematic Literature Review (SLR) that was
		  carried out to obtain an overview of the various
		  consistency concepts, problems, and solutions proposed
		  regarding behavior models. In our study, the identification
		  and selection of the primary studies was based on a
		  well-planned search strategy. The search process identified
		  a total of 1770 studies, out of which 96 have been
		  thoroughly analyzed according to our predefined SLR
		  protocol. The SLR aims to highlight the state-of-the-art of
		  software behavior model consistency checking and identify
		  potential gaps for future research. Based on research
		  topics in selected studies, we have identified seven main
		  categories: targeted software models, types of consistency
		  checking, consistency checking techniques, inconsistency
		  handling, type of study and evaluation, automation support,
		  and practical impact. The findings of the systematic review
		  also reveal suggestions for future research, such as
		  improving the quality of study design and conducting
		  evaluations, and application of research outcomes in
		  industrial settings. For this purpose, appropriate strategy
		  for inconsistency handling, better tool support for
		  consistency checking and/or development tool integration
		  should be considered in future studies.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {17},
  numpages	= {39},
  keywords	= {Software behavioral model, consistency checking,
		  consistency types, systematic literature review}
}

@InBook{	  10.1145/3015783.3015795,
  author	= {Cohen, Philip R. and Oviatt, Sharon},
  title		= {Multimodal speech and pen interfaces},
  year		= {2017},
  isbn		= {9781970001679},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3015783.3015795},
  abstract	= {This chapter describes interfaces that enable users to
		  combine digital pen and speech input for interacting with
		  computing systems. Such interfaces promise natural and
		  efficient interaction, taking advantage of skills that
		  users have developed over many years. Many applications for
		  such systems have been explored, such as speech and pen
		  systems for computer-aided design (CAD), with which an
		  architect can sketch to create and position entities while
		  speaking information about them. For instance, a user could
		  draw a hardwood floor outline while saying "threefourths
		  inch thick heart pine." In response, the CAD system would
		  create a floor of the correct shape, thickness, and
		  materials, while also updating the list of materials to
		  purchase for the job. Then the user could touch the floor
		  and say "finish with polyurethane." The user of such a
		  system could concentrate on creating the planned building,
		  without interrupting their concentration to navigate a
		  complex interface menu system. In fact, multimodal CAD
		  systems like Think3 are preferred by users, and have been
		  documented to significantly increase their productivity by
		  speeding up interaction 23% [Engineer Live 2013, Price
		  2004].This chapter will discuss how speech and pen
		  multimodal systems have been built, and also how well they
		  have performed. By pen input we include such devices as
		  light pens, styluses, wireless digital pens, and digital
		  pens that can write on paper while either storing digital
		  data, or streaming it to a receiver [Anoto 2016]. We will
		  also occasionally refer to other devices that can, like
		  digital pens, provide a continuous stream of &lt; x, y
		  &gt;coordinates---such as tracked laser pointers, finger
		  input on touch-screens, and the ubiquitous mouse. Pen input
		  devices can be used for a number of communicative
		  functions, such as handwriting letters and numbers, drawing
		  symbols, sketching diagrams or shapes, pointing, or
		  gesturing (e.g., drawing an arrow to scroll a map). See the
		  Glossary for defined terms.This chapter begins by
		  discussing users' multimodal speech and pen interaction
		  patterns, and the documented advantages of this type of
		  multimodal system (Section 10.2). Section 10.3 describes
		  the simulation infrastructure that's ideally required for
		  prototyping new systems, and the process of collecting
		  multimodal data resources. In terms of system development,
		  Sections 10.4 and 10.5 outline general signal processing
		  and information flow, and major architectural components.
		  Section 10.6 describes implemented approaches to multimodal
		  fusion and semantic integration. Section 10.7 presents
		  examples of multimodal speech and pen systems, some of
		  which are commercial applications [Tumuluri 2017], with the
		  Sketch-Thru-Plan system provided as a walk-through case
		  study. The chapter concludes with Section 10.8 by
		  discussing future directions for research and development.
		  As an aid to comprehension, readers are referred to the
		  Glossary for newly introduced terms throughout the chapter,
		  and also the Focus Questions at the end of the chapter.},
  booktitle	= {The Handbook of Multimodal-Multisensor Interfaces:
		  Foundations, User Modeling, and Common Modality
		  Combinations - Volume 1},
  pages		= {403–447},
  numpages	= {45}
}

@InProceedings{	  10.1145/3339252.3341497,
  author	= {Chora\'{s}, Micha\l{} and Pawlicki, Marek and Kozik,
		  Rafa\l{} and Demestichas, Konstantinos and Kosmides, Pavlos
		  and Gupta, Manik},
  title		= {SocialTruth Project Approach to Online Disinformation
		  (Fake News) Detection and Mitigation},
  year		= {2019},
  isbn		= {9781450371643},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3339252.3341497},
  doi		= {10.1145/3339252.3341497},
  abstract	= {The extreme growth and adoption of Social Media, in
		  combination with their poor governance and the lack of
		  quality control over the digital content being published
		  and shared, has led information veracity to a continuous
		  deterioration. Current approaches entrust content
		  verification to a single centralised authority, lack
		  resilience towards attempts to successfully "game"
		  verification checks, and make content verification
		  difficult to access and use. In response, our ambition is
		  to create an open, democratic, pluralistic and distributed
		  ecosystem that allows easy access to various verification
		  services (both internal and third-party), ensuring
		  scalability and establishing trust in a completely
		  decentralized environment. In fact, this is the ambition of
		  the EU H2020 SocialTruth project. In this paper, we present
		  the innovative project approach and the vision of effective
		  online disinformation detection for various practical
		  use-cases.},
  booktitle	= {Proceedings of the 14th International Conference on
		  Availability, Reliability and Security},
  articleno	= {68},
  numpages	= {10},
  keywords	= {detection, fake news, networks, pattern recognition,
		  safety, security},
  location	= {Canterbury, CA, United Kingdom},
  series	= {ARES '19}
}

@InBook{	  10.1145/3122865.3122866,
  title		= {Preface},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122866},
  abstract	= {The field of multimedia is dedicated to research and
		  studies that leverage multiple modalities of signals and
		  data in developing intelligent systems and technologies. Be
		  it search engine, recommendation system, streaming service,
		  interactive agent, or collaborative system, multimedia
		  plays a critical role in ensuring full understanding of
		  multimodal sensory signals, robust modeling of user-content
		  interaction, natural and rich communication experience, and
		  scalable system deployment. The goal is to utilize unique
		  contributions from each modality, integrate complementary
		  synergies, and achieve the best performance and novel
		  functions beyond what's separately available in each
		  individual medium. In this community, most contributors
		  also maintain strong activities in other disciplines such
		  as networking, computer vision, human-computer interaction,
		  and machine learning. But the field of multimedia is unique
		  in offering a rich and dynamic forum for researchers from
		  "traditional" fields to collaborate and develop new
		  solutions and knowledge that transcend the boundaries of
		  individual disciplines.The field enjoys a long history of
		  vibrant research. For example, the flagship ACM SIGMM
		  Multimedia Conference was established in 1993, celebrating
		  its 25th anniversary this year. The community also has
		  several well-known conferences and journals organized by
		  ACM, IEEE, and other groups, attracting a large number of
		  researchers and practitioners from around the world.
		  However, despite the prolific research activities and
		  outcomes, there has been less effort toward developing
		  books that serve as an introduction to the rich spectrum of
		  topics in this broad field. Most of the few books available
		  today either focus on specific subfields or basic
		  background. There is a lack of tutorial-style materials
		  covering the active topics being pursued by the leading
		  researchers at frontiers of the field.SIGMM launched a new
		  initiative to address this need in 2015, by selecting and
		  inviting 12 rising-star speakers from different subfields
		  of multimedia to deliver plenary tutorial style talks at
		  ACM Multimedia 2015. Each speaker discussed challenges and
		  the state of the art within their prospective research
		  areas in a general manner to the broad community. Topics
		  covered were comprehensive, including multimedia content
		  understanding, multimodal human-human and human-computer
		  interaction, multimedia social media, and multimedia system
		  architecture and deployment. Following the very positive
		  responses to the talks, these rising-star speakers were
		  invited to expand the content covered in their talks to
		  chapters that can be used as reference materials for
		  researchers, students, and practitioners. Each resulting
		  chapter discusses problems, technical challenges,
		  state-of-the-art approaches and performances, open issues,
		  and promising directions for future work. Collectively, the
		  chapters provide an excellent sampling of major topics
		  addressed by the community as a whole. This book, capturing
		  outcomes of such efforts, is well positioned to fill the
		  aforementioned needs by providing tutorial-style reference
		  materials for frontier topics of multimedia.Section 1 of
		  the book includes five chapters that are focused on
		  analysis and understanding of multimedia content. Topics
		  covered range from analysis of video content, audio
		  content, multimodal content about interaction of
		  freestanding conversational groups, and analysis of
		  multimedia data in the encrypted format for preserving
		  privacy on cloud servers, to efficient approximate
		  similarity search techniques for searching over large-scale
		  databases.First, Zuxuan Wu et al. review current research
		  on understanding video content by detecting the classes of
		  actions or events contained in a given video clip and
		  generation of full-sentence captions describing the content
		  in each such video. Unlike previous surveys, this review
		  focuses on solutions based on deep learning, reflecting the
		  recent trend of research in this area. The chapter also
		  gives extensive reviews of the datasets used in
		  state-of-the-art research and benchmarking
		  efforts.Extending the modality from video to audio, in
		  Chapter 2, Gerald Friedland et al. introduce the field of
		  computer audition, aiming to develop the theory behind
		  artificial systems that can extract information from sound.
		  This chapter reviews the research datasets available,
		  appropriate representations needed for audio, and a few
		  challenging problems such as automatic extraction of
		  hierarchical semantic structures from audio content and
		  automatic discovery of high-level semantic concepts from
		  massive audio data and associated metadata.The holy grail
		  of research for the multimedia community is to be able to
		  integrate and fuse information extracted from multiple
		  modalities of data. In Chapter 3, Xavier Alameda-Pineda et
		  al. present an excellent example and emergent research
		  challenges in the application of detecting social
		  interaction among freestanding conversational groups. The
		  chapter includes overviews of research issues, approaches,
		  evaluation of joint estimation of head and body poses using
		  multiPreface modality data (such as wearable sensors and
		  distributed camera networks), and results of detecting
		  dynamic group formation of interacting people.Chapter 4
		  addresses a novel emerging topic prompted by the popular
		  approach to multimedia analysis using cloud computing
		  servers. When multimedia data is sent to the cloud for
		  storage or processing, there is a risk of privacy breach
		  via unauthorized access by third parties to the content in
		  the cloud. Pradeep Atrey et al. review state-of-the-art
		  methods and open issues for processing multimedia content
		  in the encrypted domain without needing to convert data to
		  the original format. This allows content to stay in its
		  protected form while useful analysis is performed on it.In
		  Chapter 5, Herv\'{e}e Jeundefinedou surveys efficient
		  techniques for finding approximate solutions for similarity
		  search, which is of particular interest when searching
		  massive multimedia data like images, videos, and audio
		  recordings. Jeundefinedou considers various performance
		  factors like query speed, memory requirement, and search
		  accuracy. Multiple frameworks based on locality sensitive
		  hashing (LSH), quantization/ compression, and hybrid
		  combinations are also reviewed in a coherent manner.In
		  Section 2 of the book, the emphasis shifts from content
		  analysis to humancentered aspects of multimedia computing.
		  This new focus goes beyond extraction of semantic
		  information from multimedia data. Instead, the broad
		  research scope incorporates understanding of users and
		  user-content interaction so as to improve effectiveness of
		  multimedia systems in many applications, such as search and
		  recommendation.Under the human-centric theme, Chapter 6,
		  authored by Peng Cui, discusses the evolution of multimedia
		  computing paradigms from the data-centric, to the
		  content-centric, and recently to the human-centric. Cui
		  presents a new framework, called social-sensed multimedia
		  computing, to capture many key issues involved and advances
		  achieved, including understanding of user-content
		  interaction behavior, understanding of user intent,
		  multimedia representation considering user intention, and
		  integration of heterogeneous data sensed on multimedia
		  social networks.Chapter 7 follows the human-centric theme
		  and further moves the focus from processing individual
		  multimedia data streams to processing a large number of
		  heterogeneous streams in different modalities involving a
		  large number of people. Analysis of such massive streams
		  offers the possibility of detecting important situations of
		  society, such as socio-economic affairs, as well as the
		  living environment. Vivek Singh provides an overview of the
		  problem definition, research framework, and the EventShop
		  toolkit he developed for application development in this
		  emerging area.The extension to the human-centric computing
		  paradigm also calls for formal mathematical theories and
		  tools for explaining the phenomena observed, such as the
		  information propagation behaviors and the occurrences of
		  information cascades on social networks. In Chapter 8,
		  Marian-Andrei Rizoiu et al. review stochastic processes
		  such as the Hawkes point process for modeling discrete,
		  interdependent events over continuous time. These are
		  strongly related to patterns corresponding to retweet
		  cascade events on social media. Successful models like
		  these can help researchers understand information
		  dissemination patterns and predict popularity on social
		  media.Interaction between users and content reveals not
		  only the intent of the user (covered in Chapter 6), but
		  also attributes of the content as well as of the user
		  him/herself. Such interaction can be manifested in multiple
		  forms including explicit cues such as visual and verbal
		  expressions, and implicit cues such as eye movement and
		  physiological signals like brain activity and heart rate.
		  Chapter 9 includes a survey by Subramanian Ramanathan et
		  al. on how such implicit user interaction cues can be
		  explored to improve analysis of content (scene
		  understanding) and user (user emotion recognition).To
		  support research and development of emerging multimedia
		  topics discussed above, there is a critical need for new
		  generations of communication and computing systems that
		  take into account the unique requirements of multimedia,
		  such as real-time, high bandwidth, distributiveness, major
		  power consumption, and resource uncertainty. The popular
		  cloud-based computing systems, though prevalent
		  formanyapplications, are not suitable for large-scale
		  multimedia applications such as cloud-based gaming service
		  and animation rendering service.The last section of the
		  book focuses on the systems aspect, covering distinct
		  topics of multimedia fog computing (Chapter 10) and cloud
		  gaming (Chapter 11). Cheng-Hsin Hsu et al. survey the
		  emerging paradigm focused on fog computing, in which
		  computing services are crowdsourced to the edge nodes or
		  even to the client devices on the user end. This offers
		  major potential benefits in terms of low latency, location
		  awareness, scalability, and heterogeneity. However, it also
		  poses many significant challenges in areas such as resource
		  discovery, resource allocation and management, quality of
		  service, and security. Discussion of these challenges,
		  along with recent advances in this area, are presented in
		  Chapter 10.Finally, as a concrete example of large-scale
		  distributed multimedia computing systems, Chapter 11 (by
		  Kuan-Ta Chen et al.) presents a comprehensive survey of
		  cloud gaming, with emphasis on the development of platform
		  and testbed, test scenarios, and evaluation of performance,
		  in order to enhance optimal design of various components of
		  the complex cloud gaming systems. In particular, the
		  chapter overviews extensive research in areas such as
		  open-source platforms, cloud deployment, client design, and
		  communication between gaming servers and clients.The scope
		  of this book is by no means exhaustive or complete. For
		  example, it can be expanded to include other important
		  topics such as (but not limited to) multimedia content
		  generation, multimodal knowledge discovery and
		  representation, multimedia immersive networked
		  environments, and applications in areas like healthcare,
		  learning, and infrastructure. Nonetheless, the
		  comprehensive survey materials already covered in the book
		  provide an excellent foundation for exploring additional
		  topics mentioned above, and many other relevant fields.We
		  would like to give sincere acknowledgment to Dr. Svebor
		  Karaman, who has provided tremendous assistance in
		  communicating with contributors and organizing the content
		  of this book. In addition, Diane Cerra and her team at
		  Morgan &amp; Claypool Publishers have provided valuable
		  guidance and editorial help},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {xi–xv}
}

@Article{	  10.1162/coli_a_00378,
  author	= {Ranta, Aarne and Angelov, Krasimir and Gruzitis, Normunds
		  and Kolachina, Prasanth},
  title		= {Abstract Syntax as Interlingua: Scaling Up the Grammatical
		  Framework from Controlled Languages to Robust Pipelines},
  year		= {2020},
  issue_date	= {June 2020},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA},
  volume	= {46},
  number	= {2},
  issn		= {0891-2017},
  url		= {https://doi.org/10.1162/coli_a_00378},
  doi		= {10.1162/coli_a_00378},
  abstract	= {Abstract syntax is an interlingual representation used in
		  compilers. Grammatical Framework (GF) applies the abstract
		  syntax idea to natural languages. The development of GF
		  started in 1998, first as a tool for controlled language
		  implementations, where it has gained an established
		  position in both academic and commercial projects. GF
		  provides grammar resources for over 40 languages, enabling
		  accurate generation and translation, as well as grammar
		  engineering tools and components for mobile and Web
		  applications. On the research side, the focus in the last
		  ten years has been on scaling up GF to wide-coverage
		  language processing. The concept of abstract syntax offers
		  a unified view on many other approaches: Universal
		  Dependencies, WordNets, FrameNets, Construction Grammars,
		  and Abstract Meaning Representations. This makes it
		  possible for GF to utilize data from the other approaches
		  and to build robust pipelines. In return, GF can contribute
		  to data-driven approaches by methods to transfer resources
		  from one language to others, to augment data by rule-based
		  generation, to check the consistency of hand-annotated
		  corpora, and to pipe analyses into high-precision semantic
		  back ends. This article gives an overview of the use of
		  abstract syntax as interlingua through both established and
		  emerging NLP applications involving GF.},
  journal	= {Comput. Linguist.},
  month		= jun,
  pages		= {425–486},
  numpages	= {62}
}

@InBook{	  10.1145/3122865.3122872,
  author	= {Cui, Peng},
  title		= {Social-sensed multimedia computing},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122872},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {137–157},
  numpages	= {21}
}

@InBook{	  10.1145/3122865.3122868,
  author	= {Friedland, Gerald and Smaragdis, Paris and McDermott, Josh
		  and Raj, Bhiksha},
  title		= {Audition for multimedia computing},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122868},
  abstract	= {What do the fields of robotics, human-computer
		  interaction, AI, video retrieval, privacy, cybersecurity,
		  Internet of Things, and big data all have in common? They
		  all work with various sources of data: visual, textual,
		  time stamps, links, records. But there is one source of
		  data that has been almost completely ignored by the
		  academic community---sound.Our comprehension of the world
		  relies critically on audition---the ability to perceive and
		  interpret the sounds we hear. Sound is ubiquitous, and is a
		  unique source of information about our environment and the
		  events occurring in it. Just by listening, we can determine
		  whether our child's laughter originated inside or outside
		  our house, how far away they were when they laughed, and
		  whether the window through which the sound passed was open
		  or shut. The ability to derive information about the world
		  from sound is a core aspect of perceptual
		  intelligence.Auditory inferences are often complex and
		  sophisticated despite their routine occurrence. The number
		  of possible inferences is typically not enumerable, and the
		  final interpretation is not merely one of selection from a
		  fixed set. And yet humans perform such inferences
		  effortlessly, based only on sounds captured using two
		  sensors, our ears.Electronic devices can also "perceive"
		  sound. Every phone and tablet has at least one microphone,
		  as do most cameras. Any device or space can be equipped
		  with microphones at minimal expense. Indeed, machines can
		  not only "listen"; they have potential advantages over
		  humans as listening devices, in that they can communicate
		  and coordinate their experiences in ways that biological
		  systems simply cannot. Collections of devices that can
		  sense sound and communicate with each other could
		  instantiate a single electronic entity that far surpasses
		  humans in its ability to record and process information
		  from sound.And yet machines at present cannot truly hear.
		  Apart from well-developed efforts to recover structure in
		  speech and music, the state of the art in machine hearing
		  is limited to relatively impoverished descriptions of
		  recorded sounds: detecting occurrences of a limited
		  pre-specified set of sound types, and their locations.
		  Although researchers typically envision artificially
		  intelligent agents such as robots to have human-like
		  hearing abilities, at present the rich descriptions and
		  inferences humans can make about sound are entirely beyond
		  the capability of machine systems.In this chapter, we
		  suggest establishing the field of Computer Audition to
		  develop the theory behind artificial systems that extract
		  information from sound. Our objective is to enable computer
		  systems to replicate and exceed human abilities. This
		  chapter describes the challenges of this field.},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {31–50},
  numpages	= {20}
}

@InBook{	  10.1145/3122865.3122869,
  author	= {Alameda-Pineda, Xavier and Ricci, Elisa and Sebe, Nicu},
  title		= {Multimodal analysis of free-standing conversational
		  groups},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122869},
  abstract	= {"Free-standing conversational groups" are what we call the
		  elementary building blocks of social interactions formed in
		  settings when people are standing and congregate in groups.
		  The automatic detection, analysis, and tracking of such
		  structural conversational units captured on camera poses
		  many interesting challenges for the research community.
		  First, although delineating these formations is strongly
		  linked to other behavioral cues such as head and body
		  poses, finding methods that successfully describe and
		  exploit these links is not obvious. Second, the use of
		  visual data is crucial, but when analyzing crowded scenes,
		  one must account for occlusions and low-resolution images.
		  In this regard, the use of other sensing technologies such
		  as wearable devices can facilitate the analysis of social
		  interactions by complementing the visual information. Yet
		  the exploitation of multiple modalities poses other
		  challenges in terms of data synchronization, calibration,
		  and fusion. In this chapter, we discuss recent advances in
		  multimodal social scene analysis, in particular for the
		  detection of conversational groups or F-formations [Kendon
		  1990]. More precisely, a multimodal joint head and body
		  pose estimator is described and compared to other recent
		  approaches for head and body pose estimation and
		  F-formation detection. Experimental results on the recently
		  published SALSA dataset are reported, they evidence the
		  long road toward a fully automated high-precision social
		  scene analysis framework.},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {51–74},
  numpages	= {24}
}

@InBook{	  10.1145/3122865.3122876,
  author	= {Hsu, Cheng-Hsin and Hong, Hua-Jun and Elgamal, Tarek and
		  Nahrstedt, Klara and Venkatasubramanian, Nalini},
  title		= {Multimedia fog computing: minions in the cloud and crowd},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122876},
  abstract	= {In cloud computing, minions refer to virtual or physical
		  machines that carry out the actual workload. Minions in the
		  cloud hide in faraway data centers and thus cloud computing
		  is less friendly to multimedia applications. The fog
		  computing paradigm pushes minions toward edge networks. We
		  adopt a generalized definition, where minions get into end
		  devices owned by the crowd. The serious uncertainty, such
		  as dynamic network conditions, limited battery levels, and
		  unpredictable minion availability in multimedia fog
		  platforms makes them harder to be managed than cloud
		  platforms. In this chapter, we share our experience on
		  utilizing resources from the crowd to optimize multimedia
		  applications. The learned lessons shed some light on the
		  optimal design of a unified multimedia fog platform for
		  distributed multimedia applications.},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {255–286},
  numpages	= {32}
}

@InBook{	  10.1145/3122865.3122877,
  author	= {Chen, Kuan-Ta and Cai, Wei and Shea, Ryan and Huang,
		  Chun-Ying and Liu, Jiangchuan and Leung, Victor C. M. and
		  Hsu, Cheng-Hsin},
  title		= {Cloud gaming},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122877},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {287–314},
  numpages	= {28}
}

@InBook{	  10.1145/3122865.3122873,
  author	= {Singh, Vivek},
  title		= {Situation recognition using multimodal data},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122873},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {159–189},
  numpages	= {31}
}

@InBook{	  10.1145/3122865.3122875,
  author	= {Ramanathan, Subramanian and Gilani, Syed Omer and Sebe,
		  Nicu},
  title		= {Utilizing implicit user cues for multimedia analytics},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122875},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {219–251},
  numpages	= {33}
}

@InBook{	  10.1145/3122865.3122874,
  author	= {Rizoiu, Marian-Andrei and Lee, Young and Mishra, Swapnil
		  and Xie, Lexing},
  title		= {Hawkes processes for events in social media},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122874},
  abstract	= {This chapter provides an accessible introduction for point
		  processes, and especially Hawkes processes, for modeling
		  discrete, inter-dependent events over continuous time. We
		  start by reviewing the definitions and key concepts in
		  point processes. We then introduce the Hawkes process and
		  its event intensity function, as well as schemes for event
		  simulation and parameter estimation. We also describe a
		  practical example drawn from social media data---we show
		  how to model retweet cascades using a Hawkes self-exciting
		  process.We present a design of the memory kernel, and
		  results on estimating parameters and predicting popularity.
		  The code and sample event data are available in an online
		  repository.},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {191–218},
  numpages	= {28}
}

@InBook{	  10.1145/3122865.3122871,
  author	= {Jeundefinedou, Herv\'{e}},
  title		= {Efficient similarity search},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122871},
  abstract	= {This chapter addresses one of the fundamental problems
		  involved in multimedia systems, namely efficient similarity
		  search for large collections of multimedia content. This
		  problem has received a lot of attention from various
		  research communities. In particular, it is a historical
		  line of research in computational geometry and databases.
		  The computer vision and multimedia communities have adopted
		  pragmatic approaches guided by practical requirements: the
		  large sets of features required to describe image
		  collections make visual search a highly demanding task. As
		  a result, early works [Flickner et al. 1995, Fagin 1998,
		  Beis and Lowe 1997] in image indexing have foreseen the
		  interest in approximate algorithms, especially after the
		  dissemination of methods based on local description in the
		  90s, as any improvement obtained on this indexing part
		  improves the whole visual search system.Among the existing
		  approximate nearest neighbors (ANN) strategies, the popular
		  framework of Locality-Sensitive Hashing (LSH) [Indyk and
		  Motwani 1998, Gionis et al. 1999] provides theoretical
		  guarantees on the search quality with limited assumptions
		  on the underlying data distribution. It was first proposed
		  [Indyk and Motwani 1998] for the Hamming and l1 spaces, and
		  was later extended to the Euclidean/ cosine cases [Charikar
		  2002, Datar et al. 2004] or the earth mover's distance
		  [Charikar 2002, Andoni and Indyk 2006]. LSH has been
		  successfully used for local descriptors [Ke et al. 2004],
		  3D object indexing [Matei et al. 2006, Shakhnarovich et al.
		  2006], and other fields such as audio retrieval [Casey and
		  Slaney 2007, Ryynanen and Klapuri 2008]. It has also
		  received some attention in a context of private information
		  retrieval [Pathak and Raj 2012, Aghasaryan et al. 2013,
		  Furon et al. 2013].A few years ago, approaches inspired by
		  compression and more specifically quantization-based
		  approaches [Jundefinedou et al. 2011] were shown to be a
		  viable alternative to hashing methods, and shown successful
		  for efficiently searching in a billion-sized dataset.This
		  chapter discusses these different trends. It is organized
		  as follows. Section 5.1 gives some background references
		  and concepts, including evaluation issues. Most of the
		  methods and variants are exposed within the LSH framework.
		  It is worth mentioning that LSH is more of a concept than a
		  particular algorithm. The search algorithms associated with
		  LSH follow two distinct search mechanisms, the probe-cell
		  model and sketches, which are discussed in Sections 5.2 and
		  5.3, respectively. Section 5.4 describes methods inspired
		  by compression algorithms, while Section 5.5 discusses
		  hybrid approaches combining the non-exhaustiveness of the
		  cell-probe model with the advantages of sketches or
		  compression-based algorithms. Other metrics than Euclidean
		  and cosine are briefly discussed in Section 5.6.},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {105–134},
  numpages	= {30}
}

@InBook{	  10.1145/3122865.3122870,
  author	= {Atrey, Pradeep K. and Lathey, Ankita and Yakubu, Abukari
		  M.},
  title		= {Encrypted domain multimedia content analysis},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  url		= {https://doi.org/10.1145/3122865.3122870},
  booktitle	= {Frontiers of Multimedia Research},
  pages		= {75–104},
  numpages	= {30}
}

@Article{	  10.1145/3229329.3229333,
  author	= {Zhang, Jiawei and Yu, Philip S.},
  title		= {Broad Learning: An Emerging Area in Social Network
		  Analysis},
  year		= {2018},
  issue_date	= {June 2018},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {1},
  issn		= {1931-0145},
  url		= {https://doi.org/10.1145/3229329.3229333},
  doi		= {10.1145/3229329.3229333},
  abstract	= {Looking from a global perspective, the landscape of online
		  social networks is highly fragmented. A large number of
		  online social networks have appeared, which can provide
		  users with various types of services. Generally,
		  information available in these online social networks is of
		  diverse categories, which can be represented as
		  heterogeneous social networks (HSNs) formally. Meanwhile,
		  in such an age of online social media, users usually
		  participate in multiple online social networks
		  simultaneously, who can act as the anchors aligning
		  different social networks together. So multiple HSNs not
		  only represent information in each social network, but also
		  fuse information from multiple networks.Formally, the
		  online social networks sharing common users are named as
		  the aligned social networks, and these shared users are
		  called the anchor users. The heterogeneous information
		  generated by users' social activities in the multiple
		  aligned social networks provides social network
		  practitioners and researchers with the opportunities to
		  study individual user's social behaviors across multiple
		  social platforms simultaneously. This paper presents a
		  comprehensive survey about the latest research works on
		  multiple aligned HSNs studies based on the broad learning
		  setting, which covers 5 major research tasks, including
		  network alignment, link prediction, community detection,
		  information diffusion and network embedding respectively.},
  journal	= {SIGKDD Explor. Newsl.},
  month		= may,
  pages		= {24–50},
  numpages	= {27},
  keywords	= {Network Embedding, Network Alignment, Link Prediction,
		  Information Diffusion, Heterogeneous Social Networks,
		  Community Detection}
}

@Book{		  10.1145/3122865,
  editor	= {Chang, Shih-Fu},
  title		= {Frontiers of Multimedia Research},
  year		= {2017},
  isbn		= {9781970001075},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  volume	= {17},
  abstract	= {The field of multimedia is unique in offering a rich and
		  dynamic forum for researchers from "traditional" fields to
		  collaborate and develop new solutions and knowledge that
		  transcend the boundaries of individual disciplines. Despite
		  the prolific research activities and outcomes, however, few
		  efforts have been made to develop books that serve as an
		  introduction to the rich spectrum of topics covered by this
		  broad field. A few books are available that either focus on
		  specific subfields or basic background in multimedia.
		  Tutorial-style materials covering the active topics being
		  pursued by the leading researchers at frontiers of the
		  field are currently lacking. In 2015, ACM SIGMM, the
		  special interest group on multimedia, launched a new
		  initiative to address this void by selecting and inviting
		  12 rising-star speakers from different subfields of
		  multimedia research to deliver plenary tutorial-style talks
		  at the ACM Multimedia conference for 2015. Each speaker
		  discussed the challenges and state-of-the-art developments
		  of their prospective research areas in a general manner to
		  the broad community. The covered topics were comprehensive,
		  including multimedia content understanding, multimodal
		  human-human and human-computer interaction, multimedia
		  social media, and multimedia system architecture and
		  deployment. Following the very positive responses to these
		  talks, the speakers were invited to expand the content
		  covered in their talks into chapters that can be used as
		  reference material for researchers, students, and
		  practitioners. Each chapter discusses the problems,
		  technical challenges, state-of-the-art approaches and
		  performances, open issues, and promising direction for
		  future work. Collectively, the chapters provide an
		  excellent sampling of major topics addressed by the
		  community as a whole. This book, capturing some of the
		  outcomes of such efforts, is well positioned to fill the
		  aforementioned needs in providing tutorial-style reference
		  materials for frontier topics in multimedia.}
}

@Book{		  10.1145/3015783,
  editor	= {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip
		  R. and Sonntag, Daniel and Potamianos, Gerasimos and
		  Kr\"{u}ger, Antonio},
  title		= {The Handbook of Multimodal-Multisensor Interfaces:
		  Foundations, User Modeling, and Common Modality
		  Combinations - Volume 1},
  year		= {2017},
  isbn		= {9781970001679},
  publisher	= {Association for Computing Machinery and Morgan &amp;
		  Claypool},
  volume	= {14},
  abstract	= {The Handbook of Multimodal-Multisensor Interfaces provides
		  the first authoritative resource on what has become the
		  dominant paradigm for new computer interfaces-user input
		  involving new media (speech, multi-touch, gestures,
		  writing) embedded in multimodal-multisensor interfaces.
		  These interfaces support smartphones, wearables,
		  in-vehicle, robotic, and many other applications that are
		  now highly competitive commercially. This edited collection
		  is written by international experts and pioneers in the
		  field. It provides a textbook for students, and a reference
		  and technology roadmap for professionals working in this
		  rapidly emerging area. Volume 1 of the handbook presents
		  relevant theory and neuroscience foundations for guiding
		  the development of high-performance systems. Additional
		  chapters discuss approaches to user modeling, interface
		  design that supports user choice, synergistic combination
		  of modalities with sensors, and blending of multimodal
		  input and output. They also highlight an in-depth look at
		  the most common multimodal-multisensor combinations- for
		  example, touch and pen input, haptic and non-speech audio
		  output, and speech co-processed with visible lip movements,
		  gaze, gestures, or pen input. A common theme throughout is
		  support for mobility and individual differences among
		  users-including the world's rapidly growing population of
		  seniors. These handbook chapters provide walk-through
		  examples and video illustrations of different system
		  designs and their interactive use. Common terms are
		  defined, and information on practical resources is provided
		  (e.g., software tools, data resources) for hands-on project
		  work to develop and evaluate multimodal-multisensor
		  systems. In the final chapter, experts exchange views on a
		  timely and controversial challenge topic, and how they
		  believe multimodal-multisensor interfaces should be
		  designed in the future to most effectively advance human
		  performance.}
}

@Proceedings{	  10.1145/2998181,
  title		= {CSCW '17: Proceedings of the 2017 ACM Conference on
		  Computer Supported Cooperative Work and Social Computing},
  year		= {2017},
  isbn		= {9781450343350},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to CSCW 2017, the ACM 2017 Conference on Computer
		  Supported Cooperative Work and Social Computing! We are
		  excited to welcome the CSCW community back to Portland,
		  Oregon, where the second CSCW conference was held in 1988.
		  Both Portland and CSCW have matured a great deal during the
		  intervening 29 years. We hope that you will find that
		  Portland provides a stimulating environment for our
		  conference.CSCW is the premier venue for presenting
		  research in the design and use of technologies that affect
		  groups, organizations, communities, and networks. Bringing
		  together top researchers and practitioners from academia
		  and industry, CSCW explores the technical, social,
		  material, and theoretical challenges of designing
		  technology to support collaborative work and life
		  activities. CSCW welcomes a diverse range of topics and
		  research methodologies. Studies often involve the
		  development and application of novel technologies and/or
		  ethnographic studies that inform design practice or theory.
		  The mission of the conference is to share research that
		  advances the state of human knowledge and improves both the
		  design of systems and the ways they are used. The diversity
		  of work in our conference program reflects the diversity of
		  technology use in people's work, social, and civic lives as
		  well as the geographic and cultural diversity of
		  contributors.As many of you know, CSCW follows a rigorous
		  "revise and resubmit" review process that uses peer review
		  to improve submitted papers while maintaining a
		  high-quality threshold for final acceptance. We also help
		  prepare the next generation of reviewers with a mentorship
		  program in which students review papers under the guidance
		  of an experienced reviewer. This year we have the largest
		  CSCW program ever. We had 530 submitted papers and 183 were
		  accepted for presentation at the conference. The program
		  also includes 4 papers published in ACM Transactions on
		  Human- Computer Interaction (TOCHI). In addition, we will
		  feature 14 workshops, 56 posters, 12 demos, and 3
		  panels.Lili Cheng of Microsoft Research will open the
		  conference, speaking on "Conversational AI &amp; Lessons
		  Learned." Our closing plenary will feature Jorge Cham, the
		  creator of PhD Comics, who will talk about, "The Science
		  Gap." We also welcome Paul Luff and Christian Heath from
		  King's College as the recipients of this year's CSCW
		  Lasting Impact award for their influential 1998 paper,
		  "Mobility in Collaboration."},
  location	= {Portland, Oregon, USA}
}

@InProceedings{	  10.1145/3544548.3581441,
  author	= {Ashby, Trevor and Webb, Braden K and Knapp, Gregory and
		  Searle, Jackson and Fulda, Nancy},
  title		= {Personalized Quest and Dialogue Generation in Role-Playing
		  Games: A Knowledge Graph- and Language Model-based
		  Approach},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3581441},
  doi		= {10.1145/3544548.3581441},
  abstract	= {Procedural content generation (PCG) in video games offers
		  unprecedented opportunities for customization and user
		  engagement. Working within the specialized context of
		  role-playing games (RPGs), we introduce a novel framework
		  for quest and dialogue generation that places the player at
		  the core of the generative process. Drawing on a
		  hand-crafted knowledge base, our method grounds generated
		  content with in-game context while simultaneously employing
		  a large-scale language model to create fluent, unique,
		  accompanying dialogue. Through human evaluation, we confirm
		  that quests generated using this method can approach the
		  performance of hand-crafted quests in terms of fluency,
		  coherence, novelty, and creativity; demonstrate the
		  enhancement to the player experience provided by greater
		  dynamism; and provide a novel, automated metric for the
		  relevance between quest and dialogue. We view our
		  contribution as a critical step toward dynamic, co-creative
		  narrative frameworks in which humans and AI systems jointly
		  collaborate to create unique and user-specific playable
		  experiences.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {290},
  numpages	= {20},
  keywords	= {English, GPT-2, MMORPG, NPC dialogue, RPG, World of
		  Warcraft, computational creativity, dynamic quest
		  generation, human-AI co-creativity, human-computer
		  interaction, knowledge graph, knowledge-grounded text
		  generation, language model, large-scale language models,
		  narrative, natural language processing, procedural content
		  generation, quest, quests, text generation, transformers,
		  video games},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@Article{	  10.1145/3618295,
  author	= {Zhong, Lingfeng and Wu, Jia and Li, Qian and Peng, Hao and
		  Wu, Xindong},
  title		= {A Comprehensive Survey on Automatic Knowledge Graph
		  Construction},
  year		= {2023},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3618295},
  doi		= {10.1145/3618295},
  abstract	= {Automatic knowledge graph construction aims at
		  manufacturing structured human knowledge. To this end, much
		  effort has historically been spent extracting informative
		  fact patterns from different data sources. However, more
		  recently, research interest has shifted to acquiring
		  conceptualized structured knowledge beyond informative
		  data. In addition, researchers have also been exploring new
		  ways of handling sophisticated construction tasks in
		  diversified scenarios. Thus, there is a demand for a
		  systematic review of paradigms to organize knowledge
		  structures beyond data-level mentions. To meet this demand,
		  we comprehensively survey more than 300 methods to
		  summarize the latest developments in knowledge graph
		  construction. A knowledge graph is built in three steps:
		  knowledge acquisition, knowledge refinement, and knowledge
		  evolution. The processes of knowledge acquisition are
		  reviewed in detail, including obtaining entities with
		  fine-grained types and their conceptual linkages to
		  knowledge graphs; resolving coreferences; and extracting
		  entity relationships in complex scenarios. The survey
		  covers models for knowledge refinement, including knowledge
		  graph completion, and knowledge fusion. Methods to handle
		  knowledge evolution are also systematically presented,
		  including condition knowledge acquisition, condition
		  knowledge graph completion, and knowledge dynamic. We
		  present the paradigms to compare the distinction among
		  these methods along the axis of the data environment,
		  motivation, and architecture. Additionally, we also provide
		  briefs on accessible resources that can help readers to
		  develop practical knowledge graph systems. The survey
		  concludes with discussions on the challenges and possible
		  directions for future exploration.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {94},
  numpages	= {62},
  keywords	= {logic reasoning, knowledge fusion, knowledge graph
		  completion, information extraction, deep learning,
		  Knowledge graph}
}

@InProceedings{	  10.1145/3611643.3616317,
  author	= {Du, Xueying and Lou, Yiling and Liu, Mingwei and Peng, Xin
		  and Yang, Tianyong},
  title		= {KG4CraSolver: Recommending Crash Solutions via Knowledge
		  Graph},
  year		= {2023},
  isbn		= {9798400703270},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3611643.3616317},
  doi		= {10.1145/3611643.3616317},
  abstract	= {Fixing crashes is challenging, and developers often
		  discuss their encountered crashes and refer to similar
		  crashes and solutions on online Q&amp;A forums (e.g., Stack
		  Overflow). However, a crash often involves very complex
		  contexts, which includes different contextual elements,
		  e.g., purposes, environments, code, and crash traces.
		  Existing crash solution recommendation or general solution
		  recommendation techniques only use an incomplete context or
		  treat the entire context as pure texts to search relevant
		  solutions for a given crash, resulting in inaccurate
		  recommendation results. In this work, we propose a novel
		  crash solution knowledge graph (KG) to summarize the
		  complete crash context and its solution with a
		  graph-structured representation. To construct the crash
		  solution KG automatically, we propose to leverage prompt
		  learning to construct the KG from SO threads with a small
		  set of labeled data. Based on the constructed KG, we
		  further propose a novel KG-based crash solution
		  recommendation technique KG4CraSolver, which precisely
		  finds the relevant SO thread for an encountered crash by
		  finely analyzing and matching the complete crash context
		  based on the crash solution KG. The evaluation results show
		  that the constructed KG is of high quality and KG4CraSolver
		  outperforms baselines in terms of all metrics (e.g.,
		  13.4%-113.4% MRR improvements). Moreover, we perform a user
		  study and find that KG4CraSolver helps participants find
		  crash solutions 34.4% faster and 63.3% more accurately.},
  booktitle	= {Proceedings of the 31st ACM Joint European Software
		  Engineering Conference and Symposium on the Foundations of
		  Software Engineering},
  pages		= {1242–1254},
  numpages	= {13},
  keywords	= {Crash Solution Recommendation, Knowledge Graph, Stack
		  Overflow},
  location	= {San Francisco, CA, USA},
  series	= {ESEC/FSE 2023}
}

@InProceedings{	  10.1145/3581783.3612863,
  author	= {Liu, Runze and Fang, Yaqun and Yu, Fan and Tian, Ruiqi and
		  Ren, Tongwei and Wu, Gangshan},
  title		= {Deep Video Understanding with Video-Language Model},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612863},
  doi		= {10.1145/3581783.3612863},
  abstract	= {Pre-trained video-language models (VLMs) have shown
		  superior performance in high-level video understanding
		  tasks, analyzing multi-modal information, aligning with
		  Deep Video Understanding Challenge (DVUC) requirements.In
		  this paper, we explore pre-trained VLMs' potential in
		  multimodal question answering for long-form videos. We
		  propose a solution called Dual Branches Video Modeling
		  (DBVM), which combines knowledge graph (KG) and VLMs,
		  leveraging their strengths and addressing shortcomings.The
		  KG branch recognizes and localizes entities, fuses
		  multimodal features at different levels, and constructs KGs
		  with entities as nodes and relationships as edges.The VLM
		  branch applies a selection strategy to adapt input movies
		  into acceptable length and a cross-matching strategy to
		  post-process results providing accurate scene
		  descriptions.Experiments conducted on the DVUC dataset
		  validate the effectiveness of our DBVM.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {9551–9555},
  numpages	= {5},
  keywords	= {cross-matching strategy, deep video understanding,
		  knowledge graph, pre-trained video-language model},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3617184.3630130,
  author	= {Ye, Xin and Wang, Shimin and Wang, Han and Wei, Qinwei and
		  Yang, Ting and Tao, Yu},
  title		= {Application of Knowledge Graph in Financial Information
		  Security Strategy},
  year		= {2023},
  isbn		= {9798400708800},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3617184.3630130},
  doi		= {10.1145/3617184.3630130},
  abstract	= {It is very important to utilize various financial security
		  data resources for intelligent data processing and
		  analysis, which can improve risk assessment, analysis, and
		  early warning capabilities. This paper takes the knowledge
		  graph as the core to study the application of the enhanced
		  representation of the knowledge graph in financial
		  information security risk control, which can give play to
		  its advantages of knowledge integration, organizing the
		  scattered and distributed multi-source heterogeneous
		  security data, providing data analysis and knowledge
		  reasoning support for threat modeling, risk analysis,
		  attack reasoning, etc. in the network security, and thus
		  providing a basis knowledge for screening potential
		  information security risks. The establishment of knowledge
		  graph enhanced by knowledge representation and reasoning
		  has significantly improved the Identification efficiency of
		  financial risk information, as well as the accuracy and
		  integrity of the results of the strategy scheme, thus
		  reducing the workload of manual screening and integration
		  of information, and verifying the effectiveness of the
		  financial information risk knowledge graph in this paper.},
  booktitle	= {Proceedings of the 8th International Conference on Cyber
		  Security and Information Engineering},
  pages		= {188–192},
  numpages	= {5},
  keywords	= {financial information, information security, knowledge
		  graph, security strategy, text knowledge enhancement},
  location	= {Putrajaya, Malaysia},
  series	= {ICCSIE '23}
}

@InProceedings{	  10.1145/3625469.3625470,
  author	= {Wu, Wenjing and Yuan, Qi and Chen, Qiulan and Cao,
		  Yunzhong},
  title		= {Construction Safety Knowledge Graph Integrating Text and
		  Image Information},
  year		= {2023},
  isbn		= {9798400707681},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625469.3625470},
  doi		= {10.1145/3625469.3625470},
  abstract	= {To improve the extraction efficiency and visualization of
		  construction safety knowledge, this paper combines
		  knowledge graph technology with construction safety domain,
		  and proposes a basic framework of construction safety
		  knowledge ontology based on the evolution logic of safety
		  events according to the characteristics of knowledge.
		  Considering two types of safety knowledge carriers and data
		  sources, text and image, a knowledge graph is designed to
		  contain text semantic features and image features, and the
		  knowledge services based on different dimensional knowledge
		  queries are validated in the experiments. The results show
		  that the BERT-BiLSTM-CRF algorithm can be used to extract
		  entities in text, and YOLOv5-FastPose can extract excavator
		  poses from images. This paper verifies the applicability of
		  knowledge graphs for safety knowledge mining, visualization
		  and services.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Information Management and Management Science},
  pages		= {26–32},
  numpages	= {7},
  keywords	= {Pose estimation, Knowledge graph, Entity recognition,
		  Construction safety management},
  location	= {Chengdu, China},
  series	= {IMMS '23}
}

@Article{	  10.1145/3638065,
  author	= {Liu, Shenghao and Lu, Lingyun and Wang, Bang},
  title		= {On Mining User-Item Interactions via Knowledge Graph for
		  Recommendation},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638065},
  doi		= {10.1145/3638065},
  abstract	= {Introducing Knowledge Graph (KG) to facilitate recommender
		  system has become a tendency in recent years. Many existing
		  methods leverage KG to obtain side information of items to
		  promote item representation learning for enhancing
		  recommendation performance. However, they ignore that KG
		  also may contribute to better user representation learning.
		  To solve the above issue, we propose a novel algorithm,
		  KIGR (Knowledge-aware Interaction Graph for
		  Recommendation), to mine user-item interactions via
		  Knowledge Graph for assisting user representation learning.
		  Specifically, a user-item interaction is encoded by
		  attentively summing up the relation embedding about the
		  item in KG. Then an unsupervised learning method is used to
		  group the user-item interactions into different latent
		  types. Further, a user-item interaction graph is divided
		  into several subgraphs, which is referred to as
		  Knowledge-aware Interaction Graph, making each subgraph
		  only contains one latent type of interactions. Finally,
		  user representation is the fusion of user interest
		  embedding, which is learned on knowledge-aware interaction
		  graph; While item representation is learned on KG.
		  Experimental results on MovieLens, LastFM and Amazon-Book
		  validate that the proposed KIGR has a superior performance
		  compared with the SOTA algorithms.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Recomm. Syst.},
  month		= dec,
  keywords	= {Recommendation System; Knowledge Graph; Graph Neural
		  Network}
}

@InProceedings{	  10.1145/3583780.3615105,
  author	= {Wang, Yu and Ye, Feng and Li, Binquan and Jin, Gaoyang and
		  Xu, Dong and Li, Fengsheng},
  title		= {UrbanFloodKG: An Urban Flood Knowledge Graph System for
		  Risk Assessment},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615105},
  doi		= {10.1145/3583780.3615105},
  abstract	= {Increasing numbers of people live in flood-prone areas
		  worldwide. With continued development, urban flood will
		  become more frequent, which has caused casualties and
		  property damage. Researchers have been dedicating to urban
		  flood risk assessments in recent years. However, current
		  research is still facing the challenges of multi-modal data
		  fusion and knowledge representation of urban flood events.
		  Therefore, in this paper, we propose an Urban Flood
		  Knowledge Graph (UrbanFloodKG) system that enables KG to
		  support urban flood risk assessment. The system consists of
		  data layer, graph layer, algorithm layer, and application
		  layer, which implements knowledge extraction and storage
		  functions, integrates knowledge representation learning
		  models and graph neural network models to support link
		  prediction and node classification tasks. We conduct model
		  comparison experiments on link prediction and node
		  classification tasks based on urban flood event data from
		  Guangzhou, and demonstrate the effectiveness of the models
		  used. Our experiments prove that the accuracy of risk
		  assessment can reach 91% when using GEN, which provides a a
		  promising research direction for urban flood risk
		  assessment.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2574–2584},
  numpages	= {11},
  keywords	= {urban flood, link prediction, knowledge graph, graph
		  neural network},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3576842.3589161,
  author	= {Zhang, Ruipeng and Xie, Mengjun},
  title		= {A Knowledge Graph Question Answering Approach to IoT
		  Forensics},
  year		= {2023},
  isbn		= {9798400700378},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3576842.3589161},
  doi		= {10.1145/3576842.3589161},
  abstract	= {Internet of Things (IoT) forensics has been a particularly
		  challenging task for forensic practitioners due to the
		  heterogeneity of IoT environments as well as the complexity
		  and volume of IoT data. With the advent of artificial
		  intelligence, question-answering (QA) systems have emerged
		  as a potential solution for users to access sophisticated
		  forensic knowledge and data. In this light, we present a
		  novel IoT forensics framework that employs knowledge graph
		  question answering (KGQA). Our framework enables
		  investigators to access forensic artifacts and
		  cybersecurity knowledge using natural language questions
		  facilitated by a deep-learning-powered KGQA model. The
		  proposed framework demonstrates high efficacy in answering
		  natural language questions over the experimental IoT
		  forensic knowledge graph.},
  booktitle	= {Proceedings of the 8th ACM/IEEE Conference on Internet of
		  Things Design and Implementation},
  pages		= {446–447},
  numpages	= {2},
  keywords	= {Digital Forensics, Internet of Things, Knowledge Graph,
		  Ontology Design, Question Answering},
  location	= {San Antonio, TX, USA},
  series	= {IoTDI '23}
}

@Article{	  10.1145/3597022,
  author	= {Yang, Yang and Zhang, Chubing and Song, Xin and Dong,
		  Zheng and Zhu, Hengshu and Li, Wenjie},
  title		= {Contextualized Knowledge Graph Embedding for Explainable
		  Talent Training Course Recommendation},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3597022},
  doi		= {10.1145/3597022},
  abstract	= {Learning and development, or L&amp;D, plays an important
		  role in talent management, which aims to improve the
		  knowledge and capabilities of employees through a variety
		  of performance-oriented training activities. Recently, with
		  the rapid development of enterprise management information
		  systems, many research efforts and industrial practices
		  have been devoted to building personalized employee
		  training course recommender systems. Nevertheless, a
		  widespread challenge is how to provide explainable
		  recommendations with the consideration of different
		  learning motivations from talents. To this end, we propose
		  CKGE, a contextualized knowledge graph (KG) embedding
		  approach for developing an explainable training course
		  recommender system. A novel perspective of CKGE is to
		  integrate both the contextualized neighbor semantics and
		  high-order connections as motivation-aware information for
		  learning effective representations of talents and courses.
		  Specifically, in CKGE, for each entity pair (i.e., the
		  talent-course pair), we first construct a meta-graph,
		  including the neighbors of each entity and the meta-paths
		  between entities as motivation-aware information. Then, we
		  develop a novel KG-based Transformer, which can serialize
		  entities and paths in the meta-graph as a sequential input,
		  with the specially designed relational attention and
		  structural encoding mechanisms to better model the global
		  dependence of KG structured data. Meanwhile, the local path
		  mask prediction can effectively reveal the importance of
		  different paths. As a result, CKGE not only can make
		  precise predictions but also can discriminate the
		  saliencies of meta-paths in characterizing corresponding
		  preferences. Extensive experiments on real-world and public
		  datasets clearly validate the effectiveness and
		  interpretability of CKGE compared with state-of-the-art
		  baselines.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= sep,
  articleno	= {33},
  numpages	= {27},
  keywords	= {Transformer, knowledge graph, Course recommendation}
}

@InProceedings{	  10.1145/3583780.3614769,
  author	= {Pahuja, Vardaan and Wang, Boshi and Latapie, Hugo and
		  Srinivasa, Jayanth and Su, Yu},
  title		= {A Retrieve-and-Read Framework for Knowledge Graph Link
		  Prediction},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614769},
  doi		= {10.1145/3583780.3614769},
  abstract	= {Knowledge graph (KG) link prediction aims to infer new
		  facts based on existing facts in the KG. Recent studies
		  have shown that using the graph neighborhood of a node via
		  graph neural networks (GNNs) provides more useful
		  information compared to just using the query information.
		  Conventional GNNs for KG link prediction follow the
		  standard message-passing paradigm on the entire KG, which
		  leads to superfluous computation, over-smoothing of node
		  representations, and also limits their expressive power. On
		  a large scale, it becomes computationally expensive to
		  aggregate useful information from the entire KG for
		  inference. To address the limitations of existing KG link
		  prediction frameworks, we propose a novel retrieve-and-read
		  framework, which first retrieves a relevant subgraph
		  context for the query and then jointly reasons over the
		  context and the query with a high-capacity reader. As part
		  of our exemplar instantiation for the new framework, we
		  propose a novel Transformer-based GNN as the reader, which
		  incorporates graph-based attention structure and
		  cross-attention between query and context for deep fusion.
		  This simple yet effective design enables the model to focus
		  on salient context information relevant to the query.
		  Empirical results on two standard KG link prediction
		  datasets demonstrate the competitive performance of the
		  proposed method. Furthermore, our analysis yields valuable
		  insights for designing improved retrievers within the
		  framework.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1992–2002},
  numpages	= {11},
  keywords	= {transformers, knowledge graph link prediction, knowledge
		  graph completion, graph neural networks},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Article{	  10.1145/3627704,
  author	= {Li, Da and Zhu, Boqing and Yang, Sen and Xu, Kele and Yi,
		  Ming and He, Yukai and Wang, Huaimin},
  title		= {Multi-task Pre-training Language Model for Semantic
		  Network Completion},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {11},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3627704},
  doi		= {10.1145/3627704},
  abstract	= {Semantic networks, exemplified by the knowledge graph,
		  serve as a means to represent knowledge by leveraging the
		  structure of a graph. While the knowledge graph exhibits
		  promising potential in the field of natural language
		  processing, it suffers from incompleteness. This article
		  focuses on the task of completing knowledge graphs by
		  predicting linkages between entities, which is fundamental
		  yet critical. Traditional methods based on translational
		  distance struggle when dealing with unseen entities. In
		  contrast, semantic matching presents itself as a potential
		  solution due to its ability to handle such cases. However,
		  semantic matching-based approaches necessitate large-scale
		  datasets for effective training, which are typically
		  unavailable in practical scenarios, hindering their
		  competitive performance. To address this challenge, we
		  propose a novel architecture for knowledge graphs known as
		  LP-BERT, which incorporates a language model. LP-BERT
		  consists of two primary stages: multi-task pre-training and
		  knowledge graph fine-tuning. During the pre-training phase,
		  the model acquires relationship information from triples by
		  predicting either entities or relations through three
		  distinct tasks. In the fine-tuning phase, we introduce a
		  batch-based triple-style negative sampling technique
		  inspired by contrastive learning. This method significantly
		  increases the proportion of negative sampling while
		  maintaining a nearly unchanged training time. Furthermore,
		  we propose a novel data augmentation approach that
		  leverages the inverse relationship of triples to enhance
		  both the performance and robustness of the model. To
		  demonstrate the effectiveness of our proposed framework, we
		  conduct extensive experiments on three widely used
		  knowledge graph datasets: WN18RR, FB15k-237, and UMLS. The
		  experimental results showcase the superiority of our
		  methods, with LP-BERT achieving state-of-the-art
		  performance on the WN18RR and FB15k-237 datasets.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {250},
  numpages	= {20},
  keywords	= {multi-task learning, translational distance, semantic
		  matching, link prediction, Knowledge graph}
}

@InProceedings{	  10.1145/3580305.3599791,
  author	= {Zang, Xiaoling and Hu, Binbin and Chu, Jun and Zhang,
		  Zhiqiang and Zhang, Guannan and Zhou, Jun and Zhong,
		  Wenliang},
  title		= {Commonsense Knowledge Graph towards Super APP and Its
		  Applications in Alipay},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599791},
  doi		= {10.1145/3580305.3599791},
  abstract	= {The recently explosive growth of Super Apps brings great
		  convenience to people's daily life by providing a wide
		  variety of services through mini-programs, including online
		  shopping, travel, finance, and so on. Due to the
		  considerable gap between various scenarios, the restriction
		  of effective information transfer and sharing severely
		  blocks the efficient delivery of online services,
		  potentially affecting the user's app experience. To deeply
		  understand users' needs, we propose SupKG, a commonsense
		  knowledge graph towards Super APP to help comprehensively
		  characterize user behaviors across different business
		  scenarios. In particular, our SupKG is carefully
		  established from multiplex and heterogeneous data source in
		  Alipay (a well-known Super App in China), which also
		  emphasize abundant spatiotemporal relations and
		  intent-related entities to answer the fundamental question
		  in life service ''which service do users need at what time
		  and where''.On the hand, the successful application of
		  SupKG hinges on the effective form of network
		  representation ie Knowledge Graph Embedding (KGE). However,
		  a series of unsatisfying issues still need to be carefully
		  considered in the industrial environment: i) bridging
		  language representations with knowledge structure in a
		  unified manner, ii) alleviating the skewed data
		  distribution in SupKG, and iii) effectively characterizing
		  hierarchical structures in SupKG. With these motivations,
		  we develop a novel knowledge graph representation learning
		  framework for SupKG, enabling various downstream
		  applications to benefit from learned representations of
		  entities and relations. Extensive experiments on the
		  standard knowledge graph completion task demonstrate the
		  consistent and significant performance improvement of our
		  representation learning framework, which also greatly
		  benefits the supplementation of potential knowledge of
		  SupKG. Towards real-world applications in Alipay, our SupKG
		  and learned representations show the potential superiority
		  of integrating global behaviors in cold-start scenarios and
		  providing high-quality knowledge for warming up the
		  graph-based ranking.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {5509–5519},
  numpages	= {11},
  keywords	= {super apps, representation learning, knowledge graph},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3583780.3615252,
  author	= {Fang, Jinyuan and Meng, Zaiqiao and Macdonald, Craig},
  title		= {KGPR: Knowledge Graph Enhanced Passage Ranking},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615252},
  doi		= {10.1145/3583780.3615252},
  abstract	= {Passage ranking aims to rank a set of passages based on
		  their relevance to a query. Current state-of-the-art models
		  for this task typically employ a cross-encoder structure.
		  However, these models lack access to background knowledge,
		  i.e., information related to the query that can be helpful
		  in retrieving relevant passages. Knowledge Graphs (KGs)
		  provide a structured way of storing information about
		  entities and their relationships, offering valuable
		  background knowledge about entities. While KGs have been
		  used to augment pretrained language models (LMs) to perform
		  several reasoning tasks such as question answering, it
		  remains an open question of how to utilise the information
		  from KGs to enhance the performance of cross-encoders on
		  the passage ranking task. Therefore, we propose KGPR, a
		  KG-enhanced cross-encoder for the Passage Retrieval task.
		  KGPR is built upon LUKE, an entity-aware pretrained LM,
		  with an additional module that fuses information from KGs
		  into LUKE. By leveraging the background knowledge from KGs,
		  KGPR enhances the model's comprehension of queries and
		  passages, resulting in improved ranking performance.
		  Experimental results demonstrate that using KGs can enhance
		  the performance of LUKE in the passage retrieval task, and
		  KGPR can outperform state-of-the-art monoT5 cross-encoder
		  by 3.32% and 10.77% on the MS MARCO development set and
		  TREC DL-HARD query set respectively, using a model with a
		  similar number of parameters.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3880–3885},
  numpages	= {6},
  keywords	= {passage ranking, knowledge graphs, cross-encoder},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3583780.3615294,
  author	= {Xiong, Bo and Nayyeri, Mojtaba and Daza, Daniel and
		  Cochez, Michael},
  title		= {Reasoning beyond Triples: Recent Advances in Knowledge
		  Graph Embeddings},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615294},
  doi		= {10.1145/3583780.3615294},
  abstract	= {Knowledge Graphs (KGs) are a collection of facts
		  describing entities connected by relationships. KG
		  embeddings map entities and relations into a vector space
		  while preserving their relational semantics. This enables
		  effective inference of missing knowledge from the embedding
		  space. Most KG embedding approaches focused on
		  triple-shaped KGs. A great amount of real-world knowledge,
		  however, cannot simply be represented by triples. In this
		  tutorial, we give a systematic introduction to KG
		  embeddings that go beyond the triple representation. In
		  particular, the tutorial will focus on temporal facts where
		  the triples are enriched with temporal information,
		  hyper-relational facts where the triples are enriched with
		  qualifiers, n-ary facts describing relationships between
		  multiple entities, and also facts that are augmented with
		  literal and text descriptions. During the tutorial, we will
		  introduce both fundamental knowledge and advanced topics
		  for understanding recent embedding approaches for
		  beyond-triple representations.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5228–5231},
  numpages	= {4},
  keywords	= {link prediction, knowledge representation and reasoning,
		  knowledge graph embeddings, complex question answering},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Article{	  10.1145/3617379,
  author	= {Wang, Yashen and Ouyang, Xiaoye and Guo, Dayu and Zhu,
		  Xiaoling},
  title		= {MEGA: Meta-Graph Augmented Pre-Training Model for
		  Knowledge Graph Completion},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {1},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3617379},
  doi		= {10.1145/3617379},
  abstract	= {Nowadays, a large number of Knowledge Graph Completion
		  (KGC) methods have been proposed by using embedding based
		  manners, to overcome the incompleteness problem faced with
		  knowledge graph (KG). One important recent innovation in
		  Natural Language Processing (NLP) domain is the employ of
		  deep neural models that make the most of pre-training,
		  culminating in BERT, the most popular example of this line
		  of approaches today. Recently, a series of new KGC methods
		  introducing a pre-trained language model, such as KG-BERT,
		  have been developed and released compelling performance.
		  However, previous pre-training based KGC methods usually
		  train the model by using simple training task and only
		  utilize one-hop relational signals in KG, which leads that
		  they cannot model high-order semantic contexts and
		  multi-hop complex relatedness. To overcome this problem,
		  this article presents a novel pre-training framework for
		  KGC task, which especially consists of both one-hop
		  relation level task (low-order) and multi-hop meta-graph
		  level task (high-order). Hence, the proposed method can
		  capture not only the elaborate sub-graph structure but also
		  the subtle semantic information on the given KG. The
		  empirical results show the efficiency of the proposed
		  method on the widely used real-world datasets.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= oct,
  articleno	= {30},
  numpages	= {24},
  keywords	= {semantic enhancement, multi-task learning, pre-training
		  model, meta-graph, Knowledge graph completion}
}

@InProceedings{	  10.1145/3477495.3531757,
  author	= {Chen, Mingyang and Zhang, Wen and Zhu, Yushan and Zhou,
		  Hongting and Yuan, Zonggang and Xu, Changliang and Chen,
		  Huajun},
  title		= {Meta-Knowledge Transfer for Inductive Knowledge Graph
		  Embedding},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531757},
  doi		= {10.1145/3477495.3531757},
  abstract	= {Knowledge graphs (KGs) consisting of a large number of
		  triples have become widespread recently, and many knowledge
		  graph embedding (KGE) methods are proposed to embed
		  entities and relations of a KG into continuous vector
		  spaces. Such embedding methods simplify the operations of
		  conducting various in-KG tasks (e.g., link prediction) and
		  out-of-KG tasks (e.g., question answering). They can be
		  viewed as general solutions for representing KGs. However,
		  existing KGE methods are not applicable to inductive
		  settings, where a model trained on source KGs will be
		  tested on target KGs with entities unseen during model
		  training. Existing works focusing on KGs in inductive
		  settings can only solve the inductive relation prediction
		  task. They can not handle other out-of-KG tasks as general
		  as KGE methods since they don't produce embeddings for
		  entities. In this paper, to achieve inductive knowledge
		  graph embedding, we propose a model MorsE, which does not
		  learn embeddings for entities but learns transferable
		  meta-knowledge that can be used to produce entity
		  embeddings. Such meta-knowledge is modeled by
		  entity-independent modules and learned by meta-learning.
		  Experimental results show that our model significantly
		  outperforms corresponding baselines for in-KG and out-of-KG
		  tasks in inductive settings.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {927–937},
  numpages	= {11},
  keywords	= {meta-learning, meta-knowledge transfer, knowledge graph,
		  inductive knowledge graph embedding},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3539618.3592052,
  author	= {Yu, Donghan and Yang, Yiming},
  title		= {Retrieval-Enhanced Generative Model for Large-Scale
		  Knowledge Graph Completion},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3592052},
  doi		= {10.1145/3539618.3592052},
  abstract	= {The task of knowledge graph completion (KGC) is of great
		  importance. To achieve scalability when dealing with
		  large-scale knowledge graphs, recent works formulate KGC as
		  a sequence-to-sequence process, where the incomplete
		  triplet (input) and the missing entity (output) are both
		  verbalized as text sequences. However, inference with these
		  methods relies solely on the model parameters for implicit
		  reasoning and neglects the use of KG itself, which limits
		  the performance since the model lacks the capacity to
		  memorize a vast number of triplets. To tackle this issue,
		  we introduce ReSKGC, a Retrieval-enhanced Seq2seq KGC
		  model, which selects semantically relevant triplets from
		  the KG and uses them as evidence to guide output generation
		  with explicit reasoning. Our method has demonstrated
		  state-of-the-art performance on benchmark datasets
		  Wikidata5M and WikiKG90Mv2, which contain about 5M and 90M
		  entities, respectively.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2334–2338},
  numpages	= {5},
  keywords	= {information retrieval, knowledge graph completion, neural
		  network},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3539618.3591846,
  author	= {Er-Rahmadi, Btissam and Oncevay, Arturo and Ji, Yuanyi and
		  Pan, Jeff Z.},
  title		= {KATIE: A System for Key Attributes Identification in
		  Product Knowledge Graph Construction},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591846},
  doi		= {10.1145/3539618.3591846},
  abstract	= {We present part of Huawei's efforts in building a Product
		  Knowledge Graph (PKG). We want to identify which product
		  attributes (i.e. properties) are relevant and important in
		  terms of shopping decisions to product categories (i.e.
		  classes). This is particularly challenging when the
		  attributes and their values are mined from online product
		  catalogues, i.e. HTML pages. These web pages contain
		  semi-structured data, which do not follow a concerted
		  format and use diverse vocabulary to designate the same
		  features. We propose a system for key attribute
		  identification (KATIE) based on fine-tuning pre-trained
		  models (e.g., DistilBERT) to predict the applicability and
		  importance of an attribute to a category. We also propose
		  an attribute synonyms identification module that allows us
		  to discover synonymous attributes by considering not only
		  their labels' similarities but also the similarity of their
		  values sets. We have evaluated our approach to Huawei
		  categories taxonomy and a set of internally mined
		  attributes from web pages. KATIE guarantees promising
		  performance results compared to the most recent
		  baselines.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3320–3324},
  numpages	= {5},
  keywords	= {entity resolution, fine-tuning, pre-trained language
		  model, product knowledge graph, relation discovery},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3404835.3463072,
  author	= {Chung, Chanyoung and Whang, Joyce Jiyoung},
  title		= {Knowledge Graph Embedding via Metagraph Learning},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3463072},
  doi		= {10.1145/3404835.3463072},
  abstract	= {Knowledge graph embedding aims to represent entities and
		  relations in a continuous feature space while preserving
		  the structure of a knowledge graph. Most existing knowledge
		  graph embedding methods either focus only on a flat
		  structure of the given knowledge graph or exploit the
		  predefined types of entities to explore an enriched
		  structure. In this paper, we define the metagraph of a
		  knowledge graph by proposing a new affinity metric that
		  measures the structural similarity between entities, and
		  then grouping close entities by hypergraph clustering.
		  Without any prior information about entity types, a set of
		  semantically close entities is successfully merged into one
		  super-entity in our metagraph representation. We propose
		  the metagraph-based pre-training model of knowledge graph
		  embedding where we first learn representations in the
		  metagraph and initialize the entities and relations in the
		  original knowledge graph with the learned representations.
		  Experimental results show that our method is effective in
		  improving the accuracy of state-of-the-art knowledge graph
		  embedding methods.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2212–2216},
  numpages	= {5},
  keywords	= {clustering, embedding, knowledge graph, metagraph,
		  pre-train},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3632314.3632332,
  author	= {Guo, Dongdong and Ma, Haitao and Zhao, Can and Peng, Hao
		  and Du, Wenbo and Jiang, Zongrui and Zhang, Yan},
  title		= {Construction and Application of the Knowledge Graph Method
		  in Maintenance of Robot in Automotive Manufacturing
		  Industry},
  year		= {2023},
  isbn		= {9798400709401},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632314.3632332},
  doi		= {10.1145/3632314.3632332},
  abstract	= {Based on the spare parts and structure data of industrial
		  robots, the entity list of robot parts is established to
		  form a query dictionary, and entity annotation is performed
		  on the robot corpus by means of dictionary query, which
		  reduces the cost of manual annotation and ensures the
		  quality of annotation data. In the process of entity
		  recognition training, Bert+Bilstm+CRF model structure is
		  used to initially use 70% of the dictionary data for
		  annotation, and the model is trained by iteratively
		  increasing the annotation data in a continuous cycle, so
		  that the model can extract all the entities in the robot
		  corpus as much as possible. In addition, the material
		  number/model information and PM maintenance
		  content/strategy of the entity have been used as attributes
		  of the entity. Meanwhile, the experience summarized by the
		  failure model and effect analysis of industrial robots is
		  fully utilized to connect the phenomena, causes and
		  measures through the entities in order to build the
		  industrial robot knowledge graph relationships. The
		  constructed knowledge graph relationship is stored in a
		  Neo4j graphical database, making it convenient for content
		  retrieval and inquiry of application systems.In the
		  industrial robot knowledge graph application side, the
		  field maintenance personnel requirements are collected
		  through a questionnaire survey and the requirements are
		  classified into intent. A Bert+TextCNN structure model is
		  built to realize the intention recognition of user
		  inquiries. By combining entity recognition models and
		  intent classification models, the system is able to better
		  understand user inquiry needs, leading to the
		  implementation of an intelligent maintenance system for
		  industrial robots.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Intelligent Sensing and Industrial Automation},
  articleno	= {15},
  numpages	= {9},
  keywords	= {Knowledge Graph, Robot Maintenance},
  location	= {Virtual Event, China},
  series	= {ISIA '23}
}

@InProceedings{	  10.1145/3503161.3548257,
  author	= {Ma, Yue and Wang, Yali and Wu, Yue and Lyu, Ziyu and Chen,
		  Siran and Li, Xiu and Qiao, Yu},
  title		= {Visual Knowledge Graph for Human Action Reasoning in
		  Videos},
  year		= {2022},
  isbn		= {9781450392037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503161.3548257},
  doi		= {10.1145/3503161.3548257},
  abstract	= {Action recognition has been traditionally treated as a
		  high-level video classification problem. However, such a
		  manner lacks the detailed and semantic understanding of
		  body movement, which is the critical knowledge to explain
		  and infer complex human actions. To fill this gap, we
		  propose to summarize a novel visual knowledge graph from
		  over 15M detailed human annotations, for describing action
		  as the distinct composition of body parts, part movements
		  and interactive objects in videos. Based on it, we design a
		  generic multi-modal Action Knowledge Understanding (AKU)
		  framework, which can progressively infer human actions from
		  body part movements in the videos, with assistance of
		  visual-driven semantic knowledge mining. Finally, we
		  validate AKU on the recent Kinetics-TPS benchmark, which
		  contains body part parsing annotations for detailed
		  understanding of human action in videos. The results show
		  that, our AKU significantly boosts various video backbones
		  with explainable action knowledge in both supervised and
		  few shot settings, and outperforms the recent
		  knowledge-based action recognition framework, e.g., our AKU
		  achieves 83.9% accuracy on Kinetics-TPS while PaStaNet
		  achieves 63.8% accuracy under the same backbone. The codes
		  and models will be released at
		  https://github.com/mayuelala/AKU.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Multimedia},
  pages		= {4132–4141},
  numpages	= {10},
  keywords	= {action recognition, knowledge graph, video understanding},
  location	= {Lisboa, Portugal},
  series	= {MM '22}
}

@InProceedings{	  10.1145/3477314.3507031,
  author	= {Liu, Xinglan and Hussain, Hussain and Razouk, Houssam and
		  Kern, Roman},
  title		= {Effective use of BERT in graph embeddings for sparse
		  knowledge graph completion},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3507031},
  doi		= {10.1145/3477314.3507031},
  abstract	= {Graph embedding methods have emerged as effective
		  solutions for knowledge graph completion. However, such
		  methods are typically tested on benchmark datasets such as
		  Freebase, but show limited performance when applied on
		  sparse knowledge graphs with orders of magnitude lower
		  density. To compensate for the lack of structure in a
		  sparse graph, low dimensional representations of textual
		  information such as word2vec or BERT embeddings have been
		  used. This paper proposes a BERT-based method (BERT-ConvE),
		  to exploit transfer learning of BERT in combination with a
		  convolutional network model ConvE. Comparing to existing
		  text-aware approaches, we effectively make use of the
		  context dependency of BERT embeddings through optimizing
		  the features extraction strategies. Experiments on
		  ConceptNet show that the proposed method outperforms strong
		  baselines by 50% on knowledge graph completion tasks. The
		  proposed method is suitable for sparse graphs as also
		  demonstrated by empirical studies on ATOMIC and
		  sparsified-FB15k-237 datasets. Its effectiveness and
		  simplicity make it appealing for industrial applications.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {799–802},
  numpages	= {4},
  keywords	= {BERT, context aware embedding, knowledge graph embedding,
		  language model, sparse knowledge graph},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@InProceedings{	  10.1145/3487553.3524238,
  author	= {Xie, Xin and Zhang, Ningyu and Li, Zhoubo and Deng, Shumin
		  and Chen, Hui and Xiong, Feiyu and Chen, Mosha and Chen,
		  Huajun},
  title		= {From Discrimination to Generation: Knowledge Graph
		  Completion with Generative Transformer},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524238},
  doi		= {10.1145/3487553.3524238},
  abstract	= {Knowledge graph completion aims to address the problem of
		  extending a KG with missing triples. In this paper, we
		  provide an approach GenKGC, which converts knowledge graph
		  completion to sequence-to-sequence generation task with the
		  pre-trained language model. We further introduce
		  relation-guided demonstration and entity-aware hierarchical
		  decoding for better representation learning and fast
		  inference. Experimental results on three datasets show that
		  our approach can obtain better or comparable performance
		  than baselines and achieve faster inference speed compared
		  with previous methods with pre-trained language models. We
		  also release a new large-scale Chinese knowledge graph
		  dataset OpenBG500 for research purpose1.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {162–165},
  numpages	= {4},
  keywords	= {Generation, Knowledge Graph Completion, Transformer},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3583780.3615110,
  author	= {Fan, Ziwei and Liu, Zhiwei and Heinecke, Shelby and Zhang,
		  Jianguo and Wang, Huan and Xiong, Caiming and Yu, Philip
		  S.},
  title		= {Zero-shot Item-based Recommendation via Multi-task Product
		  Knowledge Graph Pre-Training},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615110},
  doi		= {10.1145/3583780.3615110},
  abstract	= {Existing recommender systems face difficulties with
		  zero-shot items, i.e. items that have no historical
		  interactions with users during the training stage. Though
		  recent works extract universal item representation via
		  pre-trained language models (PLMs), they ignore the crucial
		  item relationships. This paper presents a novel paradigm
		  for the Zero-Shot Item-based Recommendation (ZSIR) task,
		  which pre-trains a model on product knowledge graph (PKG)
		  to refine the item features from PLMs. We identify three
		  challenges for pre-training PKG, which are multi-type
		  relations in PKG, semantic divergence between item generic
		  information and relations and domain discrepancy from PKG
		  to downstream ZSIR task. We address the challenges by
		  proposing four pre-training tasks and novel task-oriented
		  adaptation (ToA) layers. Moreover, this paper discusses how
		  to fine-tune the model on new recommendation task such that
		  the ToA layers are adapted to ZSIR task. Comprehensive
		  experiments on 18 markets dataset are conducted to verify
		  the effectiveness of the proposed MPKG model.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {483–493},
  numpages	= {11},
  keywords	= {multi-task pre-training, product knowledge graph,
		  zero-shot recommendation},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3591106.3592258,
  author	= {Deng, Jiaxin and Shen, Dong and Pan, Haojie and Wu,
		  Xiangyu and Liu, Ximan and Meng, Gaofeng and Yang, Fan and
		  Gao, Tingting and Fu, Ruiji and Wang, Zhongyuan},
  title		= {A Unified Model for Video Understanding and Knowledge
		  Embedding with Heterogeneous Knowledge Graph Dataset},
  year		= {2023},
  isbn		= {9798400701788},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3591106.3592258},
  doi		= {10.1145/3591106.3592258},
  abstract	= {Video understanding is an important task in short video
		  business platforms and it has a wide application in video
		  recommendation and classification. Most of the existing
		  video understanding works only focus on the information
		  that appeared within the video content, including the video
		  frames, audio and text. However, introducing common sense
		  knowledge from the external Knowledge Graph (KG) dataset is
		  essential for video understanding when referring to the
		  content which is less relevant to the video. Owing to the
		  lack of video knowledge graph dataset, the work which
		  integrates video understanding and KG is rare. In this
		  paper, we propose a heterogeneous dataset that contains the
		  multi-modal video entity and fruitful common sense
		  relations. This dataset also provides multiple novel video
		  inference tasks like the Video-Relation-Tag (VRT) and
		  Video-Relation-Video (VRV) tasks. Furthermore, based on
		  this dataset, we propose an end-to-end model that jointly
		  optimizes the video understanding objective with knowledge
		  graph embedding, which can not only better inject factual
		  knowledge into video understanding but also generate
		  effective multi-modal entity embedding for KG.
		  Comprehensive experiments indicate that combining video
		  understanding embedding with factual knowledge benefits the
		  content-based video retrieval performance. Moreover, it
		  also helps the model generate better knowledge graph
		  embedding which outperforms traditional KGE-based methods
		  on VRT and VRV tasks with at least 42.36% and 17.73%
		  improvement in HITS@10.},
  booktitle	= {Proceedings of the 2023 ACM International Conference on
		  Multimedia Retrieval},
  pages		= {95–104},
  numpages	= {10},
  keywords	= {knowledge graph, multi-modal learning, video inference,
		  video understanding},
  location	= {Thessaloniki, Greece},
  series	= {ICMR '23}
}

@InProceedings{	  10.1145/3480001.3480022,
  author	= {Su, Mengmeng and Su, Hongyi and Zheng, Hong and Yan, Bo},
  title		= {Deep Learning For Knowledge Graph Completion With XLNET},
  year		= {2021},
  isbn		= {9781450390163},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3480001.3480022},
  doi		= {10.1145/3480001.3480022},
  abstract	= {Knowledge Graph is a graph knowledge base composed of fact
		  entities and relations. Recently, the adoption of Knowledge
		  Graph in Natural Language Processing tasks has proved the
		  efficiency and convenience of KG. Therefore, the
		  plausibility of Knowledge Graph become an import subject,
		  which is also named as KG Completion or Link Prediction.
		  The plausibility of Knowledge Graph reflects in the
		  validness of triples which is structured representation of
		  the entities and relations of Knowledge Graph. Some
		  research work has devoted to KG Completion tasks. The
		  typical methods include semantic matching models like
		  TransE or TransH and Pre-trained models like KG-BERT. In
		  this article, we propose a novel method based on the
		  pre-trained model XLNET and the classification model to
		  verify whether the triples of Knowledge Graph are valid or
		  not. This method takes description of entities or relations
		  as the input sentence text for fine-tuning. Meanwhile
		  contextualized representations with rich semantic
		  information can be obtained by XLNET, avoiding limitations
		  and shortcomings of other typical neural network models.
		  Then these representations are fed into a classifier for
		  classification. Experimental results show that there is an
		  improvement in KG Completion Tasks that the proposed method
		  has achieved.},
  booktitle	= {Proceedings of the 2021 5th International Conference on
		  Deep Learning Technologies},
  pages		= {13–19},
  numpages	= {7},
  keywords	= {GRU, KG Completion, Knowledge Graph, LSTM, XLNet},
  location	= {Qingdao, China},
  series	= {ICDLT '21}
}

@InProceedings{	  10.1145/3543873.3587585,
  author	= {Timmer, Roelien C. and Mark, Megan and Khoo, Fech Scen and
		  Ribeiro Martins, Marcella Scoczynski and Berea, Anamaria
		  and Renard, Gregory and Bugbee, Kaylin},
  title		= {NASA Science Mission Directorate Knowledge Graph
		  Discovery},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587585},
  doi		= {10.1145/3543873.3587585},
  abstract	= {The size of the National Aeronautics and Space
		  Administration (NASA) Science Mission Directorate (SMD)
		  data catalog is growing exponentially, allowing researchers
		  to make discoveries. However, making discoveries is
		  challenging and time-consuming due to the size of the data
		  catalogs, and as many concepts and data are indirectly
		  connected. This paper proposes a pipeline to generate
		  knowledge graphs (KGs) representing different NASA SMD
		  domains. These KGs can be used as the basis for dataset
		  search engines, saving researchers time and supporting them
		  in finding new connections. We collected textual data and
		  used several modern natural language processing (NLP)
		  methods to create the nodes and the edges of the KGs. We
		  explore the cross-domain connections, discuss our
		  challenges, and provide future directions to inspire
		  researchers working on similar challenges.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {795–799},
  numpages	= {5},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3562007.3562030,
  author	= {Xu, Yong and Chen, Bao and Zhen, Jingru and Ma, Guoqing
		  and Chen, Gongbin and Liu, Yan and Fang, Qun},
  title		= {NRKM: News Recommendation Based on Knowledge Graph with
		  Multi-View Learning},
  year		= {2022},
  isbn		= {9781450396851},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3562007.3562030},
  doi		= {10.1145/3562007.3562030},
  abstract	= {News recommendation is necessary to help users find
		  interesting news, improve their experience, and alleviate
		  information overload. Accurately learning news and user
		  representations is a key task in news recommendation
		  systems. News texts usually contain rich entities, however
		  existing recommender systems ignore the importance of news
		  entities. In order to effectively alleviate the above
		  problems, we design a multi-view news recommendation system
		  based on knowledge graph. First, with news headlines,
		  summaries, categories, and knowledge graph features, we
		  learn news representations using a graph interactive
		  attention network and a multi-head attention mechanism.
		  Second, we combine a recurrent neural network and an
		  interactive attention network to learn user representations
		  from user historical click news records. Finally, predict
		  the probability that the user will click on the candidate
		  news. This method effectively alleviates the problem that
		  the current news recommendation model has a weak ability to
		  capture news representations and user interest
		  representations. Experiments on real datasets show that
		  this method can effectively improve the performance of news
		  recommendation.},
  booktitle	= {Proceedings of the 2022 3rd International Conference on
		  Control, Robotics and Intelligent System},
  pages		= {123–127},
  numpages	= {5},
  keywords	= {Attention Mechanism, Knowledge Graph, News
		  Recommendation},
  location	= {Virtual Event, China},
  series	= {CCRIS '22}
}

@InProceedings{	  10.1145/3503161.3548273,
  author	= {Cao, Xianshuai and Shi, Yuliang and Wang, Jihu and Yu, Han
		  and Wang, Xinjun and Yan, Zhongmin},
  title		= {Cross-modal Knowledge Graph Contrastive Learning for
		  Machine Learning Method Recommendation},
  year		= {2022},
  isbn		= {9781450392037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503161.3548273},
  doi		= {10.1145/3503161.3548273},
  abstract	= {The explosive growth of machine learning (ML) methods is
		  overloading users with choices for learning tasks. Method
		  recommendation aims to alleviate this problem by selecting
		  the most appropriate ML methods for given learning tasks.
		  Recent research shows that the descriptive and structural
		  information of the knowledge graphs (KGs) can significantly
		  enhance the performance of ML method recommendation.
		  However, existing studies have not fully explored the
		  descriptive information in KGs, nor have they effectively
		  exploited the descriptive and structural information to
		  provide the necessary supervision. To address these
		  limitations, we distinguish descriptive attributes from the
		  traditional relationships in KGs with the rest as
		  structural connections to expand the scope of KG
		  descriptive information. Based on this insight, we propose
		  the Cross-modal Knowledge Graph Contrastive learning (CKGC)
		  approach, which regards information from descriptive
		  attributes and structural connections as two modalities,
		  learning informative node representations by maximizing the
		  agreement between the descriptive view and the structural
		  view. Through extensive experiments, we demonstrate that
		  CKGC significantly outperforms the state-of-the-art
		  baselines, achieving around 2% higher accurate
		  click-through-rate (CTR) prediction, over 30% more accurate
		  top-10 recommendation, and over 50% more accurate top-20
		  recommendation compared to the best performing existing
		  approach.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Multimedia},
  pages		= {3694–3702},
  numpages	= {9},
  keywords	= {contrastive learning, cross modalities, knowledge graph,
		  recommender system},
  location	= {Lisboa, Portugal},
  series	= {MM '22}
}

@InProceedings{	  10.1145/3615887.3627754,
  author	= {Rawsthorne, Helen Mair and Abadie, Nathalie and Kergosien,
		  Eric and Duch\^{e}ne, C\'{e}cile and Saux, \'{E}ric},
  title		= {Automatic Nested Spatial Entity and Spatial Relation
		  Extraction From Text for Knowledge Graph Creation: A
		  Baseline Approach and a Benchmark Dataset},
  year		= {2023},
  isbn		= {9798400703492},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3615887.3627754},
  doi		= {10.1145/3615887.3627754},
  abstract	= {Automatically extracting geographic information from text
		  is the key to harnessing the vast amount of spatial
		  knowledge that only exists in this unstructured form. The
		  fundamental elements of spatial knowledge include spatial
		  entities, their types and the spatial relations between
		  them. Structuring the spatial knowledge contained within
		  text as a geospatial knowledge graph, and disambiguating
		  the spatial entities, significantly facilitates its reuse.
		  The automatic extraction of geographic information from
		  text also allows the creation or enrichment of gazetteers.
		  We propose a baseline approach for nested spatial entity
		  and binary spatial relation extraction from text, a new
		  annotated French-language benchmark dataset on the maritime
		  domain that can be used to train algorithms for both
		  extraction tasks, and benchmark results for the two tasks
		  carried out individually and end-to-end. Our approach
		  involves applying the Princeton University Relation
		  Extraction system (PURE), made for flat, generic entity
		  extraction and generic binary relation extraction, to the
		  extraction of nested, spatial entities and spatial binary
		  relations. By extracting nested spatial entities and the
		  spatial relations between them, we have more information to
		  aid entity disambiguation. In our experiments we compare
		  the performance of a pretrained monolingual French BERT
		  language model with that of a pretrained multilingual BERT
		  language model, and study the effect of including
		  cross-sentence context. Our results reveal very similar
		  results for both models, although the multilingual model
		  performs slightly better in entity extraction, and the
		  monolingual model has slightly better relation extraction
		  and end-to-end performances. We observe that increasing the
		  amount of cross-sentence context improves the results for
		  entity extraction whereas it has the opposite effect on
		  relation extraction.},
  booktitle	= {Proceedings of the 7th ACM SIGSPATIAL International
		  Workshop on Geospatial Humanities},
  pages		= {21–30},
  numpages	= {10},
  keywords	= {binary spatial relation, deep learning, geographic
		  information, language model, maritime data, nested spatial
		  entity, neural network, spatial knowledge},
  location	= {Hamburg, Germany},
  series	= {GeoHumanities '23}
}

@InProceedings{	  10.1145/3580305.3599801,
  author	= {Zhang, Shiyuan and Li, Tong and Hui, Shuodi and Li,
		  Guangyu and Liang, Yanping and Yu, Li and Jin, Depeng and
		  Li, Yong},
  title		= {Deep Transfer Learning for City-scale Cellular Traffic
		  Generation through Urban Knowledge Graph},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599801},
  doi		= {10.1145/3580305.3599801},
  abstract	= {The problem of cellular traffic generation in cities
		  without historical traffic data is critical and urgently
		  needs to be solved to assist 5G base station deployments in
		  mobile networks. In this paper, we propose ADAPTIVE, a deep
		  transfer learning framework for city-scale cellular traffic
		  generation through the urban knowledge graph. ADAPTIVE
		  leverages historical data from other cities that have
		  deployed 5G networks to assist cities that are newly
		  deploying 5G networks through deep transfer learning.
		  Specifically, ADAPTIVE can align the representations of
		  base stations in the target city and source city while
		  considering the environmental factors of cities, spatial
		  and environmental contextual relations between base
		  stations, and traffic temporal patterns at base stations.
		  We next design a feature-enhanced generative adversarial
		  network, which is trained based on the historical traffic
		  data and representations of base stations in the source
		  city. By feeding the aligned target city's base station
		  representations into the trained model, we can then obtain
		  the generated traffic data for the target city. Extensive
		  experiments on real-world cellular traffic datasets show
		  that ADAPTIVE generally outperforms state-of-the-art
		  baselines by more than 40% in terms of Jensen-Shannon
		  divergence and root-mean-square error. Also, ADAPTIVE has
		  strong robustness based on the results of various
		  cross-city experiments. ADAPTIVE has been successfully
		  deployed on the 'Jiutian' Artificial Intelligence Platform
		  of China Mobile to support cellular traffic generation and
		  assist in the construction and operation of mobile
		  networks.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4842–4851},
  numpages	= {10},
  keywords	= {gan, cellular traffic, transfer learning, urban knowledge
		  graph},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3534678.3539210,
  author	= {Liu, Xiao and Yin, Da and Zheng, Jingnan and Zhang,
		  Xingjian and Zhang, Peng and Yang, Hongxia and Dong, Yuxiao
		  and Tang, Jie},
  title		= {OAG-BERT: Towards a Unified Backbone Language Model for
		  Academic Knowledge Services},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539210},
  doi		= {10.1145/3534678.3539210},
  abstract	= {Academic Knowledge Services have substantially facilitated
		  the development of human science and technology, providing
		  a plenitude of useful research tools. However, many
		  applications highly depend on ad-hoc models and expensive
		  human labeling to understand professional contents,
		  hindering deployments in real world. To create a unified
		  backbone language model for various knowledge-intensive
		  academic knowledge mining challenges, based on the world's
		  largest public academic graph Open Academic Graph (OAG), we
		  pre-train an academic language model, namely OAG-BERT, to
		  integrate massive heterogeneous entity knowledge beyond
		  scientific corpora. We develop novel pre-training
		  strategies along with zero-shot inference techniques.
		  OAG-BERT's superior performance on 9 knowledge-intensive
		  academic tasks (including 2 demo applications) demonstrates
		  its qualification to serve as a foundation for academic
		  knowledge services. Its zero-shot capability also offers
		  great potential to mitigate the need of costly annotations.
		  OAG-BERT has been deployed to multiple real-world
		  applications, such as reviewer recommendations for NSFC
		  (National Nature Science Foundation of China) and paper
		  tagging in the AMiner system. All codes and pre-trained
		  models are available via the CogDL.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3418–3428},
  numpages	= {11},
  keywords	= {heterogeneous knowledge graph, language model,
		  pre-training},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3627915.3628091,
  author	= {Ding, Kai and Liu, Jiamei and Gu, Caiyuan and Wang, Ning
		  and Fan, Xiaoyan},
  title		= {Research on Construction of Chinese Technology Literature
		  Question Answering System Based on Knowledge Graph},
  year		= {2023},
  isbn		= {9798400700590},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627915.3628091},
  doi		= {10.1145/3627915.3628091},
  abstract	= {In order to improve the knowledge service ability of the
		  library and meet the user's demand for querying Chinese
		  technology literature in natural language. This article
		  proposes a two-stage method to implement the question
		  answering system for Chinese technology literature. In the
		  first stage, the Chinese question classification system
		  based on sentence pattern is designed, and the stacking
		  framework of ensemble learning algorithm is used to
		  classify Chinese question. In the second stage, the
		  pipeline is employed to parse the natural language question
		  sentences and transfer them into Cypher statement. The
		  field of "competitive intelligence" is selected for
		  experiment. The experimental results show that the proposed
		  stacking classification model improves the metric F1 by
		  10.81% compared with the single model average. Overall, the
		  accuracy of translating Chinese questions into Cypher
		  statements reached 83.96%. The proposed method can
		  effectively convert Chinese questions into Cypher
		  statements and directly return the answer to users. It
		  provides a reference scheme for the application of the
		  question answering based on knowledge map, which can
		  improve the knowledge service ability of the library.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Computer Science and Application Engineering},
  articleno	= {34},
  numpages	= {5},
  keywords	= {Cypher Statement, Ensemble Learning, Graph Database,
		  Knowledge Graph, Q&amp;A System},
  location	= {Virtual Event, China},
  series	= {CSAE '23}
}

@InProceedings{	  10.1145/3539618.3591763,
  author	= {Yao, Yunzhi and Mao, Shengyu and Zhang, Ningyu and Chen,
		  Xiang and Deng, Shumin and Chen, Xi and Chen, Huajun},
  title		= {Schema-aware Reference as Prompt Improves Data-Efficient
		  Knowledge Graph Construction},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591763},
  doi		= {10.1145/3539618.3591763},
  abstract	= {With the development of pre-trained language models, many
		  prompt-based approaches to data-efficient knowledge graph
		  construction have been proposed and achieved impressive
		  performance. However, existing prompt-based learning
		  methods for knowledge graph construction are still
		  susceptible to several potential limitations: (i) semantic
		  gap between natural language and output structured
		  knowledge with pre-defined schema, which means model cannot
		  fully exploit semantic knowledge with the constrained
		  templates; (ii) representation learning with locally
		  individual instances limits the performance given the
		  insufficient features, which are unable to unleash the
		  potential analogical capability of pre-trained language
		  models. Motivated by these observations, we propose a
		  retrieval-augmented approach, which retrieves schema-aware
		  Reference As Prompt (RAP), for data-efficient knowledge
		  graph construction. It can dynamically leverage schema and
		  knowledge inherited from human-annotated and
		  weak-supervised data as a prompt for each sample, which is
		  model-agnostic and can be plugged into widespread existing
		  approaches. Experimental results demonstrate that previous
		  methods integrated with RAP can achieve impressive
		  performance gains in low-resource settings on five datasets
		  of relational triple extraction and event extraction for
		  knowledge graph construction Code is available in
		  https://github.com/zjunlp/RAP.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {911–921},
  numpages	= {11},
  keywords	= {event extraction, prompt-based learning, triple
		  extraction},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3477495.3531992,
  author	= {Chen, Xiang and Zhang, Ningyu and Li, Lei and Deng, Shumin
		  and Tan, Chuanqi and Xu, Changliang and Huang, Fei and Si,
		  Luo and Chen, Huajun},
  title		= {Hybrid Transformer with Multi-level Fusion for Multimodal
		  Knowledge Graph Completion},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531992},
  doi		= {10.1145/3477495.3531992},
  abstract	= {Multimodal Knowledge Graphs (MKGs), which organize
		  visual-text factual knowledge, have recently been
		  successfully applied to tasks such as information
		  retrieval, question answering, and recommendation system.
		  Since most MKGs are far from complete, extensive knowledge
		  graph completion studies have been proposed focusing on the
		  multimodal entity, relation extraction and link prediction.
		  However, different tasks and modalities require changes to
		  the model architecture, and not all images/objects are
		  relevant to text input, which hinders the applicability to
		  diverse real-world scenarios. In this paper, we propose a
		  hybrid transformer with multi-level fusion to address those
		  issues. Specifically, we leverage a hybrid transformer
		  architecture with unified input-output for diverse
		  multimodal knowledge graph completion tasks. Moreover, we
		  propose multi-level fusion, which integrates visual and
		  text representation via coarse-grained prefix-guided
		  interaction and fine-grained correlation-aware fusion
		  modules. We conduct extensive experiments to validate that
		  our MKGformer can obtain SOTA performance on four datasets
		  of multimodal link prediction, multimodal RE, and
		  multimodal NER1. https://github.com/zjunlp/MKGformer.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {904–915},
  numpages	= {12},
  keywords	= {knowledge graph completion, multimodal, named entity
		  recognition, relation extraction},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3603765.3603771,
  author	= {Diaz Gonzalez, Armando D. and Hughes, Kevin S. and Yue,
		  Songhui and Hayes, Sean T.},
  title		= {Applying BioBERT to Extract Germline Gene-Disease
		  Associations for Building a Knowledge Graph from the
		  Biomedical Literature},
  year		= {2023},
  isbn		= {9798400700637},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603765.3603771},
  doi		= {10.1145/3603765.3603771},
  abstract	= {Published biomedical information has and continues to
		  rapidly increase. The recent advancements in Natural
		  Language Processing (NLP), have generated considerable
		  interest in automating the extraction, normalization, and
		  representation of biomedical knowledge about entities such
		  as genes and diseases. Our study analyzes germline
		  abstracts in the construction of knowledge graphs of the
		  immense work that has been done in this area for genes and
		  diseases. This paper presents SimpleGermKG, an automatic
		  knowledge graph construction approach that connects
		  germline genes and diseases. For the extraction of genes
		  and diseases, we employ BioBERT, a pre-trained BERT model
		  on biomedical corpora. We propose an ontology-based and
		  rule-based algorithm to standardize and disambiguate
		  medical terms. For semantic relationships between articles,
		  genes, and diseases, we implemented a part-whole relation
		  approach to connect each entity with its data source and
		  visualize them in a graph-based knowledge representation.
		  Lastly, we discuss the knowledge graph applications,
		  limitations, and challenges to inspire the future research
		  of germline corpora. Our knowledge graph contains 297
		  genes, 130 diseases, and 46,747 triples. Graph-based
		  visualizations are used to show the results.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Information System and Data Mining},
  pages		= {37–42},
  numpages	= {6},
  keywords	= {BioBERT, entity recognition, germline mutations, knowledge
		  graph, semantic relation},
  location	= {Atlanta, USA},
  series	= {ICISDM '23}
}

@InProceedings{	  10.1145/3459637.3482003,
  author	= {Deng, Cheng and Jia, Yuting and Xu, Hui and Zhang, Chong
		  and Tang, Jingyao and Fu, Luoyi and Zhang, Weinan and
		  Zhang, Haisong and Wang, Xinbing and Zhou, Chenghu},
  title		= {GAKG: A Multimodal Geoscience Academic Knowledge Graph},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482003},
  doi		= {10.1145/3459637.3482003},
  abstract	= {The research of geoscience plays a strong role in helping
		  people gain a better understanding of the Earth. To
		  effectively represent the knowledge (KG) from enormous
		  geoscience research papers, knowledge graphs can be a
		  powerful means. In the face of enormous geoscience research
		  papers, knowledge graphs can be a powerful means to manage
		  the relationships of data and integrate knowledge extracted
		  from them. However, the existing geoscience KGs mainly
		  focus on the external connection between concepts, whereas
		  the potential abundant information contained in the
		  internal multimodal data of the paper is largely overlooked
		  for more fine-grained knowledge mining. To this end, we
		  propose GAKG, a large-scale multimodal academic KG based on
		  1.12 million papers published in various geoscience-related
		  journals. In addition to the bibliometrics elements, we
		  also extracted the internal illustrations, tables, and text
		  information of the articles, and dig out the knowledge
		  entities of the papers and the era and spatial attributes
		  of the articles, coupling multimodal academic data and
		  features. Specifically, GAKG realizes knowledge entity
		  extraction under our proposed Human-In-the-Loop framework,
		  the novelty of which is to combine the techniques of
		  machine reading and information retrieval with manual
		  annotation of geoscientists in the loop. Considering the
		  fact that literature of geoscience often contains more
		  abundant illustrations and time scale information compared
		  with that of other disciplines, we extract all the
		  geographical information and era from the geoscience
		  papers' text and illustrations, mapping papers to the atlas
		  and chronology. Based on GAKG, we build several knowledge
		  discovery benchmarks for finding geoscience communities and
		  predicting potential links. GAKG and its services have been
		  made publicly available and user-friendly.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4445–4454},
  numpages	= {10},
  keywords	= {data management, geoscience academic knowledge graph,
		  information extraction, knowledge base},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3502223.3502244,
  author	= {Ye, Ganqiang and Zhang, Wen and Bi, Zhen and Wong, Chi Man
		  and Hui, Chen and Chen, Huajun},
  title		= {Improving Knowledge Graph Representation Learning by
		  Structure Contextual Pre-training},
  year		= {2022},
  isbn		= {9781450395656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3502223.3502244},
  doi		= {10.1145/3502223.3502244},
  abstract	= {Representation learning models for Knowledge Graphs (KG)
		  have proven to be effective in encoding structural
		  information and performing reasoning over KGs. In this
		  paper, we propose a novel pre-training-then-fine-tuning
		  framework for knowledge graph representation learning, in
		  which a KG model is firstly pre-trained with triple
		  classification task, followed by discriminative fine-tuning
		  on specific downstream tasks such as entity type prediction
		  and entity alignment. Drawing on the general ideas of
		  learning deep contextualized word representations in
		  typical pre-trained language models, we propose SCoP to
		  learn pre-trained KG representations with structural and
		  contextual triples of the target triple encoded.
		  Experimental results demonstrate that fine-tuning SCoP not
		  only outperforms results of baselines on a portfolio of
		  downstream tasks but also avoids tedious task-specific
		  model design and parameter training.},
  booktitle	= {Proceedings of the 10th International Joint Conference on
		  Knowledge Graphs},
  pages		= {151–155},
  numpages	= {5},
  keywords	= {Contextual Triple, Embedding, Knowledge Graph,
		  Pre-training},
  location	= {Virtual Event, Thailand},
  series	= {IJCKG '21}
}

@InProceedings{	  10.1145/3488560.3502193,
  author	= {Su, Juntao and Dougherty, Edward T. and Jiang, Shuang and
		  Jin, Fang},
  title		= {An Interactive Knowledge Graph Based Platform for COVID-19
		  Clinical Research},
  year		= {2022},
  isbn		= {9781450391320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3488560.3502193},
  doi		= {10.1145/3488560.3502193},
  abstract	= {Since the first identified case of COVID-19 in December
		  2019, a plethora of pharmaceuticals and therapeutics have
		  been tested for COVID-19 treatment. While medical
		  advancements and breakthroughs are well underway, the sheer
		  number of studies, treatments, and associated reports makes
		  it extremely challenging to keep track of the rapidly
		  growing COVID-19 research landscape. While existing
		  scientific literature search systems provide basic document
		  retrieval, they fundamentally lack the ability to explore
		  data, and in addition, do not help develop a deeper
		  understanding of COVID-19 related clinical experiments and
		  findings. As research expands, results do so as well,
		  resulting in a position that is complicated and
		  overwhelming. To address this issue, we present a named
		  entity recognition based framework that accurately extracts
		  COVID-19 related information from clinical test results
		  articles, and generates an efficient and interactive visual
		  knowledge graph. This knowledge graph platform is user
		  friendly, and provides intuitive and convenient tools to
		  explore and analyze COVID-19 research data and results
		  including medicinal performances, side effects and target
		  populations.},
  booktitle	= {Proceedings of the Fifteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1609–1612},
  numpages	= {4},
  keywords	= {clinical results, covid-19, knowledge graph, named entity
		  recognition},
  location	= {Virtual Event, AZ, USA},
  series	= {WSDM '22}
}

@InProceedings{	  10.1145/3508230.3508252,
  author	= {Mehta, Shreyansh and Radke, Mansi and Sunkle, Sagar},
  title		= {Named Entity Recognition using Knowledge Graph Embeddings
		  and DistilBERT},
  year		= {2022},
  isbn		= {9781450387354},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3508230.3508252},
  doi		= {10.1145/3508230.3508252},
  abstract	= {Named Entity Recognition (NER) is a Natural Language
		  Processing (NLP) task of identifying entities from a
		  natural language text and classifies them into categories
		  like Person, Location, Organization etc. Pre-trained neural
		  language models (PNLM) based on transformers are
		  state-of-the-art in many NLP task including NER. Analysis
		  of output of DistilBERT, a popular PNLM, reveals that
		  mis-classifications occur when a non-entity word is at a
		  place contextually suitable for an entity. The paper is
		  based on the hypothesis that the performance of a PNLM can
		  be improved by combining it with Knowledge Graph Embeddings
		  (KGE). We show that fine-tuning of DistilBERT along with
		  NumberBatch KGE gives performance improvement over various
		  Open-domain as well as Biomedical-domain datasets.},
  booktitle	= {Proceedings of the 2021 5th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {146–150},
  numpages	= {5},
  keywords	= {Contextualized Word Representation, Knowledge Graph
		  Embeddings, Named Entity Recognition, Neural Networks},
  location	= {Sanya, China},
  series	= {NLPIR '21}
}

@InProceedings{	  10.1145/3460210.3493582,
  author	= {Jaradeh, Mohamad Yaser and Singh, Kuldeep and Stocker,
		  Markus and Auer, S\"{o}ren},
  title		= {Triple Classification for Scholarly Knowledge Graph
		  Completion},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493582},
  doi		= {10.1145/3460210.3493582},
  abstract	= {structured information representing knowledge encoded in
		  scientific publications. With the sheer volume of published
		  scientific literature comprising a plethora of
		  inhomogeneous entities and relations to describe scientific
		  concepts, these KGs are inherently incomplete. We present
		  exBERT, a method for leveraging pre-trained transformer
		  language models to perform scholarly knowledge graph
		  completion. We model triples of a knowledge graph as text
		  and perform triple classification (i.e., belongs to KG or
		  not). The evaluation shows that exBERT outperforms other
		  baselines on three scholarly KG completion datasets in the
		  tasks of triple classification, link prediction, and
		  relation prediction. Furthermore, we present two scholarly
		  datasets as resources for the research community, collected
		  from public KGs and online resources.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {225–232},
  numpages	= {8},
  keywords	= {link prediction, relation prediction, scholarly knowledge
		  graphs, triple classification},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@InProceedings{	  10.1145/3624062.3624172,
  author	= {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao,
		  Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and
		  Xie, Zhen and Cerpa, Alberto and Du, Wan},
  title		= {HPC-GPT: Integrating Large Language Model for
		  High-Performance Computing},
  year		= {2023},
  isbn		= {9798400707858},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3624062.3624172},
  doi		= {10.1145/3624062.3624172},
  abstract	= {Large Language Models (LLMs), including the LLaMA model,
		  have exhibited their efficacy across various general-domain
		  natural language processing (NLP) tasks. However, their
		  performance in high-performance computing (HPC) domain
		  tasks has been less than optimal due to the specialized
		  expertise required to interpret the model’s responses. In
		  response to this challenge, we propose HPC-GPT, a novel
		  LLaMA-based model that has been supervised fine-tuning
		  using generated QA (Question-Answer) instances for the HPC
		  domain. To evaluate its effectiveness, we concentrate on
		  two HPC tasks: managing AI models and datasets for HPC, and
		  data race detection. By employing HPC-GPT, we demonstrate
		  comparable performance with existing methods on both tasks,
		  exemplifying its excellence in HPC-related scenarios. Our
		  experiments on open-source benchmarks yield extensive
		  results, underscoring HPC-GPT’s potential to bridge the
		  performance gap between LLMs and HPC-specific tasks. With
		  HPC-GPT, we aim to pave the way for LLMs to excel in HPC
		  domains, simplifying the utilization of language models in
		  complex computing applications.},
  booktitle	= {Proceedings of the SC '23 Workshops of the International
		  Conference on High Performance Computing, Network, Storage,
		  and Analysis},
  pages		= {951–960},
  numpages	= {10},
  keywords	= {Data Race Detection, High-performance Computing, Large
		  Language Model, Neural Network., OpenMP},
  location	= {Denver, CO, USA},
  series	= {SC-W '23}
}

@Article{	  10.1109/taslp.2023.3331096,
  author	= {Wang, Yile and Zhang, Yue and Li, Peng and Liu, Yang},
  title		= {Gradual Syntactic Label Replacement for Language Model
		  Pre-Training},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3331096},
  doi		= {10.1109/TASLP.2023.3331096},
  abstract	= {Pre-training serves as a foundation of recent NLP models,
		  where language modeling tasks are performed over large
		  texts. Typical models like BERT and GPT take the corpus as
		  a whole and treat each word equally for language modeling.
		  However, recent works show that the naturally existing
		  frequency bias in the raw corpus may limit the power of the
		  language model. In this article, we propose a multi-stage
		  training strategy that gradually increases the training
		  vocabulary by modifying the training data. Specifically, we
		  leverage the syntactic structure as a bridge for infrequent
		  words and replace them with the corresponding syntactic
		  labels, then we recover their original lexical surface for
		  further training. Such strategy results in an easy-to-hard
		  curriculum learning process, where the model learns the
		  most common words and some basic syntax concepts, before
		  recognizing a large number of uncommon words via their
		  specific usages and the previously learned category
		  knowledge. Experimental results show that such a method can
		  improve the performance of both discriminative and
		  generative pre-trained language models on benchmarks and
		  various downstream tasks.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {486–496},
  numpages	= {11}
}

@Article{	  10.1145/3617174,
  author	= {Huang, Qing and Yuan, Zhiqiang and Xing, Zhenchang and
		  Peng, Xin and Xu, Xiwei and Lu, Qinghua},
  title		= {FQN Inference in Partial Code by Prompt-tuned Language
		  Model of Code},
  year		= {2023},
  issue_date	= {February 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {2},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3617174},
  doi		= {10.1145/3617174},
  abstract	= {Partial code usually involves non-fully-qualified type
		  names (non-FQNs) and undeclared receiving objects.
		  Resolving the FQNs of these non-FQN types and undeclared
		  receiving objects (referred to as type inference) is the
		  prerequisite to effective search and reuse of partial code.
		  Existing dictionary-lookup based methods build a symbolic
		  knowledge base of API names and code contexts, which
		  involve significant compilation overhead and are sensitive
		  to unseen API names and code context variations. In this
		  article, we propose using a prompt-tuned code masked
		  language model (MLM) as a neural knowledge base for type
		  inference, called POME, which is lightweight and has
		  minimal requirements on code compilation. Unlike the
		  existing symbol name and context matching for type
		  inference, POME infers the FQNs syntax and usage knowledge
		  encapsulated in prompt-tuned code MLM through a colze-style
		  fill-in-blank strategy. POME is integrated as a plug-in
		  into web and integrated development environments (IDE) to
		  assist developers in inferring FQNs in the real world. We
		  systematically evaluate POME on a large amount of source
		  code from GitHub and Stack Overflow, and explore its
		  generalization and hybrid capability. The results validate
		  the effectiveness of the POME design and its applicability
		  for partial code type inference, and they can be easily
		  extended to different programming languages (PL). POME can
		  also be used to generate a PL-hybrid type inference model
		  for providing a one-for-all solution. As the first of its
		  kind, our neural type inference method opens the door to
		  many innovative ways of using partial code.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= dec,
  articleno	= {31},
  numpages	= {32},
  keywords	= {Type inference, fully qualified names, code masked
		  language model, neural knowledge base}
}

@InProceedings{	  10.1145/3511808.3557318,
  author	= {Jin, Xin and Sun, Xia and Chen, Jiacheng and Sutcliffe,
		  Richard},
  title		= {Extracting Drug-drug Interactions from Biomedical Texts
		  using Knowledge Graph Embeddings and Multi-focal Loss},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557318},
  doi		= {10.1145/3511808.3557318},
  abstract	= {The field of Drug-drug interaction (DDI) aims to detect
		  descriptions of interactions between drugs from biomedical
		  texts. Currently, researchers have extracted DDIs using
		  pre-trained language models such as BERT, which often
		  misclassify two kinds of DDI types, "Effect" and "Int", on
		  the DDIExtraction 2013 corpus because of highly similar
		  expressions. The use of knowledge graphs can alleviate this
		  problem by incorporating different relationships for each,
		  thus allowing them to be distinguished. Thus, we propose a
		  novel framework to integrate the neural network with a
		  knowledge graph, where the features from these components
		  are complementary. Specifically, we take text features at
		  different levels into account in the neural network part.
		  This is done by firstly obtaining a word-level position
		  feature using PubMedBERT together with a convolution neural
		  network, secondly, getting a phrase-level key path feature
		  using a dependency parsing tree, thirdly, using PubMedBERT
		  with an attention mechanism to obtain a sentence-level
		  language feature, and finally, fusing these three kinds of
		  representation into a synthesized feature. We also extract
		  a knowledge feature from a drug knowledge graph which takes
		  just a few minutes to construct, then concatenate the
		  synthesized feature with the knowledge feature, feed the
		  result into a multi-layer perceptron and obtain the result
		  by a softmax classifier. In order to achieve a good
		  integration of the synthesized feature and the knowledge
		  feature, we train the model using a novel multifocal loss
		  function, KGE-MFL, which is based on a knowledge graph
		  embedding. Finally we attain state-of-the-art results on
		  the DDIExtraction 2013 dataset (micro F-score 86.24%) and
		  on the ChemProt dataset (micro F-score 77.75%), which
		  proves our framework to be effective for biomedical
		  relation extraction tasks. In particular, we fill the
		  performance gap (more than 5.57%) between methods that rely
		  on and do not rely on knowledge graph embedding on the
		  DDIExtraction 2013 corpus, when predicting the "Int" type.
		  The implementation code is available at
		  https://github.com/NWU-IPMI/DDIE-KGE-MFL.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {884–893},
  numpages	= {10},
  keywords	= {drug-drug interactions, imbalance problem, knowledge
		  graph, pubmedbert},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3579051.3579065,
  author	= {Wu, Zhanglin and Zhang, Min and Zhu, Ming and Li, Yinglu
		  and Zhu, Ting and Yang, Hao and Peng, Song and Qin, Ying},
  title		= {KG-BERTScore: Incorporating Knowledge Graph into BERTScore
		  for Reference-Free Machine Translation Evaluation},
  year		= {2023},
  isbn		= {9781450399876},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579051.3579065},
  doi		= {10.1145/3579051.3579065},
  abstract	= {BERTScore is an effective and robust automatic metric for
		  reference-based machine translation evaluation. In this
		  paper, we incorporate multilingual knowledge graph into
		  BERTScore and propose a metric named KG-BERTScore, which
		  linearly combines the results of BERTScore and bilingual
		  named entity matching for reference-free machine
		  translation evaluation. From the experimental results on
		  WMT19 QE as a metric without references shared tasks, our
		  metric KG-BERTScore gets higher overall correlation with
		  human judgements than the current state-of-the-art metrics
		  for reference-free machine translation evaluation.1
		  Moreover, the pre-trained multilingual model used by
		  KG-BERTScore and the parameter for linear combination are
		  also studied in this paper.},
  booktitle	= {Proceedings of the 11th International Joint Conference on
		  Knowledge Graphs},
  pages		= {121–125},
  numpages	= {5},
  keywords	= {BERTScore, KG-BERTScore, machine translation evaluation,
		  multilingual knowledge graph, pre-trained multilingual
		  model},
  location	= {Hangzhou, China},
  series	= {IJCKG '22}
}

@InProceedings{	  10.1145/3543507.3584185,
  author	= {Gautam, Nikita and Shumway, David and Kowalcyk, Megan and
		  Khanal, Sarthak and Caragea, Doina and Caragea, Cornelia
		  and Mcginty, Hande and Dorevitch, Samuel},
  title		= {Leveraging Existing Literature on the Web and Deep Neural
		  Models to Build a Knowledge Graph Focused on Water Quality
		  and Health Risks},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3584185},
  doi		= {10.1145/3543507.3584185},
  abstract	= {A knowledge graph focusing on water quality in relation to
		  health risks posed by water activities (such as diving or
		  swimming) is not currently available. To address this
		  limitation, we first use existing resources to construct a
		  knowledge graph relevant to water quality and health risks
		  using KNowledge Acquisition and Representation Methodology
		  (KNARM). Subsequently, we explore knowledge graph
		  completion approaches for maintaining and updating the
		  graph. Specifically, we manually identify a set of
		  domain-specific UMLS concepts and use them to extract a
		  graph of approximately 75,000 semantic triples from the
		  Semantic MEDLINE database (which contains
		  head-relation-tail triples extracted from PubMed). Using
		  the resulting knowledge graph, we experiment with the
		  KG-BERT approach for graph completion by employing
		  pre-trained BERT/RoBERTa models and also models fine-tuned
		  on a collection of water quality and health risks abstracts
		  retrieved from the Web of Science. Experimental results
		  show that KG-BERT with BERT/RoBERTa models fine-tuned on a
		  domain-specific corpus improves the performance of KG-BERT
		  with pre-trained models. Furthermore, KG-BERT gives better
		  results than several translational distance or semantic
		  matching baseline models.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {4161–4171},
  numpages	= {11},
  keywords	= {BERT, Knowledge graph, diving, health risks, knowledge
		  graph completion, water quality, water recreation},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3587716.3587723,
  author	= {Huang, Xinyi and Cheng, Lianglun and Deng, Jianfeng and
		  Wang, Tao},
  title		= {Binocular attention-based stacked BiLSTM NER model for
		  Supply chain management event knowledge graph
		  construction},
  year		= {2023},
  isbn		= {9781450398411},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587716.3587723},
  doi		= {10.1145/3587716.3587723},
  abstract	= {Extracting fine-grained event ontology knowledge based on
		  supply chain management (SCM) related corpus and
		  constructing knowledge graph (KG) has important guiding
		  significance and knowledge support for the efficient
		  implementation and development of SCM in manufacturing
		  enterprises. Recently, research on the KG of SCM has not
		  gained sufficient attention. This paper aims to propose an
		  event logical KG construction approach for SCM.
		  Specifically, a stacked BiLSTM entity recognition model
		  based on the binocular attention mechanism is proposed,
		  called the SBBAN model. Firstly, the character feature
		  attention mechanism is used to infer the key information
		  that contributes greatly to entity recognition in the text
		  sequence. Character weighted features and character
		  features splicing are used as new character input features.
		  Then the deep semantic abstract features of text sequence
		  are obtained by stacked BiLSTM. In addition, a
		  self-attention mechanism is added to obtain the deep
		  context relevant features. Experimental results show that
		  the model shows better performance in in comparison with
		  the state-of-the-art algorithms to complete the matching of
		  event argument entities and offer knowledge support for
		  SCM.},
  booktitle	= {Proceedings of the 2023 15th International Conference on
		  Machine Learning and Computing},
  pages		= {40–46},
  numpages	= {7},
  keywords	= {event logic knowledge graph, named entity recognition,
		  stacked BiLSTM, supply chain management ontology},
  location	= {Zhuhai, China},
  series	= {ICMLC '23}
}

@InProceedings{	  10.1145/3539618.3591698,
  author	= {Atif, Farah and El Khatib, Ola and Difallah, Djellel},
  title		= {BeamQA: Multi-hop Knowledge Graph Question Answering with
		  Sequence-to-Sequence Prediction and Beam Search},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591698},
  doi		= {10.1145/3539618.3591698},
  abstract	= {Knowledge Graph Question Answering (KGQA) is a task that
		  aims to answer natural language queries by extracting facts
		  from a knowledge graph. Current state-of-the-art techniques
		  for KGQA rely on text-based information from graph entity
		  and relations labels, as well as external textual corpora.
		  By reasoning over multiple edges in the graph, these can
		  accurately rank and return the most relevant entities.
		  However, one of the limitations of these methods is that
		  they cannot handle the inherent incompleteness of
		  real-world knowledge graphs and may lead to inaccurate
		  answers due to missing edges. To address this issue, recent
		  advances in graph representation learning have led to the
		  development of systems that can use link prediction
		  techniques to handle missing edges probabilistically,
		  allowing the system to reason with incomplete information.
		  However, existing KGQA frameworks that use such techniques
		  often depend on learning a transformation from the query
		  representation to the graph embedding space, which requires
		  access to a large training dataset. We present BeamQA, an
		  approach that overcomes these limitations by combining a
		  sequence-to-sequence prediction model with beam search
		  execution in the embedding space. Our model uses a
		  pre-trained large language model and synthetic question
		  generation. Our experiments demonstrate the effectiveness
		  of BeamQA when compared to other KGQA methods on two
		  knowledge graph question-answering datasets.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {781–790},
  numpages	= {10},
  keywords	= {knowledge graphs, question answering},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Article{	  10.1145/3596219,
  author	= {Zhao, Shuai and Li, Qing and Yang, Yuer and Wen, Jinming
		  and Luo, Weiqi},
  title		= {From Softmax to Nucleusmax: A Novel Sparse Language Model
		  for Chinese Radiology Report Summarization},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3596219},
  doi		= {10.1145/3596219},
  abstract	= {The Chinese radiology report summarization is a crucial
		  component in smart healthcare that employs language models
		  to summarize key findings in radiology reports and
		  communicate these findings to physicians. However, most
		  language models for radiology report summarization utilize
		  a softmax transformation in their output layer, leading to
		  dense alignments and strictly positive output
		  probabilities. This density is inefficient, reducing model
		  interpretability and giving probability mass to many
		  unrealistic outputs. To tackle this issue, we propose a
		  novel approach named nucleusmax. Nucleusmax is able to
		  mitigate dense outputs and improve model interpretability
		  by truncating the unreliable tail of the probability
		  distribution. In addition, we incorporate nucleusmax with a
		  copy mechanism, a useful technique to avoid professional
		  errors in the generated diagnostic opinions. To further
		  promote the research of radiology report summarization, we
		  also have created a Chinese radiology report summarization
		  dataset, which is freely available. Experimental results
		  showed via both automatic and human evaluation that the
		  proposed approach substantially improves the sparsity and
		  overall quality of outputs over competitive softmax models,
		  producing radiology summaries that approach the quality of
		  those authored by physicians. In general, our work
		  demonstrates the feasibility and prospect of the language
		  model to the domain of radiology and smart healthcare.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {180},
  numpages	= {21},
  keywords	= {Chinese radiology report summarization, language model,
		  softmax, abstractive summarization}
}

@InProceedings{	  10.1145/3605801.3605806,
  author	= {Gao, Shangsheng and Gao, Li and Li, Qi and Xu, Jianjun},
  title		= {Application of large language model in intelligent Q&amp;A
		  of digital government},
  year		= {2023},
  isbn		= {9798400700620},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605801.3605806},
  doi		= {10.1145/3605801.3605806},
  abstract	= {LLM (Large Language Model) is developing rapidly today,
		  and it is very important to use LLM to help existing
		  question answering systems improve. The government question
		  answering system is of great value in improving government
		  administrative efficiency. Existing intelligent questions
		  and answers mostly use the combination of keyword matching
		  and machine learning. But keyword matching is difficult to
		  understand complex and multi-round dialogue questions,
		  resulting in limited quality of reply content. Based on
		  machine learning method, it has further improved the
		  understanding of semantics, but because the understanding
		  of words in the field of government affairs is too
		  difficult and professional; and the semantic recognition of
		  colloquial questions is difficult, the performance is not
		  even as good as the question answering system based on
		  keyword matching. This paper uses the large language model
		  as a tool to help understand user questions, and integrates
		  it into the existing government question answering system.
		  On the premise of effectively utilizing the advantages of
		  the large language model, it avoids its defects. And it is
		  demonstrated by experiments that the above system has a
		  great improvement compared with the previous method.},
  booktitle	= {Proceedings of the 2023 2nd International Conference on
		  Networks, Communications and Information Technology},
  pages		= {24–27},
  numpages	= {4},
  keywords	= {Digital government, Government intelligence Q&amp;A, LLM},
  location	= {Qinghai, China},
  series	= {CNCIT '23}
}

@InProceedings{	  10.1145/3543873.3587335,
  author	= {Su\'{a}rez, Francisca and Hogan, Aidan},
  title		= {Templet: A Collaborative System for Knowledge Graph
		  Question Answering over Wikidata},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587335},
  doi		= {10.1145/3543873.3587335},
  abstract	= {We present Templet: an online question answering (QA)
		  system for Wikidata. Templet is based on the
		  collaboratively-edited repository QAWiki, which collects
		  questions in multiple natural languages along with their
		  corresponding structured queries. Templet generates
		  templates from question–query pairs on QAWiki by
		  replacing key entities with identifiers. Using
		  autocompletion, the user can type a question in natural
		  language, select a template, and again using
		  autocompletion, select the entities they wish to insert
		  into the template’s placeholders, generating a concrete
		  question, query and results. The main objectives of Templet
		  are: (i) to enable users to answer potentially complex
		  questions over Wikidata using natural language templates
		  and autocompletion; (ii)&nbsp;to encourage users to
		  collaboratively create new templates via QAWiki, which in
		  turn can benefit not only Templet, but other QA systems.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {152–155},
  numpages	= {4},
  keywords	= {Wikidata, knowledge graphs, question answering, user
		  interfaces},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3565387.3565405,
  author	= {Chen, Wentong and Fan, Chunxiao and Wu, Yuexin and Wang,
		  Yitong},
  title		= {Chinese Machine Reading Comprehension Based on Language
		  Model Containing Knowledge},
  year		= {2022},
  isbn		= {9781450396004},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3565387.3565405},
  doi		= {10.1145/3565387.3565405},
  abstract	= {Machine reading comprehension (MRC) is a task that
		  requires machines to answer relevant questions based on a
		  given context. In recent years, it has attracted extensive
		  attention with the development of deep learning and big
		  data. Considering that human beings will associate some
		  external relevant knowledge when understanding the text,
		  researchers have proposed a method of introducing knowledge
		  outside the given context to assist reading and this method
		  is called Knowledge-Based Machine Reading Comprehension
		  (KBMRC). However, the current research on this method is
		  still scattered, and the retrieval and fusion of relevant
		  knowledge are still two challenges in application,
		  especially in Chinese MRC. The contribution of this paper
		  mainly on the following three points: Firstly, in order to
		  resolve the problem of related knowledge retrieval, we
		  build up a related knowledge set. Secondly, in order to
		  resolve the problem of related knowledge fusion, we propose
		  a negative sample generation strategy and train a language
		  model containing knowledge. Finally, a twin-tower fusion
		  model is constructed based on this model. The experiments
		  on Chinese reading comprehension dataset CMRC2018 show that
		  our method has a certain improvement compared with the
		  baseline method without external knowledge.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Computer Science and Application Engineering},
  articleno	= {18},
  numpages	= {7},
  keywords	= {knowledge fusion, machine reading comprehension, natural
		  language processing, pre-trained language model},
  location	= {Virtual Event, China},
  series	= {CSAE '22}
}

@InProceedings{	  10.1145/3442381.3450043,
  author	= {Wang, Bo and Shen, Tao and Long, Guodong and Zhou, Tianyi
		  and Wang, Ying and Chang, Yi},
  title		= {Structure-Augmented Text Representation Learning for
		  Efficient Knowledge Graph Completion},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450043},
  doi		= {10.1145/3442381.3450043},
  abstract	= {Human-curated knowledge graphs provide critical supportive
		  information to various natural language processing tasks,
		  but these graphs are usually incomplete, urging
		  auto-completion of them (a.k.a. knowledge graph
		  completion). Prevalent graph embedding approaches, e.g.,
		  TransE, learn structured knowledge via representing graph
		  elements (i.e., entities/relations) into dense embeddings
		  and capturing their triple-level relationship with spatial
		  distance. However, they are hardly generalizable to the
		  elements never visited in training and are intrinsically
		  vulnerable to graph incompleteness. In contrast, textual
		  encoding approaches, e.g., KG-BERT, resort to graph
		  triple’s text and triple-level contextualized
		  representations. They are generalizable enough and robust
		  to the incompleteness, especially when coupled with
		  pre-trained encoders. But two major drawbacks limit the
		  performance: (1) high overheads due to the costly scoring
		  of all possible triples in inference, and (2) a lack of
		  structured knowledge in the textual encoder. In this paper,
		  we follow the textual encoding paradigm and aim to
		  alleviate its drawbacks by augmenting it with graph
		  embedding techniques – a complementary hybrid of both
		  paradigms. Specifically, we partition each triple into two
		  asymmetric parts as in translation-based graph embedding
		  approach, and encode both parts into contextualized
		  representations by a Siamese-style textual encoder. Built
		  upon the representations, our model employs both
		  deterministic classifier and spatial measurement for
		  representation and structure learning respectively. It thus
		  reduces the overheads by reusing graph elements’
		  embeddings to avoid combinatorial explosion, and enhances
		  structured knowledge by exploring the spatial
		  characteristics. Moreover, we develop a self-adaptive
		  ensemble scheme to further improve the performance by
		  incorporating triple scores from an existing graph
		  embedding model. In experiments, we achieve
		  state-of-the-art performance on three benchmarks and a
		  zero-shot dataset for link prediction, with highlights of
		  inference costs reduced by 1-2 orders of magnitude compared
		  to a sophisticated textual encoding method.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {1737–1748},
  numpages	= {12},
  keywords	= {Contextualized Representation, Knowledge Graph Completion,
		  Knowledge Graph Embedding, Link Prediction, Structured
		  Knowledge},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3500931.3500947,
  author	= {Sun, Yue and Li, Yuxuan and Zheng, Bo and Zhu, ShaoJun and
		  Wu, MaoNian},
  title		= {An Intelligent Question-Answering System for Myopia
		  Prevention and Control based on Knowledge Graph},
  year		= {2021},
  isbn		= {9781450395588},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3500931.3500947},
  doi		= {10.1145/3500931.3500947},
  abstract	= {China is facing a serious visual health crisis, especially
		  the trend of high incidence of myopia and low age. The
		  Chinese government pays special attention to the vision
		  health of young people. The prevention and treatment of
		  myopia in children and adolescents has become a public
		  health issue of general concern. In order to further
		  promote the prevention and control of myopia and the
		  management of vision health, this research actively tracks
		  social hot issues, expounding the construction process of
		  the knowledge graph from the three aspects of knowledge
		  acquisition, knowledge fusion, knowledge storage and
		  visualization. Based on the knowledge graph, design
		  intelligent question-answering robots, in-depth research on
		  user intentions, and realize knowledge retrieval and
		  utilization. The intelligent question answering application
		  related to myopia prevention and control proposed in this
		  article can provide references for researchers in related
		  fields.},
  booktitle	= {Proceedings of the 2nd International Symposium on
		  Artificial Intelligence for Medicine Sciences},
  pages		= {80–87},
  numpages	= {8},
  keywords	= {Knowledge Graph, Myopia, Q&amp;A(question-answering)
		  system},
  location	= {Beijing, China},
  series	= {ISAIMS '21}
}

@Article{	  10.1145/3451167,
  author	= {Wang, Yashen and Zhang, Huanhuan and Liu, Zhirun and Zhou,
		  Qiang},
  title		= {Hierarchical Concept-Driven Language Model},
  year		= {2021},
  issue_date	= {June 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {6},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3451167},
  doi		= {10.1145/3451167},
  abstract	= {For guiding natural language generation, many
		  semantic-driven methods have been proposed. While clearly
		  improving the performance of the end-to-end training task,
		  these existing semantic-driven methods still have clear
		  limitations: for example, (i) they only utilize shallow
		  semantic signals (e.g., from topic models) with only a
		  single stochastic hidden layer in their data generation
		  process, which suffer easily from noise (especially adapted
		  for short-text etc.) and lack of interpretation; (ii) they
		  ignore the sentence order and document context, as they
		  treat each document as a bag of sentences, and fail to
		  capture the long-distance dependencies and global semantic
		  meaning of a document. To overcome these problems, we
		  propose a novel semantic-driven language modeling
		  framework, which is a method to learn a Hierarchical
		  Language Model and a Recurrent Conceptualization-enhanced
		  Gamma Belief Network, simultaneously. For scalable
		  inference, we develop the auto-encoding Variational
		  Recurrent Inference, allowing efficient end-to-end training
		  and simultaneously capturing global semantics from a text
		  corpus. Especially, this article introduces concept
		  information derived from high-quality lexical knowledge
		  graph Probase, which leverages strong interpretability and
		  anti-nose capability for the proposed model. Moreover, the
		  proposed model captures not only intra-sentence word
		  dependencies, but also temporal transitions between
		  sentences and inter-sentence concept dependence.
		  Experiments conducted on several NLP tasks validate the
		  superiority of the proposed approach, which could
		  effectively infer meaningful hierarchical concept structure
		  of document and hierarchical multi-scale structures of
		  sequences, even compared with latest state-of-the-art
		  Transformer-based models.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= may,
  articleno	= {104},
  numpages	= {22},
  keywords	= {Language modeling, text generation, concept semantic
		  information, interpretation, recurrent
		  conceptualization-enhanced gamma belief network,
		  hierarchical language modeling, representation learning}
}

@InProceedings{	  10.1145/3491102.3502087,
  author	= {Lee, Yoonjoo and Chung, John Joon Young and Kim, Tae Soo
		  and Song, Jean Y and Kim, Juho},
  title		= {Promptiverse: Scalable Generation of Scaffolding Prompts
		  Through Human-AI Hybrid Knowledge Graph Annotation},
  year		= {2022},
  isbn		= {9781450391573},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3491102.3502087},
  doi		= {10.1145/3491102.3502087},
  abstract	= {Online learners are hugely diverse with varying prior
		  knowledge, but most instructional videos online are created
		  to be one-size-fits-all. Thus, learners may struggle to
		  understand the content by only watching the videos.
		  Providing scaffolding prompts can help learners overcome
		  these struggles through questions and hints that relate
		  different concepts in the videos and elicit meaningful
		  learning. However, serving diverse learners would require a
		  spectrum of scaffolding prompts, which incurs high
		  authoring effort. In this work, we introduce Promptiverse,
		  an approach for generating diverse, multi-turn scaffolding
		  prompts at scale, powered by numerous traversal paths over
		  knowledge graphs. To facilitate the construction of the
		  knowledge graphs, we propose a hybrid human-AI annotation
		  tool, Grannotate. In our study (N=24), participants
		  produced 40 times more on-par quality prompts with higher
		  diversity, through Promptiverse and Grannotate, compared to
		  hand-designed prompts. Promptiverse presents a model for
		  creating diverse and adaptive learning experiences
		  online.},
  booktitle	= {Proceedings of the 2022 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {96},
  numpages	= {18},
  keywords	= {Scaffolding prompt, human-AI hybrid annotation, knowledge
		  graph},
  location	= {New Orleans, LA, USA},
  series	= {CHI '22}
}

@InProceedings{	  10.1145/3500931.3501011,
  author	= {Guo, Chaohui and Lin, Shaofu and Huang, Zhisheng and Yao,
		  Yahong},
  title		= {Mental Health Question and Answering System Based on Bert
		  Model and Knowledge Graph Technology},
  year		= {2021},
  isbn		= {9781450395588},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3500931.3501011},
  doi		= {10.1145/3500931.3501011},
  abstract	= {With the development and progress of society, people are
		  facing increasing pressure. The emergence of this
		  phenomenon has led to a rapid increase in the incidence of
		  mental illness. In order to deal with this phenomenon, this
		  paper proposes a system of question and answering on the
		  basic knowledge of mental health (MHQ&amp;A) by using deep
		  learning retrieval technology and knowledge graph
		  technology. The system MHQ&amp;A is designed mainly for the
		  general public, to answer the basic knowledge of mental
		  health, especially the field of depression. First of all,
		  the basic and the professional question and answer data
		  about mental health were respectively obtained by the
		  reptilian bot from the "IASK" website knowledge and the
		  "Dr. Dingxiang" website. Then, the questions and answers
		  obtained through the crawler are made into a Question and
		  Answering Knowledge Graph of Basic Health Knowledge in the
		  mental health field, which is combined with semantic data
		  of antidepressants and the semantic data of depression
		  papers. Finally, a set of template matching rules is
		  designed to determine the type of problem of users. If the
		  questions are about the professional knowledge of medicine
		  or thesis, the reasoning template will be used to reason
		  and search the answer in the "Question and Answering
		  Knowledge Graph of Basic Health Knowledge in the Mental
		  Health Field". If the questions are about other basic
		  knowledge in the field of mental health, the BERT model is
		  used to vectorize the questions of users, and the matching
		  questions and corresponding answers in the MHQ&amp;A are
		  found through cosine similarity calculation. Through the
		  test of system accuracy, it is proved that the system can
		  effectively combine deep learning technology and
		  knowledge.},
  booktitle	= {Proceedings of the 2nd International Symposium on
		  Artificial Intelligence for Medicine Sciences},
  pages		= {472–476},
  numpages	= {5},
  keywords	= {Deep learning, Knowledge Graph, Mental illness, Question
		  and answering system},
  location	= {Beijing, China},
  series	= {ISAIMS '21}
}

@InProceedings{	  10.1145/3562007.3562036,
  author	= {Tian, Hao and Zhang, Xiaoxiong and Liu, Liu and Liu,
		  Shanshan and Ding, Kun},
  title		= {Knowledge Graph Completion based on Multi-task Learning
		  and Extractive Text Summarization},
  year		= {2022},
  isbn		= {9781450396851},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3562007.3562036},
  doi		= {10.1145/3562007.3562036},
  abstract	= {Knowledge completion graph is an important technology to
		  supplement knowledge graph and improve data quality.
		  However, existing knowledge graph completion methods
		  ignored the characteristics of triple's relation and
		  introduced redundant entity description information. In
		  order to improve the above problems, this paper proposed an
		  ALBERT-KGC model. The key contexts were extracted from
		  redundant entity descriptions by the extraction
		  summarization technology. Then ALBERT was used as an
		  encoder to reduce the number of model parameters, and the
		  multi-task learning method was applied to fine-tune the
		  model, effectively integrating entity and relation
		  features. The results of linking prediction experiments on
		  data sets WN18RR and FB15k-237 showed that the proposed
		  method can improve the MeanRank (MR) index by 31 and 2, and
		  the top 10 hit ratio (Hit@10) index by 1.2% and 5.8%
		  compared with the traditional methods, verifying the
		  validity of the model.},
  booktitle	= {Proceedings of the 2022 3rd International Conference on
		  Control, Robotics and Intelligent System},
  pages		= {159–164},
  numpages	= {6},
  keywords	= {ALBERT, Extractive summary generation, Knowledge
		  completion, Multi-task learning},
  location	= {Virtual Event, China},
  series	= {CCRIS '22}
}

@Article{	  10.1109/taslp.2022.3225537,
  author	= {Li, Yu and Hu, Bojie and Liu, Jian and Chen, Yufeng and
		  Xu, Jinan},
  title		= {A Neighborhood Re-Ranking Model With Relation Constraint
		  for Knowledge Graph Completion},
  year		= {2022},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3225537},
  doi		= {10.1109/TASLP.2022.3225537},
  abstract	= {Knowledge graph completion (KGC) aims to predict missing
		  links based on observed triples. However, current KGC
		  models are still limited by the following two aspects. (1)
		  the entity semantics is implicitly learned by neural
		  network and merely depends on existing facts, which mostly
		  suffers from less additional specific knowledge. Although
		  previous studies have noticed that entity type information
		  can effectively improve KGC task, most of them rely on
		  labeled type-specific data. (2) the recent graph-based
		  models mainly concentrate on Graph Neural Network (GNN) to
		  update source entity representation, regardless of the
		  separate role that neighborhood information plays and may
		  mix noisy neighbor features for target prediction. To
		  address the above two issues, we propose a neighborhood
		  re-ranking model with relation constraint for KGC task. We
		  suggest that both relation constraint and structured
		  information located in triples can boost the model
		  performance. More importantly, we automatically generate
		  explicit constraints as additional type feature to enrich
		  entity representation instead of depending on human
		  annotated labels. Meanwhile, we construct a neighborhood
		  completion module to re-rank candidate entities for full
		  use of the neighbor structure rather than traditional GNN
		  updating manner. Extensive experiments on seven benchmarks
		  demonstrate that our model achieves the competitive results
		  in comparison to the recent advanced baselines.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {411–425},
  numpages	= {15}
}

@InProceedings{	  10.1145/3487351.3489484,
  author	= {Pan, Zhenhe and Jiang, Shuang and Su, Juntao and Guo,
		  Muzhe and Zhang, Yuanlin},
  title		= {Knowledge graph based platform of COVID-19 drugs and
		  symptoms},
  year		= {2022},
  isbn		= {9781450391283},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487351.3489484},
  doi		= {10.1145/3487351.3489484},
  abstract	= {Since the first cased of COVID-19 was identified in
		  December 2019, a plethora of different drugs have been
		  tested for COVID-19 treatment, making it a daunting task to
		  keep track of the rapid growth of COVID-19 research
		  landscape. Using the existing scientific literature search
		  systems to develop a deeper understanding of COVID-19
		  related clinical experiments and results turns to be
		  increasingly complicated. In this paper, we build a named
		  entity recognition-based framework to extract information
		  accurately and generate knowledge graph efficiently from a
		  myriad of clinical test results articles. Of the tested
		  drugs to treat COVID-19, we also develop a question
		  answering system answers to medical questions regarding
		  COVID-19 related symptoms using Wikipedia articles. We
		  combine the state-of-the-art question answering model -
		  Bidirectional Encoder Representations from Transformers
		  (BERT), with Knowledge Graph to answer patients' questions
		  about treatment options for their symptoms. This generated
		  knowledge graph is user-friendly with intuitive and
		  convenient tools to find the supporting and/or
		  contradictory references of certain drugs with properties
		  such as side effects, target population, etc. The trained
		  question answering platform provides a straightforward and
		  error-tolerant way to query for treatment suggestions given
		  uses' input symptoms.},
  booktitle	= {Proceedings of the 2021 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {313–316},
  numpages	= {4},
  location	= {Virtual Event, Netherlands},
  series	= {ASONAM '21}
}

@InProceedings{	  10.1145/3503928.3503936,
  author	= {Yang, Shihan and Tang, Rui},
  title		= {Learning Knowledge Uncertainty from the Pretrained
		  Language Model},
  year		= {2022},
  isbn		= {9781450385220},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503928.3503936},
  doi		= {10.1145/3503928.3503936},
  booktitle	= {Proceedings of the 6th International Conference on
		  Information Systems Engineering},
  pages		= {37–42},
  numpages	= {6},
  keywords	= {BERT, knowledge graph embedding, knowledge reasoning,
		  knowledge representation, uncertainty},
  location	= {Shanghai, China},
  series	= {ICISE '21}
}

@InProceedings{	  10.1109/ase51524.2021.9678574,
  author	= {Su, Yanqi and Xing, Zhenchang and Peng, Xin and Xia, Xin
		  and Wang, Chong and Xu, Xiwei and Zhu, Liming},
  title		= {Reducing bug triaging confusion by learning from mistakes
		  with a bug tossing knowledge graph},
  year		= {2022},
  isbn		= {9781665403375},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE51524.2021.9678574},
  doi		= {10.1109/ASE51524.2021.9678574},
  abstract	= {Assigning bugs to the right components is the prerequisite
		  to get the bugs analyzed and fixed. Classification-based
		  techniques have been used in practice for assisting bug
		  component assignments, for example, the BugBug tool
		  developed by Mozilla. However, our study on 124,477 bugs in
		  Mozilla products reveals that erroneous bug component
		  assignments occur frequently and widely. Most errors are
		  repeated errors and some errors are even misled by the
		  BugBug tool. Our study reveals that complex component
		  designs and misleading component names and bug report
		  keywords confuse bug component assignment not only for bug
		  reporters but also developers and even bug triaging tools.
		  In this work, we propose a learning to rank framework that
		  learns to assign components to bugs from correct, erroneous
		  and irrelevant bug-component assignments in the history. To
		  inform the learning, we construct a bug tossing knowledge
		  graph which incorporates not only goal-oriented component
		  tossing relationships but also rich information about
		  component tossing community, component descriptions, and
		  historical closed and tossed bugs, from which three
		  categories and seven types of features for bug, component
		  and bug-component relation can be derived. We evaluate our
		  approach on a dataset of 98,587 closed bugs (including
		  29,100 tossed bugs) of 186 components in six Mozilla
		  products. Our results show that our approach significantly
		  improves bug component assignments for both tossed and
		  non-tossed bugs over the BugBug tool and the BugBug tool
		  enhanced with component tossing relationships, with &gt;20%
		  Top-k accuracies and &gt;30% NDCG@k (k=1,3,5,10).},
  booktitle	= {Proceedings of the 36th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {191–202},
  numpages	= {12},
  keywords	= {bug triaging, knowledge graph, learning to rank},
  location	= {Melbourne, Australia},
  series	= {ASE '21}
}

@InProceedings{	  10.1145/3551349.3556898,
  author	= {Lee, Jaehyung and Han, Kisun and Yu, Hwanjo},
  title		= {A Light Bug Triage Framework for Applying Large
		  Pre-trained Language Model},
  year		= {2023},
  isbn		= {9781450394758},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3551349.3556898},
  doi		= {10.1145/3551349.3556898},
  abstract	= {Assigning appropriate developers to the bugs is one of the
		  main challenges in bug triage. Demands for automatic bug
		  triage are increasing in the industry, as manual bug triage
		  is labor-intensive and time-consuming in large projects.
		  The key to the bug triage task is extracting semantic
		  information from a bug report. In recent years, large
		  Pre-trained Language Models (PLMs) including BERT [4] have
		  achieved dramatic progress in the natural language
		  processing (NLP) domain. However, applying large PLMs to
		  the bug triage task for extracting semantic information has
		  several challenges. In this paper, we address the
		  challenges and propose a novel framework for bug triage
		  named LBT-P, standing for Light Bug Triage framework with a
		  Pre-trained language model. It compresses a large PLM into
		  small and fast models using knowledge distillation
		  techniques and also prevents catastrophic forgetting of PLM
		  by introducing knowledge preservation fine-tuning. We also
		  develop a new loss function exploiting representations of
		  earlier layers as well as deeper layers in order to handle
		  the overthinking problem. We demonstrate our proposed
		  framework on the real-world private dataset and three
		  public real-world datasets [11]: Google Chromium, Mozilla
		  Core, and Mozilla Firefox. The result of the experiments
		  shows the superiority of LBT-P.},
  booktitle	= {Proceedings of the 37th IEEE/ACM International Conference
		  on Automated Software Engineering},
  articleno	= {3},
  numpages	= {11},
  keywords	= {BERT, Bug triage, Catastrophic forgetting, Knowledge
		  distillation, Overthinking, Pre-trained language model},
  location	= {Rochester, MI, USA},
  series	= {ASE '22}
}

@Article{	  10.1145/3607188,
  author	= {Huang, Qing and Sun, Yanbang and Xing, Zhenchang and Yu,
		  Min and Xu, Xiwei and Lu, Qinghua},
  title		= {API Entity and Relation Joint Extraction from Text via
		  Dynamic Prompt-tuned Language Model},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {1},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3607188},
  doi		= {10.1145/3607188},
  abstract	= {Extraction of Application Programming Interfaces (APIs)
		  and their semantic relations from unstructured text (e.g.,
		  Stack Overflow) is a fundamental work for software
		  engineering tasks (e.g., API recommendation). However,
		  existing approaches are rule based and sequence labeling
		  based. They must manually enumerate the rules or label data
		  for a wide range of sentence patterns, which involves a
		  significant amount of labor overhead and is exacerbated by
		  morphological and common-word ambiguity. In contrast to
		  matching or labeling API entities and relations, this
		  article formulates heterogeneous API extraction and API
		  relation extraction task as a sequence-to-sequence
		  generation task and proposes the API Entity-Relation Joint
		  Extraction framework (AERJE), an API entity-relation joint
		  extraction model based on the large pre-trained language
		  model. After training on a small number of ambiguous but
		  correctly labeled data, AERJE builds a multi-task
		  architecture that extracts API entities and relations from
		  unstructured text using dynamic prompts. We systematically
		  evaluate AERJE on a set of long and ambiguous sentences
		  from Stack Overflow. The experimental results show that
		  AERJE achieves high accuracy and discrimination ability in
		  API entity-relation joint extraction, even with zero or
		  few-shot fine-tuning.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= nov,
  articleno	= {6},
  numpages	= {25},
  keywords	= {API entity, API relation, joint extraction, dynamic
		  prompt}
}

@InProceedings{	  10.1145/3627915.3628028,
  author	= {Chen, Zipeng and Sun, Peixia and Chang, Qian and Zhao,
		  Longgang},
  title		= {Dynamic Recommendation System Based on Event Knowledge
		  Graph in Telecom O&amp;M (Telecom Operations and
		  Maintenance Field)},
  year		= {2023},
  isbn		= {9798400700590},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627915.3628028},
  doi		= {10.1145/3627915.3628028},
  abstract	= {With the accumulation of time and the increasing volume of
		  telecommunications business, the company has accumulated a
		  large number of network fault work orders. These work
		  orders record the process of various fault resolution
		  through textual information, which is of great reference
		  value to the existing network fault resolution. In order to
		  make full use of the work order data, this paper designs
		  the work order event mapping structure and combines the
		  information extraction algorithm to extract the information
		  of unstructured work order data and its inner connection.
		  To address the difficulty of extracting long text with low
		  entity coverage, a text summarization algorithm is used to
		  remove redundant information and then a pointer network,
		  globalpointer, is applied to extract entities. For event
		  information, the UIE (Unified Structure Generation for
		  Universal Information Extraction) framework is used for
		  event extraction. It is also applied to the dynamic
		  recommendation field of similar work orders in dialogue
		  scenarios. The experimental results verify the advantages
		  of the main design of work order event mapping and the
		  effectiveness of the recommendation algorithm in this
		  paper, which is of great significance for forming a
		  structured knowledge base in the telecom operation and
		  maintenance industry and supporting the intelligent
		  development of telecom operation and maintenance.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Computer Science and Application Engineering},
  articleno	= {10},
  numpages	= {6},
  keywords	= {Mapping Entity Extraction, Similar Work Order
		  Recommendations, Work Order Event},
  location	= {Virtual Event, China},
  series	= {CSAE '23}
}

@Article{	  10.1109/tcbb.2023.3248797,
  author	= {Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
  title		= {Prediction of Protein-Protein Interactions Using Vision
		  Transformer and Language Model},
  year		= {2023},
  issue_date	= {Sept.-Oct. 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {5},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2023.3248797},
  doi		= {10.1109/TCBB.2023.3248797},
  abstract	= {The knowledge of protein-protein interaction (PPI) helps
		  us to understand proteins’ functions, the causes and
		  growth of several diseases, and can aid in designing new
		  drugs. The majority of existing PPI research has relied
		  mainly on sequence-based approaches. With the availability
		  of multi-omics datasets (sequence, 3D structure) and
		  advancements in deep learning techniques, it is feasible to
		  develop a deep multi-modal framework that fuses the
		  features learned from different sources of information to
		  predict PPI. In this work, we propose a multi-modal
		  approach utilizing protein sequence and 3D structure. To
		  extract features from the 3D structure of proteins, we use
		  a pre-trained vision transformer model that has been
		  fine-tuned on the structural representation of proteins.
		  The protein sequence is encoded into a feature vector using
		  a pre-trained language model. The feature vectors extracted
		  from the two modalities are fused and then fed to the
		  neural network classifier to predict the protein
		  interactions. To showcase the effectiveness of the proposed
		  methodology, we conduct experiments on two popular PPI
		  datasets, namely, the human dataset and the
		  &lt;italic&gt;S. cerevisiae&lt;/italic&gt; dataset. Our
		  approach outperforms the existing methodologies to predict
		  PPI, including multi-modal approaches. We also evaluate the
		  contributions of each modality by designing uni-modal
		  baselines. We perform experiments with three modalities as
		  well, having gene ontology as the third modality.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= feb,
  pages		= {3215–3225},
  numpages	= {11}
}

@InProceedings{	  10.1145/3511808.3557564,
  author	= {Ju, Jinhao and Yang, Deqing and Liu, Jingping},
  title		= {Commonsense Knowledge Base Completion with Relational
		  Graph Attention Network and Pre-trained Language Model},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557564},
  doi		= {10.1145/3511808.3557564},
  abstract	= {Many commonsense knowledge graphs (CKGs) still suffer from
		  incompleteness although they have been applied in many
		  natural language processing tasks successfully. Due to the
		  scale and sparsity of CKGs, existing knowledge base
		  completion models are not still competent for CKGs. In this
		  paper, we propose a commonsense knowledge base completion
		  (CKBC) model which learns the structural representations
		  and contextual representations of CKG nodes and relations,
		  respectively by a relational graph attention network and a
		  pre-trained language model. Based on these two types of
		  representations, the scoring decoder in our model achieves
		  a more accurate prediction for a given triple. Our
		  empirical studies on the representative CKG ConceptNet
		  demonstrate our model's superiority over the
		  state-of-the-art CKBC models.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4104–4108},
  numpages	= {5},
  keywords	= {commonsense knowledge base completion, pre-trained
		  language model, relational graph attention},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3488933.3489030,
  author	= {Shu, Xinfeng and Yan, Jing and Gao, Weiran and Zhang,
		  Fan},
  title		= {Research on Military Equipment Entity Recognition and
		  Knowledge Graph Construction Method Based on
		  ALBERT-Bi-LSTM-CRF},
  year		= {2022},
  isbn		= {9781450384087},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3488933.3489030},
  doi		= {10.1145/3488933.3489030},
  abstract	= {Aiming at the problems of complex and sparse military
		  weapon data and weak association between data, this paper
		  proposes a research method for unstructured data named
		  entity recognition based on the ALBERT-Bi-LSTM-CRF model.
		  Preprocess the ALBERT model to generate a word vector based
		  on contextual information, and then input the trained word
		  vector into the BiLSTM-CRF model for further training
		  processing, and finally build a knowledge map based on the
		  military weapon field and complete the knowledge question
		  and answer function. The experiment evaluates the
		  performance of entity recognition by adding training data
		  to the ALBERT model several times. The results show that
		  the model has a good advantage in entity relationship
		  recognition in Baidu Baike and Canglang Corpus, with F1
		  values reaching 93.76% and 92.76%, respectively. It
		  provides a better way to solve the existing problems.},
  booktitle	= {Proceedings of the 2021 4th International Conference on
		  Artificial Intelligence and Pattern Recognition},
  pages		= {273–279},
  numpages	= {7},
  keywords	= {ALBERT, Bi-LSTM, Knowledge Graph, Knowledge Graph
		  Questions and Answers, NER, Word vector},
  location	= {Xiamen, China},
  series	= {AIPR '21}
}

@InProceedings{	  10.1145/3580305.3599921,
  author	= {Zhang, Xinyang and Malkov, Yury and Florez, Omar and Park,
		  Serim and McWilliams, Brian and Han, Jiawei and El-Kishky,
		  Ahmed},
  title		= {TwHIN-BERT: A Socially-Enriched Pre-trained Language Model
		  for Multilingual Tweet Representations at Twitter},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599921},
  doi		= {10.1145/3580305.3599921},
  abstract	= {Pre-trained language models (PLMs) are fundamental for
		  natural language processing applications. Most existing
		  PLMs are not tailored to the noisy user-generated text on
		  social media, and the pre-training does not factor in the
		  valuable social engagement logs available in a social
		  network. We present TwHIN-BERT, a multilingual language
		  model productionized at Twitter, trained on in-domain data
		  from the popular social network. TwHIN-BERT differs from
		  prior pre-trained language models as it is trained with not
		  only text-based self-supervision but also with a social
		  objective based on the rich social engagements within a
		  Twitter heterogeneous information network (TwHIN). Our
		  model is trained on 7 billion tweets covering over 100
		  distinct languages, providing a valuable representation to
		  model short, noisy, user-generated text. We evaluate our
		  model on various multilingual social recommendation and
		  semantic understanding tasks and demonstrate significant
		  metric improvement over established pre-trained language
		  models. We open-source TwHIN-BERT and our curated hashtag
		  prediction and social engagement benchmark datasets to the
		  research community.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {5597–5607},
  numpages	= {11},
  keywords	= {language models, social engagement, social media},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3581783.3612838,
  author	= {Li, Ruizhe and Guo, Jiahao and Li, Mingxi and Wu,
		  Zhengqian and Liang, Chao},
  title		= {A Hierarchical Deep Video Understanding Method with
		  Shot-Based Instance Search and Large Language Model},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612838},
  doi		= {10.1145/3581783.3612838},
  abstract	= {Deep video understanding (DVU) is often considered a
		  challenge due to the aim of interpreting a video with
		  storyline, which is designed to solve two levels of
		  problems: predicting the human interaction in scene-level
		  and identifying the relationship between two entities in
		  movie-level. Based on our understanding of the movie
		  characteristics and analysis of DVU tasks, in this paper,
		  we propose a four-stage method to solve the task, which
		  includes video structuring, shot based instance search,
		  interaction &amp; relation prediction and shot-scene
		  summary &amp; Question Answering (QA) with ChatGPT. In
		  these four stages, shot based instance search allows
		  accurate identification and tracking of characters at an
		  appropriate video granularity. Using ChatGPT in QA, on the
		  one hand, can narrow the answer space, on the other hand,
		  with the help of the powerful text understanding ability,
		  ChatGPT can help us answer the questions by giving
		  background knowledge. We rank first in movie-level group 2
		  and scene-level group 1, second in movie-level group 1 and
		  scene-level group 2 in ACM MM 2023 Grand Challenge.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {9425–9429},
  numpages	= {5},
  keywords	= {instance search, multi-modal feature, vedio
		  understanding},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@Article{	  10.1145/3483524,
  author	= {Liao, Xianwen and Huang, Yongzhong and Yang, Peng and
		  Chen, Lei},
  title		= {A Statistical Language Model for Pre-Trained Sequence
		  Labeling: A Case Study on Vietnamese},
  year		= {2021},
  issue_date	= {May 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3483524},
  doi		= {10.1145/3483524},
  abstract	= {By defining the computable word segmentation unit and
		  studying its probability characteristics, we establish an
		  unsupervised statistical language model (SLM) for a new
		  pre-trained sequence labeling framework in this article.
		  The proposed SLM is an optimization model, and its
		  objective is to maximize the total binding force of all
		  candidate word segmentation units in sentences under the
		  condition of no annotated datasets and vocabularies. To
		  solve SLM, we design a recursive divide-and-conquer dynamic
		  programming algorithm. By integrating SLM with the popular
		  sequence labeling models, Vietnamese word segmentation,
		  part-of-speech tagging and named entity recognition
		  experiments are performed. The experimental results show
		  that our SLM can effectively promote the performance of
		  sequence labeling tasks. Just using less than 10% of
		  training data and without using a dictionary, the
		  performance of our sequence labeling framework is better
		  than the state-of-the-art Vietnamese word segmentation
		  toolkit VnCoreNLP on the cross-dataset test. SLM has no
		  hyper-parameter to be tuned, and it is completely
		  unsupervised and applicable to any other analytic language.
		  Thus, it has good domain adaptability.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= dec,
  articleno	= {50},
  numpages	= {21},
  keywords	= {Unsupervised, statistical language model, sequence
		  labeling}
}

@Article{	  10.1109/taslp.2023.3302238,
  author	= {Chang, Youngjae and Ko, Youngjoong},
  title		= {Two-Step Masked Language Model for Domain-Adapting
		  Multi-Modal Task-Oriented Dialogue Systems},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3302238},
  doi		= {10.1109/TASLP.2023.3302238},
  abstract	= {The next generation of artificial intelligence (AI) is
		  required to be capable of proper communication to enable
		  eloquent interaction with human beings. Thus, AI models
		  require powerful language understanding and generation
		  capabilities. Therefore, designing an adequate
		  dialogue-specific task is an important topic in dialogue
		  research. In this article, we improve our model that won in
		  two out of four subtasks in SIMMC2.0 challenge using a
		  dialogue-specific pre-training task, which learns the
		  distinctive features of dialogues. In addition, we
		  introduce an efficient method to pre-train the
		  encoder-decoder transformer using an auxiliary task for the
		  encoder. Experimental results indicate that superior
		  performance is achieved compared to our previous model,
		  revealing a new approach to improve task-oriented dialogue
		  systems.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {2938–2943},
  numpages	= {6}
}

@InProceedings{	  10.1145/3581641.3584037,
  author	= {Ross, Steven I. and Martinez, Fernando and Houde,
		  Stephanie and Muller, Michael and Weisz, Justin D.},
  title		= {The Programmer’s Assistant: Conversational Interaction
		  with a Large Language Model for Software Development},
  year		= {2023},
  isbn		= {9798400701061},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581641.3584037},
  doi		= {10.1145/3581641.3584037},
  abstract	= {Large language models (LLMs) have recently been applied in
		  software engineering to perform tasks such as translating
		  code between programming languages, generating code from
		  natural language, and autocompleting code as it is being
		  written. When used within development tools, these systems
		  typically treat each model invocation independently from
		  all previous invocations, and only a specific limited
		  functionality is exposed within the user interface. This
		  approach to user interaction misses an opportunity for
		  users to more deeply engage with the model by having the
		  context of their previous interactions, as well as the
		  context of their code, inform the model’s responses. We
		  developed a prototype system – the Programmer’s
		  Assistant – in order to explore the utility of
		  conversational interactions grounded in code, as well as
		  software engineers’ receptiveness to the idea of
		  conversing with, rather than invoking, a code-fluent LLM.
		  Through an evaluation with 42 participants with varied
		  levels of programming experience, we found that our system
		  was capable of conducting extended, multi-turn discussions,
		  and that it enabled additional knowledge and capabilities
		  beyond code generation to emerge from the LLM. Despite
		  skeptical initial expectations for conversational
		  programming assistance, participants were impressed by the
		  breadth of the assistant’s capabilities, the quality of
		  its responses, and its potential for improving their
		  productivity. Our work demonstrates the unique potential of
		  conversational interactions with LLMs for co-creative
		  processes like software development.},
  booktitle	= {Proceedings of the 28th International Conference on
		  Intelligent User Interfaces},
  pages		= {491–514},
  numpages	= {24},
  keywords	= {code-fluent large language models, conversational
		  interaction, foundation models, human-centered AI},
  location	= {Sydney, NSW, Australia},
  series	= {IUI '23}
}

@InProceedings{	  10.1145/3511808.3557355,
  author	= {Liu, Yang and Sun, Zequn and Li, Guangyao and Hu, Wei},
  title		= {I Know What You Do Not Know: Knowledge Graph Embedding via
		  Co-distillation Learning},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557355},
  doi		= {10.1145/3511808.3557355},
  abstract	= {Knowledge graph (KG) embedding seeks to learn vector
		  representations for entities and relations. Conventional
		  models reason over graph structures, but they suffer from
		  the issues of graph incompleteness and long-tail entities.
		  Recent studies have used pre-trained language models to
		  learn embeddings based on the textual information of
		  entities and relations, but they cannot take advantage of
		  graph structures. In the paper, we show empirically that
		  these two kinds of features are complementary for KG
		  embedding. To this end, we propose CoLE, a Co-distillation
		  Learning method for KG Embedding that exploits the
		  complementarity of graph structures and text information.
		  Its graph embedding model employs Transformer to
		  reconstruct the representation of an entity from its
		  neighborhood subgraph. Its text embedding model uses a
		  pre-trained language model to generate entity
		  representations from the soft prompts of their names,
		  descriptions and relational neighbors. To let the two
		  models promote each other, we propose co-distillation
		  learning that allows them to distill selective knowledge
		  from each other's prediction logits. In our co-distillation
		  learning, each model serves as both a teacher and a
		  student. Experiments on benchmark datasets demonstrate that
		  the two models outperform their related baselines, and the
		  ensemble method CoLE with co-distillation learning advances
		  the state-of-the-art of KG embedding.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1329–1338},
  numpages	= {10},
  keywords	= {co-distillation learning, knowledge graph, link
		  prediction},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@Article{	  10.1109/taslp.2022.3153268,
  author	= {Wang, Chengyu and Dai, Suyang and Wang, Yipeng and Yang,
		  Fei and Qiu, Minghui and Chen, Kehan and Zhou, Wei and
		  Huang, Jun},
  title		= {ARoBERT: An ASR Robust Pre-Trained Language Model for
		  Spoken Language Understanding},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3153268},
  doi		= {10.1109/TASLP.2022.3153268},
  abstract	= {Spoken Language Understanding (SLU) aims to interpret the
		  meanings of human speeches in order to support various
		  human-machine interaction systems. A key technique for SLU
		  is Automatic Speech Recognition (ASR), which transcribes
		  speech signals into text contents. As the output texts of
		  modern ASR systems unavoidably contain errors, mainstream
		  SLU models either trained or tested on texts transcribed by
		  ASR systems would not be sufficiently error robust. We
		  present ARoBERT, an ASR Robust BERT model, which can be
		  fine-tuned to solve a variety of SLU tasks with noisy
		  inputs. To guarantee the robustness of ARoBERT, during
		  pretraining, we decrease the fluctuations of language
		  representations when some parts of the input texts are
		  replaced by homophones or synophones. Specifically, we
		  propose two novel self-supervised pre-training tasks for
		  ARoBERT, namely Phonetically-aware Masked Language Modeling
		  (PMLM) and ASR Model-adaptive Masked Language Modeling
		  (AMMLM). The PMLM task explicitly fuses the knowledge of
		  word phonetic similarities into the pre-training process,
		  which forces homophones and synophones to share similar
		  representations. In AMMLM, a data-driven algorithm is
		  further introduced to mine typical ASR errors such that
		  ARoBERT can tolerate ASR model errors. In the experiments,
		  we evaluate ARoBERT over multiple datasets. The results
		  show the superiority of ARoBERT, which consistently
		  outperforms strong baselines. We have also shown that
		  ARoBERT outperforms state-of-the-arts on a public
		  benchmark. Currently, ARoBERT has been deployed in an
		  online production system with significant improvements.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= feb,
  pages		= {1207–1218},
  numpages	= {12}
}

@InProceedings{	  10.1145/3447548.3467128,
  author	= {Liu, Lihui and Du, Boxin and Fung, Yi Ren and Ji, Heng and
		  Xu, Jiejun and Tong, Hanghang},
  title		= {KompaRe: A Knowledge Graph Comparative Reasoning System},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467128},
  doi		= {10.1145/3447548.3467128},
  abstract	= {Reasoning is a fundamental capability for harnessing
		  valuable insight, knowledge and patterns from knowledge
		  graphs. Existing work has primarily been focusing on
		  point-wise reasoning, including search, link prediction,
		  entity prediction, subgraph matching and so on. This paper
		  introduces comparative reasoning over knowledge graphs,
		  which aims to infer the commonality and inconsistency with
		  respect to multiple pieces of clues. We envision that the
		  comparative reasoning will complement and expand the
		  existing point-wise reasoning over knowledge graphs. In
		  detail, we develop KompaRe, the first of its kind prototype
		  system that provides comparative reasoning capability over
		  large knowledge graphs. We present both the system
		  architecture and its core algorithms, including knowledge
		  segment extraction, pairwise reasoning and collective
		  reasoning. Empirical evaluations demonstrate the efficacy
		  of the proposed KompaRe.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {3308–3318},
  numpages	= {11},
  keywords	= {comparative reasoning, knowledge graph fact checking,
		  knowledge graph reasoning system},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@InProceedings{	  10.1145/3459637.3482382,
  author	= {Kim, Bosung and Choi, Hyewon and Yu, Haeun and Ko,
		  Youngjoong},
  title		= {Query Reformulation for Descriptive Queries of Jargon
		  Words Using a Knowledge Graph based on a Dictionary},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482382},
  doi		= {10.1145/3459637.3482382},
  abstract	= {Query reformulation (QR) is a key factor in overcoming the
		  problems faced by the lexical chasm in information
		  retrieval (IR) systems. In particular, when searching for
		  jargon, people tend to use descriptive queries, such as "a
		  medical examination of the colon" rather than
		  "colonoscopy," or they often use them interchangeably.
		  Thus, transforming users' descriptive queries into
		  appropriate jargon queries helps to retrieve more relevant
		  documents. In this paper, we propose a new graph-based QR
		  system that uses a dictionary, where the model does not
		  require human-labeled data. Given a descriptive query, our
		  system predicts the corresponding jargon word over a graph
		  consisting of pairs of a headword and its description in
		  the dictionary. First, we train a graph neural network to
		  represent the relational properties between words and to
		  infer a jargon word using compositional information of the
		  descriptive query's words. Moreover, we propose a graph
		  search model that finds the target node in real time using
		  the relevance scores of neighborhood nodes. By adding this
		  fast graph search model to the front of the proposed
		  system, we reduce the reformulating time significantly.
		  Experimental results on two datasets show that the proposed
		  method can effectively reformulate descriptive queries to
		  corresponding jargon words as well as improve retrieval
		  performance under several search frameworks.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {854–862},
  numpages	= {9},
  keywords	= {graph neural networks, graph search, query reformulation},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3488560.3498437,
  author	= {Zhu, Yushan and Zhang, Wen and Chen, Mingyang and Chen,
		  Hui and Cheng, Xu and Zhang, Wei and Chen, Huajun},
  title		= {DualDE: Dually Distilling Knowledge Graph Embedding for
		  Faster and Cheaper Reasoning},
  year		= {2022},
  isbn		= {9781450391320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3488560.3498437},
  doi		= {10.1145/3488560.3498437},
  abstract	= {Knowledge Graph Embedding (KGE) is a popular method for KG
		  reasoning and training KGEs with higher dimension are
		  usually preferred since they have better reasoning
		  capability. However, high-dimensional KGEs pose huge
		  challenges to storage and computing resources and are not
		  suitable for resource-limited or time-constrained
		  applications, for which faster and cheaper reasoning is
		  necessary. To address this problem, we propose DualDE, a
		  knowledge distillation method to build low-dimensional
		  student KGE from pre-trained high-dimensional teacher KGE.
		  DualDE considers the dual-influence between the teacher and
		  the student. In DualDE, we propose a soft label evaluation
		  mechanism to adaptively assign different soft label and
		  hard label weights to different triples, and a two-stage
		  distillation approach to improve the student's acceptance
		  of the teacher. Our DualDE is general enough to be applied
		  to various KGEs. Experimental results show that our method
		  can successfully reduce the embedding parameters of a
		  high-dimensional KGE by 7\texttimes{} - 15\texttimes{} and
		  increase the inference speed by 2\texttimes{} -
		  6\texttimes{} while retaining a high performance. We also
		  experimentally prove the effectiveness of our soft label
		  evaluation mechanism and two-stage distillation approach
		  via ablation study.},
  booktitle	= {Proceedings of the Fifteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1516–1524},
  numpages	= {9},
  keywords	= {fast embedding, knowledge distillation, knowledge graph
		  embedding},
  location	= {Virtual Event, AZ, USA},
  series	= {WSDM '22}
}

@InProceedings{	  10.1145/3551349.3556912,
  author	= {Huang, Qing and Yuan, Zhiqiang and Xing, Zhenchang and Xu,
		  Xiwei and Zhu, Liming and Lu, Qinghua},
  title		= {Prompt-tuned Code Language Model as a Neural Knowledge
		  Base for Type Inference in Statically-Typed Partial Code},
  year		= {2023},
  isbn		= {9781450394758},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3551349.3556912},
  doi		= {10.1145/3551349.3556912},
  abstract	= {Partial code usually involves non-fully-qualified type
		  names (non-FQNs) and undeclared receiving objects.
		  Resolving the FQNs of these non-FQN types and undeclared
		  receiving objects (referred to as type inference) is the
		  prerequisite to effective search and reuse of partial code.
		  Existing dictionary-lookup based methods build a symbolic
		  knowledge base of API names and code contexts, which
		  involve significant compilation overhead and are sensitive
		  to unseen API names and code context variations. In this
		  paper, we formulate type inference as a cloze-style
		  fill-in-blank language task. Built on source code
		  naturalness, our approach fine-tunes a code masked language
		  model (MLM) as a neural knowledge base of code elements
		  with a novel “pre-train, prompt and predict” paradigm
		  from raw source code. Our approach is lightweight and has
		  minimum requirements on code compilation. Unlike existing
		  symbolic name and context matching for type inference, our
		  prompt-tuned code MLM packs FQN syntax and usage in its
		  parameters and supports fuzzy neural type inference. We
		  systematically evaluate our approach on a large amount of
		  source code from GitHub and Stack Overflow. Our results
		  confirm the effectiveness of our approach design and the
		  practicality for partial code type inference. As the first
		  of its kind, our neural type inference method opens the
		  door to many innovative ways of using partial code.},
  booktitle	= {Proceedings of the 37th IEEE/ACM International Conference
		  on Automated Software Engineering},
  articleno	= {79},
  numpages	= {13},
  location	= {Rochester, MI, USA},
  series	= {ASE '22}
}

@InProceedings{	  10.1145/3487553.3524923,
  author	= {Negreanu, Carina and Karaoglu, Alperen and Williams, Jack
		  and Chen, Shuang and Fabian, Daniel and Gordon, Andrew and
		  Lin, Chin-Yew},
  title		= {Rows from Many Sources: Enriching row completions from
		  Wikidata with a pre-trained Language Model},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524923},
  doi		= {10.1145/3487553.3524923},
  abstract	= {Row completion is the task of augmenting a given table of
		  text and numbers with additional, relevant rows. The task
		  divides into two steps: subject suggestion, the task of
		  populating the main column; and gap filling, the task of
		  populating the remaining columns. We present
		  state-of-the-art results for subject suggestion and gap
		  filling measured on a standard benchmark (WikiTables). Our
		  idea is to solve this task by harmoniously combining
		  knowledge base table interpretation and free text
		  generation. We interpret the table using the knowledge base
		  to suggest new rows and generate metadata like headers
		  through property linking. To improve candidate diversity,
		  we synthesize additional rows using free text generation
		  via GPT-3, and crucially, we exploit the metadata we
		  interpret to produce better prompts for text generation.
		  Finally, we verify that the additional synthesized content
		  can be linked to the knowledge base or a trusted web source
		  such as Wikipedia.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {1272–1280},
  numpages	= {9},
  keywords	= {free text generation, knowledge base linking, language
		  models, natural language applications, semantic knowledge,
		  tabular data},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3459637.3482491,
  author	= {Xie, Chenhao and Huang, Wenhao and Liang, Jiaqing and
		  Huang, Chengsong and Xiao, Yanghua},
  title		= {WebKE: Knowledge Extraction from Semi-structured Web with
		  Pre-trained Markup Language Model},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482491},
  doi		= {10.1145/3459637.3482491},
  abstract	= {The World Wide Web contains rich up-to-date information
		  for knowledge graph construction. However, most current
		  relation extraction techniques are designed for free text
		  and thus do not handle well semi-structured web content. In
		  this paper, we propose a novel multi-phase machine reading
		  framework, called WebKE. It processes the web content on
		  different granularity by first detecting areas of interest
		  at DOM tree node level and then extracting relational
		  triples for each area. We also propose HTMLBERT as an
		  encoder the web content. It is a pre-trained markup
		  language model that fully leverages the visual layout
		  information and DOM-tree structure, without the need of
		  hand engineered features. Experimental results show that
		  the proposed approach outperforms state-of- the-art methods
		  by a considerable gain. The source code is available at
		  https://github.com/redreamality/webke.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2211–2220},
  numpages	= {10},
  keywords	= {htmlbert, knowledge extraction, knowledge graph
		  construction, pre-trained markup language model, relation
		  extraction, semi-structured web extraction, webke},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3580305.3599233,
  author	= {Yu, Wenhao and Tong, Lingbo and Shi, Weijia and Peng,
		  Nanyun and Jiang, Meng},
  title		= {The Second Workshop on Knowledge-Augmented Methods for
		  Natural Language Processing},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599233},
  doi		= {10.1145/3580305.3599233},
  abstract	= {Language models are being developed and deployed in many
		  applications, "small"-scale and large-scale, generic and
		  specialized, text-only and multimodal, etc. Meanwhile, the
		  missingness of important knowledge causes limitations and
		  safety challenges. The knowledge includes commonsense,
		  world facts, domain expertise, personalization, and
		  especially the unique patterns that need to be discovered
		  from big data applications. Training and inference
		  processes of the language models can be and should be
		  augmented with the knowledge. The first KnowledgeNLP at
		  AAAI 2023 attracted scientists on knowledge augmentation
		  methods towards higher language intelligence. This workshop
		  offers a broad platform to share ideas and discuss various
		  topics, such as (1) synergy between knowledge and language
		  model, (2) scalable architectures that integrate NLP,
		  knowledge graph, and graph learning technologies, (3)
		  KnowledgeNLP for e-commerce, education, and healthcare, (4)
		  human factors and social good in KnowledgeNLP.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {5899–5900},
  numpages	= {2},
  keywords	= {knowledge augmentation, knowledge graph, language model,
		  question answering, text generation},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@Article{	  10.1145/3492855,
  author	= {Kou, Ziyi and Shang, Lanyu and Zhang, Yang and Wang,
		  Dong},
  title		= {HC-COVID: A Hierarchical Crowdsource Knowledge Graph
		  Approach to Explainable COVID-19 Misinformation Detection},
  year		= {2022},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {GROUP},
  url		= {https://doi.org/10.1145/3492855},
  doi		= {10.1145/3492855},
  abstract	= {The proliferation of social media has promoted the spread
		  of misinformation that raises many concerns in our society.
		  This paper focuses on a critical problem of explainable
		  COVID-19 misinformation detection that aims to accurately
		  identify and explain misleading COVID-19 claims on social
		  media. Motivated by the lack of COVID-19 relevant knowledge
		  in existing solutions, we construct a novel crowdsource
		  knowledge graph based approach to incorporate the COVID-19
		  knowledge facts by leveraging the collaborative efforts of
		  expert and non-expert crowd workers. Two important
		  challenges exist in developing our solution: i) how to
		  effectively coordinate the crowd efforts from both expert
		  and non-expert workers to generate the relevant knowledge
		  facts for detecting COVID-19 misinformation; ii) How to
		  leverage the knowledge facts from the constructed knowledge
		  graph to accurately explain the detected COVID-19
		  misinformation. To address the above challenges, we develop
		  HC-COVID, a hierarchical crowdsource knowledge graph based
		  framework that explicitly models the COVID-19 knowledge
		  facts contributed by crowd workers with different levels of
		  expertise and accurately identifies the related knowledge
		  facts to explain the detection results. We evaluate
		  HC-COVID using two public real-world datasets on social
		  media. Evaluation results demonstrate that HC-COVID
		  significantly outperforms state-of-the-art baselines in
		  terms of the detection accuracy of misleading COVID-19
		  claims and the quality of the explanations.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jan,
  articleno	= {36},
  numpages	= {25},
  keywords	= {covid19, explainable misinformation detection, human-ai
		  collaboration}
}

@InProceedings{	  10.1145/3587259.3627571,
  author	= {Hertling, Sven and Paulheim, Heiko},
  title		= {OLaLa: Ontology Matching with Large Language Models},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627571},
  doi		= {10.1145/3587259.3627571},
  abstract	= {Ontology (and more generally: Knowledge Graph) Matching is
		  a challenging task where information in natural language is
		  one of the most important signals to process. With the rise
		  of Large Language Models, it is possible to incorporate
		  this knowledge in a better way into the matching pipeline.
		  A number of decisions still need to be taken, e.g., how to
		  generate a prompt that is useful to the model, how
		  information in the KG can be formulated in prompts, which
		  Large Language Model to choose, how to provide existing
		  correspondences to the model, how to generate candidates,
		  etc. In this paper, we present a prototype that explores
		  these questions by applying zero-shot and few-shot
		  prompting with multiple open Large Language Models to
		  different tasks of the Ontology Alignment Evaluation
		  Initiative (OAEI). We show that with only a handful of
		  examples and a well-designed prompt, it is possible to
		  achieve results that are en par with supervised matching
		  systems which use a much larger portion of the ground
		  truth.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {131–139},
  numpages	= {9},
  keywords	= {Entity Resolution, Large Language Model, Ontology
		  Matching},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3583780.3615301,
  author	= {Gupta, Rajeev and Srinivasa, Srinath},
  title		= {Workshop on Enterprise Knowledge Graphs using Large
		  Language Models},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615301},
  doi		= {10.1145/3583780.3615301},
  abstract	= {Knowledge graphs are used for organizing and connecting
		  individual entities to integrate the information extracted
		  from different data sources. Typically, knowledge graphs
		  are used to connect various real-world entities like
		  persons, places, things, actions, etc. For the knowledge
		  graphs created using the enterprise data, the knowledge
		  graph entities can be of different types-static entities
		  (e.g., people, projects), communication entities (e.g.,
		  emails, meetings, documents), derived entities (e.g.,
		  rules, definitions, entities from emails), etc. The graphs
		  are used to connect these entities with enriched context
		  (as edges and node attributes) and used for powering
		  various search and recommendations applications.With the
		  advent of large language models, the whole lifecycle of
		  knowledge graphs involving -information extraction, graph
		  construction, application of graphs, querying knowledge
		  graphs, using the graph for recommendations, etc., - is
		  impacted. With large language models such as GPT, LLaMA,
		  PALM, etc., entity and relationship extraction can be
		  improved. Similarly, one can answer different types of
		  queries with the help of LLMs which were very difficult
		  without them. This workshop is about improving the
		  enterprise knowledge graphs and its applications using
		  large language models.Enterprise graphs can be of different
		  scopes-whether they contain data from individual
		  users/customers, a sub-organization, or the whole
		  enterprise. This workshop will also cover various privacy
		  and access control related issues which are typical for any
		  enterprise graph. These include privacy preserving
		  federated learning, using LLMs to extract information from
		  private data, querying the knowledge graph in a privacy
		  preserving manner, etc.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5271–5272},
  numpages	= {2},
  keywords	= {entity extraction, knowledge graph, large language model,
		  recommendations, relationship extraction},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3581783.3612252,
  author	= {Zhai, Jianyang and Zheng, Xiawu and Wang, Chang-Dong and
		  Li, Hui and Tian, Yonghong},
  title		= {Knowledge Prompt-tuning for Sequential Recommendation},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612252},
  doi		= {10.1145/3581783.3612252},
  abstract	= {Pre-trained language models (PLMs) have demonstrated
		  strong performance in sequential recommendation (SR), which
		  are utilized to extract general knowledge. However,
		  existing methods still lack domain knowledge and struggle
		  to capture users' fine-grained preferences. Meanwhile, many
		  traditional SR methods improve this issue by integrating
		  side information while suffering from information loss. To
		  summarize, we believe that a good recommendation system
		  should utilize both general and domain knowledge
		  simultaneously. Therefore, we introduce an external
		  knowledge base and propose Knowledge Prompt-tuning for
		  Sequential Recommendation (KP4SR). Specifically, we
		  construct a set of relationship templates and transform a
		  structured knowledge graph (KG) into knowledge prompts to
		  solve the problem of the semantic gap. However, knowledge
		  prompts disrupt the original data structure and introduce a
		  significant amount of noise. We further construct a
		  knowledge tree and propose a knowledge tree mask, which
		  restores the data structure in a mask matrix form, thus
		  mitigating the noise problem. We evaluate KP4SR on three
		  real-world datasets, and experimental results show that our
		  approach outperforms state-of-the-art methods on multiple
		  evaluation metrics. Specifically, compared with PLM-based
		  methods, our method improves NDCG@5 and HR@5 by 40.65% and
		  36.42% on the books dataset, 11.17% and 11.47% on the music
		  dataset, and 22.17% and 19.14% on the movies dataset,
		  respectively. Our code is publicly available at the link:
		  https://github.com/zhaijianyang/KP4SR.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {6451–6461},
  numpages	= {11},
  keywords	= {knowledge graph, pre-trained language model, prompt
		  learning, sequential recommendation},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3589132.3625641,
  author	= {Zhou, Zhilun and Ding, Jingtao and Liu, Yu and Jin, Depeng
		  and Li, Yong},
  title		= {Towards Generative Modeling of Urban Flow through
		  Knowledge-enhanced Denoising Diffusion},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589132.3625641},
  doi		= {10.1145/3589132.3625641},
  abstract	= {Although generative AI has been successful in many areas,
		  its ability to model geospatial data is still
		  underexplored. Urban flow, a typical kind of geospatial
		  data, is critical for a wide range of applications from
		  public safety and traffic management to urban planning.
		  Existing studies mostly focus on predictive modeling of
		  urban flow that predicts the future flow based on
		  historical flow data, which may be unavailable in
		  data-sparse areas or newly planned regions. Some other
		  studies aim to predict OD flow among regions but they fail
		  to model dynamic changes of urban flow over time. In this
		  work, we study a new problem of urban flow generation that
		  generates dynamic urban flow for regions without historical
		  flow data. To capture the effect of multiple factors on
		  urban flow, such as region features and urban environment,
		  we employ diffusion model to generate urban flow for
		  regions under different conditions. We first construct an
		  urban knowledge graph (UKG) to model the urban environment
		  and relationships between regions, based on which we design
		  a knowledge-enhanced spatio-temporal diffusion model
		  (KSTDiff) to generate urban flow for each region.
		  Specifically, to accurately generate urban flow for regions
		  with different flow volumes, we design a novel diffusion
		  process guided by a volume estimator, which is learnable
		  and customized for each region. Moreover, we propose a
		  knowledge-enhanced denoising network to capture the
		  spatio-temporal dependencies of urban flow as well as the
		  impact of urban environment in the denoising process.
		  Extensive experiments on four real-world datasets validate
		  the superiority of our model over state-of-the-art
		  baselines in urban flow generation. Further in-depth
		  studies demonstrate the utility of generated urban flow
		  data and the ability of our model for long-term flow
		  generation and urban flow prediction. Our code is released
		  at:
		  https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {91},
  numpages	= {12},
  keywords	= {generative model, urban flow, knowledge graph, diffusion
		  model},
  location	= {Hamburg, Germany},
  series	= {SIGSPATIAL '23}
}

@InProceedings{	  10.1145/3487553.3524622,
  author	= {Barik, Anab Maulana and Hsu, Wynne and Lee, Mong Li},
  title		= {Incorporating External Knowledge for Evidence-based Fact
		  Verification},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524622},
  doi		= {10.1145/3487553.3524622},
  abstract	= {Existing fact verification methods employ pre-trained
		  language models such as BERT for the contextual
		  representation of evidence sentences. However, such
		  representations do not take into account commonsense
		  knowledge and these methods often conclude that there is
		  not enough information to predict whether a claim is
		  supported or refuted by the evidence sentences. In this
		  work, we propose a framework called CGAT that incorporates
		  external knowledge from ConceptNet to enrich the contextual
		  representations of evidence sentences. We employ graph
		  attention models to propagate the information among the
		  evidence sentences before predicting the veracity of the
		  claim. Experiment results on the benchmark FEVER dataset
		  and UKP Snopes Corpus indicate that the proposed approach
		  leads to higher accuracy and FEVER score compared to
		  state-of-the-art claim verification methods.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {429–437},
  numpages	= {9},
  keywords	= {fact verification, knowledge graph, language model},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3485447.3511921,
  author	= {Ye, Hongbin and Zhang, Ningyu and Deng, Shumin and Chen,
		  Xiang and Chen, Hui and Xiong, Feiyu and Chen, Xi and Chen,
		  Huajun},
  title		= {Ontology-enhanced Prompt-tuning for Few-shot Learning},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511921},
  doi		= {10.1145/3485447.3511921},
  abstract	= {Few-shot Learning (FSL) is aimed to make predictions based
		  on a limited number of samples. Structured data such as
		  knowledge graphs and ontology libraries has been leveraged
		  to benefit the few-shot setting in various tasks. However,
		  the priors adopted by the existing methods suffer from
		  challenging knowledge missing, knowledge noise, and
		  knowledge heterogeneity, which hinder the performance for
		  few-shot learning. In this study, we explore knowledge
		  injection for FSL with pre-trained language models and
		  propose ontology-enhanced prompt-tuning (OntoPrompt).
		  Specifically, we develop the ontology transformation based
		  on the external knowledge graph to address the knowledge
		  missing issue, which fulfills and converts structure
		  knowledge to text. We further introduce span-sensitive
		  knowledge injection via a visible matrix to select
		  informative knowledge to handle the knowledge noise issue.
		  To bridge the gap between knowledge and text, we propose a
		  collective training algorithm to optimize representations
		  jointly. We evaluate our proposed OntoPrompt in three
		  tasks, including relation extraction, event extraction, and
		  knowledge graph completion, with eight datasets.
		  Experimental results demonstrate that our approach can
		  obtain better few-shot performance than baselines.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {778–787},
  numpages	= {10},
  keywords	= {Event Extraction, Few-shot Learning, Knowledge Graph
		  Completion, Ontology, Prompt-tuning, Relation Extraction},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3485447.3511937,
  author	= {Geng, Shijie and Fu, Zuohui and Tan, Juntao and Ge,
		  Yingqiang and de Melo, Gerard and Zhang, Yongfeng},
  title		= {Path Language Modeling over Knowledge Graphsfor
		  Explainable Recommendation},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511937},
  doi		= {10.1145/3485447.3511937},
  abstract	= {To facilitate human decisions with credible suggestions,
		  personalized recommender systems should have the ability to
		  generate corresponding explanations while making
		  recommendations. Knowledge graphs (KG), which contain
		  comprehensive information about users and products, are
		  widely used to enable this. By reasoning over a KG in a
		  node-by-node manner, existing explainable models provide a
		  KG-grounded path for each user-recommended item. Such paths
		  serve as an explanation and reflect the historical behavior
		  pattern of the user. However, not all items can be reached
		  following the connections within the constructed KG under
		  finite hops. Hence, previous approaches are constrained by
		  a recall bias in terms of existing connectivity of KG
		  structures. To overcome this, we propose a novel Path
		  Language Modeling Recommendation (PLM-Rec) framework,
		  learning a language model over KG paths consisting of
		  entities and edges. Through path sequence decoding, PLM-Rec
		  unifies recommendation and explanation in a single step and
		  fulfills them simultaneously. As a result, PLM-Rec not only
		  captures the user behaviors but also eliminates the
		  restriction to pre-existing KG connections, thereby
		  alleviating the aforementioned recall bias. Moreover, the
		  proposed technique makes it possible to conduct explainable
		  recommendation even when the KG is sparse or possesses a
		  large number of relations. Experiments and extensive
		  ablation studies on three Amazon e-commerce datasets
		  demonstrate the effectiveness and explainability of the
		  PLM-Rec framework.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {946–955},
  numpages	= {10},
  keywords	= {Explainable Recommendation, Knowledge Graph, Path Language
		  Model, Recall Bias, Recommender Systems},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3624062.3624094,
  author	= {Kousha, Pouya and Sathu, Vivekananda and Lieber, Matthew
		  and Subramoni, Hari and Panda, Dhabaleswar K.},
  title		= {Democratizing HPC Access and Use with Knowledge Graphs},
  year		= {2023},
  isbn		= {9798400707858},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3624062.3624094},
  doi		= {10.1145/3624062.3624094},
  abstract	= {The field of High-Performance Computing (HPC) is
		  undergoing rapid evolution, with an expanding and diverse
		  user base harnessing its unparalleled computational
		  capabilities. As the range of HPC applications grows,
		  newcomers to the field are faced with the daunting task of
		  optimizing their applications for efficient execution on
		  HPC systems. Traditional documentation, often spanning
		  dozens of pages, is cumbersome for finding answers and
		  ill-suited for integration with emerging conversational
		  AI-powered user interfaces like chatbots. Addressing this
		  challenge, we propose a novel HPC ontology crafted to
		  encapsulate HPC runtime relations in a scalable fashion.
		  Our proposed ontology not only facilitates the transfer and
		  querying of this knowledge but also serves as a
		  foundational pillar for our AI-powered Speech Assistant
		  Interface (SAI)[13]. This ensures reproducibility,
		  reliability, and optimal performance when executing tasks.
		  In this paper, we elucidate the relationships and
		  properties underpinning our ontology and showcase how users
		  can interact with knowledge graphs based on our proposed
		  ontology to derive insights.},
  booktitle	= {Proceedings of the SC '23 Workshops of the International
		  Conference on High Performance Computing, Network, Storage,
		  and Analysis},
  pages		= {243–251},
  numpages	= {9},
  keywords	= {Documentation, HPC, Knowledge Graph, Ontology},
  location	= {Denver, CO, USA},
  series	= {SC-W '23}
}

@InProceedings{	  10.1145/3579370.3594759,
  author	= {Alfasi, Daniel and Shapira, Tal and Bremler-Barr, Anat},
  title		= {Next-Generation Security Entity Linkage: Harnessing the
		  Power of Knowledge Graphs and Large Language},
  year		= {2023},
  isbn		= {9781450399623},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579370.3594759},
  doi		= {10.1145/3579370.3594759},
  abstract	= {With the continuous increase in reported Common
		  Vulnerabilities and Exposures (CVEs), security teams are
		  overwhelmed by vast amounts of data, which are often
		  analyzed manually, leading to a slow and inefficient
		  process. To address cybersecurity threats effectively, it
		  is essential to establish connections across multiple
		  security entity databases, including CVEs, Common Weakness
		  Enumeration (CWEs), and Common Attack Pattern Enumeration
		  and Classification (CAPECs). In this study, we introduce a
		  new approach that leverages the RotatE [4] knowledge graph
		  embedding model, initialized with embeddings from Ada
		  language model developed by OpenAI [3]. Additionally, we
		  extend this approach by initializing the embeddings for the
		  relations.},
  booktitle	= {Proceedings of the 16th ACM International Conference on
		  Systems and Storage},
  pages		= {150},
  numpages	= {1},
  keywords	= {CVE, CWE, CAPEC, knowledge graph embedding},
  location	= {Haifa, Israel},
  series	= {SYSTOR '23}
}

@InProceedings{	  10.1145/3543873.3587667,
  author	= {Lin, Shiyong and Yuan, Yiping and Jin, Carol and Pan, Yi},
  title		= {Skill Graph Construction From Semantic Understanding},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587667},
  doi		= {10.1145/3543873.3587667},
  abstract	= {LinkedIn is building a skill graph to power a skill-first
		  talent marketplace. Constructing a skill graph from a flat
		  list is not an trivial task, especially by human curation.
		  In this paper, we leverage the pre-trained large language
		  model BERT to achieve this through semantic understanding
		  on synthetically generated texts as training data. We
		  automatically create positive and negative labels from the
		  seed skill graph. The training data are encoded by
		  pre-trained language models into embeddings and they are
		  consumed by the downstream classification module to
		  classify the relationships between skill pairs.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {978–982},
  numpages	= {5},
  keywords	= {NLP, knowledge graph, link prediction},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3583780.3614938,
  author	= {Wang, Zihan and Zhao, Kai and He, Yongquan and Chen,
		  Zhumin and Ren, Pengjie and de Rijke, Maarten and Ren,
		  Zhaochun},
  title		= {Iteratively Learning Representations for Unseen Entities
		  with Inter-Rule Correlations},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614938},
  doi		= {10.1145/3583780.3614938},
  abstract	= {Recent work on knowledge graph completion (KGC) focuses on
		  acquiring embeddings of entities and relations in knowledge
		  graphs. These embedding methods necessitate that all test
		  entities be present during the training phase, resulting in
		  a time-consuming retraining process for
		  out-of-knowledge-graph (OOKG) entities. To tackle this
		  predicament, current inductive methods employ graph neural
		  networks (GNNs) to represent unseen entities by aggregating
		  information of the known neighbors, and enhance the
		  performance with additional information, such as attention
		  mechanisms or logic rules. Nonetheless, Two key challenges
		  continue to persist: (i) identifying inter-rule
		  correlations to further facilitate the inference process,
		  and (ii) capturing interactions among rule mining, rule
		  inference, and embedding to enhance both rule and embedding
		  learning.In this paper, we propose a virtual neighbor
		  network with inter-rule correlations (VNC) to address the
		  above challenges. VNC consists of three main components:
		  (i) rule mining, (ii) rule inference, and (iii) embedding.
		  To identify useful complex patterns in knowledge graphs,
		  both logic rules and inter-rule correlations are extracted
		  from knowledge graphs based on operations over relation
		  embeddings. To reduce data sparsity, virtual networks for
		  OOKG entities are predicted and assigned soft labels by
		  optimizing a rule-constrained problem. We also devise an
		  iterative framework to capture the underlying interactions
		  between rule and embedding learning. Experimental results
		  on both link prediction and triple classification tasks
		  show that the proposed VNC framework achieves
		  state-of-the-art performance on four widely-used knowledge
		  graphs.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2534–2543},
  numpages	= {10},
  keywords	= {inductive learning, knowledge graph, representation
		  learning},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Article{	  10.1145/3605943,
  author	= {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh,
		  Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and
		  Agirre, Eneko and Heintz, Ilana and Roth, Dan},
  title		= {Recent Advances in Natural Language Processing via Large
		  Pre-trained Language Models: A Survey},
  year		= {2023},
  issue_date	= {February 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3605943},
  doi		= {10.1145/3605943},
  abstract	= {Large, pre-trained language models (PLMs) such as BERT and
		  GPT have drastically changed the Natural Language
		  Processing (NLP) field. For numerous NLP tasks, approaches
		  leveraging PLMs have achieved state-of-the-art performance.
		  The key idea is to learn a generic, latent representation
		  of language from a generic task once, then share it across
		  disparate NLP tasks. Language modeling serves as the
		  generic task, one with abundant self-supervised text
		  available for extensive training. This article presents the
		  key fundamental concepts of PLM architectures and a
		  comprehensive view of the shift to PLM-driven NLP
		  techniques. It surveys work applying the pre-training then
		  fine-tuning, prompting, and text generation approaches. In
		  addition, it discusses PLM limitations and suggested
		  directions for future research.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {30},
  numpages	= {40},
  keywords	= {Large language models, foundational models, generative AI,
		  neural networks}
}

@InProceedings{	  10.1145/3580305.3599853,
  author	= {Hui, Shuodi and Wang, Huandong and Li, Tong and Yang,
		  Xinghao and Wang, Xing and Feng, Junlan and Zhu, Lin and
		  Deng, Chao and Hui, Pan and Jin, Depeng and Li, Yong},
  title		= {Large-scale Urban Cellular Traffic Generation via
		  Knowledge-Enhanced GANs with Multi-Periodic Patterns},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599853},
  doi		= {10.1145/3580305.3599853},
  abstract	= {With the rapid development of the cellular network,
		  network planning is increasingly important. Generating
		  large-scale urban cellular traffic contributes to network
		  planning via simulating the behaviors of the planned
		  network. Existing methods fail in simulating the long-term
		  temporal behaviors of cellular traffic while cannot model
		  the influences of the urban environment on the cellular
		  networks. We propose a knowledge-enhanced GAN with
		  multi-periodic patterns to generate large-scale cellular
		  traffic based on the urban environment. First, we design a
		  GAN model to simulate the multi-periodic patterns and
		  long-term aperiodic temporal dynamics of cellular traffic
		  via learning the daily patterns, weekly patterns, and
		  residual traffic between long-term traffic and periodic
		  patterns step by step. Then, we leverage urban knowledge to
		  enhance traffic generation via constructing a knowledge
		  graph containing multiple factors affecting cellular
		  traffic in the surrounding urban environment. Finally, we
		  evaluate our model on a real cellular traffic dataset. Our
		  proposed model outperforms three state-of-art generation
		  models by over 32.77%, and the urban knowledge enhancement
		  improves the performance of our model by 4.71%. Moreover,
		  our model achieves good generalization and robustness in
		  generating traffic for urban cellular networks without
		  training data in the surrounding areas.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4195–4206},
  numpages	= {12},
  keywords	= {cellular traffic, gan, generation, knowledge graph},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3459637.3482436,
  author	= {Zhang, Taolin and Cai, Zerui and Wang, Chengyu and Li,
		  Peng and Li, Yang and Qiu, Minghui and Tang, Chengguang and
		  He, Xiaofeng and Huang, Jun},
  title		= {HORNET: Enriching Pre-trained Language Representations
		  with Heterogeneous Knowledge Sources},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482436},
  doi		= {10.1145/3459637.3482436},
  abstract	= {Knowledge-Enhanced Pre-trained Language Models (KEPLMs)
		  improve the language understanding abilities of deep
		  language models by leveraging the rich semantic knowledge
		  from knowledge graphs, other than plain pre-training texts.
		  However, previous efforts mostly use homogeneous knowledge
		  (especially structured relation triples in knowledge
		  graphs) to enhance the context-aware representations of
		  entity mentions, whose performance may be limited by the
		  coverage of knowledge graphs. Also, it is unclear whether
		  these KEPLMs truly understand the injected semantic
		  knowledge due to the "black-box'' training mechanism. In
		  this paper, we propose a novel KEPLM named HORNET, which
		  integrates Heterogeneous knowledge from various structured
		  and unstructured sources into the Roberta NETwork and hence
		  takes full advantage of both linguistic and factual
		  knowledge simultaneously. Specifically, we design a hybrid
		  attention heterogeneous graph convolution network (HaHGCN)
		  to learn heterogeneous knowledge representations based on
		  the structured relation triplets from knowledge graphs and
		  the unstructured entity description texts. Meanwhile, we
		  propose the explicit dual knowledge understanding tasks to
		  help induce a more effective infusion of the heterogeneous
		  knowledge, promoting our model for learning the complicated
		  mappings from the knowledge graph embedding space to the
		  deep context-aware embedding space and vice versa.
		  Experiments show that our HORNET model outperforms various
		  KEPLM baselines on knowledge-aware tasks including
		  knowledge probing, entity typing and relation extraction.
		  Our model also achieves substantial improvement over
		  several GLUE benchmark datasets, compared to other
		  KEPLMs.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2608–2617},
  numpages	= {10},
  keywords	= {heterogeneous graph attention network, knowledge graph,
		  natural language processing, pre-trained language model},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3600160.3604991,
  author	= {Tailhardat, Lionel and Troncy, Rapha\"{e}l and Chabot,
		  Yoan},
  title		= {Leveraging Knowledge Graphs For Classifying Incident
		  Situations in ICT Systems},
  year		= {2023},
  isbn		= {9798400707728},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600160.3604991},
  doi		= {10.1145/3600160.3604991},
  abstract	= {The complexity of Information and Communications
		  Technology (ICT) systems, such as enterprise or Internet
		  access provider networks, entails uncertainty in causal
		  reasoning for efficient incident management. In this work,
		  we propose to use knowledge graphs and explicit
		  representation of incident context to enable support teams
		  to provide a quick and effective response to complex
		  incident situations. Formal analysis and expert opinions
		  are used to analyze challenges in providing knowledge about
		  relationships between events and incidents in network
		  operations. We make use of an RDF knowledge graph generated
		  from a real industrial settings and representing the
		  network topology in terms of equipments and applications,
		  past incidents and their resolutions. We then demonstrate
		  the effectiveness of using a graph embeddings-based
		  classifier to categorize incident tickets based on context
		  and link anomaly models with their logical
		  representation.},
  booktitle	= {Proceedings of the 18th International Conference on
		  Availability, Reliability and Security},
  articleno	= {83},
  numpages	= {9},
  keywords	= {Anomaly Detection, Graph Embeddings, Graph Query, Incident
		  Management, Knowledge Graph},
  location	= {Benevento, Italy},
  series	= {ARES '23}
}

@InProceedings{	  10.1145/3583780.3614739,
  author	= {Chuang, Yu-Neng and Wang, Guanchu and Chang, Chia-Yuan and
		  Lai, Kwei-Herng and Zha, Daochen and Tang, Ruixiang and
		  Yang, Fan and Reyes, Alfredo Costilla and Zhou, Kaixiong
		  and Jiang, Xiaoqian and Hu, Xia},
  title		= {DiscoverPath: A Knowledge Refinement and Retrieval System
		  for Interdisciplinarity on Biomedical Research},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614739},
  doi		= {10.1145/3583780.3614739},
  abstract	= {The exponential growth in scholarly publications
		  necessitates advanced tools for efficient article
		  retrieval, especially in interdisciplinary fields where
		  diverse terminologies are used to describe similar
		  research. Traditional keyword-based search engines often
		  fall short in assisting users who may not be familiar with
		  specific terminologies. To address this, we present a
		  knowledge graph based paper search engine for biomedical
		  research to enhance the user experience in discovering
		  relevant queries and articles. The system, dubbed
		  DiscoverPath, employs Named Entity Recognition (NER) and
		  part-of-speech (POS) tagging to extract terminologies and
		  relationships from article abstracts to create a KG. To
		  reduce information overload, DiscoverPath presents users
		  with a focused subgraph containing the queried entity and
		  its neighboring nodes and incorporates a query
		  recommendation system enabling users to iteratively refine
		  their queries. The system is equipped with an accessible
		  Graphical User Interface that provides an intuitive
		  visualization of the KG, query recommendations, and
		  detailed article information, enabling efficient article
		  retrieval, thus fostering interdisciplinary knowledge
		  exploration.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5021–5025},
  numpages	= {5},
  keywords	= {biomedical, healthcare, information retrieval system,
		  knowledge graph, recommender system},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Article{	  10.1145/3600230,
  author	= {Katwe, Praveen Kumar and Khamparia, Aditya and Gupta,
		  Deepak and Dutta, Ashit Kumar},
  title		= {Methodical Systematic Review of Abstractive Summarization
		  and Natural Language Processing Models for Biomedical
		  Health Informatics: Approaches, Metrics and Challenges},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3600230},
  doi		= {10.1145/3600230},
  abstract	= {Text summarization tasks are primarily very useful for
		  decision support systems and provide a source for useful
		  data for training of bots as they can reduce and retain the
		  useful information from the large corpus. This review
		  article is for studying the literature that already exists
		  in context of abstractive summarization and application of
		  NLP language models in biomedical and associated healthcare
		  applications. In past decade with trends like bigdata, IOT,
		  enormous amount of data is getting processed in all
		  structured, unstructured and semi structured formats. This
		  review provides a comprehensive literature survey in
		  research trends for abstractive summarization, foundations
		  of machine translation and evolution of language models.
		  This review identifies the potential of language model to
		  provide a possible methodology for improving the
		  performance and accuracy of various tasks in summarization.
		  Deep neural network-based language models have now been the
		  widely accepted state of art for various abstractive
		  summarization and there exists an enormous scope to
		  improvise and tune the language models for domain specific
		  use case. This study shows current systems lack in
		  faithfulness to original content and control of degree of
		  hallucination. This review also details on the evaluation
		  criteria and need for automated metrics and attempts to
		  provide guideline for evaluation for abstractive
		  summarization for health informatics.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  keywords	= {NLP, abstractive summarization, sentence compression,
		  sentence fusion, document summarization, language model,
		  ROUGE}
}

@InProceedings{	  10.1145/3580305.3599852,
  author	= {Xiang, Tingyan and Li, Ao and Ji, Yugang and Li, Dong},
  title		= {Knowledge Based Prohibited Item Detection on Heterogeneous
		  Risk Graphs},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599852},
  doi		= {10.1145/3580305.3599852},
  abstract	= {With the popularity of online shopping in recent years,
		  various prohibited items are continuously attacking
		  e-commerce portals. Searching and deleting such risk items
		  online has played a fundamental role in protecting the
		  health of e-commerce trades. To mitigate negative impact of
		  limited supervision and adversarial behaviors of malicious
		  sellers, current state-of-the-art work mainly introduces
		  heterogeneous graph neural network with further
		  improvements such as graph structure learning, pairwise
		  training mechanism, etc. However, performance of these
		  models is highly limited since domain knowledge is
		  indispensable for identifying prohibited items but ignored
		  by these methods. In this paper, we propose a novel
		  Knowledge Based Prohibited item Detection system (named
		  KBPD) to break through this limitation. To make full use of
		  rich risk knowledge, the proposed method introduces the
		  Risk-Domain Knowledge Graph (named RDKG), which is encoded
		  by a path-based graph neural network method. Furthermore,
		  to utilize information from both the RDKG and the
		  Heterogeneous Risk Graph (named HRG), an interactive fusion
		  framework is proposed and further improves the detection
		  performance. We collect real-world datasets from the
		  largest Chinese second-hand commodity trading platform,
		  Xianyu. Both offline and online experimental results
		  consistently demonstrate that KBPD outperforms the
		  state-of-the-art baselines. The improvement over the
		  second-best method is up to 22.67% in the AP metric.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {5260–5269},
  numpages	= {10},
  keywords	= {heterogeneous graph neural network, knowledge graph,
		  prohibited item detection},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3580305.3599439,
  author	= {Luo, Pengfei and Xu, Tong and Wu, Shiwei and Zhu, Chen and
		  Xu, Linli and Chen, Enhong},
  title		= {Multi-Grained Multimodal Interaction Network for Entity
		  Linking},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599439},
  doi		= {10.1145/3580305.3599439},
  abstract	= {Multimodal entity linking (MEL) task, which aims at
		  resolving ambiguous mentions to a multimodal knowledge
		  graph, has attracted wide attention in recent years. Though
		  large efforts have been made to explore the complementary
		  effect among multiple modalities, however, they may fail to
		  fully absorb the comprehensive expression of abbreviated
		  textual context and implicit visual indication. Even worse,
		  the inevitable noisy data may cause inconsistency of
		  different modalities during the learning process, which
		  severely degenerates the performance. To address the above
		  issues, in this paper, we propose a novel Multi-GraIned
		  Multimodal InteraCtion Network (MIMIC) framework for
		  solving the MEL task. Specifically, the unified inputs of
		  mentions and entities are first encoded by textual/visual
		  encoders separately, to extract global descriptive features
		  and local detailed features. Then, to derive the similarity
		  matching score for each mention-entity pair, we device
		  three interaction units to comprehensively explore the
		  intra-modal interaction and inter-modal fusion among
		  features of entities and mentions. In particular, three
		  modules, namely the Text-based Global-Local interaction
		  Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) and
		  Cross-Modal Fusion-based interaction Unit (CMFU) are
		  designed to capture and integrate the fine-grained
		  representation lying in abbreviated text and implicit
		  visual cues. Afterwards, we introduce a unit-consistency
		  objective function via contrastive learning to avoid
		  inconsistency and model degradation. Experimental results
		  on three public benchmark datasets demonstrate that our
		  solution outperforms various state-of-the-art baselines,
		  and ablation studies verify the effectiveness of designed
		  modules.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1583–1594},
  numpages	= {12},
  keywords	= {knowledge graph, multimodal entity linking, multimodal
		  interaction},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3587716.3587787,
  author	= {Qian, Jing and Chen, Qi and Yue, Yong and Atkinson, Katie
		  and Li, Gangmin},
  title		= {Injecting Commonsense Knowledge into Prompt Learning for
		  Zero-Shot Text Classification},
  year		= {2023},
  isbn		= {9781450398411},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587716.3587787},
  doi		= {10.1145/3587716.3587787},
  abstract	= {The combination of pre-training and fine-tuning has become
		  a default solution to Natural Language Processing (NLP)
		  tasks. The emergence of prompt learning breaks such
		  routine, especially in the scenarios of low data resources.
		  Insufficient labelled data or even unseen classes are
		  frequent problems in text classification, equipping
		  Pre-trained Language Models (PLMs) with task-specific
		  prompts helps get rid of the dilemma. However, general PLMs
		  are barely provided with commonsense knowledge. In this
		  work, we propose a KG-driven verbalizer that leverages
		  commonsense Knowledge Graph (KG) to map label words with
		  predefined classes. Specifically, we transform the mapping
		  relationships into semantic relevance in the
		  commonsense-injected embedding space. For zero-shot text
		  classification task, experimental results exhibit the
		  effectiveness of our KG-driven verbalizer on a Twitter
		  dataset for natural disasters (i.e. HumAID) compared with
		  other baselines.},
  booktitle	= {Proceedings of the 2023 15th International Conference on
		  Machine Learning and Computing},
  pages		= {427–432},
  numpages	= {6},
  keywords	= {knowledge graph, prompt learning, zero-shot text
		  classification},
  location	= {Zhuhai, China},
  series	= {ICMLC '23}
}

@Article{	  10.1145/3567829,
  author	= {Li, Jia and Song, Dandan and Wu, Zhijing},
  title		= {A Semantically Driven Hybrid Network for Unsupervised
		  Entity Alignment},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3567829},
  doi		= {10.1145/3567829},
  abstract	= {The major challenge in the task of entity alignment (EA)
		  lies in the heterogeneity of the knowledge graph. The
		  traditional solution to EA is to first map entities to the
		  same space via knowledge embedding and then calculate the
		  similarity between entities from different knowledge
		  graphs. However, these methods mainly rely on manually
		  labeled seeds of EA, which limits their applicability. Some
		  researchers have begun using pseudo-labels rather than
		  seeds for unsupervised EA. However, directly using
		  pseudo-labels causes new problems, such as noise in the
		  pseudo-labels. In this article, we propose a model called
		  the Semantically Driven Hybrid Network (SDHN) to reduce the
		  impact of noise in the pseudo-labels on the performance of
		  EA models. The SDHN consists of two modules: a
		  Teacher–Student Network (TSN) and a Rotation and Penalty
		  (RAP) module. The TSN module reduces the impact of noise in
		  two ways: (1) The TSN’s teacher network guides its
		  student network to construct pseudo-labels based on
		  semantic information instead of directly creating
		  pseudo-labels. (2) It adaptively fuses semantic information
		  into student networks to improve the final representation
		  of entity embedding. Finally, the TSN enhances the
		  performance of models of entity alignment via the RAP
		  module. The results of experiments on multiple benchmark
		  datasets showed that the SDHN outperforms state-of-the-art
		  models.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= mar,
  articleno	= {20},
  numpages	= {21},
  keywords	= {Knowledge graph, graph neural networks, entity alignment}
}

@InProceedings{	  10.1145/3539618.3594246,
  author	= {Dietz, Laura and Bast, Hannah and Chatterjee, Shubham and
		  Dalton, Jeffrey and Nie, Jian-Yun and Nogueira, Rodrigo},
  title		= {Neuro-Symbolic Representations for Information Retrieval},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3594246},
  doi		= {10.1145/3539618.3594246},
  abstract	= {This tutorial will provide an overview of recent advances
		  on neuro-symbolic approaches for information retrieval. A
		  decade ago, knowledge graphs and semantic annotations
		  technology led to active research on how to best leverage
		  symbolic knowledge. At the same time, neural methods have
		  demonstrated to be versatile and highly effective.From a
		  neural network perspective, the same representation
		  approach can service document ranking or knowledge graph
		  reasoning. End-to-end training allows to optimize complex
		  methods for downstream tasks.We are at the point where both
		  the symbolic and the neural research advances are
		  coalescing into neuro-symbolic approaches. The underlying
		  research questions are how to best combine symbolic and
		  neural approaches, what kind of symbolic/neural approaches
		  are most suitable for which use case, and how to best
		  integrate both ideas to advance the state of the art in
		  information retrieval.Materials are available online:
		  https://github.com/laura-dietz/neurosymbolic-representations-for-IR},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3436–3439},
  numpages	= {4},
  keywords	= {document representation, entities, knowledge graph, neural
		  networks, neuro-symbolic representation},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3562007.3562032,
  author	= {Zhang, Xiaoqiao},
  title		= {Prediction of User Ratings of Dianping Based on K-BERT
		  Model},
  year		= {2022},
  isbn		= {9781450396851},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3562007.3562032},
  doi		= {10.1145/3562007.3562032},
  abstract	= {In the Internet era, people usually consider the ratings
		  and reviews of stores on review platforms when choosing
		  travel locations. Today, the mainstream rating scheme for
		  total store scores is weighted by review scores, but this
		  scoring system can be negatively affected by malicious
		  scoring and uneven scoring. Problems such as incompleteness
		  and other issues will affect the authenticity of the
		  store's rating. To this end, this paper designs a K-BERT
		  Dianping user rating prediction based on K-BERT model to
		  reflect real review ratings. Compared with the traditional
		  BERT pre-training model, the K-BERT model can solve
		  knowledge-driven problems faster through knowledge graph
		  injection. In this paper, a Dianping knowledge map is
		  established. Through the steps of text preprocessing, text
		  pre-training, and Dianping dataset fine-tuning, it is found
		  that the accuracy rate of the K-BERT model in the Dianping
		  rating classification is about 95%. By comparing the model
		  with BERT, Logistic Regression , it is found that the
		  predicted effect of the K-BERT model is significantly
		  better than the above two models.},
  booktitle	= {Proceedings of the 2022 3rd International Conference on
		  Control, Robotics and Intelligent System},
  pages		= {133–140},
  numpages	= {8},
  keywords	= {Bert, Knowledge Graph, Reviews, Text Categorization},
  location	= {Virtual Event, China},
  series	= {CCRIS '22}
}

@InProceedings{	  10.1145/3487553.3524199,
  author	= {Kuculo, Tin},
  title		= {Comprehensive Event Representations using Event Knowledge
		  Graphs and Natural Language Processing},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524199},
  doi		= {10.1145/3487553.3524199},
  abstract	= {Recent work has utilised knowledge-aware approaches to
		  natural language understanding, question answering,
		  recommendation systems, and other tasks. These approaches
		  rely on well-constructed and large-scale knowledge graphs
		  that can be useful for many downstream applications and
		  empower knowledge-aware models with commonsense reasoning.
		  Such knowledge graphs are constructed through knowledge
		  acquisition tasks such as relation extraction and knowledge
		  graph completion. This work seeks to utilise and build on
		  the growing body of work that uses findings from the field
		  of natural language processing (NLP) to extract knowledge
		  from text and build knowledge graphs. The focus of this
		  research project is on how we can use transformer-based
		  approaches to extract and contextualise event information,
		  matching it to existing ontologies, to build comprehensive
		  knowledge graph-based event representations. Specifically,
		  sub-event extraction is used as a way of creating
		  sub-event-aware event representations. These event
		  representations are then further enriched through
		  fine-grained location extraction and contextualised through
		  the alignment of historically relevant quotes.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {359–363},
  numpages	= {5},
  keywords	= {event extraction, event geotagging, event representation,
		  knowledge graph, quotes},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3544548.3580907,
  author	= {Petridis, Savvas and Diakopoulos, Nicholas and Crowston,
		  Kevin and Hansen, Mark and Henderson, Keren and
		  Jastrzebski, Stan and Nickerson, Jeffrey V and Chilton,
		  Lydia B},
  title		= {AngleKindling: Supporting Journalistic Angle Ideation with
		  Large Language Models},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3580907},
  doi		= {10.1145/3544548.3580907},
  abstract	= {News media often leverage documents to find ideas for
		  stories, while being critical of the frames and narratives
		  present. Developing angles from a document such as a press
		  release is a cognitively taxing process, in which
		  journalists critically examine the implicit meaning of its
		  claims. Informed by interviews with journalists, we
		  developed AngleKindling, an interactive tool which employs
		  the common sense reasoning of large language models to help
		  journalists explore angles for reporting on a press
		  release. In a study with 12 professional journalists, we
		  show that participants found AngleKindling significantly
		  more helpful and less mentally demanding to use for
		  brainstorming ideas, compared to a prior journalistic angle
		  ideation tool. AngleKindling helped journalists deeply
		  engage with the press release and recognize angles that
		  were useful for multiple types of stories. From our
		  findings, we discuss how to help journalists customize and
		  identify promising angles, and extending AngleKindling to
		  other knowledge-work domains.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {225},
  numpages	= {16},
  keywords	= {Brainstorming, Generative AI, Ideation, Journalism, Large
		  Language Models},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@InProceedings{	  10.1145/3583780.3615514,
  author	= {Colas, Anthony and Ma, Haodi and He, Xuanli and Bai, Yang
		  and Wang, Daisy Zhe},
  title		= {Can Knowledge Graphs Simplify Text?},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615514},
  doi		= {10.1145/3583780.3615514},
  abstract	= {Knowledge Graph (KG)-to-Text Generation has seen recent
		  improvements in generating fluent and informative sentences
		  which describe a given KG. As KGs are widespread across
		  multiple domains and contain important entity-relation
		  information, and as text simplification aims to reduce the
		  complexity of a text while preserving the meaning of the
		  original text, we propose KGSimple, a novel approach to
		  unsupervised text simplification which infuses
		  KG-established techniques in order to construct a
		  simplified KG path and generate a concise text which
		  preserves the original input's meaning. Through an
		  iterative and sampling KG-first approach, our model is
		  capable of simplifying text when starting from a KG by
		  learning to keep important information while harnessing
		  KG-to-text generation to output fluent and descriptive
		  sentences. We evaluate various settings of the KGSimple
		  model on currently-available KG-to-text datasets,
		  demonstrating its effectiveness compared to unsupervised
		  text simplification models which start with a given complex
		  text. Our code is available on GitHub.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {379–389},
  numpages	= {11},
  keywords	= {KG-to-text, data-to-text, knowledge graph, natural
		  language generation, simulated annealing, text
		  simplification},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3457682.3457745,
  author	= {ZHOU, JIALE and WANG, TAO and DENG, JIANFENG},
  title		= {Corpus Construction and Entity Recognition for the Field
		  of Industrial Robot Fault Diagnosis},
  year		= {2021},
  isbn		= {9781450389310},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3457682.3457745},
  doi		= {10.1145/3457682.3457745},
  abstract	= {The fault logs record the fault information generated
		  during the operation process of industrial robots. It
		  contains a large amount of fault knowledge and solution
		  information. It is necessary to extract this information
		  and build the fault diagnosis knowledge graph of industrial
		  robots, which can support remote fault diagnosis of
		  industrial robots without human help. At present, the
		  research of fault diagnosis knowledge graph is still
		  relatively scarce. In this paper, we propose a method of
		  named entity recognition for extracting the knowledge of
		  industrial robot fault diagnosis. The contribution of our
		  paper is to establish the fault field dataset Fault-Data,
		  propose the ontology concept of the fault diagnosis field,
		  and obtain a good field recognition effect through the
		  verification of the entity recognition model of fault
		  diagnosis. Experimental results show that the F value of
		  named entity recognition reaches 91.99%, which provides a
		  certain reference significance for subsequent knowledge
		  extraction and knowledge graph construction.},
  booktitle	= {Proceedings of the 2021 13th International Conference on
		  Machine Learning and Computing},
  pages		= {410–416},
  numpages	= {7},
  keywords	= {Industrial robots, entity recognition, fault diagnosis,
		  knowledge graph},
  location	= {Shenzhen, China},
  series	= {ICMLC '21}
}

@InProceedings{	  10.1145/3511808.3557246,
  author	= {Xiong, Guanming and Bao, Junwei and Zhao, Wen and Wu,
		  Youzheng and He, Xiaodong},
  title		= {AutoQGS: Auto-Prompt for Low-Resource Knowledge-based
		  Question Generation from SPARQL},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557246},
  doi		= {10.1145/3511808.3557246},
  abstract	= {This study investigates the task of knowledge-based
		  question generation (KBQG). Conventional KBQG works
		  generated questions from fact triples in the knowledge
		  graph, which could not express complex operations like
		  aggregation and comparison in SPARQL. Moreover, due to the
		  costly annotation of large-scale SPARQL-question pairs,
		  KBQG from SPARQL under low-resource scenarios urgently
		  needs to be explored. Recently, since the generative
		  pre-trained language models (PLMs) typically trained in
		  natural language (NL)-to-NL paradigm have been proven
		  effective for low-resource generation, e.g., T5 and BART,
		  how to effectively utilize them to generate NL-question
		  from non-NL SPARQL is challenging. To address these
		  challenges, AutoQGS, an auto-prompt approach for
		  low-resource KBQG from SPARQL, is proposed. Firstly, we put
		  forward to generate questions directly from SPARQL for KBQG
		  task to handle complex operations. Secondly, we propose an
		  auto-prompter trained on large-scale unsupervised data to
		  rephrase SPARQL into NL description, smoothing the
		  low-resource transformation from non-NL SPARQL to NL
		  question with PLMs. Experimental results on the
		  WebQuestionsSP, ComlexWebQuestions 1.1, and PathQuestions
		  show that our model achieves state-of-the-art performance,
		  especially in low-resource settings. Furthermore, a corpora
		  of 330k factoid complex question-SPARQL pairs is generated
		  for further KBQG research.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2250–2259},
  numpages	= {10},
  keywords	= {complex question generation, knowledge graph, low
		  resource},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@Article{	  10.1145/3586163,
  author	= {Breit, Anna and Waltersdorfer, Laura and Ekaputra, Fajar
		  J. and Sabou, Marta and Ekelhart, Andreas and Iana, Andreea
		  and Paulheim, Heiko and Portisch, Jan and Revenko, Artem
		  and Teije, Annette Ten and Van Harmelen, Frank},
  title		= {Combining Machine Learning and Semantic Web: A Systematic
		  Mapping Study},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {14s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3586163},
  doi		= {10.1145/3586163},
  abstract	= {In line with the general trend in artificial intelligence
		  research to create intelligent systems that combine
		  learning and symbolic components, a new sub-area has
		  emerged that focuses on combining Machine Learning
		  components with techniques developed by the Semantic Web
		  community—Semantic Web Machine Learning (SWeML). Due to
		  its rapid growth and impact on several communities in
		  thepast two decades, there is a need to better understand
		  the space of these SWeML Systems, their characteristics,
		  and trends. Yet, surveys that adopt principled and unbiased
		  approaches are missing. To fill this gap, we performed a
		  systematic study and analyzed nearly 500 papers published
		  in the past decade in this area, where we focused on
		  evaluating architectural and application-specific features.
		  Our analysis identified a rapidly growing interest in SWeML
		  Systems, with a high impact on several application domains
		  and tasks. Catalysts for this rapid growth are the
		  increased application of deep learning and knowledge graph
		  technologies. By leveraging the in-depth understanding of
		  this area acquired through this study, a further key
		  contribution of this article is a classification system for
		  SWeML Systems that we publish as ontology.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {313},
  numpages	= {41},
  keywords	= {Semantic Web, Machine Learning, Artificial Intelligence,
		  knowledge graph, Knowledge Representation and Reasoning,
		  neuro-symbolic integration, Systematic Mapping Study}
}

@InProceedings{	  10.1145/3604915.3608885,
  author	= {Zhang, Gangyi},
  title		= {User-Centric Conversational Recommendation: Adapting the
		  Need of User with Large Language Models},
  year		= {2023},
  isbn		= {9798400702419},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604915.3608885},
  doi		= {10.1145/3604915.3608885},
  abstract	= {Conversational recommender systems (CRS) promise to
		  provide a more natural user experience for exploring and
		  discovering items of interest through ongoing conversation.
		  However, effectively modeling and adapting to users’
		  complex and changing preferences remains challenging. This
		  research develops user-centric methods that focus on
		  understanding and adapting to users throughout
		  conversations to provide the most helpful recommendations.
		  First, a graph-based Conversational Path Reasoning (CPR)
		  framework is proposed that represents dialogs as
		  interactive reasoning over a knowledge graph to capture
		  nuanced user interests and explain recommendations. To
		  further enhance relationship modeling, graph neural
		  networks are incorporated for improved representation
		  learning. Next, to address uncertainty in user needs, the
		  Vague Preference Multi-round Conversational Recommendation
		  (VPMCR) scenario and matching Adaptive Vague Preference
		  Policy Learning (AVPPL) solution are presented using
		  reinforcement learning to tailor recommendations to
		  evolving preferences. Finally, opportunities to leverage
		  large language models are discussed to further advance user
		  experiences via advanced user modeling, policy learning,
		  and response generation. Overall, this research focuses on
		  designing conversational recommender systems that
		  continuously understand and adapt to users’ ambiguous,
		  complex and changing needs during natural conversations.},
  booktitle	= {Proceedings of the 17th ACM Conference on Recommender
		  Systems},
  pages		= {1349–1354},
  numpages	= {6},
  keywords	= {conversational recommendation, large language model,
		  user-centric},
  location	= {Singapore, Singapore},
  series	= {RecSys '23}
}

@InProceedings{	  10.1145/3581783.3613848,
  author	= {Rao, Jiahua and Shan, Zifei and Liu, Longpo and Zhou, Yao
		  and Yang, Yuedong},
  title		= {Retrieval-based Knowledge Augmented Vision Language
		  Pre-training},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3613848},
  doi		= {10.1145/3581783.3613848},
  abstract	= {With the recent progress in large-scale vision and
		  language representation learning, Vision Language
		  Pre-training (VLP) models have achieved promising
		  improvements on various multi-modal downstream tasks.
		  Albeit powerful, these models have not fully leveraged
		  world knowledge to their advantage. A key challenge of
		  knowledge-augmented VLP is the lack of clear connections
		  between knowledge and multi-modal data. Moreover, not all
		  knowledge present in images/texts is useful, therefore
		  prior approaches often struggle to effectively integrate
		  knowledge, visual, and textual information. In this study,
		  we propose REtrieval-based knowledge Augmented Vision
		  Language (REAVL), a novel knowledge-augmented pre-training
		  framework to address the above issues. For the first time,
		  we introduce a knowledge-aware self-supervised learning
		  scheme that efficiently establishes the correspondence
		  between knowledge and multi-modal data and identifies
		  informative knowledge to improve the modeling of alignment
		  and interactions between visual and textual modalities. By
		  adaptively integrating informative knowledge with visual
		  and textual information, REAVL achieves new
		  state-of-the-art performance uniformly on knowledge-based
		  vision-language understanding and multi-modal entity
		  linking tasks, as well as competitive results on general
		  vision-language tasks while only using 0.2% pre-training
		  data of the best models. Our model shows strong sample
		  efficiency and effective knowledge utilization.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {5399–5409},
  numpages	= {11},
  keywords	= {knowledge graph, knowledge retrieval, knowledge-augmented
		  model, vision-language pre-training},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3579895.3579936,
  author	= {Qiao, Lin and Qu, Ruiting and Liu, Shenglong and Zhang, Yu
		  and Wang, Huiying and Liu, Biqi},
  title		= {Roadmap on Industrial Knowledge System for Data-Oriented
		  Intelligent Operation and Maintenance in Chinese Power
		  Industry},
  year		= {2023},
  isbn		= {9781450398039},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579895.3579936},
  doi		= {10.1145/3579895.3579936},
  abstract	= {To effectively and efficiently manage the information of
		  power industry, especially in the State Grid of China,
		  data-oriented intelligent operation and maintenance have
		  always been a crucial task. Hence, in this paper, the
		  roadmap on industrial knowledge system is presented for
		  data-oriented intelligent operation and maintenance in
		  Chinese power industry. Firstly, the background of the
		  data-oriented intelligent operation and maintenance is
		  described, and it has been pointed out that the core
		  problem is data explosion and lack of knowledge in the
		  data-oriented intelligent operation and maintenance in the
		  State Grid of China. This is important not only for the
		  construction of smart grid, but also for global energy
		  savings. Secondly, as for data-oriented intelligent
		  operation and maintenance, the State Grid data-oriented
		  knowledge graph can be constructed. Then, we study the
		  knowledge-driven multi-scenario intelligent operation
		  decision technologies and information deployment
		  technologies. Finally, the possible research direction of
		  industrial knowledge systems is briefly presented for
		  data-oriented intelligent operation and maintenance in
		  State Grid.},
  booktitle	= {Proceedings of the 2022 11th International Conference on
		  Networks, Communication and Computing},
  pages		= {267–276},
  numpages	= {10},
  keywords	= {Industrial knowledge system, Knowledge Graph,
		  data-oriented Intelligent maintenance, data-oriented
		  Intelligent operation},
  location	= {Beijing, China},
  series	= {ICNCC '22}
}

@InProceedings{	  10.1145/3543507.3583236,
  author	= {Guo, Kunpeng and Diefenbach, Dennis and Gourru, Antoine
		  and Gravier, Christophe},
  title		= {Wikidata as a seed for Web Extraction},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583236},
  doi		= {10.1145/3543507.3583236},
  abstract	= {Wikidata has grown to a knowledge graph with an impressive
		  size. To date, it contains more than 17 billion triples
		  collecting information about people, places, films, stars,
		  publications, proteins, and many more. On the other side,
		  most of the information on the Web is not published in
		  highly structured data repositories like Wikidata, but
		  rather as unstructured and semi-structured content, more
		  concretely in HTML pages containing text and tables.
		  Finding, monitoring, and organizing this data in a
		  knowledge graph is requiring considerable work from human
		  editors. The volume and complexity of the data make this
		  task difficult and time-consuming. In this work, we present
		  a framework that is able to identify and extract new facts
		  that are published under multiple Web domains so that they
		  can be proposed for validation by Wikidata editors. The
		  framework is relying on question-answering technologies. We
		  take inspiration from ideas that are used to extract facts
		  from textual collections and adapt them to extract facts
		  from Web pages. For achieving this, we demonstrate that
		  language models can be adapted to extract facts not only
		  from textual collections but also from Web pages. By
		  exploiting the information already contained in Wikidata
		  the proposed framework can be trained without the need for
		  any additional learning signals and can extract new facts
		  for a wide range of properties and domains. Following this
		  path, Wikidata can be used as a seed to extract facts on
		  the Web. Our experiments show that we can achieve a mean
		  performance of 84.07 at F1-score. Moreover, our estimations
		  show that we can potentially extract millions of facts that
		  can be proposed for human validation. The goal is to help
		  editors in their daily tasks and contribute to the
		  completion of the Wikidata knowledge graph.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {2402–2411},
  numpages	= {10},
  keywords	= {Knowledge Graph Completion, Linking, Question Answering,
		  Web Crawling, Web Extraction, Web Scraping, Wikidata},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3477495.3531683,
  author	= {Yang, Zuoxi},
  title		= {Generating Knowledge-based Explanation for Recommendation
		  from Review},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531683},
  doi		= {10.1145/3477495.3531683},
  abstract	= {Reasonable explanation is helpful to increase the trust
		  and satisfaction of user to the recommender system. Among
		  many previous studies, there is growing concern about
		  generating explanation based on review text.Collaborative
		  filtering is one of the most successful approaches to
		  predict user's preference. However, most of them suffer
		  from data sparsity problem. Researcher often utilizes
		  auxiliary data to address this problem, such as review,
		  knowledge graph (KG), image and so on. Some researchers
		  have proven that recommendation accuracy can be improved
		  via incorporating rating and review data. Besides, neural
		  network is also applied to learn more powerful
		  representations for user and item from the review data. For
		  example, convolution neural network (CNN) is used to
		  extract representation from review text by using
		  convolutional filters. Recurrent neural network (RNN) is
		  another widely used model, which can encode the sequential
		  behaviours as hidden states. However, most of them lack the
		  ability to generate explanation.In order to generate
		  explanation, there are two main approaches are used, i.e.,
		  template-based approach and generation-based approach. It
		  is usually necessary for the templated-based approach to
		  define serval templates. Then, these templates will be
		  further filled with different personalized features/words.
		  Although they can offer readable explanations, they rely
		  heavily on pre-defined templates. It causes large manual
		  efforts, limiting their explanation expression. Due to the
		  strong generation ability of natural language model, the
		  generation-based approach is capable to generate
		  explanation without templates, which can largely enhance
		  the expression of the generated sentence. Although they can
		  generate more free and flexible explanation, the
		  explanation might tend to be uninformative.To tackle these
		  challenges of the above-mentioned work, we propose a
		  Generating Knowldge-based Explanation for Recommendation
		  from Review (GKER) to provide informative explanation.
		  Unlike the traditional generation-based approach with a
		  multi-task framework, we design a single-task framework to
		  simultaneously model user's preference and explanation
		  generation. The multi-task training usually needs more
		  manual effort and time overhead. In this unitary framework,
		  we inject the user's sentiment preference into the
		  explanation generation, aiming at capturing the user's
		  interest while producing high-quality explanation.
		  Specifically, we build three graphs, including a bipartite
		  graph, a KG and a co-occur graph. All of them are
		  integrated to form a unitary graph, thus bringing the
		  semantic among user-item interaction, KG and review. Based
		  on this integrated graph, it is possible to learn more
		  effective representations for user and item. To make better
		  use of the integrated KG, a graph convolution network (GCN)
		  is utilized to obtain improved embeddings due to its
		  superior representation learning ability. We argue that
		  these embeddings can contain more semantic interaction
		  signals with the help of the integrated KG and GCN. After
		  obtaining these extensive embeddings, a multilayer
		  perceptron (MLP) layer is further employed to capture
		  non-linear interaction signals between user and item,
		  aiming at predicting user's rating accurately. The
		  predicted rating would be regarded as a sentiment indicator
		  to explore why the user likes or dislikes the target item.
		  To investigate the association between sentiment indicator
		  and the related review data, a transformer-enhanced
		  encoder-decoder architecture is designed to produce
		  informative and topic-relevant explanation. Besides, the
		  aspect semantic is added in this architecture through an
		  attention mechanism. In this framework, the transformer is
		  utilized as a "teacher" model to supervise the generation
		  of the encoder-decoder process. Finally, experiments
		  conducted on three datasets have shown the state-of-the-art
		  performance of GKER.There are some research issues for
		  discussion: 1) although KG is a useful tool for
		  recommendation accuracy and explainability, it is always
		  incomplete in the real world. Hence, it is worth completing
		  it for the recommendation. 2) Besides, as for explainable,
		  it still needs more metrics to evaluate the quality of its
		  explanation.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3494},
  numpages	= {1},
  keywords	= {generating explanation, graph convolution network,
		  knowledge graph, review},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3511808.3557459,
  author	= {Li, Jiacheng and Katsis, Yannis and Baldwin, Tyler and
		  Kim, Ho-Cheol and Bartko, Andrew and McAuley, Julian and
		  Hsu, Chun-Nan},
  title		= {SPOT: Knowledge-Enhanced Language Representations for
		  Information Extraction},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557459},
  doi		= {10.1145/3511808.3557459},
  abstract	= {Knowledge-enhanced pre-trained models for language
		  representation have been shown to be more effective in
		  knowledge base construction tasks (i.e.,~relation
		  extraction) than language models such as BERT. These
		  knowledge-enhanced language models incorporate knowledge
		  into pre-training to generate representations of entities
		  or relationships. However, existing methods typically
		  represent each entity with a separate embedding. As a
		  result, these methods struggle to represent
		  out-of-vocabulary entities and a large amount of
		  parameters, on top of their underlying token models (i.e.,
		  the transformer), must be used and the number of entities
		  that can be handled is limited in practice due to memory
		  constraints. Moreover, existing models still struggle to
		  represent entities and relationships simultaneously. To
		  address these problems, we propose a new pre-trained model
		  that learns representations of both entities and
		  relationships from token spans and span pairs in the text
		  respectively. By encoding spans efficiently with span
		  modules, our model can represent both entities and their
		  relationships but requires fewer parameters than existing
		  models. We pre-trained our model with the knowledge graph
		  extracted from Wikipedia and test it on a broad range of
		  supervised and unsupervised information extraction tasks.
		  Results show that our model learns better representations
		  for both entities and relationships than baselines, while
		  in supervised settings, fine-tuning our model outperforms
		  RoBERTa consistently and achieves competitive results on
		  information extraction tasks.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1124–1134},
  numpages	= {11},
  keywords	= {information extraction, knowledge representation, language
		  model, pre-trained model, representation learning},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3607199.3607208,
  author	= {Alam, Md Tanvirul and Bhusal, Dipkamal and Park, Youngja
		  and Rastogi, Nidhi},
  title		= {Looking Beyond IoCs: Automatically Extracting Attack
		  Patterns from External CTI},
  year		= {2023},
  isbn		= {9798400707650},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3607199.3607208},
  doi		= {10.1145/3607199.3607208},
  abstract	= {Public and commercial organizations extensively share
		  cyberthreat intelligence (CTI) to prepare systems to defend
		  against existing and emerging cyberattacks. However,
		  traditional CTI has primarily focused on tracking known
		  threat indicators such as IP addresses and domain names,
		  which may not provide long-term value in defending against
		  evolving attacks. To address this challenge, we propose to
		  use more robust threat intelligence signals called attack
		  patterns. LADDER is a knowledge extraction framework that
		  can extract text-based attack patterns from CTI reports at
		  scale. The framework characterizes attack patterns by
		  capturing the phases of an attack in Android and enterprise
		  networks and systematically maps them to the MITRE
		  ATT&amp;CK pattern framework. LADDER can be used by
		  security analysts to determine the presence of attack
		  vectors related to existing and emerging threats, enabling
		  them to prepare defenses proactively. We also present
		  several use cases to demonstrate the application of LADDER
		  in real-world scenarios. Finally, we provide a new,
		  open-access benchmark malware dataset to train future
		  cyberthreat intelligence models.},
  booktitle	= {Proceedings of the 26th International Symposium on
		  Research in Attacks, Intrusions and Defenses},
  pages		= {92–108},
  numpages	= {17},
  keywords	= {Attack Patterns, Knowledge Graph, LADDER, Threat
		  Intelligence},
  location	= {Hong Kong, China},
  series	= {RAID '23}
}

@InProceedings{	  10.1145/3581783.3612274,
  author	= {Liang, Kongming and Wang, Xinran and Zhang, Haiwen and Ma,
		  Zhanyu and Guo, Jun},
  title		= {Hierarchical Visual Attribute Learning in the Wild},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612274},
  doi		= {10.1145/3581783.3612274},
  abstract	= {Observing objects' attributes at different levels of
		  detail is a fundamental aspect of how humans perceive and
		  understand the world around them. Existing studies focused
		  on attribute prediction in a flat way, but they overlook
		  the underlying attribute hierarchy, e.g., navy blue is a
		  subcategory of blue. In recent years, large language
		  models, e.g., ChatGPT, have emerged with the ability to
		  perform an extensive range of natural language processing
		  tasks like text generation and classification. The factual
		  knowledge learned by LLM can assist us build the
		  hierarchical relations of visual attributes in the wild.
		  Based on that, we propose a model called the
		  object-specific attribute relation net, which takes
		  advantage of three types of relations among attributes -
		  positive, negative, and hierarchical - to better facilitate
		  attribute recognition in images. Guided by the extracted
		  hierarchical relations, our model can predict attributes
		  from coarse to fine. Additionally, we introduce several
		  evaluation metrics for attribute hierarchy to
		  comprehensively assess the model's ability to comprehend
		  hierarchical relations. Our extensive experiments
		  demonstrate that our proposed hierarchical annotation
		  brings improvements to the model's understanding of
		  hierarchical relations of attributes, and the
		  object-specific attribute relation net can recognize visual
		  attributes more accurately.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {3415–3423},
  numpages	= {9},
  keywords	= {attribute learning, hierarchical multi-label learning,
		  large language model},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3447548.3467203,
  author	= {Luo, Xusheng and Bo, Le and Wu, Jinhang and Li, Lin and
		  Luo, Zhiy and Yang, Yonghua and Yang, Keping},
  title		= {AliCoCo2: Commonsense Knowledge Extraction, Representation
		  and Application in E-commerce},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467203},
  doi		= {10.1145/3447548.3467203},
  abstract	= {Commonsense knowledge used by humans while doing online
		  shopping is valuable but difficult to be captured by
		  existing systems running on e-commerce platforms. While
		  construction of common- sense knowledge graphs in
		  e-commerce is non-trivial, representation learning upon
		  such graphs poses unique challenge compared to well-studied
		  open-domain knowledge graphs (e.g., Freebase). By
		  leveraging the commonsense knowledge and representation
		  techniques, various applications in e-commerce can be
		  benefited. Based on AliCoCo, the large-scale e-commerce
		  concept net assisting a series of core businesses in
		  Alibaba, we further enrich it with more commonsense
		  relations and present AliCoCo2, the first commonsense
		  knowledge graph constructed for e-commerce use. We propose
		  a multi-task encoder-decoder framework to provide effective
		  representations for nodes and edges from AliCoCo2. To
		  explore the possibility of improving e-commerce businesses
		  with commonsense knowledge, we apply newly mined
		  commonsense relations and learned embeddings to e-commerce
		  search engine and recommendation system in different ways.
		  Experimental results demonstrate that our proposed
		  representation learning method achieves state-of-the-art
		  performance on the task of knowledge graph completion
		  (KGC), and applications on search and recommendation
		  indicate great potential value of the construction and use
		  of commonsense knowledge graph in e-commerce. Besides, we
		  propose an e-commerce QA task with a new benchmark during
		  the construction of AliCoCo2, for testing machine common
		  sense in e-commerce, which can benefit research community
		  in exploring commonsense reasoning.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {3385–3393},
  numpages	= {9},
  keywords	= {common sense, e-commerce, knowledge graph embedding},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@InProceedings{	  10.1145/3582197.3582250,
  author	= {Xu, Sa},
  title		= {A Knowledge Reasoning Model Based on Non-Factoid
		  Information Enhancement},
  year		= {2023},
  isbn		= {9781450397438},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3582197.3582250},
  doi		= {10.1145/3582197.3582250},
  abstract	= {Q&amp;A system plays an increasingly important role in the
		  modern society with information explosion, and knowledge
		  reasoning model (KRM) is the main research content of
		  Q&amp;A system. Existing knowledge reasoning models are
		  mainly divided into text-based information retrieval (IR)
		  and knowledge graph embedding (KGE). KGE is superior to IR
		  in terms of storage and reasoning capabilities for massive
		  factoid information, but lacks the ability to reason
		  non-factoid information, merely focus on the mining of
		  structural information without the semantic information. We
		  proposed a knowledge reasoning model based on non-factoid
		  information enhancement (NFE-KRM) in scenic Q&amp;A. It
		  realizes the KGE integrates semantic information (SIKGE)
		  and the unified semantic embedding space (USES), so that
		  NFE-KRM has the ability to answer both factoid and
		  non-factoid questions. We have used a large number of
		  experiments to prove that SIKGE gets a better performance
		  on Mean Rank and Hits@10. NFE-KRM's F1 score and accuracy
		  on the mixed dataset are both competitive.},
  booktitle	= {Proceedings of the 2022 10th International Conference on
		  Information Technology: IoT and Smart City},
  pages		= {318–323},
  numpages	= {6},
  keywords	= {Q&amp;A system, Information Retrieval, Knowledge Graph
		  Embedding, NLP.},
  location	= {Shanghai, China},
  series	= {ICIT '22}
}

@InProceedings{	  10.1145/3500931.3500953,
  author	= {Dong, Andi and Wang, Chao and Tong, Pan and Yang, Dan and
		  Yong, Cuo},
  title		= {Research on Tibetan medicine intelligent question
		  answering system integrating confrontation training and
		  reinforcement learning},
  year		= {2021},
  isbn		= {9781450395588},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3500931.3500953},
  doi		= {10.1145/3500931.3500953},
  abstract	= {In this study, a knowledge graph (KG) based Tibetan
		  medicine intelligent question answering (QA) system model
		  was proposed based on an adversarial learning generative
		  network model, in an attempt to alleviate the scarcity of
		  medical resources, promote the heritage and innovation of
		  Tibetan medicine, and ease the shortage of Tibetan medical
		  information. In this model, the simulated answers were
		  generated via adversarial learning, and subsequently the
		  reinforcement learning was applied for feedback-based
		  optimization, with the ultimate aim of enhancing the
		  accuracy rate of this model. Besides, a triple extraction
		  method based on Tibetan features was proposed to construct
		  a KG dialog set. Finally, this model was subjected to an
		  experiment in Chinese and Tibetan datasets, with the
		  results indicating that the accuracy of this intelligent QA
		  model incorporating adversarial networks and reinforcement
		  learning was higher than other models.},
  booktitle	= {Proceedings of the 2nd International Symposium on
		  Artificial Intelligence for Medicine Sciences},
  pages		= {120–125},
  numpages	= {6},
  keywords	= {Generate countermeasure network, Knowledge extraction,
		  Knowledge graph, Reinforcement learning, Tibetan medicine},
  location	= {Beijing, China},
  series	= {ISAIMS '21}
}

@Article{	  10.1145/3457533,
  author	= {Deng, Yang and Xie, Yuexiang and Li, Yaliang and Yang, Min
		  and Lam, Wai and Shen, Ying},
  title		= {Contextualized Knowledge-aware Attentive Neural Network:
		  Enhancing Answer Selection with Knowledge},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3457533},
  doi		= {10.1145/3457533},
  abstract	= {Answer selection, which is involved in many natural
		  language processing applications, such as dialog systems
		  and question answering (QA), is an important yet
		  challenging task in practice, since conventional methods
		  typically suffer from the issues of ignoring diverse
		  real-world background knowledge. In this article, we
		  extensively investigate approaches to enhancing the answer
		  selection model with external knowledge from knowledge
		  graph (KG). First, we present a context-knowledge
		  interaction learning framework, Knowledge-aware Neural
		  Network, which learns the QA sentence representations by
		  considering a tight interaction with the external knowledge
		  from KG and the textual information. Then, we develop two
		  kinds of knowledge-aware attention mechanism to summarize
		  both the context-based and knowledge-based interactions
		  between questions and answers. To handle the diversity and
		  complexity of KG information, we further propose a
		  Contextualized Knowledge-aware Attentive Neural Network,
		  which improves the knowledge representation learning with
		  structure information via a customized Graph Convolutional
		  Network and comprehensively learns context-based and
		  knowledge-based sentence representation via the multi-view
		  knowledge-aware attention mechanism. We evaluate our method
		  on four widely used benchmark QA datasets, including
		  WikiQA, TREC QA, InsuranceQA, and Yahoo QA. Results verify
		  the benefits of incorporating external knowledge from KG
		  and show the robust superiority and extensive applicability
		  of our method.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= sep,
  articleno	= {2},
  numpages	= {33},
  keywords	= {Answer selection, knowledge graph, attention mechanism,
		  graph convolutional network}
}

@InProceedings{	  10.1145/3543873.3587617,
  author	= {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
  title		= {Automated Ontology Evaluation: Evaluating Coverage and
		  Correctness using a Domain Corpus},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587617},
  doi		= {10.1145/3543873.3587617},
  abstract	= {Ontologies conceptualize domains and are a crucial part of
		  web semantics and information systems. However, re-using an
		  existing ontology for a new task requires a detailed
		  evaluation of the candidate ontology as it may cover only a
		  subset of the domain concepts, contain information that is
		  redundant or misleading, and have inaccurate relations and
		  hierarchies between concepts. Manual evaluation of large
		  and complex ontologies is a tedious task. Thus, a few
		  approaches have been proposed for automated evaluation,
		  ranging from concept coverage to ontology generation from a
		  corpus. Existing approaches, however, are limited by their
		  dependence on external structured knowledge sources, such
		  as a thesaurus, as well as by their inability to evaluate
		  semantic relationships. In this paper, we propose a novel
		  framework to automatically evaluate the domain coverage and
		  semantic correctness of existing ontologies based on domain
		  information derived from text. The approach uses a
		  domain-tuned named-entity-recognition model to extract
		  phrasal concepts. The extracted concepts are then used as a
		  representation of the domain against which we evaluate the
		  candidate ontology’s concepts. We further employ a
		  domain-tuned language model to determine the semantic
		  correctness of the candidate ontology’s relations. We
		  demonstrate our automated approach on several large
		  ontologies from the oceanographic domain and show its
		  agreement with a manual evaluation by domain experts and
		  its superiority over the state-of-the-art.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {1127–1137},
  numpages	= {11},
  keywords	= {BERT, knowledge engineering, natural language processing,
		  ontology},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3584371.3613016,
  author	= {Oduro-Afriyie, Joel and Jamil, Hasan M},
  title		= {Enabling the Informed Patient Paradigm with Secure and
		  Personalized Medical Question Answering},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3613016},
  doi		= {10.1145/3584371.3613016},
  abstract	= {Quality patient care is a complex and multifaceted problem
		  requiring the integration of data from multiple sources. We
		  propose Medicient, a knowledge-graph-based question
		  answering system that processes heterogeneous data sources,
		  including patient health records, drug databases, and
		  medical literature, into a unified knowledge graph with
		  zero training. The knowledge graph is then utilized to
		  provide personalized recommendations for treatment or
		  medication. The system leverages the power of large
		  language models for question understanding and natural
		  language response generation, while hiding sensitive
		  patient information. We compare our system to a large
		  language model (ChatGPT), which does not have access to
		  patient health records, and show that our system provides
		  better recommendations. This study contributes to a growing
		  body of research on knowledge graphs and their applications
		  in healthcare.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {33},
  numpages	= {6},
  keywords	= {knowledge graphs, large language models, semantic graph
		  search, data integration, personal health library, informed
		  patients},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1145/3539597.3570426,
  author	= {Zhang, Xiaoyu and Xin, Xin and Li, Dongdong and Liu,
		  Wenxuan and Ren, Pengjie and Chen, Zhumin and Ma, Jun and
		  Ren, Zhaochun},
  title		= {Variational Reasoning over Incomplete Knowledge Graphs for
		  Conversational Recommendation},
  year		= {2023},
  isbn		= {9781450394079},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539597.3570426},
  doi		= {10.1145/3539597.3570426},
  abstract	= {Conversational recommender systems (CRSs) often utilize
		  external knowledge graphs (KGs) to introduce rich semantic
		  information and recommend relevant items through natural
		  language dialogues. However, original KGs employed in
		  existing CRSs are often incomplete and sparse, which limits
		  the reasoning capability in recommendation. Moreover, only
		  few of existing studies exploit the dialogue context to
		  dynamically refine knowledge from KGs for better
		  recommendation. To address the above issues, we propose the
		  Variational Reasoning over Incomplete KGs Conversational
		  Recommender (VRICR). Our key idea is to incorporate the
		  large dialogue corpus naturally accompanied with CRSs to
		  enhance the incomplete KGs; and perform dynamic knowledge
		  reasoning conditioned on the dialogue context.
		  Specifically, we denote the dialogue-specific subgraphs of
		  KGs as latent variables with categorical priors for
		  adaptive knowledge graphs refactor. We propose a
		  variational Bayesian method to approximate posterior
		  distributions over dialogue-specific subgraphs, which not
		  only leverages the dialogue corpus for restructuring
		  missing entity relations but also dynamically selects
		  knowledge based on the dialogue context. Finally, we infuse
		  the dialogue-specific subgraphs to decode the
		  recommendation and responses. We conduct experiments on two
		  benchmark CRSs datasets. Experimental results confirm the
		  effectiveness of our proposed method.},
  booktitle	= {Proceedings of the Sixteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {231–239},
  numpages	= {9},
  keywords	= {conversational recommender systems, knowledge graph
		  enhancement, knowledge refinemen, variational inference},
  location	= {Singapore, Singapore},
  series	= {WSDM '23}
}

@InProceedings{	  10.1145/3587259.3627555,
  author	= {Verkijk, Stella and Roothaert, Ritten and Pernisch, Romana
		  and Schlobach, Stefan},
  title		= {Do you catch my drift? On the usage of embedding methods
		  to measure concept shift in knowledge graphs},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627555},
  doi		= {10.1145/3587259.3627555},
  abstract	= {Automatically detecting and measuring differences between
		  evolving Knowledge Graphs (KGs) has been a topic of
		  investigation for years. With the rising popularity of
		  embedding methods, we investigate the possibility of using
		  embeddings to detect Concept Shift in evolving KGs.
		  Specifically, we go deeper into the usage of nearest
		  neighbour set comparison as the basis for a similarity
		  measure, and show why this approach is conceptually
		  problematic. As an alternative, we explore the possibility
		  of using clustering methods. This paper serves to (i)
		  inform the community about the challenges that arise when
		  using KG embeddings for the comparison of different
		  versions of a KG specifically, (ii) investigate how this is
		  supported by theories on knowledge representation and
		  semantic representation in NLP and (iii) take the first
		  steps into the direction of valuable representation of
		  semantics within KGs for comparison.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {70–74},
  numpages	= {5},
  keywords	= {Concept Shift, Knowledge Graph Embeddings, NLP,
		  Semantics},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3404835.3462865,
  author	= {Li, Junyi and Zhao, Wayne Xin and Wei, Zhicheng and Yuan,
		  Nicholas Jing and Wen, Ji-Rong},
  title		= {Knowledge-based Review Generation by Coherence Enhanced
		  Text Planning},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3462865},
  doi		= {10.1145/3404835.3462865},
  abstract	= {As a natural language generation task, it is challenging
		  to generate informative and coherent review text. In order
		  to enhance the informativeness of the generated text,
		  existing solutions typically learn to copy entities or
		  triples from knowledge graphs (KGs). However, they lack
		  overall consideration to select and arrange the
		  incorporated knowledge, which tends to cause text
		  incoherence. To address the above issue, we focus on
		  improving entity-centric coherence of the generated reviews
		  by leveraging the semantic structure of KGs. In this paper,
		  we propose a novel Coherence Enhanced Text Planning model
		  (CETP) based on knowledge graphs (KGs) to improve both
		  global and local coherence for review generation. The
		  proposed model learns a two-level text plan for generating
		  a document: (1) the document plan is modeled as a sequence
		  of sentence plans in order, and (2) the sentence plan is
		  modeled as an entity-based subgraph from KG. Local
		  coherence can be naturally enforced by KG subgraphs through
		  intra-sentence correlations between entities. For global
		  coherence, we design a hierarchical self-attentive
		  architecture with both subgraph- and node-level attention
		  to enhance the correlations between subgraphs. To our
		  knowledge, we are the first to utilize a KG-based text
		  planning model to enhance text coherence for review
		  generation. Extensive experiments on three datasets confirm
		  the effectiveness of our model on improving the content
		  coherence of generated texts.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {183–192},
  numpages	= {10},
  keywords	= {knowledge graph, review generation, text planning},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3603719.3603736,
  author	= {Xie, Bingbing and Ma, Xiaoxiao and Wu, Jia and Yang, Jian
		  and Xue, Shan and Fan, Hao},
  title		= {Heterogeneous Graph Neural Network via Knowledge Relations
		  for Fake News Detection},
  year		= {2023},
  isbn		= {9798400707469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603719.3603736},
  doi		= {10.1145/3603719.3603736},
  abstract	= {The proliferation of fake news in social media has been
		  recognized as a severe problem for society, and substantial
		  attempts have been devoted to fake news detection to
		  alleviate the detrimental impacts. Knowledge graphs (KGs)
		  comprise rich factual relations among real entities, which
		  could be utilized as ground-truth databases and enhance
		  fake news detection. However, most of the existing methods
		  only leveraged natural language processing and graph mining
		  techniques to extract features of fake news for detection
		  and rarely explored the ground knowledge in knowledge
		  graphs. In this work, we propose a novel Heterogeneous
		  Graph Neural Network via Knowledge Relations for Fake News
		  Detection (HGNNR4FD). The devised framework has four major
		  components: 1) A heterogeneous graph (HG) built upon news
		  content, including three types of nodes, i.e., news,
		  entities, and topics, and their relations. 2) A KG that
		  provides the factual basis for detecting fake news by
		  generating embeddings via relations in the KG. 3) A novel
		  attention-based heterogeneous graph neural network that can
		  aggregate information from HG and KG, and 4) a fake news
		  detector, which is capable of identifying fake news based
		  on the news embeddings generated by HGNNR4FD. We further
		  validate the performance of our method by comparison with
		  seven state-of-art baselines and verify the effectiveness
		  of the components through a thorough ablation analysis.
		  From the results, we empirically demonstrate that our
		  framework achieves superior results and yields improvement
		  over the baselines regarding evaluation metrics of
		  accuracy, precision, recall, and F1-score on four
		  real-world datasets.},
  booktitle	= {Proceedings of the 35th International Conference on
		  Scientific and Statistical Database Management},
  articleno	= {15},
  numpages	= {11},
  keywords	= {Anomaly detection, Fake news detection, Graph mining,
		  Knowledge graph},
  location	= {Los Angeles, CA, USA},
  series	= {SSDBM '23}
}

@Article{	  10.1145/3505138,
  author	= {Zhang, Peng and Hui, Wenjie and Wang, Benyou and Zhao,
		  Donghao and Song, Dawei and Lioma, Christina and Simonsen,
		  Jakob Grue},
  title		= {Complex-valued Neural Network-based Quantum Language
		  Models},
  year		= {2022},
  issue_date	= {October 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3505138},
  doi		= {10.1145/3505138},
  abstract	= {Language modeling is essential in Natural Language
		  Processing and Information Retrieval related tasks. After
		  the statistical language models, Quantum Language Model
		  (QLM) has been proposed to unify both single words and
		  compound terms in the same probability space without
		  extending term space exponentially. Although QLM achieved
		  good performance in ad hoc retrieval, it still has two
		  major limitations: (1) QLM cannot make use of supervised
		  information, mainly due to the iterative and
		  non-differentiable estimation of the density matrix, which
		  represents both queries and documents in QLM. (2) QLM
		  assumes the exchangeability of words or word dependencies,
		  neglecting the order or position information of words.This
		  article aims to generalize QLM and make it applicable to
		  more complicated matching tasks (e.g., Question Answering)
		  beyond ad hoc retrieval. We propose a complex-valued neural
		  network-based QLM solution called C-NNQLM to employ an
		  end-to-end approach to build and train density matrices in
		  a light-weight and differentiable manner, and it can
		  therefore make use of external well-trained word vectors
		  and supervised labels. Furthermore, C-NNQLM adopts
		  complex-valued word vectors whose phase vectors can
		  directly encode the order (or position) information of
		  words. Note that complex numbers are also essential in the
		  quantum theory. We show that the real-valued NNQLM
		  (R-NNQLM) is a special case of C-NNQLM.The experimental
		  results on the QA task show that both R-NNQLM and C-NNQLM
		  achieve much better performance than the vanilla QLM, and
		  C-NNQLM’s performance is on par with state-of-the-art
		  neural network models. We also evaluate the proposed
		  C-NNQLM on text classification and document retrieval
		  tasks. The results on most datasets show that the C-NNQLM
		  can outperform R-NNQLM, which demonstrates the usefulness
		  of the complex representation for words and sentences in
		  C-NNQLM.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= mar,
  articleno	= {84},
  numpages	= {31},
  keywords	= {Quantum theory, language model, question answering, neural
		  network}
}

@InProceedings{	  10.1145/3583780.3615484,
  author	= {Tu, Shangqing and Zhang, Zheyuan and Yu, Jifan and Li,
		  Chunyang and Zhang, Siyu and Yao, Zijun and Hou, Lei and
		  Li, Juanzi},
  title		= {LittleMu: Deploying an Online Virtual Teaching Assistant
		  via Heterogeneous Sources Integration and Chain of Teach
		  Prompts},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615484},
  doi		= {10.1145/3583780.3615484},
  abstract	= {Teaching assistants have played essential roles in the
		  long history of education. However, few MOOC platforms are
		  providing human or virtual teaching assistants to support
		  learning for massive online students due to the complexity
		  of real-world online education scenarios and the lack of
		  training data. In this paper, we present a virtual MOOC
		  teaching assistant, LittleMu with minimum labeled training
		  data, to provide question answering and chit-chat services.
		  Consisting of two interactive modules of heterogeneous
		  retrieval and language model prompting, LittleMu first
		  integrates structural, semi- and unstructured knowledge
		  sources to support accurate answers for a wide range of
		  questions. Then, we design delicate demonstrations named
		  "Chain of Teach" prompts to exploit the large-scale
		  pre-trained model to handle complex uncollected questions.
		  Except for question answering, we develop other educational
		  services such as knowledge-grounded chit-chat. We test the
		  system's performance via both offline evaluation and online
		  deployment. Since May 2020, our LittleMu system has served
		  over 80,000 users with over 300,000 queries from over 500
		  courses on XuetangX MOOC platform, which continuously
		  contributes to a more convenient and fair education. Our
		  code, services, and dataset will be available at
		  https://github.com/THU-KEG/VTA.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4843–4849},
  numpages	= {7},
  keywords	= {dialogue system, educational support, language model
		  prompts, virtual teaching assistant},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3477495.3531661,
  author	= {Ge, Congcong and Zeng, Xiaocan and Chen, Lu and Gao,
		  Yunjun},
  title		= {ZeroMatcher: A Cost-Off Entity Matching System},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531661},
  doi		= {10.1145/3477495.3531661},
  abstract	= {Entity Matching (EM) aims to find data instances from
		  different sources that refer to the same real-world entity.
		  The existing EM techniques can be either costly or tailored
		  for a specific data type. We present ZeroMatcher, a
		  cost-off entity matching system, which supports (i)
		  handling EM tasks with different data types, including
		  relational tables and knowledge graphs; (ii) keeping its EM
		  performance always competitive by enabling the sub-modules
		  to be updated in a lightweight manner, thus reducing
		  development costs; and (iii) performing EM without human
		  annotations to further slash the labor costs. First,
		  ZeroMatcher automatically suggests users a set of
		  appropriate modules for EM according to the data types of
		  the input datasets. Users could specify the modules for the
		  subsequent EM process according to their preferences.
		  Alternatively, users are able to customize the modules of
		  ZeroMatcher. Then, the system proceeds to the EM task,
		  where users can track the entire EM process and monitor the
		  memory usage changes in real-time. When the EM process is
		  completed, ZeroMatcher visualizes the EM results from
		  different aspects to ease the understanding for users.
		  Finally, ZeroMatcher provides EM results evaluation,
		  enabling users to compare the effectiveness among different
		  parameter settings.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3262–3266},
  numpages	= {5},
  keywords	= {entity matching, knowledge graph, relational table},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3534678.3539382,
  author	= {Wang, Xiaolei and Zhou, Kun and Wen, Ji-Rong and Zhao,
		  Wayne Xin},
  title		= {Towards Unified Conversational Recommender Systems via
		  Knowledge-Enhanced Prompt Learning},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539382},
  doi		= {10.1145/3534678.3539382},
  abstract	= {Conversational recommender systems (CRS) aim to
		  proactively elicit user preference and recommend
		  high-quality items through natural language conversations.
		  Typically, a CRS consists of a recommendation module to
		  predict preferred items for users and a conversation module
		  to generate appropriate responses. To develop an effective
		  CRS, it is essential to seamlessly integrate the two
		  modules. Existing works either design semantic alignment
		  strategies, or share knowledge resources and
		  representations between the two modules. However, these
		  approaches still rely on different architectures or
		  techniques to develop the two modules, making it difficult
		  for effective module integration. To address this problem,
		  we propose a unified CRS model named UniCRS based on
		  knowledge-enhanced prompt learning. Our approach unifies
		  the recommendation and conversation subtasks into the
		  prompt learning paradigm, and utilizes knowledge-enhanced
		  prompts based on a fixed pre-trained language model (PLM)
		  to fulfill both subtasks in a unified approach. In the
		  prompt design, we include fused knowledge representations,
		  task-specific soft tokens, and the dialogue context, which
		  can provide sufficient contextual information to adapt the
		  PLM for the CRS task. Besides, for the recommendation
		  subtask, we also incorporate the generated response
		  template as an important part of the prompt, to enhance the
		  information interaction between the two subtasks. Extensive
		  experiments on two public CRS datasets have demonstrated
		  the effectiveness of our approach. Our code is publicly
		  available at the link:
		  https://github.com/RUCAIBox/UniCRS.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1929–1937},
  numpages	= {9},
  keywords	= {conversational recommender system, pre-trained language
		  model, prompt learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3488560.3498516,
  author	= {Li, Xiangsheng and Mao, Jiaxin and Ma, Weizhi and Wu,
		  Zhijing and Liu, Yiqun and Zhang, Min and Ma, Shaoping and
		  Wang, Zhaowei and He, Xiuqiang},
  title		= {A Cooperative Neural Information Retrieval Pipeline with
		  Knowledge Enhanced Automatic Query Reformulation},
  year		= {2022},
  isbn		= {9781450391320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3488560.3498516},
  doi		= {10.1145/3488560.3498516},
  abstract	= {This paper presents a neural information retrieval
		  pipeline that integrates cooperative learning of query
		  reformulation and neural retrieval models. Our pipeline
		  first exploits an automatic query reformulator to
		  reformulate the user-issued query and then submits the
		  reformulated query to the neural retrieval model. We
		  simultaneously optimize the quality of reformulated queries
		  and ranking performance with an alternate training strategy
		  where query reformulator and neural retrieval model learn
		  from the feedback of each other. Besides, we incorporate
		  knowledge information into automatic query reformulation.
		  The reformulated queries are further improved and
		  contribute to a better ranking performance of the following
		  neural retrieval model. We study two representative neural
		  retrieval models KNRM and BERT in our pipeline. Experiments
		  on two datasets show that our pipeline consistently
		  improves the retrieval performance of the original neural
		  retrieval models while only increases negligible time on
		  automatic query reformulation.},
  booktitle	= {Proceedings of the Fifteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {553–561},
  numpages	= {9},
  keywords	= {knowledge graph, neural ir, query reformulation},
  location	= {Virtual Event, AZ, USA},
  series	= {WSDM '22}
}

@InProceedings{	  10.1145/3543507.3583317,
  author	= {Liu, Yu and Hua, Wen and Xin, Kexuan and Hosseini, Saeid
		  and Zhou, Xiaofang},
  title		= {TEA: Time-aware Entity Alignment in Knowledge Graphs},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583317},
  doi		= {10.1145/3543507.3583317},
  abstract	= {Entity alignment (EA) aims to identify equivalent entities
		  between knowledge graphs (KGs), which is a key technique to
		  improve the coverage of existing KGs. Current EA models
		  largely ignore the importance of time information contained
		  in KGs and treat relational facts or attribute values of
		  entities as time-invariant. However, real-world entities
		  could evolve over time, making the knowledge of the aligned
		  entities very different in multiple KGs. This may cause
		  incorrect matching between KGs if such entity dynamics is
		  ignored. In this paper, we propose a time-aware entity
		  alignment (TEA) model that discovers the entity evolving
		  behaviour by exploring the time contexts in KGs and
		  aggregates various contextual information to make the
		  alignment decision. In particular, we address two main
		  challenges in the TEA model: 1) How to identify
		  highly-correlated temporal facts; 2) How to capture entity
		  dynamics and incorporate it to learn a more informative
		  entity representation for the alignment task. Experiments
		  on real-world datasets1 verify the superiority of our TEA
		  model over state-of-the-art entity aligners.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {2591–2599},
  numpages	= {9},
  keywords	= {Entity alignment, context evolving, knowledge graph,
		  predicate clustering, time context encoder},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3563657.3595996,
  author	= {Kim, Jeongyeon and Suh, Sangho and Chilton, Lydia B and
		  Xia, Haijun},
  title		= {Metaphorian: Leveraging Large Language Models to Support
		  Extended Metaphor Creation for Science Writing},
  year		= {2023},
  isbn		= {9781450398930},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3563657.3595996},
  doi		= {10.1145/3563657.3595996},
  abstract	= {Science writers commonly use extended metaphors to
		  communicate unfamiliar concepts in a more accessible way to
		  a wider audience. However, creating metaphors for science
		  writing is challenging even for professional writers;
		  according to our formative study (n=6), finding inspiration
		  and extending metaphors with coherent structures were
		  critical yet significantly challenging tasks for them. We
		  contribute Metaphorian, a system that supports science
		  writers with the creation of scientific metaphors by
		  facilitating the search, extension, and iterative revision
		  of metaphors. Metaphorian uses a large language model-based
		  workflow inspired by the heuristic rules revealed from a
		  study with six professional writers. A user study (n=16)
		  revealed that Metaphorian significantly enhances
		  satisfaction, confidence, and inspiration in metaphor
		  writing without decreasing writers’ sense of agency. We
		  discuss design implications for creativity support for
		  figurative writing in science.},
  booktitle	= {Proceedings of the 2023 ACM Designing Interactive Systems
		  Conference},
  pages		= {115–135},
  numpages	= {21},
  keywords	= {Creativity Support Tools, GPT-3, Large Language Model,
		  Metaphors, Science Writing, Writing Support},
  location	= {Pittsburgh, PA, USA},
  series	= {DIS '23}
}

@Article{	  10.1145/3588947,
  author	= {Balsebre, Pasquale and Yao, Dezhong and Cong, Gao and
		  Huang, Weiming and Hai, Zhen},
  title		= {Mining Geospatial Relationships from Text},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {1},
  url		= {https://doi.org/10.1145/3588947},
  doi		= {10.1145/3588947},
  abstract	= {A geospatial Knowledge Graph (KG) is a heterogeneous
		  information network, capable of representing relationships
		  between spatial entities in a machine-interpretable format,
		  and has tremendous applications in logistics and social
		  networks. Existing efforts to build a geospatial KG, have
		  mainly used sparse spatial relationships, e.g., a district
		  located inside a city, which provide only marginal benefits
		  compared to a traditional database. In spite of the
		  substantial advances in the tasks of link prediction and
		  knowledge graph completion, identifying geospatial
		  relationships remains challenging, particularly due to the
		  fact that spatial entities are represented with
		  single-point geometries, and textual attributes are
		  frequently missing. In this study, we present GTMiner, a
		  novel framework capable of jointly modeling Geospatial and
		  Textual information to construct a knowledge graph, by
		  mining three useful spatial relationships from a geospatial
		  database, in an end-to-end fashion. The system is divided
		  into three components: (1) a Candidate Selection module, to
		  efficiently select a small number of candidate pairs; (2) a
		  Relation Prediction component to predict spatial
		  relationships between the entities; (3) a KG Refinement
		  procedure, to improve both coverage and correctness of a
		  geospatial knowledge graph. We carry out experiments on
		  four cities' geospatial databases, from publicly-available
		  sources and compare with existing algorithms for link
		  prediction and geospatial data integration. Finally, we
		  conduct an ablation study to motivate our design choices
		  and an efficiency analysis to show that the time required
		  by GTMiner for training and inference is comparable, or
		  even shorter, than existing solutions.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {93},
  numpages	= {26},
  keywords	= {area, geokg, geometry, geospatial, graph, information,
		  interest, kg, knowledge, language, learning, model, of,
		  ontology, poi, point, pois, prediction, relation,
		  relationship, relationships, representation, spatial,
		  text}
}

@InProceedings{	  10.1145/3539597.3573038,
  author	= {Yong, Shan Jie and Dong, Kuicai and Sun, Aixin},
  title		= {DOCoR: Document-level OpenIE with Coreference Resolution},
  year		= {2023},
  isbn		= {9781450394079},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539597.3573038},
  doi		= {10.1145/3539597.3573038},
  abstract	= {Open Information Extraction (OpenIE) extracts relational
		  fact tuples in the form of &lt;subject, relation,
		  object&gt; from text. Most existing OpenIE solutions
		  operate at sentence level and extract relational tuples
		  solely from a sentence. However, many sentences exist as a
		  part of paragraph or a document, where coreferencing is
		  common. In this demonstration, we present a system which
		  refines the semantic tuples generated by OpenIE with the
		  aid of a coreference resolution tool. Specifically, all
		  coreferential mentions across the entire document are
		  identified and grouped into coreferential clusters. Objects
		  and subjects in the extracted tuples from OpenIE which
		  match any coreferential mentions are then resolved with a
		  suitable representative term. In this way, our system is
		  able to resolve both anaphoric and cataphoric references,
		  to achieve Document-level OpenIE with Coreference
		  Resolution (DOCoR). The demonstration video can be viewed
		  at https://youtu.be/o9ZSWCBvlDs},
  booktitle	= {Proceedings of the Sixteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1204–1207},
  numpages	= {4},
  keywords	= {coreference resolution, knowledge graph construction},
  location	= {Singapore, Singapore},
  series	= {WSDM '23}
}

@InProceedings{	  10.1145/3543873.3587318,
  author	= {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
  title		= {OntoEval: an Automated Ontology Evaluation System},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587318},
  doi		= {10.1145/3543873.3587318},
  abstract	= {Developing semantically-aware web services requires
		  comprehensive and accurate ontologies. Evaluating an
		  existing ontology or adapting it is a labor-intensive and
		  complex task for which no automated tools exist.
		  Nevertheless, in this paper we propose a tool that aims at
		  making this vision come true, i.e., we present a tool for
		  the automated evaluation of ontologies that allows one to
		  rapidly assess an ontology’s coverage of a domain and
		  identify specific problems in the ontology’s structure.
		  The tool evaluates the domain coverage and correctness of
		  parent-child relations of a given ontology based on domain
		  information derived from a text corpus representing the
		  domain. The tool provides both overall statistics and
		  detailed analysis of sub-graphs of the ontology. In the
		  demo, we show how these features can be used for the
		  iterative improvement of an ontology.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {82–85},
  numpages	= {4},
  keywords	= {BERT, knowledge engineering, natural language processing,
		  ontology},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3579051.3579053,
  author	= {Chen, Zhuo and Huang, Yufeng and Chen, Jiaoyan and Geng,
		  Yuxia and Fang, Yin and Pan, Jeff Z. and Zhang, Ningyu and
		  Zhang, Wen},
  title		= {LaKo: Knowledge-driven Visual Question Answering via Late
		  Knowledge-to-Text Injection},
  year		= {2023},
  isbn		= {9781450399876},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579051.3579053},
  doi		= {10.1145/3579051.3579053},
  abstract	= {Visual question answering (VQA) often requires an
		  understanding of visual concepts and language semantics,
		  which relies on external knowledge. Most existing methods
		  exploit pre-trained language models or/and unstructured
		  text, but the knowledge in these resources are often
		  incomplete and noisy. Some other methods prefer to use
		  knowledge graphs (KGs) which often have intensive
		  structured knowledge, but the research is still quite
		  preliminary. In this paper, we propose LaKo, a
		  knowledge-driven VQA method via Late Knowledge-to-text
		  Injection. To effectively incorporate an external KG, we
		  transfer triples into textual format and propose a late
		  injection mechanism for knowledge fusion. Finally we
		  address VQA as a text generation task with an effective
		  encoder-decoder paradigm, which achieves state-of-the-art
		  results on OKVQA datasets.},
  booktitle	= {Proceedings of the 11th International Joint Conference on
		  Knowledge Graphs},
  pages		= {20–29},
  numpages	= {10},
  keywords	= {Knowledge Graph, Knowledge-to-Text, Late Knowledge
		  Injection, Visual Question Answering},
  location	= {Hangzhou, China},
  series	= {IJCKG '22}
}

@InProceedings{	  10.1145/3594778.3594877,
  author	= {Chai, Andrew and Vezvaei, Alireza and Golab, Lukasz and
		  Kargar, Mehdi and Srivastava, Divesh and Szlichta, Jaroslaw
		  and Zihayat, Morteza},
  title		= {EAGER: Explainable Question Answering Using Knowledge
		  Graphs},
  year		= {2023},
  isbn		= {9798400702013},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3594778.3594877},
  doi		= {10.1145/3594778.3594877},
  abstract	= {We present EAGER: a tool for answering questions expressed
		  in natural language. Core to EAGER is a modular pipeline
		  for generating a knowledge graph from raw text without
		  human intervention. Notably, EAGER uses the knowledge graph
		  to answer questions and to explain the reasoning behind the
		  derivation of answers. Our demonstration will showcase both
		  the automated knowledge graph generation pipeline and the
		  explainable question answering functionality. Lastly, we
		  outline open problems and directions for future work.},
  booktitle	= {Proceedings of the 6th Joint Workshop on Graph Data
		  Management Experiences &amp; Systems (GRADES) and Network
		  Data Analytics (NDA)},
  articleno	= {4},
  numpages	= {5},
  location	= {Seattle, WA, USA},
  series	= {GRADES-NDA '23}
}

@InProceedings{	  10.1145/3544548.3581260,
  author	= {Zhang, Xiaoyu and Li, Jianping and Chi, Po-Wei and
		  Chandrasegaran, Senthil and Ma, Kwan-Liu},
  title		= {ConceptEVA: Concept-Based Interactive Exploration and
		  Customization of Document Summaries},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3581260},
  doi		= {10.1145/3544548.3581260},
  abstract	= {With the most advanced natural language processing and
		  artificial intelligence approaches, effective summarization
		  of long and multi-topic documents—such as academic
		  papers—for readers from different domains still remains a
		  challenge. To address this, we introduce ConceptEVA, a
		  mixed-initiative approach to generate, evaluate, and
		  customize summaries for long and multi-topic documents.
		  ConceptEVA incorporates a custom multi-task longformer
		  encoder decoder to summarize longer documents. Interactive
		  visualizations of document concepts as a network reflecting
		  both semantic relatedness and co-occurrence help users
		  focus on concepts of interest. The user can select these
		  concepts and automatically update the summary to emphasize
		  them. We present two iterations of ConceptEVA evaluated
		  through an expert review and a within-subjects study. We
		  find that participants’ satisfaction with customized
		  summaries through ConceptEVA is higher than their own
		  manually-generated summary, while incorporating critique
		  into the summaries proved challenging. Based on our
		  findings, we make recommendations for designing
		  summarization systems incorporating mixed-initiative
		  interactions.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {204},
  numpages	= {16},
  keywords	= {Document Summarization, Interactive Visual Analytics,
		  Knowledge Graph, Mixed-Initiative Interfaces},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@InProceedings{	  10.1145/3583780.3615126,
  author	= {Dong, Hang and Chen, Jiaoyan and He, Yuan and Horrocks,
		  Ian},
  title		= {Ontology Enrichment from Texts: A Biomedical Dataset for
		  Concept Discovery and Placement},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615126},
  doi		= {10.1145/3583780.3615126},
  abstract	= {Mentions of new concepts appear regularly in texts and
		  require automated approaches to harvest and place them into
		  Knowledge Bases (KB), e.g., ontologies and taxonomies.
		  Existing datasets suffer from three issues, (i) mostly
		  assuming that a new concept is pre-discovered and cannot
		  support out-of-KB mention discovery; (ii) only using the
		  concept label as the input along with the KB and thus
		  lacking the contexts of a concept label; and (iii) mostly
		  focusing on concept placement w.r.t a taxonomy of atomic
		  concepts, instead of complex concepts, i.e., with logical
		  operators. To address these issues, we propose a new
		  benchmark, adapting MedMentions dataset (PubMed abstracts)
		  with SNOMED CT versions in 2014 and 2017 under the Diseases
		  sub-category and the broader categories of Clinical
		  finding, Procedure, and Pharmaceutical / biologic product.
		  We provide usage on the evaluation with the dataset for
		  out-of-KB mention discovery and concept placement, adapting
		  recent Large Language Model based methods.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5316–5320},
  numpages	= {5},
  keywords	= {SNOMED CT, biomedical ontologies, concept placement,
		  entity linking, language models, ontology enrichment, text
		  mining},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3539618.3591973,
  author	= {Feng, Jiazhan and Tao, Chongyang and Shen, Tao and Liu,
		  Chang and Zhao, Dongyan},
  title		= {Dimension-Prompts Boost Commonsense Consolidation},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591973},
  doi		= {10.1145/3539618.3591973},
  abstract	= {Neural knowledge models emerged and advanced
		  common-sense-centric knowledge grounding. They parameterize
		  a small seed curated commonsense knowledge graph (CS-KG) in
		  a language model to generalize more. A current trend is to
		  scale the seed up by directly mixing multiple sources of
		  CS-KG (e.g., ATOMIC, ConceptNet) into one model. But, such
		  brute-force mixing inevitably hinders effective knowledge
		  consolidation due to i) ambiguous, polysemic, and/or
		  inconsistent relations across sources and ii) knowledge
		  learned in an entangled manner despite distinct types
		  (e.g., causal, temporal). To mitigate this, we adopt a
		  concept of commonsense knowledge dimension and propose a
		  brand-new dimension-disentangled knowledge model (D2KM)
		  learning paradigm with multiple sources. That is, a
		  generative language model with dimension-specific soft
		  prompts is trained to disentangle knowledge acquisitions
		  along with different dimensions and facilitate potential
		  intra-dimension consolidation across CS-KG sources.
		  Experiments show our knowledge model outperforms its
		  baselines in both standard and zero-shot scenarios.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1934–1938},
  numpages	= {5},
  keywords	= {commonsense knowledge construction, neural knowledge
		  models},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3486001.3486240,
  author	= {Hagen, Morten and Arora, Piyush and Ghosh, Rahul and
		  Thomas, Dawn and Joshi, Salil R},
  title		= {Class-Based Order-Independent Models of Natural Language
		  for Bayesian Auto-Complete Inference},
  year		= {2021},
  isbn		= {9781450385947},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486001.3486240},
  doi		= {10.1145/3486001.3486240},
  abstract	= {We introduce a model for auto-complete of general queries
		  via Bayesian inference. To that end, we address three
		  issues: First, the problem of predicting a word given
		  previous words in a text. Usually, the context words are
		  treated as a directional sequence. In our approach, we
		  introduce a set-based class language model with
		  order-independence, modeling the context words as a set of
		  classes. Second, towards the task of predicting the next
		  word’s class based on the classes of previous words plus
		  an incomplete word prefix, we present a Bayesian framework
		  that incorporates the set-based class language model in
		  conjunction with an ontology. Third, regarding the
		  auto-complete problem, we provide complete query
		  suggestions via abstract class-space search which
		  determines similar historical queries that contain the
		  classes of previous words plus the next word’s predicted
		  class. Subsequently, we apply the model to auto-complete
		  inference in a system setting, in which users can access
		  data via natural language queries.},
  booktitle	= {Proceedings of the First International Conference on AI-ML
		  Systems},
  articleno	= {20},
  numpages	= {7},
  keywords	= {Bayesian inference, Class-based language model,
		  auto-complete, order-independence},
  location	= {Bangalore, India},
  series	= {AIMLSystems '21}
}

@InProceedings{	  10.1145/3535508.3545550,
  author	= {Wei, Anqi and Wang, Liangjiang},
  title		= {Deep sequence representation learning for predicting human
		  proteins with liquid-liquid phase separation propensity and
		  synaptic functions},
  year		= {2022},
  isbn		= {9781450393867},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3535508.3545550},
  doi		= {10.1145/3535508.3545550},
  abstract	= {With advancements in next-generation sequencing
		  techniques, the whole protein sequence repertoire has
		  increased to a great extent. In the meantime, deep learning
		  techniques have promoted the development of computational
		  methods to interpret large-scale proteomic data and
		  facilitate functional studies of proteins. Inferring
		  properties from protein amino acid sequences has been a
		  long-standing problem in Bioinformatics. Extensive studies
		  have successfully applied natural language processing (NLP)
		  techniques for the representation learning of protein
		  sequences. In this paper, we applied the deep sequence
		  model - UDSMProt, to fine-tune and evaluate two protein
		  prediction tasks: (1) predict proteins with liquid-liquid
		  phase separation propensity and (2) predict synaptic
		  proteins. Our results have shown that, without prior domain
		  knowledge and only based on protein sequences, the
		  fine-tuned language models achieved high classification
		  accuracies and outperformed baseline models using
		  compositional k-mer features in both tasks. Hence, it is
		  promising to apply the protein language model to some
		  learning tasks and the fine-tuned models can be used to
		  predict protein candidates for biological studies.},
  booktitle	= {Proceedings of the 13th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  articleno	= {41},
  numpages	= {8},
  keywords	= {liquid-liquid phase separation, protein language model,
		  synaptic proteins},
  location	= {Northbrook, Illinois},
  series	= {BCB '22}
}

@InProceedings{	  10.1145/3474085.3475648,
  author	= {Zhu, Yushan and Zhao, Huaixiao and Zhang, Wen and Ye,
		  Ganqiang and Chen, Hui and Zhang, Ningyu and Chen, Huajun},
  title		= {Knowledge Perceived Multi-modal Pretraining in
		  E-commerce},
  year		= {2021},
  isbn		= {9781450386517},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3474085.3475648},
  doi		= {10.1145/3474085.3475648},
  abstract	= {In this paper, we address multi-modal pretraining of
		  product data in the field of E-commerce. Current
		  multi-modal pretraining methods proposed for image and text
		  modalities lack robustness in the face of modality-missing
		  and modality-noise, which are two pervasive problems of
		  multi-modal product data in real E-commerce scenarios. To
		  this end, we propose a novel method, K3M, which introduces
		  knowledge modality in multi-modal pretraining to correct
		  the noise and supplement the missing of image and text
		  modalities. The modal-encoding layer extracts the features
		  of each modality. The modal-interaction layer is capable of
		  effectively modeling the interaction of multiple
		  modalities, where an initial-interactive feature fusion
		  model is designed to maintain the independence of image
		  modality and text modality, and a structure aggregation
		  module is designed to fuse the information of image, text,
		  and knowledge modalities. We pretrain K3M with three
		  pretraining tasks, including masked object modeling (MOM),
		  masked language modeling (MLM), and link prediction
		  modeling (LPM). Experimental results on a real-world
		  E-commerce dataset and a series of product-based downstream
		  tasks demonstrate that K3M achieves significant
		  improvements in performances than the baseline and
		  state-of-the-art methods when modality-noise or
		  modality-missing exists.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Multimedia},
  pages		= {2744–2752},
  numpages	= {9},
  keywords	= {knowledge graph, modality missing, modality noise,
		  multi-modal pretraining},
  location	= {Virtual Event, China},
  series	= {MM '21}
}

@InProceedings{	  10.1145/3587259.3627560,
  author	= {Schneider, Florian and Dash, Sarthak and Bagchi, Sugato
		  and Mihindukulasooriya, Nandana and Gliozzo, Alfio
		  Massimiliano},
  title		= {NLFOA: Natural Language Focused Ontology Alignment},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627560},
  doi		= {10.1145/3587259.3627560},
  abstract	= {For Ontology Alignment (OA), the task is to align
		  semantically equivalent concepts and relations from
		  different ontologies. This task plays a crucial role in
		  many downstream tasks and applications in academia and
		  industry. Since manually aligning ontologies is inefficient
		  and costly, numerous approaches exist to do this
		  automatically. However, most approaches are tailored to
		  specific domains, are rule-based systems or based on
		  feature engineering, and require external knowledge. The
		  most recent advances in the field of OA rely on the widely
		  proven effectiveness of pre-trained language models to
		  represent the human-generated language that describes the
		  entities in an ontology. However, these approaches
		  additionally require sophisticated algorithms or Graph
		  Neural Networks to exploit an ontology’s graphical
		  structure to achieve state-of-the-art performance. In this
		  work, we present NLFOA, or Natural Language Focused
		  Ontology Alignment, which purely focuses on the natural
		  language contained in ontologies to process the
		  ontology’s semantics as well as graphical structure. An
		  evaluation of our approach on common OA datasets shows
		  superior results when finetuning with only a small number
		  of training samples. Additionally, it demonstrates strong
		  results in a zero-shot setting which could be employed in
		  an active learning setup to reduce human labor when
		  manually aligning ontologies significantly.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {114–121},
  numpages	= {8},
  keywords	= {Ontology Alignment Sentence Transformers Zero-Shot},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3512527.3531361,
  author	= {Wang, Xuan and Chen, Jiajun and Tang, Hao and Zhu,
		  Zhigang},
  title		= {MultiCLU: Multi-stage Context Learning and Utilization for
		  Storefront Accessibility Detection and Evaluation},
  year		= {2022},
  isbn		= {9781450392389},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3512527.3531361},
  doi		= {10.1145/3512527.3531361},
  abstract	= {In this work, a storefront accessibility image dataset is
		  collected from Google street view and is labeled with three
		  main objects for storefront accessibility: doors (for store
		  entrances), doorknobs (for accessing the entrances) and
		  stairs (for leading to the entrances). Then MultiCLU, a new
		  multi-stage context learning and utilization approach, is
		  proposed with the following four stages: Context in
		  Labeling (CIL), Context in Training (CIT), Context in
		  Detection (CID) and Context in Evaluation (CIE). The CIL
		  stage automatically extends the label for each knob to
		  include more local contextual information. In the CIT
		  stage, a deep learning method is used to project the visual
		  information extracted by a Faster R-CNN based object
		  detector to semantic space generated by a Graph
		  Convolutional Network. The CID stage uses the spatial
		  relation reasoning between categories to refine the
		  confidence score. Finally in the CIE stage, a new loose
		  evaluation metric for storefront accessibility, especially
		  for knob category, is proposed to efficiently help BLV
		  users to find estimated knob locations. Our experiment
		  results show that the proposed MultiCLU framework can
		  achieve significantly better performance than the baseline
		  detector using Faster R-CNN, with +13.4% on mAP and +15.8%
		  on recall, respectively. Our new evaluation metric also
		  introduces a new way to evaluate storefront accessibility
		  objects, which could benefit BLV group in real life.},
  booktitle	= {Proceedings of the 2022 International Conference on
		  Multimedia Retrieval},
  pages		= {304–312},
  numpages	= {9},
  keywords	= {context learning, convolutional neural networks, graph
		  convolutional network, knowledge graph, object detection},
  location	= {Newark, NJ, USA},
  series	= {ICMR '22}
}

@Article{	  10.1145/3418598,
  author	= {Li, Yamin and Zhang, Jun and Yang, Zhongliang and Zhang,
		  Ru},
  title		= {Topic-aware Neural Linguistic Steganography Based on
		  Knowledge Graphs},
  year		= {2021},
  issue_date	= {May 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {2},
  issn		= {2691-1922},
  url		= {https://doi.org/10.1145/3418598},
  doi		= {10.1145/3418598},
  abstract	= {The core challenge of steganography is always how to
		  improve the hidden capacity and the concealment. Most
		  current generation-based linguistic steganography methods
		  only consider the probability distribution between text
		  characters, and the emotion and topic of the generated
		  steganographic text are uncontrollable. Especially for long
		  texts, generating several sentences related to a topic and
		  displaying overall coherence and discourse-relatedness can
		  ensure better concealment. In this article, we address the
		  problem of generating coherent multi-sentence texts for
		  better concealment, and a topic-aware neural linguistic
		  steganography method that can generate a steganographic
		  paragraph with a specific topic is present. We achieve a
		  topic-controllable steganographic long text generation by
		  encoding the related entities and their relationships from
		  Knowledge Graphs. Experimental results illustrate that the
		  proposed method can guarantee both the quality of the
		  generated steganographic text and its relevance to a
		  specific topic. The proposed model can be widely used in
		  covert communication, privacy protection, and many other
		  areas of information security.},
  journal	= {ACM/IMS Trans. Data Sci.},
  month		= apr,
  articleno	= {10},
  numpages	= {13},
  keywords	= {Neural networks, linguistic steganography, knowledge
		  graph, topic aware, text generation}
}

@InProceedings{	  10.1145/3472634.3472667,
  author	= {Xu, Hanchen and Chen, Zhenxiang and Wang, Shanshan and
		  Jiang, Xiaoqing},
  title		= {Chinese NER Using ALBERT and Multi-word Information},
  year		= {2021},
  isbn		= {9781450385671},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3472634.3472667},
  doi		= {10.1145/3472634.3472667},
  abstract	= {Recently, many Chinese Named Entity Recognition (NER)
		  problems which utilize the character-based approach have a
		  good performance, the character-based approach becomes the
		  dominant current of the approaches, but there are two
		  issues in the existing models. Firstly, the traditional
		  character vector representation is too single to express
		  the polysemia of characters. Secondly, words contain more
		  information than characters, but the existing models cannot
		  use the word information sufficiently. To address the above
		  issues, the AM-BiLSTM model is provided in this work. With
		  the introduction of ALBERT pre-training language model and
		  Multi-word Information(MWI), the enhanced character
		  embedding is composed. We perform experiments on two
		  datasets and the results demonstrate that the capability of
		  our method is improved outstandingly in comparison with the
		  existing NER models.},
  booktitle	= {Proceedings of the ACM Turing Award Celebration Conference
		  - China},
  pages		= {141–145},
  numpages	= {5},
  keywords	= {ALBERT, BiLSTM, Named Entity Recognition, pre-training
		  language model},
  location	= {Hefei, China},
  series	= {ACM TURC '21}
}

@InProceedings{	  10.1145/3404835.3463100,
  author	= {Huang, Zhiqi and Rahimi, Razieh and Yu, Puxuan and Shang,
		  Jingbo and Allan, James},
  title		= {AutoName: A Corpus-Based Set Naming Framework},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3463100},
  doi		= {10.1145/3404835.3463100},
  abstract	= {We propose AutoName, an unsupervised framework that
		  extracts a name for a set of query entities from a
		  large-scale text corpus. Entity-set naming is useful in
		  many tasks related to natural language processing and
		  information retrieval such as session-based and
		  conversational information seeking. Previous studies mainly
		  extract set names from knowledge bases which provide highly
		  reliable entity relations, but suffer from limited coverage
		  of entities and set names that represent broad semantic
		  classes. To address these problems, AutoName generates
		  hypernym-anchored candidate phrases via probing a
		  pre-trained language model and the entities' context in
		  documents. Phrases are then clustered to identify ones that
		  describe common concepts among query entities. Finally,
		  AutoName ranks refined phrases based on the co-occurrences
		  of their words with query entities and the conceptual
		  integrity of their respective clusters. We built a new
		  benchmark dataset for this task, consisting of 130 entity
		  sets with name labels. Experimental results show that
		  AutoName generates coherent and meaningful set names and
		  significantly outperforms all baselines.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2101–2105},
  numpages	= {5},
  keywords	= {conceptual clustering, entity set naming, language model
		  probing},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3474085.3479220,
  author	= {Anand, Vishal and Ramesh, Raksha and Jin, Boshen and Wang,
		  Ziyin and Lei, Xiaoxiao and Lin, Ching-Yung},
  title		= {MultiModal Language Modelling on Knowledge Graphs for Deep
		  Video Understanding},
  year		= {2021},
  isbn		= {9781450386517},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3474085.3479220},
  doi		= {10.1145/3474085.3479220},
  abstract	= {The natural language processing community has had a major
		  interest in auto-regressive [4, 13] and span-prediction
		  based language models [7] recently, while knowledge graphs
		  are often referenced for common-sense based reasoning and
		  fact-checking models. In this paper, we present an
		  equivalence representation of span-prediction based
		  language models and knowledge-graphs to better leverage
		  recent developments of language modelling for multi-modal
		  problem statements. Our method performed well, especially
		  with sentiment understanding for multi-modal inputs, and
		  discovered potential bias in naturally occurring videos
		  when compared with movie-data interaction-understanding. We
		  also release a dataset of an auto-generated questionnaire
		  with ground-truths consisting of labels spanning across 120
		  relationships, 99 sentiments, and 116 interactions, among
		  other labels for finer-grained analysis of model
		  comparisons in the community.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Multimedia},
  pages		= {4868–4872},
  numpages	= {5},
  keywords	= {intent detection, knowledge graphs, language model, scene
		  description, slot filling, speaker diarization,
		  transformers},
  location	= {Virtual Event, China},
  series	= {MM '21}
}

@InProceedings{	  10.1145/3442381.3449943,
  author	= {Li, Xiangsheng and Mao, Jiaxin and Ma, Weizhi and Liu,
		  Yiqun and Zhang, Min and Ma, Shaoping and Wang, Zhaowei and
		  He, Xiuqiang},
  title		= {Topic-enhanced knowledge-aware retrieval model for diverse
		  relevance estimation},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3449943},
  doi		= {10.1145/3442381.3449943},
  abstract	= {Relevance measures the relation between query and document
		  which contains several different dimensions, e.g., semantic
		  similarity, topical relatedness, cognitive relevance (the
		  relations in the aspect of knowledge), usefulness,
		  timeliness, utility and so on. However, existing retrieval
		  models mainly focus on semantic similarity and cognitive
		  relevance while ignore other possible dimensions to model
		  relevance. Topical relatedness, as an important dimension
		  to measure relevance, is not well studied in existing
		  neural information retrieval. In this paper, we propose a
		  Topic Enhanced Knowledge-aware retrieval Model (TEKM) that
		  jointly learns semantic similarity, knowledge relevance and
		  topical relatedness to estimate relevance between query and
		  document. We first construct a neural topic model to learn
		  topical information and generate topic embeddings of a
		  query. Then we combine the topic embeddings with a
		  knowledge-aware retrieval model to estimate different
		  dimensions of relevance. Specifically, we exploit kernel
		  pooling to soft match topic embeddings with word and entity
		  in a unified embedding space to generate fine-grained
		  topical relatedness. The whole model is trained in an
		  end-to-end manner. Experiments on a large-scale publicly
		  available benchmark dataset show that TEKM outperforms
		  existing retrieval models. Further analysis also shows how
		  topic relatedness is modeled to improve traditional
		  retrieval model with semantic similarity and knowledge
		  relevance.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {756–767},
  numpages	= {12},
  keywords	= {Kernel pooling, Knowledge graph, Neural IR, Neural topic
		  model},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3442442.3452326,
  author	= {Lees, Alyssa and Barbosa, Luciano and Korn, Flip and Souza
		  Silva, Levy de and Wu, You and Yu, Cong},
  title		= {Collocating News Articles with Structured Web Tables✱},
  year		= {2021},
  isbn		= {9781450383134},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442442.3452326},
  doi		= {10.1145/3442442.3452326},
  abstract	= {In today’s news deluge, it can often be overwhelming to
		  understand the significance of a news article or verify the
		  facts within. One approach to address this challenge is to
		  identify relevant data so that crucial statistics or facts
		  can be highlighted for the user to easily digest, and thus
		  improve the user’s comprehension of the news story in a
		  larger context. In this paper, we look toward structured
		  tables on the Web, especially the high quality data tables
		  from Wikipedia, to assist in news understanding.
		  Specifically, we aim to automatically find tables related
		  to a news article. For that, we leverage the content and
		  entities extracted from news articles and their matching
		  tables to fine-tune a Bidirectional Transformers (BERT)
		  model. The resulting model is, therefore, an encoder
		  tailored for article-to-table match. To find the matching
		  tables for a given news article, the fine-tuned BERT model
		  encodes each table in the corpus and the news article into
		  their respective embedding vectors. The tables with the
		  highest cosine similarities to the news article in this new
		  representation space are considered the possible matches.
		  Comprehensive experimental analyses show that the new
		  approach significantly outperforms the baselines over a
		  large, weakly-labeled, dataset obtained from Web click logs
		  as well as a small, crowdsourced, evaluation set.
		  Specifically, our approach achieves near 90% accuracy@5 as
		  opposed to baselines varying between 30% and 64%.},
  booktitle	= {Companion Proceedings of the Web Conference 2021},
  pages		= {393–401},
  numpages	= {9},
  keywords	= {Bidirectional Encoders with Transformers, Encoders,
		  Knowledge Graph, Structured Data, WebTables},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3485447.3511928,
  author	= {Zhou, Yucheng and Geng, Xiubo and Shen, Tao and Long,
		  Guodong and Jiang, Daxin},
  title		= {EventBERT: A Pre-Trained Model for Event Correlation
		  Reasoning},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511928},
  doi		= {10.1145/3485447.3511928},
  abstract	= {Event correlation reasoning infers whether a natural
		  language paragraph containing multiple events conforms to
		  human common sense. For example, “Andrew was very drowsy,
		  so he took a long nap, and now he is very alert” is sound
		  and reasonable. In contrast, “Andrew was very drowsy, so
		  he stayed up a long time, now he is very alert” does not
		  comply with human common sense. Such reasoning capability
		  is essential for many downstream tasks, such as script
		  reasoning, abductive reasoning, narrative incoherence,
		  story cloze test, etc. However, conducting event
		  correlation reasoning is challenging due to a lack of large
		  amounts of diverse event-based knowledge and difficulty in
		  capturing correlation among multiple events. In this paper,
		  we propose EventBERT, a pre-trained model to encapsulate
		  eventuality knowledge from unlabeled text. Specifically, we
		  collect a large volume of training examples by identifying
		  natural language paragraphs that describe multiple
		  correlated events and further extracting event spans in an
		  unsupervised manner. We then propose three novel event- and
		  correlation-based learning objectives to pre-train an event
		  correlation model on our created training corpus.
		  Experimental results show EventBERT outperforms strong
		  baselines on four downstream tasks, and achieves
		  state-of-the-art results on most of them. Moreover, it
		  outperforms existing pre-trained models by a large margin,
		  e.g., 6.5 ∼ 23%, in zero-shot learning of these tasks.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {850–859},
  numpages	= {10},
  keywords	= {Contrastive learning, Event correlation reasoning,
		  Language model, Pre-Training model, Zero-shot learning},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3584371.3612942,
  author	= {Kabir, Anowarul and Moldwin, Asher and Shehu, Amarda},
  title		= {A Comparative Analysis of Transformer-based Protein
		  Language Models for Remote Homology Prediction},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3612942},
  doi		= {10.1145/3584371.3612942},
  abstract	= {Protein language models based on the transformer
		  architecture are increasingly shown to learn rich
		  representations from protein sequences that improve
		  performance on a variety of downstream protein prediction
		  tasks. These tasks encompass a wide range of predictions,
		  including prediction of secondary structure, subcellular
		  localization, evolutionary relationships within protein
		  families, as well as superfamily and family membership.
		  There is recent evidence that such models also implicitly
		  learn structural information. In this paper we put this to
		  the test on a hallmark problem in computational biology,
		  remote homology prediction. We employ a rigorous setting,
		  where, by lowering sequence identity, we clarify whether
		  the problem of remote homology prediction has been solved.
		  Among various interesting findings, we report that current
		  state-of-the-art, large models are still underperforming in
		  the "twilight zone" of very low sequence identity.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {97},
  numpages	= {9},
  keywords	= {remote homology, transformer, large language model},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1145/3485447.3511941,
  author	= {Islam, Sk Mainul and Bhattacharya, Sourangshu},
  title		= {AR-BERT: Aspect-relation enhanced Aspect-level Sentiment
		  Classification with Multi-modal Explanations},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511941},
  doi		= {10.1145/3485447.3511941},
  abstract	= {Aspect level sentiment classification (ALSC) is a
		  difficult problem with state-of-the-art models showing less
		  than 80% macro-F1 score on benchmark datasets. Existing
		  models do not incorporate information on aspect-aspect
		  relations in knowledge graphs (KGs), e.g. DBpedia. Two main
		  challenges stem from inaccurate disambiguation of aspects
		  to KG entities, and the inability to learn aspect
		  representations from the large KGs in joint training with
		  ALSC models. We propose AR-BERT, a novel two-level
		  global-local entity embedding scheme that allows efficient
		  joint training of KG-based aspect embeddings and ALSC
		  models. A novel incorrect disambiguation detection
		  technique addresses the problem of inaccuracy in aspect
		  disambiguation. We also introduce the problem of
		  determining mode significance in multi-modal explanation
		  generation, and propose a two step solution. The proposed
		  methods show a consistent improvement of 2.5 − 4.1
		  percentage points, over the recent BERT-based baselines on
		  benchmark datasets.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {987–998},
  numpages	= {12},
  keywords	= {Explainable Deep Learning, Knowledge Graph Embedding,
		  Sentiment Analysis},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3570991.3571058,
  author	= {Gupta, Akshay and Kumar, Suresh and Kumar P, Sreenivasa},
  title		= {Solving age-word problems using domain ontology and BERT},
  year		= {2023},
  isbn		= {9781450397971},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570991.3571058},
  doi		= {10.1145/3570991.3571058},
  abstract	= {An age word problem (ageWP) typically involves sentences
		  that express relationships between the age of the agents
		  and asks for the age of one of them. Automatically solving
		  ageWPs is a challenging task as we need to tackle temporal
		  relationships between the agent’s ages, frame and solve
		  the equations for the required unknowns. To the best of our
		  knowledge, there exists only one ageWP dataset consisting
		  of just 124 examples. The dataset is too small to employ a
		  learning-based solver, mainly consisting of ageWPs with
		  simple temporal relationships. To address this issue, in
		  our earlier work, we designed a description-logic based
		  ontology (ageWP-ont) for the domain of age word problems
		  and utilized it to automatically generate a large number of
		  ageWPs. Sentences in these ageWPs relate the ages of agents
		  in a temporally complex manner. In this paper, we focus on
		  solving these problems. We analyzed an existing
		  learning-based solver of algebraic word-problems that uses
		  a traditional machine learning approach and found that the
		  solver can be adapted to our domain. But we found that this
		  approach does not seem to perform well, perhaps due to the
		  complex nature of the ageWPs. As we have the ontology of
		  the domain on hand, we propose a new approach of utilizing
		  it in the deep-learning based NLU component of the
		  solution. We annotate parts of the ageWP sentences with
		  class-names from ageWP-ont and train a BERT-based language
		  model (LM) that learns to predict the instances for these
		  classes in the given sentences. An RDF graph is populated
		  with these values and serves as a concrete problem-specific
		  instance of the ontology. The dataset for training the LM
		  is automatically generated with the help of ageWP-ont.
		  Finally, for the actual solving of a given ageWP, we make
		  use of its RDF graph and employ Semantic Web Rule Language
		  (SWRL) rules. We implemented the proposed system and
		  achieved 68.8% accuracy. The work demonstrates that
		  combining deep learning with ontologies can give impressive
		  results.},
  booktitle	= {Proceedings of the 6th Joint International Conference on
		  Data Science &amp; Management of Data (10th ACM IKDD CODS
		  and 28th COMAD)},
  pages		= {95–103},
  numpages	= {9},
  keywords	= {Age-word problem solver, BERT, OWL-DL ontology, SWRL},
  location	= {Mumbai, India},
  series	= {CODS-COMAD '23}
}

@InProceedings{	  10.1145/3543507.3583300,
  author	= {Ko, Yunyong and Ryu, Seongeun and Han, Soeun and Jeon,
		  Youngseung and Kim, Jaehoon and Park, Sohyun and Han,
		  Kyungsik and Tong, Hanghang and Kim, Sang-Wook},
  title		= {KHAN: Knowledge-Aware Hierarchical Attention Networks for
		  Accurate Political Stance Prediction},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583300},
  doi		= {10.1145/3543507.3583300},
  abstract	= {The political stance prediction for news articles has been
		  widely studied to mitigate the echo chamber effect –
		  people fall into their thoughts and reinforce their
		  pre-existing beliefs. The previous works for the political
		  stance problem focus on (1) identifying political factors
		  that could reflect the political stance of a news article
		  and (2) capturing those factors effectively. Despite their
		  empirical successes, they are not sufficiently justified in
		  terms of how effective their identified factors are in the
		  political stance prediction. Motivated by this, in this
		  work, we conduct a user study to investigate important
		  factors in political stance prediction, and observe that
		  the context and tone of a news article (implicit) and
		  external knowledge for real-world entities appearing in the
		  article (explicit) are important in determining its
		  political stance. Based on this observation, we propose a
		  novel knowledge-aware approach to political stance
		  prediction (KHAN), employing (1) hierarchical attention
		  networks (HAN) to learn the relationships among words and
		  sentences in three different levels and (2) knowledge
		  encoding (KE) to incorporate external knowledge for
		  real-world entities into the process of political stance
		  prediction. Also, to take into account the subtle and
		  important difference between opposite political stances, we
		  build two independent political knowledge graphs (KG)
		  (i.e., KG-lib and KG-con) by ourselves and learn to fuse
		  the different political knowledge. Through extensive
		  evaluations on three real-world datasets, we demonstrate
		  the superiority of KHAN in terms of (1) accuracy, (2)
		  efficiency, and (3) effectiveness.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1572–1583},
  numpages	= {12},
  keywords	= {echo chamber effect, hierarchical attention networks,
		  knowledge graph embedding, political stance prediction},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3604915.3608812,
  author	= {Zhao, Zhipeng and Zhou, Kun and Wang, Xiaolei and Zhao,
		  Wayne Xin and Pan, Fan and Cao, Zhao and Wen, Ji-Rong},
  title		= {Alleviating the Long-Tail Problem in Conversational
		  Recommender Systems},
  year		= {2023},
  isbn		= {9798400702419},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604915.3608812},
  doi		= {10.1145/3604915.3608812},
  abstract	= {Conversational recommender systems (CRS) aim to provide
		  the recommendation service via natural language
		  conversations. To develop an effective CRS, high-quality
		  CRS datasets are very crucial. However, existing CRS
		  datasets suffer from the long-tail issue, i.e., a large
		  proportion of items are rarely (or even never) mentioned in
		  the conversations, which are called long-tail items. As a
		  result, the CRSs trained on these datasets tend to
		  recommend frequent items, and the diversity of the
		  recommended items would be largely reduced, making users
		  easier to get bored. To address this issue, this paper
		  presents LOT-CRS, a novel framework that focuses on
		  simulating and utilizing a balanced CRS dataset (i.e.,
		  covering all the items evenly) for improving LOng-Tail
		  recommendation performance of CRSs. In our approach, we
		  design two pre-training tasks to enhance the understanding
		  of simulated conversation for long-tail items, and adopt
		  retrieval-augmented fine-tuning with label smoothness
		  strategy to further improve the recommendation of long-tail
		  items. Extensive experiments on two public CRS datasets
		  have demonstrated the effectiveness and extensibility of
		  our approach, especially on long-tail recommendation. Our
		  code is publicly available at the link:
		  https://github.com/Oran-Ac/LOT-CRS.},
  booktitle	= {Proceedings of the 17th ACM Conference on Recommender
		  Systems},
  pages		= {374–385},
  numpages	= {12},
  keywords	= {Conversational Recommender System, Long-tail Problem,
		  Pre-trained Language Model},
  location	= {Singapore, Singapore},
  series	= {RecSys '23}
}

@Article{	  10.1109/taslp.2023.3316422,
  author	= {Gao, Nan and Wang, Yongjian and Chen, Peng and Tang,
		  Jijun},
  title		= {Boosting Short Text Classification by Solving the OOV
		  Problem},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3316422},
  doi		= {10.1109/TASLP.2023.3316422},
  abstract	= {In the field of natural language processing, text
		  classification has received a lot of attention. Compared
		  with long texts, short texts have fewer words and lack
		  contextual semantic information. Existing approaches enrich
		  short text information by linking the external knowledge
		  graph, but they ignore the out-of-vocabulary (OOV) problem
		  during entity linking, especially when dealing with
		  domain-oriented data, which has some rare words or
		  domain-specific nouns. In this article, to alleviate the
		  OOV problem caused by linking the external knowledge
		  graph(KG), we propose a domain knowledge graph and entity
		  complementation strategy to improve the performance of
		  short text classification. Specifically, the external
		  knowledge graph is used to enrich the information of short
		  texts. The self-build domain knowledge graph is used to
		  solve the problem of entities failing to link to the
		  external knowledge graph. Finally, we conduct experiments
		  on various datasets: 1. a labeled Chinese electronic domain
		  dataset; 2. an open-source dataset to test the performance
		  of our algorithm in different data distribution scenarios.
		  The results demonstrate our dual knowledge graph model
		  outperforms the state-of-the-art short text classification
		  methods, especially when the OOV problem is severe.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= sep,
  pages		= {4014–4024},
  numpages	= {11}
}

@Article{	  10.1145/3580488,
  author	= {Li, Lei and Zhang, Yongfeng and Chen, Li},
  title		= {Personalized Prompt Learning for Explainable
		  Recommendation},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3580488},
  doi		= {10.1145/3580488},
  abstract	= {Providing user-understandable explanations to justify
		  recommendations could help users better understand the
		  recommended items, increase the system’s ease of use, and
		  gain users’ trust. A typical approach to realize it is
		  natural language generation. However, previous works mostly
		  adopt recurrent neural networks to meet the ends, leaving
		  the potentially more effective pre-trained Transformer
		  models under-explored. In fact, user and item IDs, as
		  important identifiers in recommender systems, are
		  inherently in different semantic space as words that
		  pre-trained models were already trained on. Thus, how to
		  effectively fuse IDs into such models becomes a critical
		  issue. Inspired by recent advancement in prompt learning,
		  we come up with two solutions: find alternative words to
		  represent IDs (called discrete prompt learning) and
		  directly input ID vectors to a pre-trained model (termed
		  continuous prompt learning). In the latter case, ID vectors
		  are randomly initialized but the model is trained in
		  advance on large corpora, so they are actually in different
		  learning stages. To bridge the gap, we further propose two
		  training strategies: sequential tuning and recommendation
		  as regularization. Extensive experiments show that our
		  continuous prompt learning approach equipped with the
		  training strategies consistently outperforms strong
		  baselines on three datasets of explainable
		  recommendation.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= mar,
  articleno	= {103},
  numpages	= {26},
  keywords	= {Explainable recommendation, Transformer, pre-trained
		  language model, prompt learning}
}

@InProceedings{	  10.1145/3600100.3623720,
  author	= {He, Fang and Wang, Dan and Sun, Yaojie},
  title		= {Ontology Integration for Building Systems and Energy
		  Storage Systems},
  year		= {2023},
  isbn		= {9798400702303},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600100.3623720},
  doi		= {10.1145/3600100.3623720},
  abstract	= {A building ontology defines the concepts and organization
		  of building data. Such knowledge can be assistance with
		  automatic data access and support data-driven applications
		  in buildings. With technological advances in batteries and
		  energy storage, an increasing number of data-driven
		  building applications now involve both building systems and
		  energy storage systems (ESS), e.g., peak load shaving
		  (PLS). However, existing building ontologies, e.g., Brick,
		  are not designed to include concepts from ESS systems.
		  Given the emergence of building-ESS applications, it has
		  become important to develop ontologies that can cover
		  knowledge about both building and ESS systems. Building
		  systems and ESS systems fall under different industry
		  sectors and there are building ontologies and ESS
		  ontologies that have been developed independently. To
		  maximally reuse existing knowledge, we leverage ontology
		  integration technologies. We present a building-energy
		  storage ontology integration (BESOI) system that can extend
		  a building ontology with appropriate ESS ontologies. Our
		  system handles ambiguity, incoherence, and redundancy
		  problems in ontology integration. We evaluate BESOI on four
		  building-ESS applications by extending Brick, a notable
		  building ontology, with different ESS ontologies. The
		  results show that BESOI can extend the coverage of Brick
		  from 68.09% to 95.74% on the concepts of applications.},
  booktitle	= {Proceedings of the 10th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {212–215},
  numpages	= {4},
  keywords	= {Building Application, Energy Storage System, Metadata,
		  Ontology Integration},
  location	= {Istanbul, Turkey},
  series	= {BuildSys '23}
}

@InProceedings{	  10.1145/3534678.3539258,
  author	= {Liu, Fenglin and Yang, Bang and You, Chenyu and Wu, Xian
		  and Ge, Shen and Woicik, Adelaide and Wang, Sheng},
  title		= {Graph-in-Graph Network for Automatic Gene Ontology
		  Description Generation},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539258},
  doi		= {10.1145/3534678.3539258},
  abstract	= {Gene Ontology (GO) is the primary gene function knowledge
		  base that enables computational tasks in biomedicine. The
		  basic element of GO is a term, which includes a set of
		  genes with the same function. Existing research efforts of
		  GO mainly focus on predicting gene term associations. Other
		  tasks, such as generating descriptions of new terms, are
		  rarely pursued. In this paper, we propose a novel task: GO
		  term description generation. This task aims to
		  automatically generate a sentence that describes the
		  function of a GO term belonging to one of the three
		  categories, i.e., molecular function, biological process,
		  and cellular component. To address this task, we propose a
		  Graph-in-Graph network that can efficiently leverage the
		  structural information of GO. The proposed network
		  introduces a two-layer graph: the first layer is a graph of
		  GO terms where each node is also a graph (gene graph). Such
		  a Graph-in-Graph network can derive the biological
		  functions of GO terms and generate proper descriptions. To
		  validate the effectiveness of the proposed network, we
		  build three large-scale benchmark datasets. By
		  incorporating the proposed Graph-in-Graph network, the
		  performances of seven different sequence-to-sequence models
		  can be substantially boosted across all evaluation metrics,
		  with up to 34.7%, 14.5%, and 39.1% relative improvements in
		  BLEU, ROUGE-L, and METEOR, respectively.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1060–1068},
  numpages	= {9},
  keywords	= {bioinformatics, gene ontology, graph representations,
		  natural language generation, sequence-to-sequence
		  learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3487553.3524250,
  author	= {GUO, Kunpeng and Defretiere, Clement and Diefenbach,
		  Dennis and Gravier, Christophe and Gourru, Antoine},
  title		= {QAnswer: Towards Question Answering Search over Websites},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524250},
  doi		= {10.1145/3487553.3524250},
  abstract	= {Question Answering (QA) is increasingly used by search
		  engines to provide results to their end-users, yet very few
		  websites currently use QA technologies for their search
		  functionality. To illustrate the potential of QA
		  technologies for the website search practitioner, we
		  demonstrate web searches that combine QA over knowledge
		  graphs and QA over free text – each being usually tackled
		  separately. We also discuss the different benefits and
		  drawbacks of both approaches for web site searches. We use
		  the case studies made of websites hosted by the Wikimedia
		  Foundation (namely Wikipedia and Wikidata). Differently
		  from a search engine (e.g. Google, Bing, etc), the data are
		  indexed integrally, i.e. we do not index only a subset, and
		  they are indexed exclusively, i.e. we index only data
		  available on the corresponding website.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {252–255},
  numpages	= {4},
  keywords	= {Knowledge Graph Question Answering, Open Domain Question
		  Answering, Question Answering, Website Search},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3511808.3557319,
  author	= {Lyu, Yuanjie and Zhu, Chen and Xu, Tong and Yin, Zikai and
		  Chen, Enhong},
  title		= {Faithful Abstractive Summarization via Fact-aware
		  Consistency-constrained Transformer},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557319},
  doi		= {10.1145/3511808.3557319},
  abstract	= {Abstractive summarization is a classic task in Natural
		  Language Generation (NLG), which aims to produce a concise
		  summary of the original document. Recently, great efforts
		  have been made on sequence-to-sequence neural networks to
		  generate abstractive sum- maries with a high level of
		  fluency. However, prior arts mainly focus on the
		  optimization of token-level likelihood, while the rich
		  semantic information in documents has been largely ignored.
		  In this way, the summarization results could be vulnerable
		  to hallucinations, i.e., the semantic-level inconsistency
		  between a summary and corresponding original document. To
		  deal with this challenge, in this paper, we propose a novel
		  fact-aware abstractive summarization model, named
		  Entity-Relation Pointer Generator Network (ERPGN).
		  Specially, we attempt to formalize the facts in original
		  document as a factual knowledge graph, and then generate
		  the high-quality summary via directly modeling consistency
		  between summary and the factual knowledge graph. To that
		  end, we first leverage two pointer net- work structures to
		  capture the fact in original documents. Then, to enhance
		  the traditional token-level likelihood loss, we design two
		  extra semantic-level losses to measure the disagreement
		  between a summary and facts from its original document.
		  Extensive experi- ments on public datasets demonstrate that
		  our ERPGN framework could outperform both classic
		  abstractive summarization models and the state-of-the-art
		  fact-aware baseline methods, with significant improvement
		  in terms of faithfulness.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1410–1419},
  numpages	= {10},
  keywords	= {abstractive summarization, factual consistency,
		  transformer},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3543507.3583314,
  author	= {Luo, Yun and Liu, Zihan and Li, Stan Z. and Zhang, Yue},
  title		= {Improving (Dis)agreement Detection with Inductive Social
		  Relation Information From Comment-Reply Interactions},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583314},
  doi		= {10.1145/3543507.3583314},
  abstract	= {(Dis)agreement detection aims to identify the authors’
		  attitudes or positions (agree, disagree, neutral) towards a
		  specific text. It is limited for existing methods merely
		  using textual information for identifying (dis)agreements,
		  especially for cross-domain settings. Social relation
		  information can play an assistant role in the
		  (dis)agreement task besides textual information. We propose
		  a novel method to extract such relation information from
		  (dis)agreement data into an inductive social relation
		  graph, merely using the comment-reply pairs without any
		  additional platform-specific information. The inductive
		  social relation globally considers the historical
		  discussion and the relation between authors. Textual
		  information based on a pre-trained language model and
		  social relation information encoded by pre-trained RGCN are
		  jointly considered for (dis)agreement detection.
		  Experimental results show that our model achieves
		  state-of-the-art performance for both the in-domain and
		  cross-domain tasks on the benchmark – DEBAGREEMENT. We
		  find social relations can boost the performance of the
		  (dis)agreement detection model, especially for the
		  long-token comment-reply pairs, demonstrating the
		  effectiveness of the social relation graph. We also explore
		  the effect of the knowledge graph embedding methods, the
		  information fusing method, and the time interval in
		  constructing the social relation graph, which shows the
		  effectiveness of our model.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1584–1593},
  numpages	= {10},
  keywords	= {Disagreement Detection, Opinion Mining, Social Relation,
		  Stance Detection},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@Article{	  10.1145/3624734,
  author	= {Sun, Jiamou and Xing, Zhenchang and Xia, Xin and Lu,
		  Qinghua and Xu, Xiwei and Zhu, Liming},
  title		= {Aspect-level Information Discrepancies across
		  Heterogeneous Vulnerability Reports: Severity, Types and
		  Detection Methods},
  year		= {2023},
  issue_date	= {February 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {2},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3624734},
  doi		= {10.1145/3624734},
  abstract	= {Vulnerable third-party libraries pose significant threats
		  to software applications that reuse these libraries. At an
		  industry scale of reuse, manual analysis of third-party
		  library vulnerabilities can be easily overwhelmed by the
		  sheer number of vulnerabilities continually collected from
		  diverse sources for thousands of reused libraries. Our
		  study of four large-scale, actively maintained
		  vulnerability databases (NVD, IBM X-Force, ExploitDB, and
		  Openwall) reveals the wide presence of information
		  discrepancies, in terms of seven vulnerability aspects,
		  i.e., product, version, component, vulnerability type, root
		  cause, attack vector, and impact, between the reports for
		  the same vulnerability from heterogeneous sources. It would
		  be beneficial to integrate and cross-validate multi-source
		  vulnerability information, but it demands automatic aspect
		  extraction and aspect discrepancy detection. In this work,
		  we experimented with a wide range of NLP methods to extract
		  named entities (e.g., product) and free-form phrases (e.g.,
		  root cause) from textual vulnerability reports and to
		  detect semantically different aspect mentions between the
		  reports. Our experiments confirm the feasibility of
		  applying NLP methods to automate aspect-level vulnerability
		  analysis and identify the need for domain customization of
		  general NLP methods. Based on our findings, we propose a
		  discrepancy-aware, aspect-level vulnerability knowledge
		  graph and a KG-based web portal that integrates diversified
		  vulnerability key aspect information from heterogeneous
		  vulnerability databases. Our conducted user study proves
		  the usefulness of our web portal. Our study opens the door
		  to new types of vulnerability integration and management,
		  such as vulnerability portraits of a product and
		  explainable prediction of silent vulnerabilities.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= dec,
  articleno	= {49},
  numpages	= {38},
  keywords	= {Vulnerability key aspect, information discrepancy,
		  hetergeneous vulnerability reports}
}

@InProceedings{	  10.1145/3543873.3587361,
  author	= {Choudhury, Sutanay and Agarwal, Khushbu and Ham, Colby and
		  Tamang, Suzanne},
  title		= {MediSage: An AI Assistant for Healthcare via Composition
		  of Neural-Symbolic Reasoning Operators},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587361},
  doi		= {10.1145/3543873.3587361},
  abstract	= {We introduce MediSage, an AI decision support assistant
		  for medical professionals and caregivers that simplifies
		  the way in which they interact with different modalities of
		  electronic health records (EHRs) through a conversational
		  interface. It provides step-by-step reasoning support to an
		  end-user to summarize patient health, predict patient
		  outcomes and provide comprehensive and personalized
		  healthcare recommendations. MediSage provides these
		  reasoning capabilities by using a knowledge graph that
		  combines general purpose clinical knowledge resources with
		  recent-most information from the EHR data. By combining the
		  structured representation of knowledge with the predictive
		  power of neural models trained over both EHR and knowledge
		  graph data, MediSage brings explainability by construction
		  and represents a stepping stone into the future through
		  further integration with biomedical language models.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {258–261},
  numpages	= {4},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3578741.3578781,
  author	= {Song, Yu and Zhang, Wenxuan and Ye, Yajuan and Zhang,
		  Chenghao and Zhang, Kunli},
  title		= {Knowledge-Enhanced Relation Extraction in Chinese EMRs},
  year		= {2023},
  isbn		= {9781450399067},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3578741.3578781},
  doi		= {10.1145/3578741.3578781},
  abstract	= {Electronic Medical Records (EMRs) is one of the important
		  data sources of clinical information. Relation extraction
		  is a key step to extract rich medical knowledge from EMRs,
		  which has been studied by many scholars. However, there are
		  some problems in EMRs corpus, such as entity nesting and
		  relation overlapping, which make it difficult to achieve
		  ideal results in EMRs relation extraction task. Previous
		  studies rarely considered the fusion of knowledge graph
		  containing rich and valuable structured knowledge, which
		  leads to semantic ambiguity and other issues. Aiming at the
		  above problems, Relation Extraction model based on
		  Knowledge Graph and Chinese character Radical
		  information(RE-KGR) model is proposed in this paper to
		  study the relation extraction of EMRs in diabetic patients.
		  Firstly, knowledge information is extracted from knowledge
		  graph and embedded by GCN. At the same time, the
		  corresponding radical features of Chinese characters are
		  fused to enhance the semantic information of the input
		  text. Compared with other baseline models, the DEMRC and
		  DiaKG experiments of EMRs datasets of diabetic patients
		  were improved by 1.32% and 2.19%.},
  booktitle	= {Proceedings of the 2022 5th International Conference on
		  Machine Learning and Natural Language Processing},
  pages		= {196–201},
  numpages	= {6},
  keywords	= {Electronic medical records, Integrating knowledge,
		  Relation extraction},
  location	= {Sanya, China},
  series	= {MLNLP '22}
}

@InProceedings{	  10.1145/3584376.3584482,
  author	= {Zhao, Jingsheng and Cui, Mingyu and Gao, Xiang and Yan,
		  Shuai and Ni, Qihui},
  title		= {Chinese Named Entity Recognition Based on BERT and Lexicon
		  Enhancement},
  year		= {2023},
  isbn		= {9781450398343},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584376.3584482},
  doi		= {10.1145/3584376.3584482},
  abstract	= {Named entity recognition is an important part of
		  information extraction and knowledge graph construction,
		  and is the basic work of natural language processing.
		  Chinese named entity recognition mainly adopts word-based
		  and character-based methods, word-based methods rely on
		  word segmentation and common word segmentation methods have
		  word segmentation errors, which easily cause error
		  propagation, character-based methods avoid this error but
		  do not make full use of lexicon information. The
		  performance of Chinese named entity recognition can be
		  effectively improved by introducing lexicon information
		  into character-based named entity recognition. In this
		  paper, we propose a BERT-IDCNN-CRF model combined with the
		  SoftLexicon method. First, the BERT pre-training language
		  model is used to train the character embedding vector, and
		  the lexicon information is obtained by the SoftLexicon
		  method. Then, the lexicon information is combined with the
		  character vector representation obtained by training. Next,
		  the fused vector representation is input to the IDCNN model
		  for further training. Finally, the recognition results of
		  Chinese named entities are obtained by the CRF model. The
		  experimental results show that the F1 value can reach
		  95.95%, 70.63% and 95.28% on Resume, Weibo and MSRA
		  datasets, and the training speed is faster than
		  BERT-BiLSTM-CRF.},
  booktitle	= {Proceedings of the 2022 4th International Conference on
		  Robotics, Intelligent Control and Artificial Intelligence},
  pages		= {597–604},
  numpages	= {8},
  location	= {Dongguan, China},
  series	= {RICAI '22}
}

@Article{	  10.1145/3582262,
  author	= {Touma, Roudy and Hajj, Hazem and El-Hajj, Wassim and
		  Shaban, Khaled},
  title		= {Automated Generation of Human-readable Natural Arabic Text
		  from RDF Data},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3582262},
  doi		= {10.1145/3582262},
  abstract	= {With the advances in Natural Language Processing (NLP),
		  the industry has been moving towards human-directed
		  artificial intelligence (AI) solutions. Recently, chatbots
		  and automated news generation have captured a lot of
		  attention. The goal is to automatically generate readable
		  text from tabular data or web data commonly represented in
		  Resource Description Framework (RDF) format. The problem
		  can then be formulated as Data-to-text (D2T) generation
		  from structured non-linguistic data into human-readable
		  natural language. Despite the significant work done for the
		  English language, no efforts are being directed towards
		  low-resource languages like the Arabic language. This work
		  promotes the development of the first RDF data-to-text
		  (D2T) generation system for the Arabic language while
		  trying to address the low-resource limitation. We develop
		  several models for the Arabic D2T task using transfer
		  learning from large language models (LLM) such as AraBERT,
		  AraGPT2, and mT5. These models include a baseline Bi-LSTM
		  Sequence-to-Sequence (Seq2Seq) model, as well as
		  encoder-decoder transformers like BERT2BERT, BERT2GPT, and
		  T5. We then provide a detailed comparative study
		  highlighting the strengths and limitations of these methods
		  setting the stage for further advancement in the field. We
		  also introduce a new Arabic dataset (AraWebNLG) that can be
		  used for new model development in the field. To ensure a
		  comprehensive evaluation, general-purpose automated metrics
		  (BLEU and Perplexity scores) are used as well as
		  task-specific human evaluation metrics related to the
		  accuracy of the content selection and fluency of the
		  generated text. The results highlight the importance of
		  pre-training on a large corpus of Arabic data and show that
		  transfer learning from AraBERT gives the best performance.
		  Text-to-text pre-training using mT5 achieves second best
		  performance results even with multilingual weights.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= mar,
  articleno	= {98},
  numpages	= {13},
  keywords	= {Low-resource languages, data-to-text, RDF, language
		  models, neural networks, datasets}
}

@InProceedings{	  10.1145/3477495.3531997,
  author	= {Dong, Qian and Liu, Yiding and Cheng, Suqi and Wang,
		  Shuaiqiang and Cheng, Zhicong and Niu, Shuzi and Yin,
		  Dawei},
  title		= {Incorporating Explicit Knowledge in Pre-trained Language
		  Models for Passage Re-ranking},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531997},
  doi		= {10.1145/3477495.3531997},
  abstract	= {Passage re-ranking is to obtain a permutation over the
		  candidate passage set from retrieval stage. Re-rankers have
		  been boomed by Pre-trained Language Models (PLMs) due to
		  their overwhelming advantages in natural language
		  understanding. However, existing PLM based re-rankers may
		  easily suffer from vocabulary mismatch and lack of domain
		  specific knowledge. To alleviate these problems, explicit
		  knowledge contained in knowledge graph is carefully
		  introduced in our work. Specifically, we employ the
		  existing knowledge graph which is incomplete and noisy, and
		  first apply it in passage re-ranking task. To leverage a
		  reliable knowledge, we propose a novel knowledge graph
		  distillation method and obtain a knowledge meta graph as
		  the bridge between query and passage. To align both kinds
		  of embedding in the latent space, we employ PLM as text
		  encoder and graph neural network over knowledge meta graph
		  as knowledge encoder. Besides, a novel knowledge injector
		  is designed for the dynamic interaction between text and
		  knowledge encoder. Experimental results demonstrate the
		  effectiveness of our method especially in queries requiring
		  in-depth domain knowledge.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1490–1501},
  numpages	= {12},
  keywords	= {language models, learning to rank, semantic matching},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3511808.3557219,
  author	= {Tong, Hanwen and Xie, Chenhao and Liang, Jiaqing and He,
		  Qianyu and Yue, Zhiang and Liu, Jingping and Xiao, Yanghua
		  and Wang, Wenguang},
  title		= {A Context-Enhanced Generate-then-Evaluate Framework for
		  Chinese Abbreviation Prediction},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557219},
  doi		= {10.1145/3511808.3557219},
  abstract	= {As a popular form of lexicalization, abbreviation is
		  widely used in both oral and written language and plays an
		  important role in various Natural Language Processing
		  applications. However, current approaches cannot ensure
		  that the predicted abbreviation preserves the meaning of
		  its full form and maintains fluency. In this paper, we
		  introduce a fresh perspective to evaluate the quality of
		  abbreviations within their textual contexts with
		  pre-trained language model. To this end, we propose a novel
		  two-stage generate-then-evaluate framework enhanced by
		  context, which consists of a generation model to generate
		  multiple candidate abbreviations and an evaluation model to
		  evaluate their quality within their contexts. Experimental
		  results show that our framework consistently outperforms
		  all the existing approaches, achieving 53.2% Hit@1
		  performance with a 5.6 points improvement compared to its
		  previous best result. Our code and data are publicly
		  available at https://github.com/HavenTong/CEGE.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1945–1954},
  numpages	= {10},
  keywords	= {chinese abbreviation prediction, context-enhanced
		  framework, generate-then-evaluate framework},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@Article{	  10.1145/3512467,
  author	= {Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu,
		  Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  title		= {A Survey of Knowledge-enhanced Text Generation},
  year		= {2022},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {11s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3512467},
  doi		= {10.1145/3512467},
  abstract	= {The goal of text-to-text generation is to make machines
		  express like a human in many applications such as
		  conversation, summarization, and translation. It is one of
		  the most important yet challenging tasks in natural
		  language processing (NLP). Various neural encoder-decoder
		  models have been proposed to achieve the goal by learning
		  to map input text to output text. However, the input text
		  alone often provides limited knowledge to generate the
		  desired output, so the performance of text generation is
		  still far from satisfaction in many real-world scenarios.
		  To address this issue, researchers have considered
		  incorporating (i) internal knowledge embedded in the input
		  text and (ii) external knowledge from outside sources such
		  as knowledge base and knowledge graph into the text
		  generation system. This research topic is known as
		  knowledge-enhanced text generation. In this survey, we
		  present a comprehensive review of the research on this
		  topic over the past five years. The main content includes
		  two parts: (i) general methods and architectures for
		  integrating knowledge into text generation; (ii) specific
		  techniques and applications according to different forms of
		  knowledge data. This survey can have broad audiences,
		  researchers and practitioners, in academia and industry.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {227},
  numpages	= {38},
  keywords	= {Natural language generation, Knowledge-enhanced Methods}
}

@InProceedings{	  10.1145/3459637.3482007,
  author	= {Wang, Haiwen and Zhou, Le and Zhang, Weinan and Wang,
		  Xinbing},
  title		= {LiteratureQA: A Qestion Answering Corpus with Graph
		  Knowledge on Academic Literature},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482007},
  doi		= {10.1145/3459637.3482007},
  abstract	= {In this paper, we introduce LiteratureQA, a large question
		  answering (QA) corpus consisting of publicly available
		  academic papers. Different from other QA corpus,
		  LiteratureQA has its unique challenges such as how to
		  leverage the structured knowledge of citation networks. We
		  further examine some popular QA method and present a
		  benchmark approach of answering academic questions by
		  combining both semantic text and graph knowledge to improve
		  the prevalent pre-training model. We hope this resource
		  could help research and development of tasks for machine
		  reading over academic text.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4623–4632},
  numpages	= {10},
  keywords	= {academic corpus, academic knowledge graph, academic
		  network, machine reading comprehension, question
		  answering},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.1145/3593804,
  author	= {Huang, Tao and Hu, Shengze and Lin, Keke and Yang, Huali
		  and Zhang, Hao and Song, Houbing and Lv, Zhihan},
  title		= {Sequence Generation Model Integrating Domain Ontology for
		  Mathematical question tagging},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3593804},
  doi		= {10.1145/3593804},
  abstract	= {In online learning systems, tagging knowledge points for
		  questions is a fundamental task. Automatic tagging
		  technology uses intelligent algorithms to automatically tag
		  knowledge points for questions to reduce manpower and time
		  costs. However, the current knowledge point tagging
		  technology cannot satisfy the situation that mathematics
		  questions often involve a variable number of knowledge
		  points, lacks the consideration of the characteristics of
		  the mathematics field, and ignores the internal connection
		  between knowledge points. To address the above issues, we
		  propose a Sequence Generation Model Integrating Domain
		  Ontology for Mathematical question tagging (SOMPT). SOMPT
		  performs data augmentation for text and then obtains
		  intermediate text based on domain ontology replacement to
		  facilitate deep learning model to understand mathematical
		  question text. SOMPT is able to obtain dynamic word vector
		  embedding to optimize the textual representation for math
		  questions. What’s more, our model can capture the
		  relationship between tags to generate knowledge points more
		  accurately in the way of sequence generation. The
		  comparative experimental results show that our proposed
		  model has an excellent tagging ability for mathematical
		  questions. Moreover, the sequence generation module in
		  SOMPT can be applied on other multi-label classification
		  tasks and be on par with the state-of-the-art performance
		  models.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  keywords	= {Mathematical question tagging, Deep learning, Language
		  models, Sequence generation}
}

@InProceedings{	  10.1145/3544549.3583931,
  author	= {Sun, Yuqian and Xu, Ying and Cheng, Chenhang and Li, Yihua
		  and Lee, Chang Hee and Asadipour, Ali},
  title		= {Explore the Future Earth with Wander 2.0: AI Chatbot
		  Driven By Knowledge-base Story Generation and Text-to-image
		  Model},
  year		= {2023},
  isbn		= {9781450394222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544549.3583931},
  doi		= {10.1145/3544549.3583931},
  abstract	= {People always envision the future of earth through science
		  fiction (Sci-fi), so can we create a unique experience of
		  "visiting the future earth" through the lens of artificial
		  intelligence (AI)? We introduce Wander 2.0, an AI chatbot
		  that co-creates sci-fi stories through knowledge-based
		  story generation on daily communication platforms like
		  WeChat and Discord. Using location information from Google
		  Maps, Wander generates narrative travelogues about specific
		  locations (e.g. Paris) through a large-scale language model
		  (LLM). Additionally, using the large-scale text-to-image
		  model (LTGM) Stable Diffusion, Wander transfers future
		  scenes that match both the text description and location
		  photo, facilitating future imagination. The project also
		  includes a real-time visualization of the human-AI
		  collaborations on a future map. Through journeys with
		  visitors from all over the world, Wander demonstrates how
		  AI can serve as a subjective interface linking fiction and
		  reality. Our research shows that multi-modal AI systems
		  have the potential to extend the artistic experience and
		  creative world-building through adaptive and unique content
		  generation for different people. Wander 2.0 is available at
		  http://wander001.com/},
  booktitle	= {Extended Abstracts of the 2023 CHI Conference on Human
		  Factors in Computing Systems},
  articleno	= {450},
  numpages	= {5},
  keywords	= {Artificial intelligence, chatbot, design fiction, gaming,
		  human-AI interaction, interactive fiction},
  location	= {Hamburg, Germany},
  series	= {CHI EA '23}
}

@InProceedings{	  10.1145/3587259.3627574,
  author	= {Ram\'{o}n-Ferrer, Virginia and Badenes-Olmedo, Carlos and
		  Corcho, Oscar},
  title		= {Automatic Topic Label Generation using Conversational
		  Models},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627574},
  doi		= {10.1145/3587259.3627574},
  abstract	= {In probabilistic topic models, a topic is characterised by
		  a set of words, with a probability associated to each of
		  them. Even though it is not necessary to understand the
		  meaning of topics to perform common downstream tasks where
		  topic models are used, such as topic inference or document
		  similarity, there have been attempts to uncover the
		  semantics of topics by providing labels to them, consisting
		  in a couple of concepts. In this paper we propose a
		  methodology, Conversational Probabilistic Topic Labelling
		  (CPTL), to study whether conversational models can be used
		  to generate labels that describe probabilistic topics given
		  their most representative keywords. We evaluate and compare
		  the performance of a selection of conversational models for
		  the topic label generation task with the performance of a
		  task-specific language model trained to generate topic
		  labels.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {17–24},
  numpages	= {8},
  keywords	= {conversational model, probabilistic topic labelling, topic
		  label, topic label generation},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3503161.3548387,
  author	= {Li, Rengang and Xu, Cong and Guo, Zhenhua and Fan, Baoyu
		  and Zhang, Runze and Liu, Wei and Zhao, Yaqian and Gong,
		  Weifeng and Wang, Endong},
  title		= {AI-VQA: Visual Question Answering based on Agent
		  Interaction with Interpretability},
  year		= {2022},
  isbn		= {9781450392037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503161.3548387},
  doi		= {10.1145/3503161.3548387},
  abstract	= {Visual Question Answering (VQA) serves as a proxy for
		  evaluating the scene understanding of an intelligent agent
		  by answering questions about images. Most VQA benchmarks to
		  date are focused on those questions that can be answered
		  through understanding visual content in the scene, such as
		  simple counting, visual attributes, and even a little
		  challenging questions that require extra encyclopedic
		  knowledge. However, humans have a remarkable capacity to
		  reason dynamic interaction on the scene, which is beyond
		  the literal content of an image and has not been
		  investigated so far. In this paper, we propose Agent
		  Interaction Visual Question Answering (AI-VQA), a task
		  investigating deep scene understanding if the agent takes a
		  certain action. For this task, a model not only needs to
		  answer action-related questions but also to locate the
		  objects in which the interaction occurs for guaranteeing it
		  truly comprehends the action. Accordingly, we make a new
		  dataset based on Visual Genome and ATOMIC knowledge graph,
		  including more than 19,000 manually annotated questions,
		  and will make it publicly available. Besides, we also
		  provide an annotation of the reasoning path while
		  developing the answer for each question. Based on the
		  dataset, we further propose a novel method, called ARE,
		  that can comprehend the interaction and explain the reason
		  based on a given event knowledge base. Experimental results
		  show that our proposed method outperforms the baseline by a
		  clear margin.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Multimedia},
  pages		= {5274–5282},
  numpages	= {9},
  keywords	= {dataset, vision and language, visual question answer},
  location	= {Lisboa, Portugal},
  series	= {MM '22}
}

@Article{	  10.1109/taslp.2021.3120636,
  author	= {Zhu, Biru and Zhang, Xingyao and Gu, Ming and Deng,
		  Yangdong},
  title		= {Knowledge Enhanced Fact Checking and Verification},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3120636},
  doi		= {10.1109/TASLP.2021.3120636},
  abstract	= {As the Internet and social media offer increasing
		  opportunities for organizations and individuals to
		  publicize online contents, it has become essential to
		  develop effective means to identify misinformation like
		  fake news. Recently, fact checking systems have been
		  regarded as a promising tool to automatically deal with
		  large amounts of information. How to effectively take
		  advantage of existing unstructured document knowledge bases
		  and structured knowledge graphs to build robust fact
		  checking systems, however, remains to be a challenge. In
		  this paper, we propose a knowledge enhanced fact checking
		  system, which leverages the Wikidata5M knowledge graph and
		  Wikipedia documents to incorporate external knowledge into
		  the claim to be checked for more robust and accurate fact
		  checking. First, we devise a contextualized knowledge graph
		  selection method to identify the most relevant sub-graph
		  with the checked claim from the large knowledge graph. We
		  then construct a novel claim-evidence-knowledge graph and
		  use a graph attention network to integrate natural language
		  evidence with structured knowledge triplets by allowing
		  them to propagate information among each other. By
		  integrating the claim, retrieved evidence and selected
		  knowledge triplets in a unified claim-evidence-knowledge
		  graph, our method improves the label accuracy of predicted
		  claims by more than 4% on the FEVER dataset over
		  state-of-the-art fact checking models.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= oct,
  pages		= {3132–3143},
  numpages	= {12}
}

@InProceedings{	  10.1145/3485447.3511943,
  author	= {Wang, Suyuchen and Zhao, Ruihui and Zheng, Yefeng and Liu,
		  Bang},
  title		= {QEN: Applicable Taxonomy Completion via Evaluating Full
		  Taxonomic Relations},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511943},
  doi		= {10.1145/3485447.3511943},
  abstract	= {Taxonomy is a fundamental type of knowledge graph for a
		  wide range of web applications like searching and
		  recommendation systems. To keep a taxonomy automatically
		  updated with the latest concepts, the taxonomy completion
		  task matches a pair of proper hypernym and hyponym in the
		  original taxonomy with the new concept as its parent and
		  child. Previous solutions utilize term embeddings as input
		  and only evaluate the parent-child relations between the
		  new concept and the hypernym-hyponym pair. Such methods
		  ignore the important sibling relations, and are not
		  applicable in reality since term embeddings are not
		  available for the latest concepts. They also suffer from
		  the relational noise of the “pseudo-leaf” node, which
		  is a null node acting as a node’s hyponym to enable the
		  new concept to be a leaf node. To tackle the above
		  drawbacks, we propose the Quadruple Evaluation Network
		  (QEN), a novel taxonomy completion framework that utilizes
		  easily accessible term descriptions as input, and applies
		  pretrained language model and code attention for accurate
		  inference while reducing online computation. QEN evaluates
		  both parent-child and sibling relations to both enhance the
		  accuracy and reduce the noise brought by pseudo-leaf.
		  Extensive experiments on three real-world datasets in
		  different domains with different sizes and term description
		  sources prove the effectiveness and robustness of QEN on
		  overall performance and especially the performance for
		  adding non-leaf nodes, which largely surpasses previous
		  methods and achieves the new state-of-the-art of the
		  task.1},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {1008–1017},
  numpages	= {10},
  keywords	= {Self-supervised Learning, Taxonomic Relations, Taxonomy
		  Completion},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3477495.3532077,
  author	= {Ren, Zhaochun and Tian, Zhi and Li, Dongdong and Ren,
		  Pengjie and Yang, Liu and Xin, Xin and Liang, Huasheng and
		  de Rijke, Maarten and Chen, Zhumin},
  title		= {Variational Reasoning about User Preferences for
		  Conversational Recommendation},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3532077},
  doi		= {10.1145/3477495.3532077},
  abstract	= {Conversational recommender systems (CRSs) provide
		  recommendations through interactive conversations. CRSs
		  typically provide recommendations through relatively
		  straightforward interactions, where the system continuously
		  inquires about a user's explicit attribute-aware
		  preferences and then decides which items to recommend. In
		  addition, topic tracking is often used to provide naturally
		  sounding responses. However, merely tracking topics is not
		  enough to recognize a user's real preferences in a
		  dialogue.In this paper, we address the problem of
		  accurately recognizing and maintaining user preferences in
		  CRSs. Three challenges come with this problem: (1) An
		  ongoing dialogue only provides the user's short-term
		  feedback; (2) Annotations of user preferences are not
		  available; and (3) There may be complex semantic
		  correlations among items that feature in a dialogue. We
		  tackle these challenges by proposing an end-to-end
		  variational reasoning approach to the task of
		  conversational recommendation. We model both long-term
		  preferences and short-term preferences as latent variables
		  with topical priors for explicit long-term and short-term
		  preference exploration, respectively. We use an efficient
		  stochastic gradient variational Bayesian (SGVB) estimator
		  for optimizing the derived evidence lower bound. A policy
		  network is then used to predict topics for a clarification
		  utterance or items for a recommendation response. The use
		  of explicit sequences of preferences with multi-hop
		  reasoning in a heterogeneous knowledge graph helps to
		  provide more accurate conversational recommendation
		  results.Extensive experiments conducted on two benchmark
		  datasets show that our proposed method outperforms
		  state-of-the-art baselines in terms of both objective and
		  subjective evaluation metric},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {165–175},
  numpages	= {11},
  keywords	= {conversational recommendation, task-oriented dialogue
		  systems, user preference tracking, variational inference},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3511808.3557617,
  author	= {Anelli, Vito Walter and Biancofiore, Giovanni Maria and De
		  Bellis, Alessandro and Di Noia, Tommaso and Di Sciascio,
		  Eugenio},
  title		= {Interpretability of BERT Latent Space through Knowledge
		  Graphs},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557617},
  doi		= {10.1145/3511808.3557617},
  abstract	= {The advent of pretrained language have renovated the ways
		  of handling natural languages, improving the quality of
		  systems that rely on them. BERT played a crucial role in
		  revolutionizing the Natural Language Processing (NLP) area.
		  However, the deep learning framework it implements lacks
		  interpretability. Thus, recent research efforts aimed to
		  explain what BERT learns from the text sources exploited to
		  pre-train its linguistic model. In this paper, we analyze
		  the latent vector space resulting from the BERT
		  context-aware word embeddings. We focus on assessing
		  whether regions of the BERT vector space hold an explicit
		  meaning attributable to a Knowledge Graph (KG). First, we
		  prove the existence of explicitly meaningful areas through
		  the Link Prediction (LP) task. Then, we demonstrate these
		  regions being linked to explicit ontology concepts of a KG
		  by learning classification patterns. To the best of our
		  knowledge, this is the first attempt at interpreting the
		  BERT learned linguistic knowledge through a KG relying on
		  its pretrained context-aware word embeddings.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3806–3810},
  numpages	= {5},
  keywords	= {deep learning, knowledge graphs, natural language
		  processing},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3447548.3467138,
  author	= {Hao, Junheng and Lei, Chuan and Efthymiou, Vasilis and
		  Quamar, Abdul and \"{O}zcan, Fatma and Sun, Yizhou and
		  Wang, Wei},
  title		= {MEDTO: Medical Data to Ontology Matching Using Hybrid
		  Graph Neural Networks},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467138},
  doi		= {10.1145/3447548.3467138},
  abstract	= {Medical ontologies are widely used to describe and
		  organize medical terminologies and to support many critical
		  applications on healthcare databases. These ontologies are
		  often manually curated (e.g., UMLS, SNOMED CT, and MeSH) by
		  medical experts. Medical databases, on the other hand, are
		  often created by database administrators, using different
		  terminology and structures. The discrepancies between
		  medical ontologies and databases compromise
		  interoperability between them. Data to ontology matching is
		  the process of finding semantic correspondences between
		  tables in databases to standard ontologies. Existing
		  solutions such as ontology matching have mostly focused on
		  engineering features from terminological, structural, and
		  semantic model information extracted from the ontologies.
		  However, this is often labor intensive and the accuracy
		  varies greatly across different ontologies. Worse yet, the
		  ontology capturing a medical database is often not given in
		  practice. In this paper, we propose MEDTO, a novel
		  end-to-end framework that consists of three innovative
		  techniques: (1) a lightweight yet effective method that
		  bootstrap a semantically rich ontology from a given medical
		  database, (2) a hyperbolic graph convolution layer that
		  encodes hierarchical concepts in the hyperbolic space, and
		  (3) a heterogeneous graph layer that encodes both local and
		  global context information of a concept. Experiments on two
		  real-world medical datasets matching against SNOMED CT show
		  significant improvements compared to the state-of-the-art
		  methods. MEDTO also consistently achieves competitive
		  results on a benchmark from the Ontology Alignment
		  Evaluation Initiative.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {2946–2954},
  numpages	= {9},
  keywords	= {graph neural network, medical data, ontology matching},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@InProceedings{	  10.1145/3436369.3436390,
  author	= {Wang, Qingchuan and E, Haihong},
  title		= {A BERT-Based Named Entity Recognition in Chinese
		  Electronic Medical Record},
  year		= {2021},
  isbn		= {9781450387835},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3436369.3436390},
  doi		= {10.1145/3436369.3436390},
  abstract	= {Named entity recognition, aiming at identifying and
		  classifying named entity mentioned in the structured or
		  unstructured text, is a fundamental subtask for information
		  extraction in natural language processing (NLP). With the
		  development of electronic medical records, obtaining the
		  key and effective information in electronic document
		  through named entity identification has become an
		  increasingly popular research direction. In this article,
		  we adapt a recently introduced pre-trained language model
		  BERT for named entity recognition in electronic medical
		  records to solve the problem of missing context information
		  and we add an extra mechanism to capture the relationship
		  between words. Based on this, (1) the entities can be
		  represented by sentence-level vector, with the forward as
		  well as backward information of the sentence, which can be
		  directly used by downstream tasks; (2) the model acquires
		  the representation of word in context and learn the
		  potential relation between words to decrease the influence
		  of inconsistent entity markup problem of a text. We conduct
		  experiments an electronic medical record dataset proposed
		  by China Conference on Knowledge Graph and Semantic
		  Computing in 2019. The experimental result shows that our
		  proposed method has an improvement compared with the
		  traditional methods.},
  booktitle	= {Proceedings of the 2020 9th International Conference on
		  Computing and Pattern Recognition},
  pages		= {13–17},
  numpages	= {5},
  keywords	= {BERT, Named entity recognition, attention mechanism,
		  electronic medical records},
  location	= {Xiamen, China},
  series	= {ICCPR '20}
}

@InProceedings{	  10.1145/3507548.3507591,
  author	= {Wang, Chenxi and Luo, Xudong},
  title		= {A Legal Question Answering System Based on BERT&nbsp;},
  year		= {2022},
  isbn		= {9781450384155},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3507548.3507591},
  doi		= {10.1145/3507548.3507591},
  abstract	= {With the development of artificial intelligence
		  technology, intelligent question-answering systems in
		  general fields have been widely accepted by people.
		  However, the development of intelligent question-answering
		  systems in limited areas is not very satisfactory.
		  Moreover, due to the diversification of Chinese
		  expressions, matching user input problems with prior
		  problems is very important. This paper proposes a scheme to
		  obtain the problem vector representation based on the BERT
		  model. In addition, the Milvus vector search engine is used
		  in this paper, which can not only provide store vector
		  representation information but also calculate vector
		  similarity. Finally, we return the answer through the
		  database. When the threshold value of our proposed scheme
		  is 0.2, the recall rate reaches 86%, and the mismatch rate
		  reaches 84%. The results verify that the system has
		  relatively good performance.},
  booktitle	= {Proceedings of the 2021 5th International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {278–283},
  numpages	= {6},
  keywords	= {Intelligent question
		  answering,&nbsp;BERT,&nbsp;Pre-trained language model,
		  Milvus&nbsp;vector search engine, Similarity calculation},
  location	= {Beijing, China},
  series	= {CSAI '21}
}

@InProceedings{	  10.1145/3539618.3591781,
  author	= {Li, Mingchen and Huang, Lifu},
  title		= {Understand the Dynamic World: An End-to-End Knowledge
		  Informed Framework for Open Domain Entity State Tracking},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591781},
  doi		= {10.1145/3539618.3591781},
  abstract	= {Open domain entity state tracking aims to predict
		  reasonable state changes of entities (i.e., [attribute] of
		  [entity] was [before_state] and [after_state] afterwards)
		  given the action descriptions. It's important to many
		  reasoning tasks to support human everyday activities.
		  However, it's challenging as the model needs to predict an
		  arbitrary number of entity state changes caused by the
		  action while most of the entities are implicitly relevant
		  to the actions and their attributes as well as states are
		  from open vocabularies. To tackle these challenges, we
		  propose a novel end-to-end Knowledge Informed framework for
		  open domain Entity State Tracking, namely KIEST, which
		  explicitly retrieves the relevant entities and attributes
		  from external knowledge graph (i.e., ConceptNet) and
		  incorporates them to autoregressively generate all the
		  entity state changes with a novel dynamic knowledge grained
		  encoder-decoder framework. To enforce the logical coherence
		  among the predicted entities, attributes, and states, we
		  design a new constraint decoding strategy and employ a
		  coherence reward to improve the decoding process.
		  Experimental results show that our proposed KIEST framework
		  significantly outperforms the strong baselines on the
		  public benchmark dataset - OpenPI},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {842–851},
  numpages	= {10},
  keywords	= {coherent reward, constraint decoding, knowledge informed
		  genera- tion, open domain entity state tracking},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3580305.3599819,
  author	= {Hu, Xinyue and Gu, Lin and An, Qiyuan and Zhang, Mengliang
		  and Liu, Liangchen and Kobayashi, Kazuma and Harada,
		  Tatsuya and Summers, Ronald M. and Zhu, Yingying},
  title		= {Expert Knowledge-Aware Image Difference Graph
		  Representation Learning for Difference-Aware Medical Visual
		  Question Answering},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599819},
  doi		= {10.1145/3580305.3599819},
  abstract	= {To contribute to automating the medical vision-language
		  model, we propose a novel Chest-Xray Different Visual
		  Question Answering (VQA) task. Given a pair of main and
		  reference images, this task attempts to answer several
		  questions on both diseases and, more importantly, the
		  differences between them. This is consistent with the
		  radiologist's diagnosis practice that compares the current
		  image with the reference before concluding the report. We
		  collect a new dataset, namely MIMIC-Diff-VQA, including
		  700,703 QA pairs from 164,324 pairs of main and reference
		  images. Compared to existing medical VQA datasets, our
		  questions are tailored to the
		  Assessment-Diagnosis-Intervention-Evaluation treatment
		  procedure used by clinical professionals. Meanwhile, we
		  also propose a novel expert knowledge-aware graph
		  representation learning model to address this task. The
		  proposed baseline model leverages expert knowledge such as
		  anatomical structure prior, semantic, and spatial knowledge
		  to construct a multi-relationship graph, representing the
		  image differences between two images for the image
		  difference VQA task. The dataset and code can be found at
		  https://github.com/Holipori/MIMIC-Diff-VQA. We believe this
		  work would further push forward the medical vision language
		  model.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4156–4165},
  numpages	= {10},
  keywords	= {datasets, medical imaging, visual question answering},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3587716.3587743,
  author	= {Chan, Chunkit and Chan, Tszho},
  title		= {Discourse-Aware Prompt for Argument Impact
		  Classification},
  year		= {2023},
  isbn		= {9781450398411},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587716.3587743},
  doi		= {10.1145/3587716.3587743},
  abstract	= {Discourse information behind the arguments attracts a lot
		  of attention from the field of Natural Language Processing
		  (NLP) and computational argumentation. Durmus et
		  al.&nbsp;[10] launched a new study on the influence of
		  discourse contexts on determining argument impact. Argument
		  Impact Classification is an intriguing but challenging task
		  to classify whether the argumentative unit or an argument
		  is impactful in a conversation. This paper empirically
		  demonstrates that the discourse marker (e.g., "for
		  example," "in other words") can be represented by the
		  learnable continuous prompt to align with discourse
		  information existing in Pre-trained Language Model (PLM).
		  This discourse information helps the Pre-trained Language
		  Model understand the input template and elicit the
		  discourse information to improve the performance on this
		  task. Therefore, based on this intuition, we propose a
		  prompt model DAPA and surpass the previous state-of-the-art
		  model with a 2.5% F1 score.},
  booktitle	= {Proceedings of the 2023 15th International Conference on
		  Machine Learning and Computing},
  pages		= {165–171},
  numpages	= {7},
  keywords	= {argument mining, natural language processing, neural
		  networks},
  location	= {Zhuhai, China},
  series	= {ICMLC '23}
}

@InProceedings{	  10.1145/3569951.3597596,
  author	= {Wang, Mei-Yu and Uran, Julian and Buitrago, Paola},
  title		= {Deep Learning Benchmark Studies on an Advanced AI
		  Engineering Testbed from the Open Compass Project},
  year		= {2023},
  isbn		= {9781450399852},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3569951.3597596},
  doi		= {10.1145/3569951.3597596},
  abstract	= {We present the Open Compass project’s pilot deep
		  learning benchmark results with various AI accelerators.
		  Those accelerators are NVIDIA V-100 and A-100, AMD MI100,
		  as well as emerging novel accelerators such as Cerebras
		  CS-2 and Graphcore. We evaluate their performance on
		  various deep learning training tasks. We then discuss key
		  insights from our experiments and share experiences about
		  evaluating and integrating those novel AI accelerators with
		  our supercomputing systems.},
  booktitle	= {Practice and Experience in Advanced Research Computing
		  2023: Computing for the Common Good},
  pages		= {255–259},
  numpages	= {5},
  keywords	= {Artificial Intelligence, BERT, Benchmarking, Bridges-2,
		  Graphics Processing Unit, Intelligence Processing Unit,
		  Large Language Model, Neocortex, UNet, Wafer-Scale Engine},
  location	= {Portland, OR, USA},
  series	= {PEARC '23}
}

@InProceedings{	  10.1145/3580305.3599246,
  author	= {Zhang, Jiarui and Ilievski, Filip and Ma, Kaixin and
		  Kollaa, Aravinda and Francis, Jonathan and Oltramari,
		  Alessandro},
  title		= {A Study of Situational Reasoning for Traffic
		  Understanding},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599246},
  doi		= {10.1145/3580305.3599246},
  abstract	= {Intelligent Traffic Monitoring (ITMo) technologies hold
		  the potential for improving road safety/security and for
		  enabling smart city infrastructure. Understanding traffic
		  situations requires a complex fusion of perceptual
		  information with domain-specific and causal commonsense
		  knowledge. Whereas prior work has provided benchmarks and
		  methods for traffic monitoring, it remains unclear whether
		  models can effectively align these information sources and
		  reason in novel scenarios. To address this assessment gap,
		  we devise three novel text-based tasks for situational
		  reasoning in the traffic domain: i) BDD-QA, which evaluates
		  the ability of Language Models (LMs) to perform situational
		  decision-making, ii) TV-QA, which assesses LMs' abilities
		  to reason about complex event causality, and iii) HDT-QA,
		  which evaluates the ability of models to solve human
		  driving exams. We adopt four knowledge-enhanced methods
		  that have shown generalization capability across language
		  reasoning tasks in prior work, based on natural language
		  inference, commonsense knowledge-graph self-supervision,
		  multi-QA joint training, and dense retrieval of domain
		  information. We associate each method with a relevant
		  knowledge source, including knowledge graphs, relevant
		  benchmarks, and driving manuals. In extensive experiments,
		  we benchmark various knowledge-aware methods against the
		  three datasets, under zero-shot evaluation; we provide
		  in-depth analyses of model performance on data partitions
		  and examine model predictions categorically, to yield
		  useful insights on traffic understanding, given different
		  background knowledge and reasoning strategies.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3262–3272},
  numpages	= {11},
  keywords	= {language models, question answering, traffic
		  understanding, zero-shot evaluation},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3500931.3501009,
  author	= {Yu, Tao and Yong, Cuo},
  title		= {Deep Convolutional Neural Network Diabetic Entity
		  Relationship Extraction Model Based on Enhanced Semantic
		  Representation},
  year		= {2021},
  isbn		= {9781450395588},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3500931.3501009},
  doi		= {10.1145/3500931.3501009},
  abstract	= {Diabetes, as the number one chronic disease in China,
		  plagues the lives of people of different age groups and
		  causes great distress to people. This paper extracts the
		  semantic relationships among diabetes entities through the
		  entity relationship extraction technique, which helps to
		  build the knowledge graph of diabetes domain. The
		  experimental results show that the deep convolutional
		  neural network (ESPDCNN) based on enhanced semantic
		  representation proposed in this paper effectively improves
		  the accuracy of the diabetic entity relationship extraction
		  model.},
  booktitle	= {Proceedings of the 2nd International Symposium on
		  Artificial Intelligence for Medicine Sciences},
  pages		= {460–465},
  numpages	= {6},
  keywords	= {DiaKG, Diabetes, ESPDCNN, Entity relationship extraction},
  location	= {Beijing, China},
  series	= {ISAIMS '21}
}

@InProceedings{	  10.1145/3578741.3578791,
  author	= {Yu, Yishu},
  title		= {Beyond Natural Language Processing: Building Knowledge
		  Graphs to Assist Scientists Understand COVID-19 Concepts},
  year		= {2023},
  isbn		= {9781450399067},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3578741.3578791},
  doi		= {10.1145/3578741.3578791},
  abstract	= {To combat COVID-19, scientists must digest the vast amount
		  of relevant biomedical knowledge in the literature to
		  understand disease mechanisms and related biological
		  functions. Nearly 3,000 scientific papers are published on
		  PubMed every day. This knowledge bottleneck has resulted in
		  severe delays in developing COVID-19 vaccines and drugs.
		  Our research produces a hierarchy of knowledge concepts
		  related to COVID-19, designed to assist scientists in
		  answering questions and generating summaries. It aims to
		  discover scientific and comprehensive knowledge to extract
		  fine-grained multimedia elements (i.e., physical and visual
		  structures, relational events and events, and chemical
		  knowledge). Our project is toward one step in natural
		  language understanding: detailed contextual sentences,
		  subgraphs, and knowledge subgraphs are the first time to be
		  automatically generated, and relations and coreferences of
		  COVID-19 mentions will be sketched. Extensive results show
		  that our method outperforms other state-of-the-art methods.
		  In addition, we have published the generated knowledge
		  graph on Google Drive1 and released the source in the
		  Github2.},
  booktitle	= {Proceedings of the 2022 5th International Conference on
		  Machine Learning and Natural Language Processing},
  pages		= {245–251},
  numpages	= {7},
  keywords	= {COVID-19, Knowledge Graphs, Natural Language Processing},
  location	= {Sanya, China},
  series	= {MLNLP '22}
}

@Article{	  10.1145/3502720,
  author	= {Lo, Pei-Chi and Lim, Ee-Peng},
  title		= {Contextual Path Retrieval: A Contextual Entity Relation
		  Embedding-based Approach},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3502720},
  doi		= {10.1145/3502720},
  abstract	= {Contextual path retrieval (CPR) refers to the task of
		  finding contextual path(s) between a pair of entities in a
		  knowledge graph that explains the connection between them
		  in a given context. For this novel retrieval task, we
		  propose the Embedding-based Contextual Path Retrieval
		  (ECPR) framework. ECPR is based on a three-component
		  structure that includes a context encoder and path encoder
		  that encode query context and path, respectively, and a
		  path ranker that assigns a ranking score to each candidate
		  path to determine the one that should be the contextual
		  path. For context encoding, we propose two novel context
		  encoding methods, i.e., context-fused entity embeddings and
		  contextualized embeddings. For path encoding, we propose
		  PathVAE, an inductive embedding approach to generate path
		  representations. Finally, we explore two path-ranking
		  approaches. In our evaluation, we construct a synthetic
		  dataset from Wikipedia and two real datasets of Wikinews
		  articles constructed through crowdsourcing. Our experiments
		  show that methods based on ECPR framework outperform
		  baseline methods, and that our two proposed context
		  encoders yield significantly better performance than
		  baselines. We also analyze a few case studies to show the
		  distinct features of ECPR-based methods.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {1},
  numpages	= {38},
  keywords	= {Knowledge base, reasoning, information retrieval,
		  embedding learning}
}

@InProceedings{	  10.1145/3580305.3599387,
  author	= {Wang, Xiaolei and Zhou, Kun and Tang, Xinyu and Zhao,
		  Wayne Xin and Pan, Fan and Cao, Zhao and Wen, Ji-Rong},
  title		= {Improving Conversational Recommendation Systems via
		  Counterfactual Data Simulation},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599387},
  doi		= {10.1145/3580305.3599387},
  abstract	= {Conversational recommender systems~(CRSs) aim to provide
		  recommendation services via natural language conversations.
		  Although a number of approaches have been proposed for
		  developing capable CRSs, they typically rely on sufficient
		  training data for training. Since it is difficult to
		  annotate recommendation-oriented dialogue datasets,
		  existing CRS approaches often suffer from the issue of
		  insufficient training due to the scarcity of training
		  data.To address this issue, in this paper, we propose a
		  CounterFactual data simulation approach for CRS, named
		  CFCRS, to alleviate the issue of data scarcity in CRSs. Our
		  approach is developed based on the framework of
		  counterfactual data augmentation, which gradually
		  incorporates the rewriting to the user preference from a
		  real dialogue without interfering with the entire
		  conversation flow. To develop our approach, we characterize
		  user preference and organize the conversation flow by the
		  entities involved in the dialogue, and design a multi-stage
		  recommendation dialogue simulator based on a conversation
		  flow language model. Under the guidance of the learned user
		  preference and dialogue schema, the flow language model can
		  produce reasonable, coherent conversation flows, which can
		  be further realized into complete dialogues. Based on the
		  simulator, we perform the intervention at the
		  representations of the interacted entities of target users,
		  and design an adversarial training method with a curriculum
		  schedule that can gradually optimize the data augmentation
		  strategy. Extensive experiments show that our approach can
		  consistently boost the performance of several competitive
		  CRSs, and outperform other data augmentation methods,
		  especially when the training data is limited. Our code is
		  publicly available at https://github.com/RUCAIBox/CFCRS.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2398–2408},
  numpages	= {11},
  keywords	= {conversational recommender system, counterfactual data
		  augmentation, prompt learning},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3511808.3557446,
  author	= {Burgdorf, Andreas and Paulus, Alexander and Pomp,
		  Andr\'{e} and Meisen, Tobias},
  title		= {DocSemMap 2.0: Semantic Labeling based on Textual Data
		  Documentations Using Seq2Seq Context Learner},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557446},
  doi		= {10.1145/3511808.3557446},
  abstract	= {Methods for automated semantic labeling of data are an
		  indispensable basis for increasing the usability of data.
		  On the one hand, they contribute to the homogenization of
		  the annotations and thus to the increase in quality; on the
		  other hand, they reduce the modeling effort, provided that
		  the quality of the used methodology is sufficient. In the
		  past, research has focused primarily on data- and
		  label-based methods. Another approach that has received
		  recent attention is the incorporation of textual data
		  documentations to support the automatic mapping of datasets
		  to a knowledge graph. However, upon deeper analysis, our
		  recent approach called DocSemMap gives away potential in a
		  number of places. In this paper, we extend the current
		  state of the art approach by uncovering existing
		  shortcomings and presenting our own improvements. Using a
		  sequence-to-sequence model (Seq2Seq), we exploit the
		  context of datasets. An additional introduced classifier
		  provides the linkage of documentation and labels for
		  prediction. Our extended approach achieves a sustainable
		  improvement in comparison to the reference approach.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {98–107},
  numpages	= {10},
  keywords	= {classifier, natural language processing, semantic mapping,
		  seq2seq},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3581783.3612403,
  author	= {Sun, Hongbo and He, Xiangteng and Zhou, Jiahuan and Peng,
		  Yuxin},
  title		= {Fine-Grained Visual Prompt Learning of Vision-Language
		  Models for Image Recognition},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612403},
  doi		= {10.1145/3581783.3612403},
  abstract	= {Large-scale pre-trained vision-language (VL) models have
		  shown powerful generic representation capabilities for
		  adapting to downstream tasks with limited training data,
		  which are data-efficient solutions to various applications
		  such as image recognition. In order to enhance the adaption
		  performance, most existing methods attempt to introduce
		  learnable vectors into the text prompt to generate adaptive
		  classification weights for the class in the downstream
		  task. However, they generally focus on the text side while
		  neglecting adaptive visual feature generation on the image
		  side, which is insufficient to fit the downstream task
		  data. In this paper, we propose fine-grained visual prompt
		  learning (FG-VPL) of vision-language models for image
		  recognition with few training samples, and the main
		  contributions are: (1) Fine-grained visual prompt is
		  introduced into the image encoder of the vision-language
		  model for focusing on the target object and conducting
		  information interaction within the object, which
		  facilitates generating discriminative visual features for
		  image recognition. (2) A two-pathway adaptive recognition
		  module is proposed to narrow the domain gap and utilize
		  both the cross-modal knowledge of the vision-language model
		  and the visual information of the few-sample training set
		  for classifying images with the help of feature adapters.
		  We conduct extensive experiments on 11 image recognition
		  benchmark datasets under the few training samples setting,
		  which demonstrate that our proposed approach can achieve
		  state-of-the-art performance. The code is available at
		  https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {5828–5836},
  numpages	= {9},
  keywords	= {fine-grained visual prompt learning, image recognition
		  with few training samples, vision-language models},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3581783.3611964,
  author	= {Yang, Qian and Chen, Qian and Wang, Wen and Hu, Baotian
		  and Zhang, Min},
  title		= {Enhancing Multi-modal Multi-hop Question Answering via
		  Structured Knowledge and Unified Retrieval-Generation},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3611964},
  doi		= {10.1145/3581783.3611964},
  abstract	= {Multi-modal multi-hop question answering involves
		  answering a question by reasoning over multiple input
		  sources from different modalities. Existing methods often
		  retrieve evidences separately and then use a language model
		  to generate an answer based on the retrieved evidences, and
		  thus do not adequately connect candidates and are unable to
		  model the interdependent relations during retrieval.
		  Moreover, the pipelined approaches of retrieval and
		  generation might result in poor generation performance when
		  retrieval performance is low. To address these issues, we
		  propose a Structured Knowledge and Unified
		  Retrieval-Generation (SKURG) approach. SKURG employs an
		  Entity-centered Fusion Encoder to align sources from
		  different modalities using shared entities. It then uses a
		  unified Retrieval-Generation Decoder to integrate
		  intermediate retrieval results for answer generation and
		  also adaptively determine the number of retrieval steps.
		  Extensive experiments on two representative multi-modal
		  multi-hop QA datasets MultimodalQA and WebQA demonstrate
		  that SKURG outperforms the state-of-the-art models in both
		  source retrieval and answer generation performance with
		  fewer parameters1.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {5223–5234},
  numpages	= {12},
  keywords	= {cross-modal reasoning, multi-modal retrieval, question
		  answering},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3534678.3539080,
  author	= {El-Kishky, Ahmed and Markovich, Thomas and Park, Serim and
		  Verma, Chetan and Kim, Baekjin and Eskander, Ramy and
		  Malkov, Yury and Portman, Frank and Samaniego, Sof\'{\i}a
		  and Xiao, Ying and Haghighi, Aria},
  title		= {TwHIN: Embedding the Twitter Heterogeneous Information
		  Network for Personalized Recommendation},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539080},
  doi		= {10.1145/3534678.3539080},
  abstract	= {Social networks, such as Twitter, form a heterogeneous
		  information network (HIN) where nodes represent domain
		  entities (e.g., user, content, advertiser, etc.) and edges
		  represent one of many entity interactions (e.g, a user
		  re-sharing content or "following" another). Interactions
		  from multiple relation types can encode valuable
		  information about social network entities not fully
		  captured by a single relation; for instance, a user's
		  preference for accounts to follow may depend on both
		  user-content engagement interactions and the other users
		  they follow. In this work, we investigate knowledge-graph
		  embeddings for entities in the Twitter HIN (TwHIN); we show
		  that these pretrained representations yield significant
		  offline and online improvement for a diverse range of
		  downstream recommendation and classification tasks:
		  personalized ads rankings, account follow-recommendation,
		  offensive content detection, and search ranking. We discuss
		  design choices and practical challenges of deploying
		  industry-scale HIN embeddings, including compressing them
		  to reduce end-to-end model latency and handling parameter
		  drift across versions.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2842–2850},
  numpages	= {9},
  keywords	= {Twitter, embedding, graph embedding, heterogeneous
		  information network, recommendation system, social
		  network},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@Article{	  10.1145/3588938,
  author	= {Tu, Jianhong and Fan, Ju and Tang, Nan and Wang, Peng and
		  Li, Guoliang and Du, Xiaoyong and Jia, Xiaofeng and Gao,
		  Song},
  title		= {Unicorn: A Unified Multi-tasking Model for Supporting
		  Matching Tasks in Data Integration},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {1},
  url		= {https://doi.org/10.1145/3588938},
  doi		= {10.1145/3588938},
  abstract	= {Data matching - which decides whether two data elements
		  (e.g., string, tuple, column, or knowledge graph entity)
		  are the "same" (a.k.a. a match) - is a key concept in data
		  integration, such as entity matching and schema matching.
		  The widely used practice is to build task-specific or even
		  dataset-specific solutions, which are hard to generalize
		  and disable the opportunities of knowledge sharing that can
		  be learned from different datasets and multiple tasks. In
		  this paper, we propose Unicorn, a unified model for
		  generally supporting common data matching tasks. Unicorn
		  can enable knowledge sharing by learning from multiple
		  tasks and multiple datasets, and can also support zero-shot
		  prediction for new tasks with zero labeled
		  matching/non-matching pairs. However, building such a
		  unified model is challenging due to heterogeneous formats
		  of input data elements and various matching semantics of
		  multiple tasks. To address the challenges, Unicorn employs
		  one generic Encoder that converts any pair of data elements
		  (a, b) into a learned representation, and uses a Matcher,
		  which is a binary classifier, to decide whether a matches
		  b. To align matching semantics of multiple tasks, Unicorn
		  adopts a mixture-of-experts model that enhances the learned
		  representation into a better representation. We conduct
		  extensive experiments using 20 datasets on seven
		  well-studied data matching tasks, and find that our unified
		  model can achieve better performance on most tasks and on
		  average, compared with the state-of-the-art specific models
		  trained for ad-hoc tasks and datasets separately. Moreover,
		  Unicorn can also well serve new matching tasks with
		  zero-shot learning.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {84},
  numpages	= {26},
  keywords	= {data integration, data matching, multi-task learning}
}

@InProceedings{	  10.1145/3600211.3604702,
  author	= {Rismani, Shalaleh and Moon, AJung},
  title		= {What does it mean to be a responsible AI practitioner: An
		  ontology of roles and skills},
  year		= {2023},
  isbn		= {9798400702310},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3600211.3604702},
  doi		= {10.1145/3600211.3604702},
  abstract	= {With the growing need to regulate AI systems across a wide
		  variety of application domains, a new set of occupations
		  has emerged in the industry. The so-called responsible
		  Artificial Intelligence (AI) practitioners or AI ethicists
		  are generally tasked with interpreting and operationalizing
		  best practices for ethical and safe design of AI systems.
		  Due to the nascent nature of these roles, however, it is
		  unclear to future employers and aspiring AI ethicists what
		  specific function these roles serve and what skills are
		  necessary to serve the functions. Without clarity on these,
		  we cannot train future AI ethicists with meaningful
		  learning objectives. In this work, we examine what
		  responsible AI practitioners do in the industry and what
		  skills they employ on the job. We propose an ontology of
		  existing roles alongside skills and competencies that serve
		  each role. We created this ontology by examining the job
		  postings for such roles over a two-year period (2020-2022)
		  and conducting expert interviews with fourteen individuals
		  who currently hold such a role in the industry. Our
		  ontology contributes to business leaders looking to build
		  responsible AI teams and provides educators with a set of
		  competencies that an AI ethics curriculum can prioritize.},
  booktitle	= {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {584–595},
  numpages	= {12},
  keywords	= {Competency Framework, Education, Responsible AI
		  Practitioner},
  location	= {Montr\'{e}al, QC, Canada},
  series	= {AIES '23}
}

@InProceedings{	  10.1145/3584684.3597263,
  author	= {Ilani, Arnon and Dolev, Shlomi},
  title		= {Invited Paper: Common Public Knowledge for Enhancing
		  Machine Learning Data Sets},
  year		= {2023},
  isbn		= {9798400701283},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584684.3597263},
  doi		= {10.1145/3584684.3597263},
  abstract	= {In this study, we show the advantages of incorporating
		  multi-source knowledge from publicly available sources,
		  such as ChatGPT and Wikipedia, into existing datasets to
		  enhance the performance of machine learning models for
		  routine tasks, such as classification. specifically, we
		  propose the utilization of supplementary data from external
		  sources and demonstrate the utility of widely accessible
		  knowledge in the context of the Forest Cover Type
		  Prediction task launched by the Roosevelt National Forest
		  of Northern Colorado. Additionally, we exhibit an
		  improvement in classification accuracy for the Isolated
		  Letter Speech Recognition dataset when incorporating
		  information on regional accents in the prediction of spoken
		  English letter names.},
  booktitle	= {Proceedings of the 5th Workshop on Advanced Tools,
		  Programming Languages, and PLatforms for Implementing and
		  Evaluating Algorithms for Distributed Systems},
  articleno	= {2},
  numpages	= {10},
  keywords	= {ontology, machine learning, random forests, feature
		  engineering, world knowledge, speech recognition, isolated
		  letter, forest management, tree cover type, ChatGPT},
  location	= {Orlando, FL, USA},
  series	= {ApPLIED 2023}
}

@InProceedings{	  10.1145/3469213.3470406,
  author	= {Cai, Xinhui and Fang, Jiandong and Zhao, Yudong},
  title		= {Joint extraction of Chinese entitys and relations based on
		  hierarchical labeling},
  year		= {2021},
  isbn		= {9781450390200},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3469213.3470406},
  doi		= {10.1145/3469213.3470406},
  abstract	= {Entitys and relations triples are the basic information
		  units that make up the knowledge graph, and represent some
		  simple and specific information. This paper proposes a
		  hierarchical annotation model based on BERT to jointly
		  extract entity relation triples from text. First, the
		  subject of the triples is marked by the labeling method. On
		  this basis, for each predefined relationship, the
		  half-pointer and half-marking method is used, and the
		  double pointer is used to mark the beginning and end
		  positions of the corresponding object in the text, and then
		  the Complete extraction of triples can effectively solve
		  the problem that traditional methods are difficult to
		  extract triples with overlapping entities.},
  booktitle	= {2021 2nd International Conference on Artificial
		  Intelligence and Information Systems},
  articleno	= {199},
  numpages	= {5},
  location	= {Chongqing, China},
  series	= {ICAIIS 2021}
}

@InProceedings{	  10.1145/3543507.3583457,
  author	= {Zhao, Mingjun and Wang, Mengzhen and Ma, Yinglong and Niu,
		  Di and Wu, Haijiang},
  title		= {CEIL: A General Classification-Enhanced Iterative Learning
		  Framework for Text Clustering},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583457},
  doi		= {10.1145/3543507.3583457},
  abstract	= {Text clustering, as one of the most fundamental challenges
		  in unsupervised learning, aims at grouping semantically
		  similar text segments without relying on human annotations.
		  With the rapid development of deep learning, deep
		  clustering has achieved significant advantages over
		  traditional clustering methods. Despite the effectiveness,
		  most existing deep text clustering methods rely heavily on
		  representations pre-trained in general domains, which may
		  not be the most suitable solution for clustering in
		  specific target domains. To address this issue, we propose
		  CEIL, a novel Classification-Enhanced Iterative Learning
		  framework for short text clustering, which aims at
		  generally promoting the clustering performance by
		  introducing a classification objective to iteratively
		  improve feature representations. In each iteration, we
		  first adopt a language model to retrieve the initial text
		  representations, from which the clustering results are
		  collected using our proposed Category Disentangled
		  Contrastive Clustering (CDCC) algorithm. After strict data
		  filtering and aggregation processes, samples with clean
		  category labels are retrieved, which serve as supervision
		  information to update the language model with the
		  classification objective via a prompt learning approach.
		  Finally, the updated language model with improved
		  representation ability is used to enhance clustering in the
		  next iteration. Extensive experiments demonstrate that the
		  CEIL framework significantly improves the clustering
		  performance over iterations, and is generally effective on
		  various clustering algorithms. Moreover, by incorporating
		  CEIL on CDCC, we achieve the state-of-the-art clustering
		  performance on a wide range of short text clustering
		  benchmarks outperforming other strong baseline methods.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1784–1792},
  numpages	= {9},
  keywords	= {Classification-enhanced Clustering, Iterative Framework,
		  Text Clustering},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3485447.3512039,
  author	= {Weinzierl, Maxwell and Harabagiu, Sanda},
  title		= {Identifying the Adoption or Rejection of Misinformation
		  Targeting COVID-19 Vaccines in Twitter Discourse},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3512039},
  doi		= {10.1145/3485447.3512039},
  abstract	= {Although billions of COVID-19 vaccines have been
		  administered, too many people remain hesitant.
		  Misinformation about the COVID-19 vaccines, propagating on
		  social media, is believed to drive hesitancy towards
		  vaccination. However, exposure to misinformation does not
		  necessarily indicate misinformation adoption. In this paper
		  we describe a novel framework for identifying the stance
		  towards misinformation, relying on attitude consistency and
		  its properties. The interactions between attitude
		  consistency, adoption or rejection of misinformation and
		  the content of microblogs are exploited in a novel neural
		  architecture, where the stance towards misinformation is
		  organized in a knowledge graph. This new neural framework
		  is enabling the identification of stance towards
		  misinformation about COVID-19 vaccines with
		  state-of-the-art results. The experiments are performed on
		  a new dataset of misinformation towards COVID-19 vaccines,
		  called CoVaxLies, collected from recent Twitter discourse.
		  Because CoVaxLies provides a taxonomy of the misinformation
		  about COVID-19 vaccines, we are able to show which type of
		  misinformation is mostly adopted and which is mostly
		  rejected.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {3196–3205},
  numpages	= {10},
  keywords	= {COVID-19, misinformation, social media, stance, twitter,
		  vaccine},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3584371.3612953,
  author	= {Quintana, Felix and Treangen, Todd and Kavraki, Lydia},
  title		= {Leveraging Large Language Models for Predicting Microbial
		  Virulence from Protein Structure and Sequence},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3612953},
  doi		= {10.1145/3584371.3612953},
  abstract	= {In the aftermath of COVID-19, screening for pathogens has
		  never been a more relevant problem. However, computational
		  screening for pathogens is challenging due to a variety of
		  factors, including (i) the complexity and role of the host,
		  (ii) virulence factor divergence and dynamics, and (iii)
		  population and community-level dynamics. Considering a
		  potential pathogen's molecular interactions, specifically
		  individual proteins and protein interactions can help
		  pinpoint a potential protein of a given microbe to cause
		  disease. However, existing tools for pathogen screening
		  rely on existing annotations (KEGG, GO, etc), making the
		  assessment of novel and unannotated proteins more
		  challenging. Here, we present an LLM-inspired approach that
		  considers protein sequence and structure to predict protein
		  virulence. We present a two-stage model incorporating
		  evolutionary features captured from the DistilProtBert
		  language model and protein structure in a graph
		  convolutional network. Our model performs better than
		  sequence alone for virulence function when high-quality
		  structures are present, thus representing a path forward
		  for virulence prediction of novel and unannotated
		  proteins.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {103},
  numpages	= {6},
  keywords	= {protein function, virulence prediction, graph-based
		  models, large language models},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1145/3477495.3531954,
  author	= {Li, Yinghui and Li, Yangning and He, Yuxin and Yu, Tianyu
		  and Shen, Ying and Zheng, Hai-Tao},
  title		= {Contrastive Learning with Hard Negative Entities for
		  Entity Set Expansion},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531954},
  doi		= {10.1145/3477495.3531954},
  abstract	= {Entity Set Expansion (ESE) is a promising task which aims
		  to expand entities of the target semantic class described
		  by a small seed entity set. Various NLP and IR applications
		  will benefit from ESE due to its ability to discover
		  knowledge. Although previous ESE methods have achieved
		  great progress, most of them still lack the ability to
		  handle hard negative entities (i.e., entities that are
		  difficult to distinguish from the target entities), since
		  two entities may or may not belong to the same semantic
		  class based on different granularity levels we analyze on.
		  To address this challenge, we devise an entity-level masked
		  language model with contrastive learning to refine the
		  representation of entities. In addition, we propose the
		  ProbExpan, a novel probabilistic ESE framework utilizing
		  the entity representation obtained by the aforementioned
		  language model to expand entities. Extensive experiments
		  and detailed analyses on three datasets show that our
		  method outperforms previous state-of-the-art methods.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1077–1086},
  numpages	= {10},
  keywords	= {contrastive learning, entity set expansion, knowledge
		  discovery},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3490322.3490336,
  author	= {Li, Zhengmin and Yun, Hongyan and Guo, Zhenbo and Qi,
		  Jianjun},
  title		= {Medical Named Entity Recognition Based on Multi Feature
		  Fusion of BERT},
  year		= {2022},
  isbn		= {9781450385091},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3490322.3490336},
  doi		= {10.1145/3490322.3490336},
  abstract	= {In order to solve the problem that traditional word
		  vectors are difficult to express the context semantics and
		  the feature extraction of traditional model is single, a
		  multi-feature fusion model named
		  BERT-BiLSTM-IDCNN-Attention-CRF for Named Entity
		  Recognition is proposed, which uses BERT to model the
		  context semantic relationship of word vectors and fuse the
		  context features and local features extracted by BiLSTM and
		  IDCNN respectively. The proposed model is tested on Chinese
		  Electronic Medical Record (EMR) dataset issued by China
		  Conference on Knowledge Graph and Semantic Computing 2020
		  (CCKS2020).Compared with the baseline models such as
		  BiLSTM-CRF, the experiment on CCKS2020 data shows that
		  BERT-BiLSTM-IDCNN-Attention-CRF achieves 1.27% improvement
		  in F1. The experimental results show that the proposed
		  model can better identify the medical entities in EMR.},
  booktitle	= {Proceedings of the 4th International Conference on Big
		  Data Technologies},
  pages		= {86–91},
  numpages	= {6},
  keywords	= {BERT, BiLSTM, CCKS2020, IDCNN, Named Entity Recognition,
		  multi feature fusion},
  location	= {Zibo, China},
  series	= {ICBDT '21}
}

@InProceedings{	  10.1145/3460210.3493573,
  author	= {Li, Xue and Magliacane, Sara and Groth, Paul},
  title		= {The Challenges of Cross-Document Coreference Resolution
		  for Email},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493573},
  doi		= {10.1145/3460210.3493573},
  abstract	= {Long-form conversations such as email are an important
		  source of information for knowledge capture. For tasks such
		  as knowledge graph construction, conversational search, and
		  entity linking, being able to resolve entities from across
		  documents is important. Building on recent work on within
		  document coreference resolution for email, we study for the
		  first time a cross-document formulation of the problem. Our
		  results show that the current state-of-the-art deep
		  learning models for general cross-document coreference
		  resolution are insufficient for email conversations. Our
		  experiments show that the general task is challenging and,
		  importantly for knowledge intensive tasks, coreference
		  resolution models that only treat entity mentions perform
		  worse. Based on these results, we outline the work needed
		  to address this challenging task.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {273–276},
  numpages	= {4},
  keywords	= {challenges, conversational data, cross-document
		  coreference resolution, email conversations, entity
		  resolution},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@InProceedings{	  10.1145/3580305.3599401,
  author	= {Kim, Taeho and Yu, Juwon and Shin, Won-Yong and Lee,
		  Hyunyoung and Im, Ji-hui and Kim, Sang-Wook},
  title		= {LATTE: A Framework for Learning Item-Features to Make a
		  Domain-Expert for Effective Conversational Recommendation},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599401},
  doi		= {10.1145/3580305.3599401},
  abstract	= {For high-quality conversational recommender systems (CRS),
		  it is important to recommend the suitable items by
		  capturing the items' features mentioned in the dialog and
		  to explain the appropriate ones among the various features
		  of the recommended item. We argue that the CRS model should
		  be a domain-expert who is (1) knowledgeable about the
		  relationships between items and their various features and
		  (2) able to explain the recommended item with its features
		  relevant to dialog context. To this end, we propose a novel
		  framework, named as LATTE, to pre-train each core module in
		  CRS (i.e., the recommendation and the conversation module)
		  through abundant external data. For the recommendation
		  module, we pre-train the recommendation module to
		  comprehensively understand the relationships between items
		  and their various features by leveraging both multi-reviews
		  and a knowledge graph. For pre-training the conversation
		  module, we create the synthetic dialogs, which contain
		  responses providing the explanation relevant to the dialog
		  context by using all the items' features and dialog
		  templates. Through extensive experiments on two public CRS
		  datasets, we demonstrate that LATTE exhibits (1) the
		  effectiveness of each module in LATTE, (2) the superiority
		  over 7 state-of-the art methods, and (3) the
		  interpretations based on visualization.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1144–1153},
  numpages	= {10},
  keywords	= {conversational recommender systems, domain-expert,
		  explanation.},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@InProceedings{	  10.1145/3404835.3463259,
  author	= {Jain, Aman and Kothyari, Mayank and Kumar, Vishwajeet and
		  Jyothi, Preethi and Ramakrishnan, Ganesh and Chakrabarti,
		  Soumen},
  title		= {Select, Substitute, Search: A New Benchmark for
		  Knowledge-Augmented Visual Question Answering},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3463259},
  doi		= {10.1145/3404835.3463259},
  abstract	= {Multimodal IR, spanning text corpus, knowledge graph and
		  images, called outside knowledge visual question answering
		  (OKVQA), is of much recent interest. However, the popular
		  data set has serious limitations. A surprisingly large
		  fraction of queries do not assess the ability to integrate
		  cross-modal information. Instead, some are independent of
		  the image, some depend on speculation, some require OCR or
		  are otherwise answerable from the image alone. To add to
		  the above limitations, frequency-based guessing is very
		  effective because of (unintended) widespread answer
		  overlaps between the train and test folds. Overall, it is
		  hard to determine when state-of-the-art systems exploit
		  these weaknesses rather than really infer the answers,
		  because they are opaque and their 'reasoning' process is
		  uninterpretable. An equally important limitation is that
		  the dataset is designed for the quantitative assessment
		  only of the end-to-end answer retrieval task, with no
		  provision for assessing the correct(semantic)
		  interpretation of the input query. In response, we identify
		  a key structural idiom in OKVQA ,viz., S3 (select,
		  substitute and search), and build a new data set and
		  challenge around it. Specifically, the questioner
		  identifies an entity in the image and asks a question
		  involving that entity which can be answered only by
		  consulting a knowledge graph or corpus passage mentioning
		  the entity. Our challenge consists of (i)OKVQA_S3, a subset
		  of OKVQA annotated based on the structural idiom and
		  (ii)S3VQA, a new dataset built from scratch. We also
		  present a neural but structurally transparent OKVQA system,
		  S3, that explicitly addresses our challenge dataset, and
		  outperforms recent competitive baselines. We make our code
		  and data available at https://s3vqa.github.io/.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2491–2498},
  numpages	= {8},
  keywords	= {multimodal question answering, open-domain question
		  answering, query reformulation},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3539618.3591700,
  author	= {Chen, Zhongwu and Xu, Chengjin and Su, Fenglong and Huang,
		  Zhen and Dou, Yong},
  title		= {Incorporating Structured Sentences with Time-enhanced BERT
		  for Fully-inductive Temporal Relation Prediction},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591700},
  doi		= {10.1145/3539618.3591700},
  abstract	= {Temporal relation prediction in incomplete temporal
		  knowledge graphs (TKGs) is a popular temporal knowledge
		  graph completion (TKGC) problem in both transductive and
		  inductive settings. Traditional embedding-based TKGC models
		  (TKGE) rely on structured connections and can only handle a
		  fixed set of entities, i.e., the transductive setting. In
		  the inductive setting where test TKGs contain emerging
		  entities, the latest methods are based on symbolic rules or
		  pre-trained language models (PLMs). However, they suffer
		  from being inflexible and not time-specific, respectively.
		  In this work, we extend the fully-inductive setting, where
		  entities in the training and test sets are totally
		  disjoint, into TKGs and take a further step towards a more
		  flexible and time-sensitive temporal relation prediction
		  approach SST-BERT,incorporating Structured Sentences with
		  Time-enhanced BERT. Our model can obtain the entity history
		  and implicitly learn rules in the semantic space by
		  encoding structured sentences, solving the problem of
		  inflexibility. We propose to use a time masking MLM task to
		  pre-train BERT in a corpus rich in temporal tokens
		  specially generated for TKGs, enhancing the time
		  sensitivity of SST-BERT. To compute the probability of
		  occurrence of a target quadruple, we aggregate all its
		  structured sentences from both temporal and semantic
		  perspectives into a score. Experiments on the transductive
		  datasets and newly generated fully-inductive benchmarks
		  show that SST-BERT successfully improves over
		  state-of-the-art baselines.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {889–899},
  numpages	= {11},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3460231.3478882,
  author	= {F\"{a}rber, Michael and Leisinger, Ann-Kathrin},
  title		= {DataHunter: A System for Finding Datasets Based on
		  Scientific Problem Descriptions},
  year		= {2021},
  isbn		= {9781450384582},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460231.3478882},
  doi		= {10.1145/3460231.3478882},
  abstract	= {The number of datasets is steadily rising, making it
		  increasingly difficult for researchers and practitioners in
		  the various scientific disciplines to be aware of all
		  datasets, particularly of the most relevant datasets for a
		  given research problem. To this end, dataset search engines
		  have been proposed. However, they are based on the users’
		  keywords and thus have difficulties in determining
		  precisely fitting datasets for complex research problems.
		  In this paper, we propose the system at
		  http://data-hunter.io that recommends suitable datasets to
		  users based on given research problem descriptions. It is
		  based on fastText for the text representation and text
		  classification, the Data Set Knowledge Graph (DSKG) with
		  metadata about almost 1,700 unique datasets, as well as
		  88,000 paper abstracts as research problem descriptions for
		  training the model. Overall, our system demonstrates that
		  recommending datasets facilitates data provisioning and
		  reuse according to the FAIR principles and that dataset
		  recommendation is a promising future research direction.},
  booktitle	= {Proceedings of the 15th ACM Conference on Recommender
		  Systems},
  pages		= {749–752},
  numpages	= {4},
  keywords	= {FAIR principles, datasets, machine learning,
		  recommendation, text classification},
  location	= {Amsterdam, Netherlands},
  series	= {RecSys '21}
}

@InProceedings{	  10.1145/3543507.3583220,
  author	= {Sakota, Marija and Peyrard, Maxime and West, Robert},
  title		= {Descartes: Generating Short Descriptions of Wikipedia
		  Articles},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583220},
  doi		= {10.1145/3543507.3583220},
  abstract	= {Wikipedia is one of the richest knowledge sources on the
		  Web today. In order to facilitate navigating, searching,
		  and maintaining its content, Wikipedia’s guidelines state
		  that all articles should be annotated with a so-called
		  short description indicating the article’s topic (e.g.,
		  the short description of beer is “Alcoholic drink made
		  from fermented cereal grains”). Nonetheless, a large
		  fraction of articles (ranging from 10.2% in Dutch to 99.7%
		  in Kazakh) have no short description yet, with detrimental
		  effects for millions of Wikipedia users. Motivated by this
		  problem, we introduce the novel task of automatically
		  generating short descriptions for Wikipedia articles and
		  propose Descartes, a multilingual model for tackling it.
		  Descartes integrates three sources of information to
		  generate an article description in a target language: the
		  text of the article in all its language versions, the
		  already-existing descriptions (if any) of the article in
		  other languages, and semantic type information obtained
		  from a knowledge graph. We evaluate a Descartes model
		  trained for handling 25 languages simultaneously, showing
		  that it beats baselines (including a strong
		  translation-based baseline) and performs on par with
		  monolingual models tailored for specific languages. A human
		  evaluation on three languages further shows that the
		  quality of Descartes’s descriptions is largely
		  indistinguishable from that of human-written descriptions;
		  e.g., 91.3% of our English descriptions (vs. 92.1% of
		  human-written descriptions) pass the bar for inclusion in
		  Wikipedia, suggesting that Descartes is ready for
		  production, with the potential to support human editors in
		  filling a major gap in today’s Wikipedia across
		  languages.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1446–1456},
  numpages	= {11},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3570991.3571028,
  author	= {Kumar, Suresh and Kumar P, Sreenivasa},
  title		= {Using domain ontology to identify consistent and
		  inconsistent cases from LSTM-generated transfer type AWPs},
  year		= {2023},
  isbn		= {9781450397971},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570991.3571028},
  doi		= {10.1145/3570991.3571028},
  booktitle	= {Proceedings of the 6th Joint International Conference on
		  Data Science &amp; Management of Data (10th ACM IKDD CODS
		  and 28th COMAD)},
  pages		= {289–290},
  numpages	= {2},
  location	= {Mumbai, India},
  series	= {CODS-COMAD '23}
}

@Article{	  10.1109/taslp.2021.3110126,
  author	= {Zhang, Ningyu and Ye, Hongbin and Deng, Shumin and Tan,
		  Chuanqi and Chen, Mosha and Huang, Songfang and Huang, Fei
		  and Chen, Huajun},
  title		= {Contrastive Information Extraction With Generative
		  Transformer},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3110126},
  doi		= {10.1109/TASLP.2021.3110126},
  abstract	= {Information extraction tasks such as entity relation
		  extraction and event extraction are of great importance for
		  natural language processing and knowledge graph
		  construction. In this paper, we revisit the end-to-end
		  information extraction task for sequence generation. Since
		  generative information extraction may struggle to capture
		  long-term dependencies and generate unfaithful triples, we
		  introduce a novel model, contrastive information extraction
		  with a generative transformer. Specifically, we introduce a
		  single shared transformer module for an
		  encoder-decoder-based generation. To generate faithful
		  results, we propose a novel triplet contrastive training
		  object. Moreover, we introduce two mechanisms to further
		  improve model performance (i.e., batch-wise dynamic
		  attention-masking and triple-wise calibration).
		  Experimental results on five datasets (i.e., NYT, WebNLG,
		  MIE, ACE-2005, and MUC-4) show that our approach achieves
		  better performance than
		  baselines.&lt;sup&gt;1&lt;/sup&gt;},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= sep,
  pages		= {3077–3088},
  numpages	= {12}
}

@InProceedings{	  10.1145/3487553.3524648,
  author	= {Garcia-Olano, Diego and Onoe, Yasumasa and Ghosh,
		  Joydeep},
  title		= {Improving and Diagnosing Knowledge-Based Visual Question
		  Answering via Entity Enhanced Knowledge Injection},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524648},
  doi		= {10.1145/3487553.3524648},
  abstract	= {Knowledge-Based Visual Question Answering (KBVQA) is a
		  bi-modal task requiring external world knowledge in order
		  to correctly answer a text question and associated image.
		  Recent single modality text work has shown knowledge
		  injection into pre-trained language models, specifically
		  entity enhanced knowledge graph embeddings, can improve
		  performance on downstream entity-centric tasks. In this
		  work, we empirically study how and whether such methods,
		  applied in a bi-modal setting, can improve an existing VQA
		  system’s performance on the KBVQA task. We experiment
		  with two large publicly available VQA datasets, (1) KVQA
		  which contains mostly rare Wikipedia entities and (2) OKVQA
		  which is less entity-centric and more aligned with common
		  sense reasoning. Both lack explicit entity spans, and we
		  study the effect of different weakly supervised and manual
		  methods for obtaining them. Additionally, we analyze how
		  recently proposed bi-modal and single modal attention
		  explanations are affected by the incorporation of such
		  entity enhanced representations. Our results show
		  substantially improved performance on the KBVQA task
		  without the need for additional costly pre-training, and we
		  provide insights for when entity knowledge injection helps
		  improve a model’s understanding. We provide code and
		  enhanced datasets for reproducibility1.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {705–715},
  numpages	= {11},
  keywords	= {entity learning, explainability, knowledge injection,
		  multi-modal learning, visual question answering, weak
		  supervision},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3545801.3545806,
  author	= {Su, Hailong and Di, Jin and Yin, Xinhong and Li, Xianbo},
  title		= {Research on Intelligent Recommendation of Science and
		  Technology Resource Data Based on Semantic Intelligence
		  Analysis},
  year		= {2022},
  isbn		= {9781450396097},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3545801.3545806},
  doi		= {10.1145/3545801.3545806},
  abstract	= {Aiming at the practical problems such as imperfect
		  semantic analysis function of recommendation service and
		  low sharing degree among scientific data in the existing
		  science and technology resources sharing, the center of
		  knowledge association algorithm with intelligence is built
		  through the mutual reflection evidence among different data
		  subjects such as talents, enterprises, platforms,
		  industries and scientific payoffs to realize multi-angle,
		  multi-dimensional and multi-association data portrait.
		  Through business understanding, data extraction, data
		  processing, feature extraction, model construction, model
		  deduction, model application, model evaluation and other
		  mining steps, the data mining algorithm center with
		  intelligence is established. Based on knowledge graph, an
		  intelligent recommendation model for scientific data is
		  proposed, and we construct vector model bank through
		  machine learning, and realize intelligent recommendation
		  and accurate pushing of scientific resources based on
		  massive data of scientific research such as papers,
		  patents, projects, and scientific reports. The problems of
		  data centralization and unification, data systematization
		  mapping, data application convenience, and data multiform
		  and multi-scene application are effectively solved in the
		  actual research work. By summarizing the theories,
		  technologies and methods related to data sharing and
		  application of S&amp;T resources, this study aims to
		  provide useful references for the wisdom upgrading of
		  S&amp;T resource sharing services and scientific and
		  technical information service model innovation under the
		  big data environment.},
  booktitle	= {Proceedings of the 7th International Conference on Big
		  Data and Computing},
  pages		= {29–36},
  numpages	= {8},
  keywords	= {data sharing, intelligent recommendation, science and
		  technology resources, semantic analysis},
  location	= {Shenzhen, China},
  series	= {ICBDC '22}
}

@Article{	  10.5555/3648699.3648939,
  author	= {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob
		  and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and
		  Barham, Paul and Chung, Hyung Won and Sutton, Charles and
		  Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and
		  Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek
		  and Barnes, Parker and Tay, Yi and Shazeer, Noam and
		  Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and
		  Hutchinson, Ben and Pope, Reiner and Bradbury, James and
		  Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin,
		  Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat,
		  Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia,
		  Xavier and Misra, Vedant and Robinson, Kevin and Fedus,
		  Liam and Zhou, Denny and Ippolito, Daphne and Luan, David
		  and Lim, Hyeontaek and Zoph, Barret and Spiridonov,
		  Alexander and Sepassi, Ryan and Dohan, David and Agrawal,
		  Shivani and Omernick, Mark and Dai, Andrew M. and Pillai,
		  Thanumalayan Sankaranarayana and Pellat, Marie and
		  Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and
		  Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and
		  Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat,
		  Orhan and Catasta, Michele and Wei, Jason and
		  Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and
		  Petrov, Slav and Fiedel, Noah},
  title		= {PaLM: scaling language modeling with pathways},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {JMLR.org},
  volume	= {24},
  number	= {1},
  issn		= {1532-4435},
  abstract	= {Large language models have been shown to achieve
		  remarkable performance across a variety of natural language
		  tasks using few-shot learning, which drastically reduces
		  the number of task-specific training examples needed to
		  adapt the model to a particular application. To further our
		  understanding of the impact of scale on few-shot learning,
		  we trained a 540- billion parameter, densely activated,
		  Transformer language model, which we call Pathways Language
		  Model (PaLM).We trained PaLM on 6144 TPU v4 chips using
		  Pathways, a new ML system which enables highly efficient
		  training across multiple TPU Pods. We demonstrate continued
		  benefits of scaling by achieving state-of-the-art few-shot
		  learning results on hundreds of language understanding and
		  generation benchmarks. On a number of these tasks, PaLM
		  540B achieves breakthrough performance, outperforming the
		  finetuned state-of-the-art on a suite of multi-step
		  reasoning tasks, and outperforming average human
		  performance on the recently released BIG-bench benchmark. A
		  significant number of BIG-bench tasks showed discontinuous
		  improvements from model scale, meaning that performance
		  steeply increased as we scaled to our largest model. PaLM
		  also has strong capabilities in multilingual tasks and
		  source code generation, which we demonstrate on a wide
		  array of benchmarks. We additionally provide a
		  comprehensive analysis on bias and toxicity, and study the
		  extent of training data memorization with respect to model
		  scale. Finally, we discuss the ethical considerations
		  related to large language models and discuss potential
		  mitigation strategies.},
  journal	= {J. Mach. Learn. Res.},
  month		= jan,
  articleno	= {240},
  numpages	= {113},
  keywords	= {large language models, few-shot learning, natural language
		  processing, scalable deep learning}
}

@InProceedings{	  10.1145/3459930.3469524,
  author	= {Mohan, Sunil and Angell, Rico and Monath, Nicholas and
		  McCallum, Andrew},
  title		= {Low resource recognition and linking of biomedical
		  concepts from a large ontology},
  year		= {2021},
  isbn		= {9781450384506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459930.3469524},
  doi		= {10.1145/3459930.3469524},
  abstract	= {Tools to explore scientific literature are essential for
		  scientists, especially in biomedicine, where about a
		  million new papers are published every year. Many such
		  tools provide users the ability to search for specific
		  entities (e.g. proteins, diseases) by tracking their
		  mentions in papers. PubMed, the most well known database of
		  biomedical papers, relies on human curators to add these
		  annotations. This can take several weeks for new papers,
		  and not all papers get tagged. Machine learning models have
		  been developed to facilitate the semantic indexing of
		  scientific papers. However their performance on the more
		  comprehensive ontologies of biomedical concepts does not
		  reach the levels of typical entity recognition problems
		  studied in NLP. In large part this is due to their low
		  resources, where the ontologies are large, there is a lack
		  of descriptive text defining most entities, and labeled
		  data can only cover a small portion of the ontology. In
		  this paper, we develop a new model that overcomes these
		  challenges by (1) generalizing to entities unseen at
		  training time, and (2) incorporating linking predictions
		  into the mention segmentation decisions. Our approach
		  achieves new state-of-the-art results for the UMLS ontology
		  in both traditional recognition/linking (+8 F1 pts) as well
		  as semantic indexing-based evaluation (+10 F1 pts).},
  booktitle	= {Proceedings of the 12th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {54},
  numpages	= {10},
  keywords	= {UMLS, biomedical concept recognition, deep learning, named
		  entity recognition and linking},
  location	= {Gainesville, Florida},
  series	= {BCB '21}
}

@InProceedings{	  10.1145/3503161.3547889,
  author	= {Ge, Jiannan and Xie, Hongtao and Min, Shaobo and Li,
		  Pandeng and Zhang, Yongdong},
  title		= {Dual Part Discovery Network for Zero-Shot Learning},
  year		= {2022},
  isbn		= {9781450392037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503161.3547889},
  doi		= {10.1145/3503161.3547889},
  abstract	= {Zero-Shot Learning (ZSL) aims to recognize unseen classes
		  by transferring knowledge from seen classes. Recent methods
		  focus on learning a common semantic space to align visual
		  and attribute information. However, they always over-relied
		  on provided attributes and ignored the category
		  discriminative information that contributes to accurate
		  unseen class recognition, resulting in weak
		  transferability. To this end, we propose a novel Dual Part
		  Discovery Network (DPDN) that considers both attribute and
		  category discriminative information by discovering
		  attribute-guided parts and category-guided parts
		  simultaneously to improve knowledge transfer. Specifically,
		  for attribute-guided parts discovery, DPDN can localize the
		  regions with specific attribute information and
		  significantly bridge the gap between visual and semantic
		  information guided by the given attributes. For
		  category-guided parts discovery, the local parts are
		  explored to discover other important regions that bring
		  latent crucial details ignored by attributes, with the
		  guidance of adaptive category prototypes. To better mine
		  the transferable knowledge, we impose class correlations
		  constraints to regularize the category prototypes. Finally,
		  attribute- and category-guided parts complement each other
		  and provide adequate discriminative subtle information for
		  more accurate unseen class recognition. Extensive
		  experimental results demonstrate that DPDN can discover
		  discriminative parts and outperform state-of-the-art
		  methods on three standard benchmarks.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Multimedia},
  pages		= {3244–3252},
  numpages	= {9},
  keywords	= {joint embedding, object recognition, zero-shot learning},
  location	= {Lisboa, Portugal},
  series	= {MM '22}
}

@InProceedings{	  10.1145/3603781.3603812,
  author	= {Chen, Yi and Song, Xingshen and Deng, Jinsheng and Cao,
		  Jihao},
  title		= {Few-shot Question Answering with Entity-Aware Prompt},
  year		= {2023},
  isbn		= {9798400700705},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603781.3603812},
  doi		= {10.1145/3603781.3603812},
  abstract	= {Providing simple task descriptions or prompts in natural
		  language for large pre-trained language models yields
		  impressive few-shot learning results in different tasks,
		  such as text classification, knowledge probing, machine
		  translation, and named entity recognition. In this paper,
		  we apply this idea to question-answering task to fine-tune
		  pre-trained language models by constructing entity-type
		  prompts. Specifically, we augment the context sequences
		  with semantic labels to enhance the understanding of
		  pre-trained models, and dynamically adjust the prompts via
		  intention recognition of the questions. Our proposition is
		  simple yet powerful over traditional fine-tune training
		  strategies and robust under few-shot conditions. The
		  contributions of our work are as follows: 1. We proposed a
		  few-shot learning method with entity-aware prompts for
		  question-answering tasks to fine-tune the pre-trained
		  language model. 2. Based on the SQuAD dataset, we extract a
		  subset with 1,131 samples containing different categories
		  of answer type, in which the answers to all questions are
		  entities. 3. Experiments on multiple pre-trained language
		  models validate that our method can effectively improve the
		  performance of few-shot learning of question-answering
		  tasks over the promptless ones.},
  booktitle	= {Proceedings of the 2023 4th International Conference on
		  Computing, Networks and Internet of Things},
  pages		= {185–190},
  numpages	= {6},
  keywords	= {Question Answering, entity awareness, prompt learning},
  location	= {Xiamen, China},
  series	= {CNIOT '23}
}

@Article{	  10.1145/3573201,
  author	= {Ma, Xuan and Yang, Xiaoshan and Xu, Changsheng},
  title		= {Multi-Source Knowledge Reasoning Graph Network for
		  Multi-Modal Commonsense Inference},
  year		= {2023},
  issue_date	= {July 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {4},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3573201},
  doi		= {10.1145/3573201},
  abstract	= {As a crucial part of natural language processing,
		  event-centered commonsense inference task has attracted
		  increasing attention. With a given observed event, the
		  intention and reaction of the people involved in the event
		  are required to be inferred with artificial intelligent
		  algorithms. To solve this problem, sequence-to-sequence
		  methods are widely studied, where the event is first
		  encoded into a specific representation and then decoded to
		  generate the results. However, all the existing methods
		  learn the event representation only with the textual
		  information, while the visual information is ignored, which
		  is actually helpful for the commonsense reference. In this
		  article, we first define a new task of multi-modal
		  commonsense reference with both textual and visual
		  information. A new event-centered multi-modal dataset is
		  also provided. Then we propose a multi-source knowledge
		  reasoning graph network to solve this task, where three
		  kinds of relational knowledge are considered. Multi-modal
		  correlations are learned to get the event’s multi-modal
		  representation from a global perspective. Intra-event
		  object relations are explored to capture the fine-grained
		  event feature with an object graph. Inter-event semantic
		  relations are also explored through the external knowledge
		  to understand the semantic associations among events with
		  an event graph. We conduct extensive experiments on the new
		  dataset, and the results show the effectiveness of our
		  method.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= mar,
  articleno	= {141},
  numpages	= {17},
  keywords	= {Knowledge reasoning, multi-modal commonsense inference,
		  graph neural network}
}

@InProceedings{	  10.1145/3486713.3486730,
  author	= {Koutsomitropoulos, Dimitrios},
  title		= {Validating Ontology-based Annotations of Biomedical
		  Resources using Zero-shot Learning},
  year		= {2021},
  isbn		= {9781450385107},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486713.3486730},
  doi		= {10.1145/3486713.3486730},
  abstract	= {Authoritative thesauri in the form of web ontologies offer
		  a sound representation of domain knowledge and can act as a
		  reference point for automated semantic tagging. On the
		  other hand, current language models achieve to capture
		  contextualized semantics of text corpora and can be
		  leveraged towards this goal. We present an approach for
		  injecting subject annotations using query term expansion
		  against such ontologies in the biomedical domain. For the
		  user to have an indication of the usefulness of these
		  suggestions we further propose an online method for
		  validating the quality of annotations using NLI models such
		  as BART and XLM-R. To circumvent training barriers posed by
		  very large label sets and scarcity of data we rely on
		  zero-shot classification and show that semantic matching
		  can contribute above-average thematic annotations. Also, a
		  web-based validation service can be attractive for human
		  curators vs. the overhead of pretraining large,
		  domain-tailored classification models.},
  booktitle	= {The 12th International Conference on Computational
		  Systems-Biology and Bioinformatics},
  pages		= {37–43},
  numpages	= {7},
  keywords	= {MeSH, Thesaurus, biomedical indexing, classification,
		  language models, machine learning, semantic matching},
  location	= {Virtual (GMT+7 Bangkok Time), Thailand},
  series	= {CSBio2021}
}

@InProceedings{	  10.5555/3545946.3598706,
  author	= {Xu, Weilai and Charles, Fred and Hargood, Charlie},
  title		= {Generating Stylistic and Personalized Dialogues for
		  Virtual Agents in Narratives},
  year		= {2023},
  isbn		= {9781450394321},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Virtual agents interact with each other through dialogues
		  in various types of narratives (e.g. narrative films). In
		  this paper, we propose an approach on the basis of DialoGPT
		  pre-trained language model, which explores the impact of
		  dialogue generation with different levels of agents'
		  personalities derived from narrative films based on the
		  Big-Five model, as well as with three different embedding
		  methods. From the experimental results using automatic
		  metrics and human judgments, we investigate and analyze the
		  impact of different settings on narrative dialogue
		  generation. Also, we demonstrate that our approach is able
		  to generate dialogues with increased variety that correctly
		  reflect the corresponding target personality.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Autonomous Agents and Multiagent Systems},
  pages		= {737–746},
  numpages	= {10},
  keywords	= {deep learning, dialogue generation, narratives, virtual
		  agents},
  location	= {London, United Kingdom},
  series	= {AAMAS '23}
}

@InProceedings{	  10.1145/3459637.3482387,
  author	= {Wisniewski, Dawid and Potoniec, Jedrzej and Lawrynowicz,
		  Agnieszka},
  title		= {SeeQuery: An Automatic Method for Recommending
		  Translations of Ontology Competency Questions into
		  SPARQL-OWL},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482387},
  doi		= {10.1145/3459637.3482387},
  abstract	= {Ontology authoring is a complicated and error-prone
		  process since the knowledge being modeled is expressed
		  using logic-based formalisms, in which logical consequences
		  of the knowledge have to be foreseen. To make that process
		  easier, competency questions (CQs), being questions
		  expressed in natural language are often stated to trace
		  both the correctness and completeness of the ontology at a
		  given time. However, CQs have to be translated into a
		  formal language, like ontology query language (SPARQL-OWL),
		  to query the ontology. Since the translation step is
		  time-consuming and requires familiarity with the query
		  language used, in this paper, we propose an automatic
		  method named SeeQuery, which recommends SPARQL-OWL queries
		  being translations of CQs stated against a given ontology.
		  It consists of a pipeline of transformations based on
		  template matching and filling, being motivated by the
		  biggest to date publicly available CQ to SPARQL-OWL
		  datasets. We provide a detailed description of SeeQuery and
		  evaluate the method on a separate set of 2 ontologies with
		  their CQs. It is, to date, the only automatic method
		  available for recommending SPARQL-OWL queries out of CQs.
		  The source code of SeeQuery is available at:
		  https://github.com/dwisniewski/SeeQuery.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2119–2128},
  numpages	= {10},
  keywords	= {automatic translation, competency questions, ontology
		  authoring, semantic similarity, sparql-owl},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3459637.3481902,
  author	= {Zhang, Chao and Zhou, Jingbo and Zang, Xiaoling and Xu,
		  Qing and Yin, Liang and He, Xiang and Liu, Lin and Xiong,
		  Haoyi and Dou, Dejing},
  title		= {CHASE: Commonsense-Enriched Advertising on Search Engine
		  with Explicit Knowledge},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3481902},
  doi		= {10.1145/3459637.3481902},
  abstract	= {While online advertising is one of the major sources of
		  income for search engines, pumping up the incomes from
		  business advertisements while ensuring the user experience
		  becomes a challenging but emerging area. Designing
		  high-quality advertisements with persuasive content has
		  been proved as a way to increase revenues through improving
		  the Click-Through Rate (CTR). However, it is difficult to
		  scale up the design of high-quality ads, due to the lack of
		  automation in creativity. In this paper, we present
		  Commonsense-Enriched Advertisement on Search Engine (CHASE)
		  --- a system for the automatic generation of persuasive
		  ads. CHASE adopts a specially designed language model that
		  fuses the keywords, commonsense-related texts, and
		  marketing contents to generate persuasive advertisements.
		  Specifically, the language model has been pre-trained using
		  massive contents of explicit knowledge and fine-tuned with
		  well-constructed quasi-parallel corpora with effective
		  control of the proportion of commonsense in the generated
		  ads and fitness to the ads' keywords. The effectiveness of
		  the proposed method CHASE has been verified by real-world
		  web traffics for search and manual evaluation. In A/B
		  tests, the advertisements generated by CHASE would bring
		  11.13% CTR improvement. The proposed model has been
		  deployed to cover three advertisement domains (which are
		  kid education, psychological counseling, and beauty
		  e-commerce) at Baidu, the world's largest Chinese search
		  engine, with adding revenue of about 1 million RMB (Chinese
		  Yuan) per day.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4352–4361},
  numpages	= {10},
  keywords	= {advertising, commonsense-enriched advertisement,
		  description generation, persuasive advertisement,
		  quasi-parallel corpora},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3539618.3594250,
  author	= {Liao, Lizi and Yang, Grace Hui and Shah, Chirag},
  title		= {Proactive Conversational Agents in the Post-ChatGPT
		  World},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3594250},
  doi		= {10.1145/3539618.3594250},
  abstract	= {ChatGPT and similar large language model (LLM) based
		  conversational agents have brought shock waves to the
		  research world. Although astonished by their human-like
		  performance, we find they share a significant weakness with
		  many other existing conversational agents in that they all
		  take a passive approach in responding to user queries. This
		  limits their capacity to understand the users and the task
		  better and to offer recommendations based on a broader
		  context than a given conversation. Proactiveness is still
		  missing in these agents, including their ability to
		  initiate a conversation, shift topics, or offer
		  recommendations that take into account a more extensive
		  context. To address this limitation, this tutorial reviews
		  methods for equipping conversational agents with proactive
		  interaction abilities.The full-day tutorial is divided into
		  four parts, including multiple interactive exercises. We
		  will begin the tutorial with an interactive exercise and
		  cover the design of existing conversational systems
		  architecture and challenges. The content includes coverage
		  of LLM-based recent advancements such as ChatGPT and Bard,
		  along with reinforcement learning with human feedback
		  (RLHF) technique. Then we will introduce the concept of
		  proactive conversation agents and preset recent
		  advancements in proactiveness of conversational agents,
		  including actively driving conversations by asking
		  questions, topic shifting, and methods that support
		  strategic planning of conversation. Next, we will discuss
		  important issues in conversational responses' quality
		  control, including safety, appropriateness, language
		  detoxication, hallucination, and alignment. Lastly, we will
		  launch another interactive exercise and discussion with the
		  audience to arrive at concluding remarks, prospecting open
		  challenges and new directions. By exploring new techniques
		  for enhancing conversational agents' proactive behavior to
		  improve user engagement, this tutorial aims to help
		  researchers and practitioners develop more effective
		  conversational agents that can better understand and
		  respond to user needs proactively and safely.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3452–3455},
  numpages	= {4},
  keywords	= {conversational ai, conversational search, proactive
		  conversation},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3543507.3583541,
  author	= {Mosharrof, Adib and Fereidouni, Moghis and Siddique,
		  A.B.},
  title		= {Toward Open-domain Slot Filling via Self-supervised
		  Co-training},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583541},
  doi		= {10.1145/3543507.3583541},
  abstract	= {Slot filling is one of the critical tasks in modern
		  conversational systems. The majority of existing literature
		  employs supervised learning methods, which require labeled
		  training data for each new domain. Zero-shot learning and
		  weak supervision approaches, among others, have shown
		  promise as alternatives to manual labeling. Nonetheless,
		  these learning paradigms are significantly inferior to
		  supervised learning approaches in terms of performance. To
		  minimize this performance gap and demonstrate the
		  possibility of open-domain slot filling, we propose a
		  Self-supervised Co-training framework, called , that
		  requires zero in-domain manually labeled training examples
		  and works in three phases. Phase one acquires two sets of
		  complementary pseudo labels automatically. Phase two
		  leverages the power of the pre-trained language model BERT,
		  by adapting it for the slot filling task using these sets
		  of pseudo labels. In phase three, we introduce a
		  self-supervised co-training mechanism, where both models
		  automatically select high-confidence soft labels to further
		  improve the performance of the other in an iterative
		  fashion. Our thorough evaluations show that outperforms
		  state-of-the-art models by 45.57% and 37.56% on SGD and
		  MultiWoZ datasets, respectively. Moreover, our proposed
		  framework achieves comparable performance when compared to
		  state-of-the-art fully supervised models.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1928–1937},
  numpages	= {10},
  keywords	= {co-training, open-domain slot filling, weak supervision.},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3442381.3450117,
  author	= {Fang, Tianqing and Zhang, Hongming and Wang, Weiqi and
		  Song, Yangqiu and He, Bin},
  title		= {DISCOS: Bridging the Gap between Discourse Knowledge and
		  Commonsense Knowledge},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450117},
  doi		= {10.1145/3442381.3450117},
  abstract	= {Commonsense knowledge is crucial for artificial
		  intelligence systems to understand natural language.
		  Previous commonsense knowledge acquisition approaches
		  typically rely on human annotations (for example, ATOMIC)
		  or text generation models (for example, COMET.) Human
		  annotation could provide high-quality commonsense
		  knowledge, yet its high cost often results in relatively
		  small scale and low coverage. On the other hand, generation
		  models have the potential to automatically generate more
		  knowledge. Nonetheless, machine learning models often fit
		  the training data well and thus struggle to generate
		  high-quality novel knowledge. To address the limitations of
		  previous approaches, in this paper, we propose an
		  alternative commonsense knowledge acquisition framework
		  DISCOS (from DIScourse to COmmonSense), which automatically
		  populates expensive complex commonsense knowledge to more
		  affordable linguistic knowledge resources. Experiments
		  demonstrate that we can successfully convert discourse
		  knowledge about eventualities from ASER, a large-scale
		  discourse knowledge graph, into if-then commonsense
		  knowledge defined in ATOMIC without any additional
		  annotation effort. Further study suggests that DISCOS
		  significantly outperforms previous supervised approaches in
		  terms of novelty and diversity with comparable quality. In
		  total, we can acquire 3.4M ATOMIC-like inferential
		  commonsense knowledge by populating ATOMIC on the core part
		  of ASER. Codes and data are available at
		  https://github.com/HKUST-KnowComp/DISCOS-commonsense.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {2648–2659},
  numpages	= {12},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3543434.3543590,
  author	= {Ortiz-Rodriguez, Fernando and Tiwari, Sanju and Panchal,
		  Ronak and Medina-Quintero, Jose Melchor and Barrera,
		  Ruben},
  title		= {MEXIN: Multidialectal Ontology supporting NLP approach to
		  improve government electronic communication with the
		  Mexican Ethnic Groups},
  year		= {2022},
  isbn		= {9781450397490},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543434.3543590},
  doi		= {10.1145/3543434.3543590},
  abstract	= {The government services usually target all citizens, but
		  sometimes physical services nor technology-based services
		  do not cover all people. This research aims to tackle
		  services given to underrepresented citizens in Mexico
		  (Indigenous people) and apply NLP techniques supported by
		  ontologies to achieve accurate translation to most dialects
		  spoken in Mexico. The scope of this paper only tests with
		  Mayan dialect spoken primarily in the Mexican peninsula.},
  booktitle	= {Proceedings of the 23rd Annual International Conference on
		  Digital Government Research},
  pages		= {461–463},
  numpages	= {3},
  keywords	= {NLP, Ontologies, Semantic Web, eGovernment},
  location	= {Virtual Event, Republic of Korea},
  series	= {dg.o '22}
}

@InProceedings{	  10.1145/3447548.3467144,
  author	= {Xia, Yuan and Wang, Chunyu and Shi, Zhenhui and Zhou,
		  Jingbo and Lu, Chao and Huang, Haifeng and Xiong, Hui},
  title		= {Medical Entity Relation Verification with Large-scale
		  Machine Reading Comprehension},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467144},
  doi		= {10.1145/3447548.3467144},
  abstract	= {Medical entity relation verification is a crucial step to
		  build a practical and enterprise medical knowledge graph
		  (MKG) because high-precision medical entity relation is a
		  key requirement for many MKG-based applications. Existing
		  relation verification approaches for general knowledge
		  graphs are not designed for considering medical domain
		  knowledge, although it is central to achieve high-quality
		  entity relation verification for MKG. To this end, in this
		  paper, we introduce a system for medical entity relation
		  verification with large-scale machine reading
		  comprehension. The proposed system is tailored to overcome
		  the unique challenges of medical relation verification
		  including high variants of medical terms, the high
		  difficulty of evidence searching in complex medical
		  documents, and the lack of evidence labels for supervision.
		  To deal with the problem of variants of medical terms, we
		  introduce a synonym-aware retrieve model to retrieve the
		  potential evidence implicitly verifying the given claim. To
		  better utilize the medical domain knowledge, a
		  relation-aware evidence detector and a medical
		  ontology-enhanced aggregator are developed to improve the
		  performance of the relation verification module. Moreover,
		  to overcome the challenge of providing high-quality
		  evidence due to the lack of labels, we introduce an
		  interactive collaborative-training method to iteratively
		  improve the evidence accuracy. Finally, we conduct
		  extensive experiments to demonstrate that the performance
		  of our proposed system is superior to all comparable
		  models. We also demonstrate that our system can
		  significantly reduce the annotation time by medical experts
		  in real-world verification tasks. It can help to improve
		  the efficiency by nearly 300%. In particular, our system
		  has been embedded into the Baidu Clinical Decision Support
		  System.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {3765–3774},
  numpages	= {10},
  keywords	= {clinical decision support, fact verification, relation
		  extraction},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@InProceedings{	  10.1145/3534678.3539020,
  author	= {Ren, Houxing and Wang, Jingyuan and Zhao, Wayne Xin},
  title		= {Generative Adversarial Networks Enhanced Pre-training for
		  Insufficient Electronic Health Records Modeling},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539020},
  doi		= {10.1145/3534678.3539020},
  abstract	= {In recent years, automatic computational systems based on
		  deep learning are widely used in medical fields, such as
		  automatic diagnosing and disease prediction. Most of these
		  systems are designed for data sufficient scenarios.
		  However, due to the disease rarity or privacy, the medical
		  data are always insufficient. When applying these
		  data-hungry deep learning models with insufficient data, it
		  is likely to lead to issues of over-fitting and cause
		  serious performance problems. Many data augmentation
		  methods have been proposed to solve the data insufficiency
		  problem, such as using GAN (Generative Adversarial
		  Networks) to generate training data. However, the augmented
		  data usually contains lots of noise. Directly using them to
		  train sensitive medical models is very difficult to achieve
		  satisfactory results.To overcome this problem, we propose a
		  novel deep model learning method for insufficient EHR
		  (Electronic Health Record) data modeling, namely GRACE,
		  which stands GeneRative Adversarial networks enhanCed
		  prE-training. In the method, we propose an
		  item-relation-aware GAN to capture changing trends and
		  correlations among data for generating high-quality EHR
		  records. Furthermore, we design a pre-training mechanism
		  consisting of a masked records prediction task and a
		  real-fake contrastive learning task to learn
		  representations for EHR data using both generated and real
		  data. After the pre-training, only the representations of
		  real data is used to train the final prediction model. In
		  this way, we can fully exploit useful information in
		  generated data through pre-training, and also avoid the
		  problems caused by directly using noisy generated data to
		  train the final prediction model. The effectiveness of the
		  proposed method is evaluated using extensive experiments on
		  three healthcare-related real-world datasets. We also
		  deploy our method in a maternal and child health care
		  hospital for the online test. Both offline and online
		  experimental results demonstrate the effectiveness of the
		  proposed method. We believe doctors and patients can
		  benefit from our effective learning method in various
		  healthcare-related applications.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3810–3818},
  numpages	= {9},
  keywords	= {healthcare informatics, pre-training, representation
		  learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3591106.3592223,
  author	= {Nebbia, Giacomo and Kovashka, Adriana},
  title		= {Hypernymization of named entity-rich captions for
		  grounding-based multi-modal pretraining},
  year		= {2023},
  isbn		= {9798400701788},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3591106.3592223},
  doi		= {10.1145/3591106.3592223},
  abstract	= {Named entities are ubiquitous in text that naturally
		  accompanies images, especially in domains such as news or
		  Wikipedia articles. In previous work, named entities have
		  been identified as a likely reason for low performance of
		  image-text retrieval models pretrained on Wikipedia and
		  evaluated on named entities-free benchmark datasets.
		  Because they are rarely mentioned, named entities could be
		  challenging to model. They also represent missed learning
		  opportunities for self-supervised models: the link between
		  named entity and object in the image may be missed by the
		  model, but it would not be if the object were mentioned
		  using a more common term. In this work, we investigate
		  hypernymization as a way to deal with named entities for
		  pretraining grounding-based multi-modal models and for
		  fine-tuning on open-vocabulary detection. We propose two
		  ways to perform hypernymization: (1) a “manual”
		  pipeline relying on a comprehensive ontology of concepts,
		  and (2) a “learned” approach where we train a language
		  model to learn to perform hypernymization. We run
		  experiments on data from Wikipedia and from The New York
		  Times. We report improved pretraining performance on
		  objects of interest following hypernymization, and we show
		  the promise of hypernymization on open-vocabulary
		  detection, specifically on classes not seen during
		  training.},
  booktitle	= {Proceedings of the 2023 ACM International Conference on
		  Multimedia Retrieval},
  pages		= {67–75},
  numpages	= {9},
  keywords	= {grounding, hypernymization, named entities,
		  open-vocabulary detection},
  location	= {Thessaloniki, Greece},
  series	= {ICMR '23}
}

@InProceedings{	  10.1145/3444370.3444559,
  author	= {Hou, Ruihui and Li, Hanhao and Feng, Honghai and Li,
		  Yunpeng and Li, Jun and Shen, Yatian},
  title		= {An entity relation extraction algorithm based on
		  BERT(wwm-ext)-BiGRU-Attention},
  year		= {2021},
  isbn		= {9781450387828},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3444370.3444559},
  doi		= {10.1145/3444370.3444559},
  abstract	= {Entity relation extraction is one of the basic steps of
		  knowledge Graph. It identifies the relations between
		  entities. A BERT-Bidirectional gated recurrent
		  units-Attention mechanism (BERT-BiGRU-Attention) model has
		  been proposed, but it is based on the single Chinese
		  character based masking. Due to the complexity of Chinese
		  grammar structure and the semantic diversity, a
		  BERT(wwm-ext) was proposed based on the whole Chinese word
		  masking. In this paper we propose a
		  BERT(wwm-ext)-BiGRU-Attention model. The experimental
		  result shows that for the purpose of entity relation
		  extraction the precision is 93.60%, recall rate is 91.90%,
		  and F1 value 92.53%, which are higher than the
		  BERT-BiGRU-Attention and its precision is 91.80%, recall
		  rate is 90.16%, and F1 value is 90.97%. Since
		  BERT(wwm-ext)-BIGRU-Attention gets higher precision, F1
		  value, and higher recall rate, it has better effects on the
		  Chinese entity relation extraction tasks.},
  booktitle	= {Proceedings of the 2020 International Conference on
		  Cyberspace Innovation of Advanced Technologies},
  pages		= {130–135},
  numpages	= {6},
  keywords	= {Attention mechanism, Bidirectional gated recurrent units,
		  Entity relation extraction, Natural language processing},
  location	= {Guangzhou, China},
  series	= {CIAT 2020}
}

@InProceedings{	  10.1145/3404835.3462870,
  author	= {Ge, Congcong and Liu, Xiaoze and Chen, Lu and Zheng,
		  Baihua and Gao, Yunjun},
  title		= {Make It Easy: An Effective End-to-End Entity Alignment
		  Framework},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3462870},
  doi		= {10.1145/3404835.3462870},
  abstract	= {Entity alignment (EA) is a prerequisite for enlarging the
		  coverage of a unified knowledge graph. Previous EA
		  approaches either restrain the performance due to
		  inadequate information utilization or need labor-intensive
		  pre-processing to get external or reliable information to
		  perform the EA task. This paper proposes EASY, an effective
		  end-to-end EA framework, which is able to (i) remove the
		  labor-intensive pre-processing by fully discovering the
		  name information provided by the entities themselves; and
		  (ii) jointly fuse the features captured by the names of
		  entities and the structural information of the graph to
		  improve the EA results. Specifically, EASY first introduces
		  NEAP, a highly effective name-based entity alignment
		  procedure, to obtain an initial alignment that has
		  reasonable accuracy and meanwhile does not require much
		  memory consumption or any complex training process. Then,
		  EASY invokes SRS, a novel structure-based refinement
		  strategy, to iteratively correct the misaligned entities
		  generated by NEAP to further enhance the entity alignment.
		  Extensive experiments demonstrate the superiority of our
		  proposed EASY with significant improvement against 13
		  existing state-of-the-art competitors.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {777–786},
  numpages	= {10},
  keywords	= {entity alignment, entity name, graph structure, iterative
		  training},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3477495.3531809,
  author	= {Yan, Guojun and Pei, Jiahuan and Ren, Pengjie and Ren,
		  Zhaochun and Xin, Xin and Liang, Huasheng and de Rijke,
		  Maarten and Chen, Zhumin},
  title		= {ReMeDi: Resources for Multi-domain, Multi-service, Medical
		  Dialogues},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531809},
  doi		= {10.1145/3477495.3531809},
  abstract	= {AcpMDS aim to assist doctors and patients with a range of
		  professional medical services, i.e., diagnosis, treatment
		  and consultation. The development of acpMDS is hindered
		  because of a lack of resources. In particular.
		  beginenumerate* [label=(arabic*) ] item there is no dataset
		  with large-scale medical dialogues that covers multiple
		  medical services and contains fine-grained medical labels
		  (i.e., intents, actions, slots, values), and item there is
		  no set of established benchmarks for acpMDS for
		  multi-domain, multi-service medical dialogues.
		  endenumerate* In this paper, we present acsReMeDi, a set of
		  aclReMeDi acusedReMeDi. \O{}urResources consists of two
		  parts, the \O{}urResources dataset and the \O{}urResources
		  benchmarks. The \O{}urResources dataset contains 96,965
		  conversations between doctors and patients, including 1,557
		  conversations with fine-gained labels. It covers 843 types
		  of diseases, 5,228 medical entities, and 3 specialties of
		  medical services across 40 domains. To the best of our
		  knowledge, the \O{}urResources dataset is the only medical
		  dialogue dataset that covers multiple domains and services,
		  and has fine-grained medical labels. The second part of the
		  \O{}urResources resources consists of a set of
		  state-of-the-art models for (medical) dialogue generation.
		  The \O{}urResources benchmark has the following methods:
		  beginenumerate* item pretrained models (i.e., BERT-WWM,
		  BERT-MED, GPT2, and MT5) trained, validated, and tested on
		  the \O{}urResources dataset, and item a acfSCL method to
		  expand the \O{}urResources dataset and enhance the training
		  of the state-of-the-art pretrained models. endenumerate* We
		  describe the creation of the \O{}urResources dataset, the
		  \O{}urResources benchmarking methods, and establish
		  experimental results using the \O{}urResources benchmarking
		  methods on the \O{}urResources dataset for future research
		  to compare against. With this paper, we share the dataset,
		  implementations of the benchmarks, and evaluation scripts.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3013–3024},
  numpages	= {12},
  keywords	= {dialogue benchmarks, dialogue dataset, medical dialogues},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@Article{	  10.1145/3631504.3631518,
  author	= {Amer-Yahia, Sihem and Bonifati, Angela and Chen, Lei and
		  Li, Guoliang and Shim, Kyuseok and Xu, Jianliang and Yang,
		  Xiaochun},
  title		= {From Large Language Models to Databases and Back: A
		  Discussion on Research and Education},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {52},
  number	= {3},
  issn		= {0163-5808},
  url		= {https://doi.org/10.1145/3631504.3631518},
  doi		= {10.1145/3631504.3631518},
  abstract	= {In recent years, large language models (LLMs) have
		  garnered increasing attention from both academia and
		  industry due to their potential to facilitate natural
		  language processing (NLP) and generate highquality text.
		  Despite their benefits, however, the use of LLMs is raising
		  concerns about the reliability of knowledge extraction. The
		  combination of DB research and data science has advanced
		  the state of the art in solving real-world problems, such
		  as merchandise recommendation and hazard prevention [30].
		  In this discussion, we explore the challenges and
		  opportunities related to LLMs in DB and data science
		  research and education.},
  journal	= {SIGMOD Rec.},
  month		= nov,
  pages		= {49–56},
  numpages	= {8}
}

@InProceedings{	  10.1145/3622896.3622920,
  author	= {Cui, Ronghui and Li, Xudong},
  title		= {A Comprehensive Survey on Text Filling Algorithms: A
		  Research Review},
  year		= {2023},
  isbn		= {9798400708190},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3622896.3622920},
  doi		= {10.1145/3622896.3622920},
  abstract	= {Starting from the concept, application scenarios and
		  research significance of text filling, this paper divides
		  text filling algorithms into two categories: traditional
		  methods and deep learning, summarizes the development
		  process of text filling algorithms in detail, focuses on
		  describing the algorithm principles of text filling
		  algorithms in recent years, and summarizes and evaluates
		  text filling algorithms from the aspects of experimental
		  results, advantages and disadvantages. In addition, this
		  paper details the most commonly used text datasets and the
		  most popular evaluation indicators - BLEU and Perplexity.
		  Finally, this paper summarizes the research on text filling
		  algorithm and looks forward to the future development
		  trend. This paper aims to provide researchers with a
		  comprehensive review of text filling algorithms to guide
		  and prospect future development.},
  booktitle	= {Proceedings of the 2023 4th International Conference on
		  Control, Robotics and Intelligent System},
  pages		= {141–147},
  numpages	= {7},
  keywords	= {Deep learning, Generative adversarial network (GAN),
		  Recurrent neural network (RNN), Text filling},
  location	= {Guangzhou, China},
  series	= {CCRIS '23}
}

@InProceedings{	  10.1145/3544548.3581566,
  author	= {Liu, Xingyu "Bruce" and Kirilyuk, Vladimir and Yuan,
		  Xiuxiu and Olwal, Alex and Chi, Peggy and Chen, Xiang
		  "Anthony" and Du, Ruofei},
  title		= {Visual Captions: Augmenting Verbal Communication with
		  On-the-fly Visuals},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3581566},
  doi		= {10.1145/3544548.3581566},
  abstract	= {Video conferencing solutions like Zoom, Google Meet, and
		  Microsoft Teams are becoming increasingly popular for
		  facilitating conversations, and recent advancements such as
		  live captioning help people better understand each other.
		  We believe that the addition of visuals based on the
		  context of conversations could further improve
		  comprehension of complex or unfamiliar concepts. To explore
		  the potential of such capabilities, we conducted a
		  formative study through remote interviews (N=10) and
		  crowdsourced a dataset of over 1500 sentence-visual pairs
		  across a wide range of contexts. These insights informed
		  Visual Captions, a real-time system that integrates with a
		  video conferencing platform to enrich verbal communication.
		  Visual Captions leverages a fine-tuned large language model
		  to proactively suggest relevant visuals in open-vocabulary
		  conversations. We present findings from a lab study (N=26)
		  and an in-the-wild case study (N=10), demonstrating how
		  Visual Captions can help improve communication through
		  visual augmentation in various scenarios.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {108},
  numpages	= {20},
  keywords	= {AI agent, augmented communication, augmented reality,
		  collaborative work, dataset, large language models, online
		  meeting, text-to-visual, video-mediated communication},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@InProceedings{	  10.1145/3587259.3627561,
  author	= {Chhikara, Prateek and Zhang, Jiarui and Ilievski, Filip
		  and Francis, Jonathan and Ma, Kaixin},
  title		= {Knowledge-enhanced Agents for Interactive Text Games},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627561},
  doi		= {10.1145/3587259.3627561},
  abstract	= {Communication via natural language is a key aspect of
		  machine intelligence, and it requires computational models
		  to learn and reason about world concepts, with varying
		  levels of supervision. Significant progress has been made
		  on fully-supervised non-interactive tasks, such as
		  question-answering and procedural text understanding. Yet,
		  various sequential interactive tasks, as in text-based
		  games, have revealed limitations of existing approaches in
		  terms of coherence, contextual awareness, and their ability
		  to learn effectively from the environment. In this paper,
		  we propose a knowledge-injection framework for improved
		  functional grounding of agents in text-based games.
		  Specifically, we consider two forms of domain knowledge
		  that we inject into learning-based agents: memory of
		  previous correct actions and affordances of relevant
		  objects in the environment. Our framework supports two
		  representative model classes: reinforcement learning agents
		  and language model agents. Furthermore, we devise multiple
		  injection strategies for the above domain knowledge types
		  and agent architectures, including injection via knowledge
		  graphs and augmentation of the existing input encoding
		  strategies. We experiment with four models on the 10 tasks
		  in the ScienceWorld&nbsp;text-based game environment, to
		  illustrate the impact of knowledge injection on various
		  model configurations and challenging task settings. Our
		  findings provide crucial insights into the interplay
		  between task properties, model architectures, and domain
		  knowledge for interactive contexts.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {157–165},
  numpages	= {9},
  keywords	= {Interactive Task Learning, Knowledge Injection, Natural
		  Language Communication, Text-based Games},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@InProceedings{	  10.1145/3500931.3500939,
  author	= {Gao, Feng and Zhou, LunSheng and Gu, JinGuang},
  title		= {Entity Pair Recognition using Semantic Enrichment and
		  Adversarial Training for Chinese Drug Knowledge
		  Extraction},
  year		= {2021},
  isbn		= {9781450395588},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3500931.3500939},
  doi		= {10.1145/3500931.3500939},
  abstract	= {Existing knowledge extraction methods in pharmacy often
		  use natural language processing tools and deep learning
		  model to identify drug entities and extract their
		  relationships from drug instructions, thus obtaining
		  drug-drug or drug-disease knowledge. However, sentences in
		  drug instructions may contain multiple drug-related
		  entities, and existing methods lack the capability of
		  identifying valid the "drug-drug" or "drug-disease" entity
		  pairs. This will introduce significant noise data in the
		  subsequent tasks such as entity relationship extraction and
		  knowledge graph construction. Meanwhile, some mentions in
		  the sentence can have hierarchical relations even if they
		  do not form valid entity pairs, such information is also
		  crucial to knowledge extraction. To solve these two
		  problems, this paper proposes an entity pair verification
		  model based on entity semantic enhancement and adversarial
		  training. Through the experiment on more than 2000 kinds of
		  drug instructions data, the experimental results show that
		  the F1 value of the model for entity pair verification is
		  up to 98.65%, which is up to 9.37% compared with the
		  existing methods.},
  booktitle	= {Proceedings of the 2nd International Symposium on
		  Artificial Intelligence for Medicine Sciences},
  pages		= {35–42},
  numpages	= {8},
  keywords	= {entity pair verification, knowledge induction, medical
		  field, subclass and hyponym},
  location	= {Beijing, China},
  series	= {ISAIMS '21}
}

@Article{	  10.1145/3436819,
  author	= {Wang, Yu and Sun, Yining and Ma, Zuchang and Gao, Lisheng
		  and Xu, Yang},
  title		= {A Hybrid Model for Named Entity Recognition on Chinese
		  Electronic Medical Records},
  year		= {2021},
  issue_date	= {March 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3436819},
  doi		= {10.1145/3436819},
  abstract	= {Electronic medical records (EMRs) contain valuable
		  information about the patients, such as clinical symptoms,
		  diagnostic results, and medications. Named entity
		  recognition (NER) aims to recognize entities from
		  unstructured text, which is the initial step toward the
		  semantic understanding of the EMRs. Extracting medical
		  information from Chinese EMRs could be a more complicated
		  task because of the difference between English and Chinese.
		  Some researchers have noticed the importance of Chinese NER
		  and used the recurrent neural network or convolutional
		  neural network (CNN) to deal with this task. However, it is
		  interesting to know whether the performance could be
		  improved if the advantages of the RNN and CNN can be both
		  utilized. Moreover, RoBERTa-WWM, as a pre-training model,
		  can generate the embeddings with word-level features, which
		  is more suitable for Chinese NER compared with Word2Vec. In
		  this article, we propose a hybrid model. This model first
		  obtains the entities identified by bidirectional long
		  short-term memory and CNN, respectively, and then uses two
		  hybrid strategies to output the final results relying on
		  these entities. We also conduct experiments on raw medical
		  records from real hospitals. This dataset is provided by
		  the China Conference on Knowledge Graph and Semantic
		  Computing in 2019 (CCKS 2019). Results demonstrate that the
		  hybrid model can improve performance significantly.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {35},
  numpages	= {12},
  keywords	= {Named entity recognition, Chinese electronic medical
		  records, neural networks, hybrid models}
}

@Article{	  10.1109/taslp.2021.3058616,
  author	= {Zhang, Zhuosheng and Li, Junlong and Zhao, Hai},
  title		= {Multi-Turn Dialogue Reading Comprehension With Pivot Turns
		  and Knowledge},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3058616},
  doi		= {10.1109/TASLP.2021.3058616},
  abstract	= {Multi-turn dialogue reading comprehension aims to teach
		  machines to read dialogue contexts and solve tasks such as
		  response selection and answering questions. The major
		  challenges involve noisy history contexts and especial
		  prerequisites of commonsense knowledge that is unseen in
		  the given material. Existing works mainly focus on context
		  and response matching approaches. This work thus makes the
		  first attempt to tackle the above two challenges by
		  extracting substantially important turns as pivot
		  utterances and utilizing external knowledge to enhance the
		  representation of context. We propose a pivot-oriented deep
		  selection model (PoDS) on top of the Transformer-based
		  language models for dialogue comprehension. In detail, our
		  model first picks out the pivot utterances from the
		  conversation history according to the semantic matching
		  with the candidate response or question, if any. Besides,
		  knowledge items related to the dialogue context are
		  extracted from a knowledge graph as external knowledge.
		  Then, the pivot utterances and the external knowledge are
		  combined together with a well-designed mechanism for
		  refining predictions. Experimental results on four dialogue
		  comprehension benchmark tasks show that our proposed model
		  achieves great improvements on baselines. A series of
		  empirical comparisons are conducted to show how our
		  selection strategies and the extra knowledge injection
		  influence the results.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= feb,
  pages		= {1161–1173},
  numpages	= {13}
}

@InProceedings{	  10.1145/3539618.3592092,
  author	= {Lin, Hsien-Chin and Feng, Shutong and Geishauser,
		  Christian and Lubis, Nurul and van Niekerk, Carel and Heck,
		  Michael and Ruppik, Benjamin and Vukovic, Renato and
		  Gasi\'{c}, Milica},
  title		= {EmoUS: Simulating User Emotions in Task-Oriented
		  Dialogues},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3592092},
  doi		= {10.1145/3539618.3592092},
  abstract	= {Existing user simulators (USs) for task-oriented dialogue
		  systems only model user behaviour on semantic and natural
		  language levels without considering the user persona and
		  emotions. Optimising dialogue systems with generic user
		  policies, which cannot model diverse user behaviour driven
		  by different emotional states, may result in a high
		  drop-off rate when deployed in the real world. Thus, we
		  present EmoUS, a user simulator that learns to simulate
		  user emotions alongside user behaviour. EmoUS generates
		  user emotions, semantic actions, and natural language
		  responses based on the user goal, the dialogue history, and
		  the user persona. By analysing what kind of system
		  behaviour elicits what kind of user emotions, we show that
		  EmoUS can be used as a probe to evaluate a variety of
		  dialogue systems and in particular their effect on the
		  user's emotional state. Developing such methods is
		  important in the age of large language model chat-bots and
		  rising ethical concerns.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2526–2531},
  numpages	= {6},
  keywords	= {dialogue system, emotion simulation, user simulation},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Article{	  10.1109/taslp.2022.3145320,
  author	= {Liu, Yongkang and Huang, Qingbao and Li, Jing and Mo,
		  Linzhang and Cai, Yi and Li, Qing},
  title		= {SSAP: Storylines and Sentiment Aware Pre-Trained Model for
		  Story Ending Generation},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3145320},
  doi		= {10.1109/TASLP.2022.3145320},
  abstract	= {As an interesting but under-explored task, story ending
		  generation aims at generating an appropriate ending for an
		  incomplete story. The challenges of the task are to deeply
		  understand the story context, mine the storylines hidden in
		  the story, and generate rational endings in logic and
		  sentiment. Although existing pre-trained approaches have
		  been proven effective to this task, how to learn to
		  generate endings with appropriate plots and sufficient
		  sentimental information still remains a major challenge.
		  One possible reason is that an over reliance on external
		  commonsense knowledge beyond the storylines and sentimental
		  trends information hidden in the story context could lead
		  to generation deviating from the main theme. To address
		  this issue, we propose a two-stage
		  &lt;bold&gt;S&lt;/bold&gt;troylines and
		  &lt;bold&gt;S&lt;/bold&gt;entiment
		  &lt;bold&gt;A&lt;/bold&gt;ware
		  &lt;bold&gt;P&lt;/bold&gt;re-trained model (SSAP) for
		  generating sentimentally relevant story endings. We apply a
		  classifier for discriminating the sentiment of the story,
		  and then employ a pre-trained language model, combining
		  with storylines information, to conditionally generate
		  sentences that match both the logic and sentiment of the
		  story. Automatic and manual evaluations show that, without
		  integrating external knowledge, our model can produce more
		  consistent and diverse story endings than state-of-the-art
		  baselines.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jan,
  pages		= {686–694},
  numpages	= {9}
}

@InProceedings{	  10.1145/3487351.3489443,
  author	= {Paschalides, Demetris and Pallis, George and Dikaiakos,
		  Marios D.},
  title		= {POLAR: a holistic framework for the modelling of
		  polarization and identification of polarizing topics in
		  news media},
  year		= {2022},
  isbn		= {9781450391283},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487351.3489443},
  doi		= {10.1145/3487351.3489443},
  abstract	= {Polarization is an alarming trend in modern societies with
		  serious implications on social cohesion and the democratic
		  process. Typically, polarization manifests itself in the
		  public discourse in politics, governance and ideology. In
		  recent years, however, polarization arises increasingly in
		  a wider range of issues, from identity and culture to
		  healthcare and the environment. As the public and private
		  discourse moves online, polarization feeds in and is fed by
		  phenomena like fake news and hate speech. The
		  identification and analysis of online polarization is
		  challenging because of the massive scale, diversity, and
		  unstructured nature of online content, and the rapid and
		  unpredictable evolution of polarizing issues. Therefore, we
		  need effective ways to identify, quantify, and represent
		  polarization and polarizing topics algorithmically and at
		  scale. In this work, we introduce POLAR - an unsupervised,
		  large-scale framework for modeling and identifying
		  polarizing topics in any domain, without prior
		  domain-specific knowledge. POLAR comprises a processing
		  pipeline that analyzes a corpus of an arbitrary number of
		  news articles to construct a hierarchical knowledge graph
		  that models polarization and identify polarizing topics
		  discussed in the corpus. Our evaluation shows that POLAR is
		  able to identify and rank polarizing topics accurately and
		  efficiently.},
  booktitle	= {Proceedings of the 2021 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {348–355},
  numpages	= {8},
  keywords	= {inter-group conflict, natural language processing,
		  polarization, polarizing topic extraction, signed
		  networks},
  location	= {Virtual Event, Netherlands},
  series	= {ASONAM '21}
}

@Article{	  10.1145/3582688,
  author	= {Song, Yisheng and Wang, Ting and Cai, Puyu and Mondal,
		  Subrota K. and Sahoo, Jyoti Prakash},
  title		= {A Comprehensive Survey of Few-shot Learning: Evolution,
		  Applications, Challenges, and Opportunities},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {13s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3582688},
  doi		= {10.1145/3582688},
  abstract	= {Few-shot learning (FSL) has emerged as an effective
		  learning method and shows great potential. Despite the
		  recent creative works in tackling FSL tasks, learning valid
		  information rapidly from just a few or even zero samples
		  remains a serious challenge. In this context, we
		  extensively investigated 200+ FSL papers published in top
		  journals and conferences in the past three years, aiming to
		  present a timely and comprehensive overview of the most
		  recent advances in FSL with a fresh perspective and to
		  provide an impartial comparison of the strengths and
		  weaknesses of existing work. To avoid conceptual confusion,
		  we first elaborate and contrast a set of relevant concepts
		  including few-shot learning, transfer learning, and
		  meta-learning. Then, we inventively extract prior knowledge
		  related to few-shot learning in the form of a pyramid,
		  which summarizes and classifies previous work in detail
		  from the perspective of challenges. Furthermore, to enrich
		  this survey, we present in-depth analysis and insightful
		  discussions of recent advances in each subsection. What is
		  more, taking computer vision as an example, we highlight
		  the important application of FSL, covering various research
		  hotspots. Finally, we conclude the survey with unique
		  insights into technology trends and potential future
		  research opportunities to guide FSL follow-up research.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {271},
  numpages	= {40},
  keywords	= {Few-shot learning, one-shot learning, zero-shot learning,
		  low-shot learning, meta-learning, prior knowledge}
}

@InProceedings{	  10.1145/3581783.3612516,
  author	= {Sun, Zhongfan and Hu, Yongli and Gao, Qingqing and Jiang,
		  Huajie and Gao, Junbin and Sun, Yanfeng and Yin, Baocai},
  title		= {Breaking the Barrier Between Pre-training and Fine-tuning:
		  A Hybrid Prompting Model for Knowledge-Based VQA},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612516},
  doi		= {10.1145/3581783.3612516},
  abstract	= {Considerable performance gains have been achieved for
		  knowledge-based visual question answering due to the
		  visual-language pre-training models with
		  pre-training-then-fine-tuning paradigm. However, because
		  the targets of the pre-training and fine-tuning stages are
		  different, there is an evident barrier that prevents the
		  cross-modal comprehension ability developed in the
		  pre-training stage from fully endowing the fine-tuning
		  task. To break this barrier, in this paper, we propose a
		  novel hybrid prompting model for knowledge-based VQA, which
		  inherits and incorporates the pre-training and fine-tuning
		  tasks with a shared objective. Specifically, based on
		  static declaration prompt, we construct a consistent goal
		  with the fine-tuning via masked language modeling to
		  inherit capabilities of pre-training task, while selecting
		  the top-t relevant knowledge in a dense retrieval manner.
		  Additionally, a dynamic knowledge prompt is learned from
		  retrieved knowledge, which not only alleviates the length
		  constraint on inputs for visual-language pre-trained models
		  but also assists in providing answer features via
		  fine-tuning. Combining and unifying the aims of the two
		  stages could fully exploit the abilities of pre-training
		  and fine-tuning to predict answer. We evaluate the proposed
		  model on the OKVQA dataset, and the result shows that our
		  model outperforms the state-of-the-art methods based on
		  visual-language pre-training models with a noticeable
		  performance gap and even exceeds the large-scale language
		  model of GPT-3, which proves the benefits of the hybrid
		  prompts and the advantages of unifying pre-training to
		  fine-tuning.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {4065–4073},
  numpages	= {9},
  keywords	= {knowledge integration, multi-modal fusion, visual question
		  answering},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3477495.3531971,
  author	= {Gerritse, Emma J. and Hasibi, Faegheh and de Vries, Arjen
		  P.},
  title		= {Entity-aware Transformers for Entity Search},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531971},
  doi		= {10.1145/3477495.3531971},
  abstract	= {Pre-trained language models such as BERT have been a key
		  ingredient to achieve state-of-the-art results on a variety
		  of tasks in natural language processing and, more recently,
		  also in information retrieval. Recent research even claims
		  that BERT is able to capture factual knowledge about entity
		  relations and properties, the information that is commonly
		  obtained from knowledge graphs. This paper investigates the
		  following question: Do BERT-based entity retrieval models
		  benefit from additional entity information stored in
		  knowledge graphs? To address this research question, we map
		  entity embeddings into the same input space as a
		  pre-trained BERT model and inject these entity embeddings
		  into the BERT model. This entity-enriched language model is
		  then employed on the entity retrieval task. We show that
		  the entity-enriched BERT model improves effectiveness on
		  entity-oriented queries over a regular BERT model,
		  establishing a new state-of-the-art result for the entity
		  retrieval task, with substantial improvements for complex
		  natural language queries and queries requesting a list of
		  entities with a certain property. Additionally, we show
		  that the entity information provided by our entity-enriched
		  model particularly helps queries related to less popular
		  entities. Last, we observe empirically that the
		  entity-enriched BERT models enable fine-tuning on limited
		  training data, which otherwise would not be feasible due to
		  the known instabilities of BERT in few-sample fine-tuning,
		  thereby contributing to data-efficient training of BERT for
		  entity search.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1455–1465},
  numpages	= {11},
  keywords	= {bert, entity embeddings, entity retrieval, transformers},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3603163.3609075,
  author	= {Ro\ss{}ner, Daniel and Atzenbeck, Claus and Brooker, Sam},
  title		= {SPORE: A Storybreaking Machine},
  year		= {2023},
  isbn		= {9798400702327},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603163.3609075},
  doi		= {10.1145/3603163.3609075},
  abstract	= {The paper presents SPORE, a Spatial Recommender System. As
		  we enter a period of unprecedented collaboration between
		  authors and computers, where artificial intelligence in
		  particular seems likely to act increasingly in a
		  co-authoring capacity, SPORE offers a different approach to
		  collaboration. More organic and exploratory than other
		  automated or procedural systems, SPORE aims to mimic the
		  process of storybreaking that already exists in the
		  creative industries.},
  booktitle	= {Proceedings of the 34th ACM Conference on Hypertext and
		  Social Media},
  articleno	= {1},
  numpages	= {6},
  keywords	= {Mother, education, hypertext, linguistics, recommender
		  system, spatial hypertext, storytelling, tropes},
  location	= {Rome, Italy},
  series	= {HT '23}
}

@InProceedings{	  10.1145/3539597.3570415,
  author	= {Wu, Taiqiang and Bai, Xingyu and Guo, Weigang and Liu,
		  Weijie and Li, Siheng and Yang, Yujiu},
  title		= {Modeling Fine-grained Information via Knowledge-aware
		  Hierarchical Graph for Zero-shot Entity Retrieval},
  year		= {2023},
  isbn		= {9781450394079},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539597.3570415},
  doi		= {10.1145/3539597.3570415},
  abstract	= {Zero-shot entity retrieval, aiming to link mentions to
		  candidate entities under the zero-shot setting, is vital
		  for many tasks in Natural Language Processing. Most
		  existing methods represent mentions/entities via the
		  sentence embeddings of corresponding context from the
		  Pre-trained Language Model. However, we argue that such
		  coarse-grained sentence embeddings can not fully model the
		  mentions/entities, especially when the attention scores
		  towards mentions/entities are relatively low. In this work,
		  we propose GER, a Graph enhanced Entity Retrieval
		  framework, to capture more fine-grained information as
		  complementary to sentence embeddings. We extract the
		  knowledge units from the corresponding context and then
		  construct a mention/entity centralized graph. Hence, we can
		  learn the fine-grained information about mention/entity by
		  aggregating information from these knowledge units. To
		  avoid the graph bottleneck for the central mention/entity
		  node, we construct a hierarchical graph and design a novel
		  Hierarchical Graph Attention Network~(HGAN). Experimental
		  results on popular benchmarks demonstrate that our proposed
		  GER framework performs better than previous
		  state-of-the-art models.},
  booktitle	= {Proceedings of the Sixteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1021–1029},
  numpages	= {9},
  keywords	= {fine-grained information, zero-shot entity retrieval},
  location	= {Singapore, Singapore},
  series	= {WSDM '23}
}

@InProceedings{	  10.1145/3581783.3611898,
  author	= {Xi, Nan and Meng, Jingjing and Yuan, Junsong},
  title		= {Chain-of-Look Prompting for Verb-centric Surgical Triplet
		  Recognition in Endoscopic Videos},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3611898},
  doi		= {10.1145/3581783.3611898},
  abstract	= {Surgical triplet recognition aims to recognize surgical
		  activities as triplets (i.e.,&lt;instrument, verb, target
		  &gt;), which provides fine-grained information essential
		  for surgical scene understanding. Existing methods for
		  surgical triplet recognition rely on compositional methods
		  that recognize the instrument, verb, and target
		  simultaneously. In contrast, our method, called
		  chain-of-look prompting, casts the problem of surgical
		  triplet recognition as visual prompt generation from
		  large-scale vision-language (VL) models, and explicitly
		  decomposes the task into a series of video reasoning
		  processes. Chain-of-Look prompting is inspired by: (1) the
		  chain-of-thought prompting in natural language processing,
		  which divides a problem into a sequence of intermediate
		  reasoning steps; (2) the inter-dependency between motion
		  and visual appearance in the human vision system. Since
		  surgical activities are conveyed by the actions of
		  physicians, we regard the verbs as the carrier of semantics
		  in surgical endoscopic videos. Additionally, we utilize the
		  BioMed large language model to calibrate the generated
		  visual prompt features for surgical scenarios. Our approach
		  captures the visual reasoning processes underlying surgical
		  activities and achieves better performance compared to the
		  state-of-the-art methods on the largest surgical triplet
		  recognition dataset, CholecT50. The code is available at
		  https://github.com/southnx/CoLSurgical.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {5007–5016},
  numpages	= {10},
  keywords	= {chain-of-look prompting, endoscopic videos, surgical
		  triplet recognition, verb-centric},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@Article{	  10.1109/taslp.2021.3079812,
  author	= {Xie, Zhiwen and Zhu, Runjie and Liu, Jin and Zhou,
		  Guangyou and Huang, Jimmy Xiangji},
  title		= {Hierarchical Neighbor Propagation With Bidirectional Graph
		  Attention Network for Relation Prediction},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3079812},
  doi		= {10.1109/TASLP.2021.3079812},
  abstract	= {The graph attention network (GAT) &lt;xref ref-type="bibr"
		  rid="ref1"&gt;[1]&lt;/xref&gt; has started to become a
		  mainstream neural network architecture since 2018, yielding
		  remarkable performance gains in various natural language
		  processing (NLP) tasks. Although GAT has reached the
		  state-of-the-art (SOTA) performance as a recent success in
		  &lt;italic&gt;relation prediction&lt;/italic&gt; in
		  knowledge graph, the current model is still limited by the
		  following two aspects: (1) the existing model only
		  considers the neighbors from the inbound-direction of the
		  given entity, but ignores the rich neighborhood information
		  from outbound-directions; (2) the existing model only uses
		  the &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt;-th
		  hop output to learn the multi-hop embeddings, which leads
		  to the loss of a large amount of early-stage embedding
		  information (e.g., one-hop) at the graph attention step. In
		  this study, we propose a novel bidirectional graph
		  attention network (BiGAT) to learn the hierarchical
		  neighbor propagation. In our proposed BiGAT, an
		  inbound-directional GAT and an outbound-directional GAT are
		  introduced to capture sufficient neighborhood information
		  before propagating the bidirectional neighborhood
		  information to learn the multi-hop feature embeddings in a
		  hierarchical manner. Experiments conducted on the four
		  publicly available datasets show that BiGAT achieves the
		  competitive results in comparison to other SOTA methods.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= may,
  pages		= {1762–1773},
  numpages	= {12}
}

@InProceedings{	  10.1145/3437963.3441748,
  author	= {Vakulenko, Svitlana and Longpre, Shayne and Tu, Zhucheng
		  and Anantha, Raviteja},
  title		= {Question Rewriting for Conversational Question Answering},
  year		= {2021},
  isbn		= {9781450382977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3437963.3441748},
  doi		= {10.1145/3437963.3441748},
  abstract	= {Conversational question answering (QA) requires the
		  ability to correctly interpret a question in the context of
		  previous conversation turns. We address the conversational
		  QA task by decomposing it into question rewriting and
		  question answering subtasks. The question rewriting (QR)
		  subtask is specifically designed to reformulate ambiguous
		  questions, which depend on the conversational context, into
		  unambiguous questions that can be correctly interpreted
		  outside of the conversational context. We introduce a
		  conversational QA architecture that sets the new state of
		  the art on the TREC CAsT 2019 passage retrieval dataset.
		  Moreover, we show that the same QR model improves QA
		  performance on the QuAC dataset with respect to answer span
		  extraction, which is the next step in QA after passage
		  retrieval. Our evaluation results indicate that the QR
		  model we proposed achieves near human-level performance on
		  both datasets and the gap in performance on the end-to-end
		  conversational QA task is attributed mostly to the errors
		  in QA.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {355–363},
  numpages	= {9},
  keywords	= {conversational search, question answering, question
		  rewriting},
  location	= {Virtual Event, Israel},
  series	= {WSDM '21}
}

@Article{	  10.1109/taslp.2022.3140482,
  author	= {Mao, Qianren and Li, Jianxin and Lin, Chenghua and Chen,
		  Congwen and Peng, Hao and Wang, Lihong and Yu, Philip S.},
  title		= {Adaptive Pre-Training and Collaborative Fine-Tuning: A
		  Win-Win Strategy to Improve Review Analysis Tasks},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3140482},
  doi		= {10.1109/TASLP.2022.3140482},
  abstract	= {Summarizing user reviews and classifying user sentiment
		  are two critical tasks for modern e-commerce platforms.
		  These two tasks can benefit each other by capturing the
		  shared linguistic features. However, such a relationship
		  has not been fully exploited by existing research on
		  domain-specific contextual representations. This work
		  explores a win-win strategy for a multi-task framework with
		  three stages: general pre-training, adaptive pre-training,
		  and collaborative fine-tuning. The task-adaptive continual
		  pre-training on a language model can obtain domain-specific
		  contextual representations, further used to improve two
		  related tasks, sentiment classification and review
		  summarization during the collaborative fine-tuning.
		  Meanwhile, to effectively capture sentiment-oriented
		  domain-specific contextual representations, we introduce a
		  novel task-adaptive pre-training procedure, which adds a
		  sentiment prediction task during the adaptive pre-training.
		  Extensive experiments conducted on two adaption scenarios
		  of a general-to-single domain and a general-to-multiple
		  domain show that our framework outperforms state-of-the-art
		  methods.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jan,
  pages		= {622–634},
  numpages	= {13}
}

@InProceedings{	  10.1145/3498891.3501259,
  author	= {Aranovich, Ra\'{u}l and Wu, Muting and Yu, Dian and Katsy,
		  Katya and Ahmadnia, Benyamin and Bishop, Matthew and
		  Filkov, Vladimir and Sagae, Kenji},
  title		= {Beyond NVD: Cybersecurity meets the Semantic Web.},
  year		= {2022},
  isbn		= {9781450385732},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3498891.3501259},
  doi		= {10.1145/3498891.3501259},
  abstract	= {Cybersecurity experts rely on the knowledge stored in
		  databases like the NVD to do their work, but these are not
		  the only sources of information about threats and
		  vulnerabilities. Much of that information flows through
		  social media channels. In this paper we argue that security
		  experts and general users alike can benefit from the
		  technologies of the Semantic Web, merging heterogeneous
		  sources of knowledge in an ontological representation. We
		  present a system that has an ontology of vulnerabilities at
		  its core, but that is enhanced with NLP tools to identify
		  cybersecurity-related information in social media and to
		  launch queries over heterogeneous data sources. The
		  transformative power of Semantic Web technologies for
		  cybersecurity, which has been proven in the biomedical
		  field, is evaluated and discussed.},
  booktitle	= {Proceedings of the 2021 New Security Paradigms Workshop},
  pages		= {59–69},
  numpages	= {11},
  keywords	= {cybersecurity, neural networks, nlp, ontology, social
		  media},
  location	= {Virtual Event, USA},
  series	= {NSPW '21}
}

@InProceedings{	  10.1145/3586183.3606779,
  author	= {Veluri, Bandhav and Itani, Malek and Chan, Justin and
		  Yoshioka, Takuya and Gollakota, Shyamnath},
  title		= {Semantic Hearing: Programming Acoustic Scenes with
		  Binaural&nbsp;Hearables},
  year		= {2023},
  isbn		= {9798400701320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3586183.3606779},
  doi		= {10.1145/3586183.3606779},
  abstract	= {Imagine being able to listen to the birds chirping in a
		  park without hearing the chatter from other hikers, or
		  being able to block out traffic noise on a busy street
		  while still being able to hear emergency sirens and car
		  honks. We introduce semantic hearing, a novel capability
		  for hearable devices that enables them to, in real-time,
		  focus on, or ignore, specific sounds from real-world
		  environments, while also preserving the spatial cues. To
		  achieve this, we make two technical contributions: 1) we
		  present the first neural network that can achieve binaural
		  target sound extraction in the presence of interfering
		  sounds and background noise, and 2) we design a training
		  methodology that allows our system to generalize to
		  real-world use. Results show that our system can operate
		  with 20 sound classes and that our transformer-based
		  network has a runtime of 6.56 ms on a connected smartphone.
		  In-the-wild evaluation with participants in previously
		  unseen indoor and outdoor scenarios shows that our
		  proof-of-concept system can extract the target sounds and
		  generalize to preserve the spatial cues in its binaural
		  output. Project page with code:
		  https://semantichearing.cs.washington.edu},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {89},
  numpages	= {15},
  keywords	= {Spatial computing, attention, binaural target sound
		  extraction, causal neural networks, earable computing,
		  noise cancellation},
  location	= {San Francisco, CA, USA},
  series	= {UIST '23}
}

@InProceedings{	  10.1145/3442381.3449860,
  author	= {Ye, Muchao and Cui, Suhan and Wang, Yaqing and Luo, Junyu
		  and Xiao, Cao and Ma, Fenglong},
  title		= {MedPath: Augmenting Health Risk Prediction via Medical
		  Knowledge Paths},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3449860},
  doi		= {10.1145/3442381.3449860},
  abstract	= {The broad adoption of electronic health records (EHR) data
		  and the availability of biomedical knowledge graphs (KGs)
		  on the web have provided clinicians and researchers
		  unprecedented resources and opportunities for conducting
		  health risk predictions to improve healthcare quality and
		  medical resource allocation. Existing methods have focused
		  on improving the EHR feature representations using
		  attention mechanisms, time-aware models, or external
		  knowledge. However, they ignore the importance of using
		  personalized information to make predictions. Besides, the
		  reliability of their prediction interpretations needs to be
		  improved since their interpretable attention scores are not
		  explicitly reasoned from disease progression paths. In this
		  paper, we propose MedPath to solve these challenges and
		  augment existing risk prediction models with the ability to
		  use personalized information and provide reliable
		  interpretations inferring from disease progression paths.
		  Firstly, MedPath extracts personalized knowledge graphs
		  (PKGs) containing all possible disease progression paths
		  from observed symptoms to target diseases from a
		  large-scale online medical knowledge graph. Next, to
		  augment existing EHR encoders for achieving better
		  predictions, MedPath learns a PKG embedding by conducting
		  multi-hop message passing from symptom nodes to target
		  disease nodes through a graph neural network encoder. Since
		  MedPath reasons disease progression by paths existing in
		  PKGs, it can provide explicit explanations for the
		  prediction by pointing out how observed symptoms can
		  finally lead to target diseases. Experimental results on
		  three real-world medical datasets show that MedPath is
		  effective in improving the performance of eight
		  state-of-the-art methods with higher F1 scores and AUCs.
		  Our case study also demonstrates that MedPath can greatly
		  improve the explicitness of the risk prediction
		  interpretation.1},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {1397–1409},
  numpages	= {13},
  keywords	= {graph neural network, healthcare informatics, model
		  interpretability, risk prediction},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3581641.3584088,
  author	= {Brachman, Michelle and Pan, Qian and Do, Hyo Jin and
		  Dugan, Casey and Chaudhary, Arunima and Johnson, James M.
		  and Rai, Priyanshu and Chakraborti, Tathagata and Gschwind,
		  Thomas and Laredo, Jim A and Miksovic, Christoph and
		  Scotton, Paolo and Talamadupula, Kartik and Thomas, Gegi},
  title		= {Follow the Successful Herd: Towards Explanations for
		  Improved Use and Mental Models of Natural Language
		  Systems},
  year		= {2023},
  isbn		= {9798400701061},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581641.3584088},
  doi		= {10.1145/3581641.3584088},
  abstract	= {While natural language systems continue improving, they
		  are still imperfect. If a user has a better understanding
		  of how a system works, they may be able to better
		  accomplish their goals even in imperfect systems. We
		  explored whether explanations can support effective
		  authoring of natural language utterances and how those
		  explanations impact users’ mental models in the context
		  of a natural language system that generates small programs.
		  Through an online study (n=252), we compared two main types
		  of explanations: 1) system-focused, which provide
		  information about how the system processes utterances and
		  matches terms to a knowledge base, and 2) social, which
		  provide information about how other users have successfully
		  interacted with the system. Our results indicate that
		  providing social suggestions of terms to add to an
		  utterance helped users to repair and generate correct flows
		  more than system-focused explanations or social
		  recommendations of words to modify. We also found that
		  participants commonly understood some mechanisms of the
		  natural language system, such as the matching of terms to a
		  knowledge base, but they often lacked other critical
		  knowledge, such as how the system handled structuring and
		  ordering. Based on these findings, we make design
		  recommendations for supporting interactions with and
		  understanding of natural language systems.},
  booktitle	= {Proceedings of the 28th International Conference on
		  Intelligent User Interfaces},
  pages		= {220–239},
  numpages	= {20},
  keywords	= {AI explainability, mental models, natural language
		  interaction},
  location	= {Sydney, NSW, Australia},
  series	= {IUI '23}
}

@Article{	  10.1145/3611651,
  author	= {Wang, Benyou and Xie, Qianqian and Pei, Jiahuan and Chen,
		  Zhihong and Tiwari, Prayag and Li, Zhao and Fu, Jie},
  title		= {Pre-trained Language Models in Biomedical Domain: A
		  Systematic Survey},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3611651},
  doi		= {10.1145/3611651},
  abstract	= {Pre-trained language models (PLMs) have been the de facto
		  paradigm for most natural language processing tasks. This
		  also benefits the biomedical domain: researchers from
		  informatics, medicine, and computer science communities
		  propose various PLMs trained on biomedical datasets, e.g.,
		  biomedical text, electronic health records, protein, and
		  DNA sequences for various biomedical tasks. However, the
		  cross-discipline characteristics of biomedical PLMs hinder
		  their spreading among communities; some existing works are
		  isolated from each other without comprehensive comparison
		  and discussions. It is nontrivial to make a survey that not
		  only systematically reviews recent advances in biomedical
		  PLMs and their applications but also standardizes
		  terminology and benchmarks. This article summarizes the
		  recent progress of pre-trained language models in the
		  biomedical domain and their applications in downstream
		  biomedical tasks. Particularly, we discuss the motivations
		  of PLMs in the biomedical domain and introduce the key
		  concepts of pre-trained language models. We then propose a
		  taxonomy of existing biomedical PLMs that categorizes them
		  from various perspectives systematically. Plus, their
		  applications in biomedical downstream tasks are
		  exhaustively discussed, respectively. Last, we illustrate
		  various limitations and future trends, which aims to
		  provide inspiration for the future research.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {55},
  numpages	= {52},
  keywords	= {Biomedical domain, pre-trained language models, natural
		  language processing}
}

@InProceedings{	  10.1145/3532106.3533533,
  author	= {Gero, Katy Ilonka and Liu, Vivian and Chilton, Lydia},
  title		= {Sparks: Inspiration for Science Writing using Language
		  Models},
  year		= {2022},
  isbn		= {9781450393584},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3532106.3533533},
  doi		= {10.1145/3532106.3533533},
  abstract	= {Large-scale language models are rapidly improving,
		  performing well on a wide variety of tasks with little to
		  no customization. In this work we investigate how language
		  models can support science writing, a challenging writing
		  task that is both open-ended and highly constrained. We
		  present a system for generating “sparks”, sentences
		  related to a scientific concept intended to inspire
		  writers. We find that our sparks are more coherent and
		  diverse than a competitive language model baseline, and
		  approach a human-written gold standard. We run a user study
		  with 13 STEM graduate students writing on topics of their
		  own selection and find three main use cases of
		  sparks—inspiration, translation, and perspective—each
		  of which correlates with a unique interaction pattern. We
		  also find that while participants were more likely to
		  select higher quality sparks, the average quality of sparks
		  seen by a given participant did not correlate with their
		  satisfaction with the tool. We end with a discussion about
		  what impacts human satisfaction with AI support tools,
		  considering participant attitudes towards influence, their
		  openness to technology, as well as issues of plagiarism,
		  trustworthiness, and bias in AI.},
  booktitle	= {Proceedings of the 2022 ACM Designing Interactive Systems
		  Conference},
  pages		= {1002–1019},
  numpages	= {18},
  keywords	= {co-creativity, creativity support tools, natural language
		  processing, science writing, writing support},
  location	= {Virtual Event, Australia},
  series	= {DIS '22}
}

@Article{	  10.1109/taslp.2022.3153255,
  author	= {Gao, Silin and Takanobu, Ryuichi and Bosselut, Antoine and
		  Huang, Minlie},
  title		= {End-to-End Task-Oriented Dialog Modeling With
		  Semi-Structured Knowledge Management},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3153255},
  doi		= {10.1109/TASLP.2022.3153255},
  abstract	= {Current task-oriented dialog (TOD) systems mostly manage
		  structured knowledge (e.g. databases and tables) to guide
		  the goal-oriented conversations. However, they fall short
		  of handling dialogs which also involve unstructured
		  knowledge (e.g. reviews and documents). In this article, we
		  formulate a task of modeling TOD grounded on a fusion of
		  structured and unstructured knowledge. To address this
		  task, we propose a TOD system with semi-structured
		  knowledge management, SeKnow, which extends the belief
		  state to manage knowledge with both structured and
		  unstructured contents. Furthermore, we introduce two
		  implementations of SeKnow based on a non-pretrained
		  sequence-to-sequence model and a pretrained language model,
		  respectively. Both implementations use the end-to-end
		  manner to jointly optimize dialog modeling grounded on
		  structured and unstructured knowledge. We conduct
		  experiments on a modified version of MultiWOZ 2.1 dataset,
		  Mod-MultiWOZ 2.1, where dialogs are processed to involve
		  semi-structured knowledge. Experimental results show that
		  SeKnow has strong performances in both end-to-end dialog
		  and intermediate knowledge management, compared to existing
		  TOD systems and their extensions with pipeline knowledge
		  management schemes.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= feb,
  pages		= {2173–2187},
  numpages	= {15}
}

@InProceedings{	  10.1145/3488560.3498514,
  author	= {Zhou, Yuanhang and Zhou, Kun and Zhao, Wayne Xin and Wang,
		  Cheng and Jiang, Peng and Hu, He},
  title		= {C²-CRS: Coarse-to-Fine Contrastive Learning for
		  Conversational Recommender System},
  year		= {2022},
  isbn		= {9781450391320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3488560.3498514},
  doi		= {10.1145/3488560.3498514},
  abstract	= {Conversational recommender systems (CRS) aim to recommend
		  suitable items to users through natural language
		  conversations. For developing effective CRSs, a major
		  technical issue is how to accurately infer user preference
		  from very limited conversation context. To address issue, a
		  promising solution is to incorporate external data for
		  enriching the context information. However, prior studies
		  mainly focus on designing fusion models tailored for some
		  specific type of external data, which is not general to
		  model and utilize multi-type external data. To effectively
		  leverage multi-type external data, we propose a novel
		  coarse-to-fine contrastive learning framework to improve
		  data semantic fusion for CRS. In our approach, we first
		  extract and represent multi-grained semantic units from
		  different data signals, and then align the associated
		  multi-type semantic units in a coarse-to-fine way. To
		  implement this framework, we design both coarse-grained and
		  fine-grained procedures for modeling user preference, where
		  the former focuses on more general, coarse-grained semantic
		  fusion and the latter focuses on more specific,
		  fine-grained semantic fusion. Such an approach can be
		  extended to incorporate more kinds of external data.
		  Extensive experiments on two public CRS datasets have
		  demonstrated the effectiveness of our approach in both
		  recommendation and conversation tasks.},
  booktitle	= {Proceedings of the Fifteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1488–1496},
  numpages	= {9},
  keywords	= {contrastive learning, conversational recommender system},
  location	= {Virtual Event, AZ, USA},
  series	= {WSDM '22}
}

@InProceedings{	  10.1145/3442381.3450090,
  author	= {Wang, Daheng and Shiralkar, Prashant and Lockard, Colin
		  and Huang, Binxuan and Dong, Xin Luna and Jiang, Meng},
  title		= {TCN: Table Convolutional Network for Web Table
		  Interpretation},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450090},
  doi		= {10.1145/3442381.3450090},
  abstract	= {Information extraction from semi-structured webpages
		  provides valuable long-tailed facts for augmenting
		  knowledge graph. Relational Web tables are a critical
		  component containing additional entities and attributes of
		  rich and diverse knowledge. However, extracting knowledge
		  from relational tables is challenging because of sparse
		  contextual information. Existing work linearize table cells
		  and heavily rely on modifying deep language models such as
		  BERT which only captures related cells information in the
		  same table. In this work, we propose a novel relational
		  table representation learning approach considering both the
		  intra- and inter-table contextual information. On one hand,
		  the proposed Table Convolutional Network model employs the
		  attention mechanism to adaptively focus on the most
		  informative intra-table cells of the same row or column;
		  and, on the other hand, it aggregates inter-table
		  contextual information from various types of implicit
		  connections between cells across different tables.
		  Specifically, we propose three novel aggregation modules
		  for (i) cells of the same value, (ii) cells of the same
		  schema position, and (iii) cells linked to the same page
		  topic. We further devise a supervised multi-task training
		  objective for jointly predicting column type and pairwise
		  column relation, as well as a table cell recovery objective
		  for pre-training. Experiments on real Web table datasets
		  demonstrate our method can outperform competitive baselines
		  by of F1 for column type prediction and by of F1 for
		  pairwise column relation prediction.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {4020–4032},
  numpages	= {13},
  keywords	= {Web table, information extraction, knowledge extraction},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3523181.3523197,
  author	= {Jiang, Shan and Wu, Huanhuan and Luo, Lingyun},
  title		= {Infusing Biomedical Knowledge into BERT for Chinese
		  Biomedical NLP Tasks with Adversarial Training},
  year		= {2022},
  isbn		= {9781450387453},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3523181.3523197},
  doi		= {10.1145/3523181.3523197},
  abstract	= {Biomedical text mining is becoming increasingly important.
		  Recently, biomedical pre-trained language models such as
		  BioBERT and SciBERT, which can capture biomedical knowledge
		  from text, have achieved promising results in biomedical
		  NLP tasks. However, most biomedical pre-trained language
		  models rely on the traditional masked language model (MLM)
		  pre-training strategy, which cannot fully capture the
		  semantic relations of context. It is challenging to learn
		  biomedical knowledge via language models in the Chinese
		  biomedical fields due to the lack of training resources and
		  the extreme complexity and diversity of Chinese medical
		  terminologies. To this end, we propose MedBERT-adv, which
		  utilizes a biomedical knowledge infusion method that can
		  effectively complement BERT-like models. Instead of using
		  time-consuming medical expert annotation and inaccurate
		  automatic annotation, we use the article structure in Baidu
		  Encyclopedia as a weakly supervised signal, utilizing each
		  medical term and its category as labels to pre-train the
		  model. We also leverage adversarial training strategies
		  like FGM for fine-tuning downstream tasks to further
		  improve the performance of MedBERT-adv. We experimented
		  with MedBERT-adv on the Chinese biomedical dataset CBLUE
		  using eight NLP tasks. Among all of them, our proposed
		  model obtained an average 1.8% improvement in average score
		  than four baseline models, demonstrating the effectiveness
		  of MedBERT-adv on Chinese biomedical text mining.},
  booktitle	= {2022 3rd Asia Service Sciences and Software Engineering
		  Conference},
  pages		= {108–114},
  numpages	= {7},
  location	= {Macau, Macao},
  series	= {ASSE' 22}
}

@InProceedings{	  10.1145/3532213.3532254,
  author	= {Zhang, Xiangliang and Jia, Yangli and Zhang, Zhenling and
		  Kang, Qi and Zhang, Yongchen and Jia, Hongling},
  title		= {Improving End-to-End Biomedical Question Answering
		  System},
  year		= {2022},
  isbn		= {9781450396110},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3532213.3532254},
  doi		= {10.1145/3532213.3532254},
  abstract	= {Biomedical question answering refers to extracting an
		  answer based on given questions and related documents.
		  Existing biomedical question answering research either
		  focuses on a specific stage, such as machine reading
		  comprehension, or uses traditional rule-based methods and
		  ontology with complex construction processes. In this
		  paper, we demonstrate the application of simple but
		  powerful neural-based approaches in improving the
		  end-to-end biomedical question answering system. We employ
		  the BM25-based documents retriever, BERT-based neural
		  ranker, and an answer extraction stage using the BioBERT
		  pre-trained language model. In view of the lack of
		  sufficient training data in the biomedical domain, domain
		  adaptation and data augmentation are adopted to address the
		  question answering task, so as to further reinforce the
		  system performance. Based on our self-built standard
		  large-volume retrieve corpus and neural ranker corpus, we
		  get competitive results on BioASQ8b.},
  booktitle	= {Proceedings of the 8th International Conference on
		  Computing and Artificial Intelligence},
  pages		= {274–279},
  numpages	= {6},
  keywords	= {Biomedical question answering, Data augmentation, Document
		  re-ranking},
  location	= {Tianjin, China},
  series	= {ICCAI '22}
}

@InProceedings{	  10.1145/3477495.3532037,
  author	= {Zhao, Mengxue and Yang, Yang and Li, Miao and Wang,
		  Jingang and Wu, Wei and Ren, Pengjie and de Rijke, Maarten
		  and Ren, Zhaochun},
  title		= {Personalized Abstractive Opinion Tagging},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3532037},
  doi		= {10.1145/3477495.3532037},
  abstract	= {An opinion tag is a sequence of words on a specific aspect
		  of a product or service. Opinion tags reflect key
		  characteristics of product reviews and help users quickly
		  understand their content in e-commerce portals. The task of
		  abstractive opinion tagging has previously been proposed to
		  automatically generate a ranked list of opinion tags for a
		  given review. However, current models for opinion tagging
		  are not personalized, even though personalization is an
		  essential ingredient of engaging user interactions,
		  especially in e-commerce. In this paper, we focus on the
		  task of personalized abstractive opinion tagging. There are
		  two main challenges when developing models for the
		  end-to-end generation of personalized opinion tags:
		  sparseness of reviews and difficulty to integrate
		  multi-type signals, i.e., explicit review signals and
		  implicit behavioral signals. To address these challenges,
		  we propose an end-to-end model, named POT, that consists of
		  three main components: (1) a review-based explicit
		  preference tracker component based on a hierarchical
		  heterogeneous review graph to track user preferences from
		  reviews; (2)a behavior-based implicit preference tracker
		  component using a heterogeneous behavior graph to track the
		  user preferences from implicit behaviors; and (3) a
		  personalized rank-aware tagging component to generate a
		  ranked sequence of personalized opinion tags. In our
		  experiments, we evaluate POT on a real-world dataset
		  collected from e-commerce platforms and the results
		  demonstrate that it significantly outperforms strong
		  baselines.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1066–1076},
  numpages	= {11},
  keywords	= {abstractive summarization, e-commerce, personalization,
		  review analysis},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3443279.3443314,
  author	= {Wan, Tianyu and Wang, Wenhui and Zhou, Hui},
  title		= {Research on Information Extraction of Municipal Solid
		  Waste Crisis using BERT-LSTM-CRF},
  year		= {2021},
  isbn		= {9781450377607},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3443279.3443314},
  doi		= {10.1145/3443279.3443314},
  abstract	= {There is much research on the phenomenon of municipal
		  solid waste (MSW) and its improvement measures, and the
		  method of information extraction be adopted to obtain the
		  potential knowledge of MSW from the existing relevant
		  research literature. Due to the complexity and diversity of
		  the MSW, unsupervised training of target texts can be
		  achieved through information data based on manual
		  annotation. According to the characteristics of the BERT
		  language model, a common method in natural language
		  processing(NLP), the pre-trained BERT(Bidirectional Encoder
		  Representation from Transformers) model with LSTM-CRF(Long
		  Short Term Memory-Conditional Random Field) architecture is
		  used in the information extraction of MSW crisis to extract
		  entities and relationships between entities from natural
		  language texts. By the method of calculating and evaluating
		  the extraction effect, it provided technical support for
		  further study of its crisis conversion.},
  booktitle	= {Proceedings of the 4th International Conference on Natural
		  Language Processing and Information Retrieval},
  pages		= {205–209},
  numpages	= {5},
  keywords	= {Bert-lstm-crf model, Information Extraction, Machine
		  learning, Solid Waste Crisis},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '20}
}

@Article{	  10.1109/taslp.2022.3197316,
  author	= {Gan, Leilei and Teng, Zhiyang and Zhang, Yue and Zhu,
		  Linchao and Wu, Fei and Yang, Yi},
  title		= {SemGloVe: Semantic Co-Occurrences for GloVe From BERT},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3197316},
  doi		= {10.1109/TASLP.2022.3197316},
  abstract	= {GloVe learns word embeddings by leveraging statistical
		  information from word co-occurrence matrices. However, word
		  pairs in the matrices are extracted from a predefined local
		  context window, which might lead to limited word pairs and
		  potentially semantic irrelevant word pairs. In this paper,
		  we propose &lt;italic&gt;SemGloVe&lt;/italic&gt;, which
		  distills &lt;italic&gt;semantic
		  co-occurrences&lt;/italic&gt; from BERT into static GloVe
		  word embeddings. Particularly, we propose two models to
		  extract co-occurrence statistics based on either the masked
		  language model or the multi-head attention weights of BERT.
		  Our methods can extract word pairs limited by the local
		  window assumption, and can define the co-occurrence weights
		  by directly considering the semantic distance between word
		  pairs. Experiments on several word similarity datasets and
		  external tasks show that SemGloVe can outperform GloVe.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {2696–2704},
  numpages	= {9}
}

@Article{	  10.1109/taslp.2022.3224286,
  author	= {Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
  title		= {Minimising Biasing Word Errors for Contextual ASR With the
		  Tree-Constrained Pointer Generator},
  year		= {2022},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3224286},
  doi		= {10.1109/TASLP.2022.3224286},
  abstract	= {Contextual knowledge is essential for reducing speech
		  recognition errors on high-valued long-tail words. This
		  paper proposes a novel tree-constrained pointer generator
		  (TCPGen) component that enables end-to-end ASR models to
		  bias towards a list of long-tail words obtained using
		  external contextual information. With only a small overhead
		  in memory use and computation cost, TCPGen can structure
		  thousands of biasing words efficiently into a symbolic
		  prefix-tree, and creates a neural shortcut between the tree
		  and the final ASR output to facilitate the recognition of
		  the biasing words. To enhance TCPGen, we further propose a
		  novel minimum biasing word error (MBWE) loss that directly
		  optimises biasing word errors during training, along with a
		  biasing-word-driven language model discounting (BLMD)
		  method during the test. All contextual ASR systems were
		  evaluated on the public Librispeech audiobook corpus and
		  the data from the dialogue state tracking challenges (DSTC)
		  with the biasing lists extracted from the dialogue-system
		  ontology. Consistent word error rate (WER) reductions were
		  achieved with TCPGen, which were particularly significant
		  on the biasing words with around 40% relative reductions in
		  the recognition error rates. MBWE and BLMD further improved
		  the effectiveness of TCPGen, and achieved more significant
		  WER reductions on the biasing words. TCPGen also achieved
		  zero-shot learning of words not in the audio training set
		  with large WER reductions on the out-of-vocabulary words in
		  the biasing list.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {345–354},
  numpages	= {10}
}

@Article{	  10.1109/tcbb.2021.3108718,
  author	= {Nourani, Esmaeil and Asgari, Ehsaneddin and McHardy, Alice
		  C. and Mofrad, Mohammad R.K.},
  title		= {TripletProt: Deep Representation Learning of Proteins
		  Based On Siamese Networks},
  year		= {2021},
  issue_date	= {Nov.-Dec. 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2021.3108718},
  doi		= {10.1109/TCBB.2021.3108718},
  abstract	= {Pretrained representations have recently gained attention
		  in various machine learning applications. Nonetheless, the
		  high computational costs associated with training these
		  models have motivated alternative approaches for
		  representation learning. Herein we introduce TripletProt, a
		  new approach for protein representation learning based on
		  the Siamese neural networks. Representation learning of
		  biological entities which capture essential features can
		  alleviate many of the challenges associated with supervised
		  learning in bioinformatics. The most important distinction
		  of our proposed method is relying on the protein-protein
		  interaction (PPI) network. The computational cost of the
		  generated representations for any potential application is
		  significantly lower than comparable methods since the
		  length of the representations is significantly smaller than
		  that in other approaches. TripletProt offers great
		  potentials for the protein informatics tasks and can be
		  widely applied to similar tasks. We evaluate TripletProt
		  comprehensively in protein functional annotation tasks
		  including sub-cellular localization (14 categories) and
		  gene ontology prediction (more than 2000 classes), which
		  are both challenging multi-class, multi-label
		  classification machine learning problems. We compare the
		  performance of TripletProt with the state-of-the-art
		  approaches including a recurrent language model-based
		  approach (i.e., UniRep), as well as a protein-protein
		  interaction (PPI) network and sequence-based method (i.e.,
		  DeepGO). Our TripletProt showed an overall improvement of
		  F1 score in the above mentioned comprehensive functional
		  annotation tasks, solely relying on the PPI network.
		  Availability: The source code and datasets are available at
		  &lt;uri&gt;https://github.com/EsmaeilNourani/TripletProt&lt;/uri&gt;.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= aug,
  pages		= {3744–3753},
  numpages	= {10}
}

@Article{	  10.1145/3588767,
  author	= {Huang, Hu and Zhang, Bowen and Li, Yangyang and Zhang,
		  Baoquan and Sun, Yuxi and Luo, Chuyao and Peng, Cheng},
  title		= {Knowledge-enhanced Prompt-tuning for Stance Detection},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3588767},
  doi		= {10.1145/3588767},
  abstract	= {Investigating public attitudes on social media is
		  important in opinion mining systems. Stance detection aims
		  to analyze the attitude of an opinionated text (e.g.,
		  favor, neutral, or against) toward a given target. Existing
		  methods mainly address this problem from the perspective of
		  fine-tuning. Recently, prompt-tuning has achieved success
		  in natural language processing tasks. However, conducting
		  prompt-tuning methods for stance detection in real-world
		  remains a challenge for several reasons: (1) The text form
		  of stance detection is usually short and informal, which
		  makes it difficult to design label words for the
		  verbalizer. (2) The tweet text may not explicitly give the
		  attitude. Instead, users may use various hashtags or
		  background knowledge to express stance-aware perspectives.
		  In this article, we first propose a prompt-tuning-based
		  framework that performs stance detection in a cloze
		  question manner. Specifically, a knowledge-enhanced
		  prompt-tuning framework (KEprompt) method is designed,
		  which consists of an automatic verbalizer (AutoV) and
		  background knowledge injection (BKI). Specifically, in
		  AutoV, we introduce a semantic graph to build a better
		  mapping from the predicted word of the pretrained language
		  model and detection labels. In BKI, we first propose a
		  topic model for learning hashtag representation and
		  introduce ConceptGraph as the supplement of the target. At
		  last, we present a challenging dataset for stance
		  detection, where all stance categories are expressed in an
		  implicit manner. Extensive experiments on a large
		  real-world dataset demonstrate the superiority of KEprompt
		  over state-of-the-art methods.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {159},
  numpages	= {20},
  keywords	= {Stance detection, deep learning, prompt-tuning framework}
}

@InProceedings{	  10.1145/3446132.3446397,
  author	= {Shi, Xin and Zeng, Xiaoyang and Wu, Jie and Hou, Mengshu
		  and Zhu, Hao},
  title		= {Context Event Features and Event Embedding Enhanced Event
		  Detection},
  year		= {2021},
  isbn		= {9781450388115},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3446132.3446397},
  doi		= {10.1145/3446132.3446397},
  abstract	= {Extracting valuable information from text has always been
		  a hot point for research and event detection is an
		  essential subtask of information extraction. Most existing
		  methods of event detection only focus on sentence-level
		  information and do not consider the correlation between
		  different event types. To address these problems, in this
		  paper, we propose a novel pre-trained language model based
		  event detection framework named CFEE that utilizes
		  document-level information and event correlation to enhance
		  the event detection task. To obtain event correlation, we
		  project all event types into a shared semantic space
		  through a Skip-gram model, where the event correlation can
		  be represented as the distance between event embeddings. In
		  order to capture document-level information, we utilize a
		  bidirectional recurrent neural network to fuse the context
		  information. Experiments on the ACE2005 dataset demonstrate
		  that our proposed model is better than most existing
		  methods, and also demonstrate the effectiveness of event
		  correlation and document-level information.},
  booktitle	= {Proceedings of the 2020 3rd International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  articleno	= {70},
  numpages	= {6},
  keywords	= {Bert, Document-level Information, Event Embedding, Event
		  detection},
  location	= {Sanya, China},
  series	= {ACAI '20}
}

@InProceedings{	  10.1145/3487553.3524237,
  author	= {Schelb, Julian and Ehrmann, Maud and Romanello, Matteo and
		  Spitz, Andreas},
  title		= {ECCE:&nbsp;Entity-centric&nbsp;Corpus&nbsp;Exploration
		  Using&nbsp;Contextual&nbsp;Implicit&nbsp;Networks},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524237},
  doi		= {10.1145/3487553.3524237},
  abstract	= {In the Digital Age, the analysis and exploration of
		  unstructured document collections is of central importance
		  to members of investigative professions, whether they might
		  be scholars, journalists, paralegals, or analysts. In many
		  of their domains, entities play a key role in the discovery
		  of implicit relations between the contents of documents and
		  thus serve as natural entry points to a detailed manual
		  analysis, such as the prototypical 5Ws in journalism or
		  stock symbols in finance. To assist in these analyses,
		  entity-centric networks have been proposed as a language
		  model that represents document collections as a
		  cooccurrence graph of entities and terms, and thereby
		  enables the visual exploration of corpora. Here, we present
		  ECCE, a web-based application that implements
		  entity-centric networks, augments them with contextual
		  language models, and provides users with the ability to
		  upload, manage, and explore document collections. Our
		  application is available as a web-based service at
		  http://dimtools.uni.kn/ecce.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {278–281},
  numpages	= {4},
  keywords	= {Entity network, cooccurrence network, corpus exploration},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3442442.3451384,
  author	= {Nguyen, Nhu Khoa and Boros, Emanuela and Lejeune, Ga\"{e}l
		  and Doucet, Antoine and Delahaut, Thierry},
  title		= {L3i_LBPAM at the FinSim-2 task: Learning Financial
		  Semantic Similarities with Siamese Transformers},
  year		= {2021},
  isbn		= {9781450383134},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442442.3451384},
  doi		= {10.1145/3442442.3451384},
  abstract	= {In this paper, we present the different methods proposed
		  for the FinSIM-2 Shared Task 2021 on Learning Semantic
		  Similarities for the Financial domain. The main focus of
		  this task is to evaluate the classification of financial
		  terms into corresponding top-level concepts (also known as
		  hypernyms) that were extracted from an external ontology.
		  We approached the task as a semantic textual similarity
		  problem. By relying on a siamese network with pre-trained
		  language model encoders, we derived semantically meaningful
		  term embeddings and computed similarity scores between them
		  in a ranked manner. Additionally, we exhibit the results of
		  different baselines in which the task is tackled as a
		  multi-class classification problem. The proposed methods
		  outperformed our baselines and proved the robustness of the
		  models based on textual similarity siamese network.},
  booktitle	= {Companion Proceedings of the Web Conference 2021},
  pages		= {302–306},
  numpages	= {5},
  keywords	= {Hypernym detection, semantic similarities, siamese
		  networks},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3581783.3612322,
  author	= {Wang, Jieming and Li, Ziyan and Yu, Jianfei and Yang, Li
		  and Xia, Rui},
  title		= {Fine-Grained Multimodal Named Entity Recognition and
		  Grounding with a Generative Framework},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612322},
  doi		= {10.1145/3581783.3612322},
  abstract	= {Multimodal Named Entity Recognition (MNER) aims to locate
		  and classify named entities mentioned in a pair of text and
		  image. However, most previous MNER works focus on
		  extracting entities in the form of text but failing to
		  ground text symbols to their corresponding visual objects.
		  Moreover, existing MNER studies primarily classify entities
		  into four coarse-grained entity types, which are often
		  insufficient to map them to their real-world referents. To
		  solve these limitations, we introduce a task named
		  Fine-grained Multimodal Named Entity Recognition and
		  Grounding (FMNERG) in this paper, which aims to
		  simultaneously extract named entities in text, their
		  fine-grained entity types, and their grounded visual
		  objects in image. Moreover, we construct a Twitter dataset
		  for the FMNERG task, and further propose a T5-based
		  multImodal GEneration fRamework (TIGER), which formulates
		  FMNERG as a generation problem by converting all the
		  entity-type-object triples into a target sequence and
		  adapts a pre-trained sequence-to-sequence model T5 to
		  directly generate the target sequence from an image-text
		  input pair. Experimental results demonstrate that TIGER
		  performs significantly better than a number of baseline
		  systems on the annotated Twitter dataset. Our dataset
		  annotation and source code are publicly released at
		  https://github.com/NUSTM/FMNERG.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {3934–3943},
  numpages	= {10},
  keywords	= {fine-grained named entity recognition, generative
		  framework, multimodal named entity recognition, visual
		  grounding},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@Proceedings{	  10.1145/3622758,
  title		= {Onward! 2023: Proceedings of the 2023 ACM SIGPLAN
		  International Symposium on New Ideas, New Paradigms, and
		  Reflections on Programming and Software},
  year		= {2023},
  isbn		= {9798400703881},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 2023 ACM SIGPLAN International Symposium on
		  New Ideas, New Paradigms, and Reflections on Programming
		  and Software (Onward! 2023), the premier multidisciplinary
		  conference focused on everything to do with programming and
		  software, including processes, methods, languages,
		  communities and applications. Onward! is more radical, more
		  visionary, and more open than other conferences to ideas
		  that are well-argued but not yet fully proven. We welcome
		  different ways of thinking about, approaching, and
		  reporting on programming language and software engineering
		  research.
		  
		  Onward! 2023 is co-located with SPLASH 2023, running from
		  Sunday 22nd of October till Friday 27th of October, in
		  Cascais, Portugal. We are delighted to have Felienne
		  Hermans giving the Onward! keynote, on Wednesday 25th of
		  October, on "Creating a learnable and inclusive programming
		  language".
		  
		  All papers and essays that lie here before you received at
		  least three reviews, leading to a decision of accept,
		  reject, or conditional accept. Authors of conditionally
		  accepted papers were provided with explicit requirements
		  for acceptance, and were carefully re-reviewed in the
		  second phase. The essays track received six submissions,
		  out of which four were accepted. The papers track accepted
		  nine out of nineteen submissions.
		  
		  We hope that the papers and essays in these proceedings
		  will stimulate and challenge your thinking about
		  programming and software engineering, and we are looking
		  forward to many discussions at the conference.},
  location	= {Cascais, Portugal}
}

@InProceedings{	  10.1145/3447548.3467164,
  author	= {Lin, Rongmei and He, Xiang and Feng, Jie and Zalmout,
		  Nasser and Liang, Yan and Xiong, Li and Dong, Xin Luna},
  title		= {PAM: Understanding Product Images in Cross Product
		  Category Attribute Extraction},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467164},
  doi		= {10.1145/3447548.3467164},
  abstract	= {Understanding product attributes plays an important role
		  in improving online shopping experience for customers and
		  serves asan integral part for constructing a product
		  knowledge graph. Most existing methods focus on attribute
		  extraction from text description or utilize visual
		  information from product images such as shape and color.
		  Compared to the inputs considered in prior works, a product
		  image in fact contains more information, represented by a
		  rich mixture of words and visual clues with a layout
		  carefully designed to impress customers. This work proposes
		  a more inclusive framework that fully utilizes these
		  different modalities for attribute extraction.Inspired by
		  recent works in visual question answering, we use a
		  transformer based sequence to sequence model to fuse
		  representations of product text, Optical Character
		  Recognition (OCR) tokens and visual objects detected in the
		  product image. The framework is further extended with the
		  capability to extract attribute value across multiple
		  product categories with a single model, by training the
		  decoder to predict both product category and attribute
		  value and conditioning its output on product category. The
		  model provides a unified attribute extraction solution
		  desirable at an e-commerce platform that offers numerous
		  product categories with a diverse body of product
		  attributes. We evaluated the model on two product
		  attributes, one with many possible values and one with a
		  small set of possible values, over 14 product categories
		  and found the model could achieve 15% gain on the Recall
		  and 10% gain on the F1 score compared to existing methods
		  using text-only features.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {3262–3270},
  numpages	= {9},
  keywords	= {e-commerce, knowledge extraction, multi-modality
		  learning},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@InProceedings{	  10.1145/3571884.3604313,
  author	= {Kernan Freire, Samuel and Foosherian, Mina and Wang,
		  Chaofan and Niforatos, Evangelos},
  title		= {Harnessing Large Language Models for Cognitive Assistants
		  in Factories},
  year		= {2023},
  isbn		= {9798400700149},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571884.3604313},
  doi		= {10.1145/3571884.3604313},
  abstract	= {As agile manufacturing expands and workforce mobility
		  increases, the importance of efficient knowledge transfer
		  among factory workers grows. Cognitive Assistants (CAs)
		  with Large Language Models (LLMs), like GPT-3.5, can bridge
		  knowledge gaps and improve worker performance in
		  manufacturing settings. This study investigates the
		  opportunities, risks, and user acceptance of LLM-powered
		  CAs in two factory contexts: textile and detergent
		  production. Several opportunities and risks are identified
		  through a literature review, proof-of-concept
		  implementation, and focus group sessions. Factory
		  representatives raise concerns regarding data security,
		  privacy, and the reliability of LLMs in high-stake
		  environments. By following design guidelines regarding
		  persistent memory, real-time data integration, security,
		  privacy, and ethical concerns, LLM-powered CAs can become
		  valuable assets in manufacturing settings and other
		  industries.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Conversational User Interfaces},
  articleno	= {44},
  numpages	= {6},
  keywords	= {cognitive assistant, conversational user interfaces,
		  human-centered AI, industry 5.0, knowledge management,
		  knowledge sharing},
  location	= {Eindhoven, Netherlands},
  series	= {CUI '23}
}

@Article{	  10.1145/3589338,
  author	= {Storey, Veda C. and Lukyanenko, Roman and Castellanos,
		  Arturo},
  title		= {Conceptual Modeling: Topics, Themes, and Technology
		  Trends},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {14s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3589338},
  doi		= {10.1145/3589338},
  abstract	= {Conceptual modeling is an important part of information
		  systems development and use that involves identifying and
		  representing relevant aspects of reality. Although the past
		  decades have experienced continuous digitalization of
		  services and products that impact business and society,
		  conceptual modeling efforts are still required to support
		  new technologies as they emerge. This paper surveys
		  research on conceptual modeling over the past five decades
		  and shows how its topics and trends continue to evolve to
		  accommodate emerging technologies, while remaining grounded
		  in basic constructs. We survey over 5,300 papers that
		  address conceptual modeling topics from the 1970s to the
		  present, which are collected from 35 multidisciplinary
		  journals and conferences, and use them as the basis from
		  which to analyze the progression of conceptual modeling.
		  The important role that conceptual modeling should play in
		  our evolving digital world is discussed, and future
		  research directions proposed.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {317},
  numpages	= {38},
  keywords	= {Conceptual modeling, digital world, database, information
		  systems, information technology, structured literature
		  review, clustering analysis}
}

@Proceedings{	  10.1145/3578741,
  title		= {MLNLP '22: Proceedings of the 2022 5th International
		  Conference on Machine Learning and Natural Language
		  Processing},
  year		= {2022},
  isbn		= {9781450399067},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sanya, China}
}

@Article{	  10.1109/taslp.2021.3065823,
  author	= {Li, Zekang and Li, Zongjia and Zhang, Jinchao and Feng,
		  Yang and Zhou, Jie},
  title		= {Bridging Text and Video: A Universal Multimodal
		  Transformer for Audio-Visual Scene-Aware Dialog},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3065823},
  doi		= {10.1109/TASLP.2021.3065823},
  abstract	= {Audio-Visual Scene-Aware Dialog (AVSD) is a task to
		  generate responses when chatting about a given video, which
		  is organized as a track of the 8th Dialog System Technology
		  Challenge (DSTC8). There are two challenges in this task:
		  1) making effective interaction among different modalities;
		  2) better understanding dialogues and generating
		  informative responses. To tackle the challenges, we propose
		  a universal multimodal transformer and introduce the
		  multi-task learning method to learn joint representations
		  among different modalities as well as generate informative
		  and fluent responses by leveraging the pre-trained language
		  model. Our method extends the natural language generation
		  pre-trained model to multimodal dialogue generation task,
		  which allows fine-tuning language models to capture
		  information across both visual and textual modalities. Our
		  system achieves the best performance in the objective
		  evaluation in both DSTC7-AVSD and DSTC8-AVSD dataset and
		  achieves an impressive 98.4% of the human performance based
		  on human ratings in the DSTC8-AVSD challenge.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {2476–2483},
  numpages	= {8}
}

@Proceedings{	  10.1145/3605801,
  title		= {CNCIT '23: Proceedings of the 2023 2nd International
		  Conference on Networks, Communications and Information
		  Technology},
  year		= {2023},
  isbn		= {9798400700620},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Qinghai, China}
}

@InProceedings{	  10.1145/3581783.3612425,
  author	= {Lin, Hongpeng and Ruan, Ludan and Xia, Wenke and Liu,
		  Peiyu and Wen, Jingyuan and Xu, Yixin and Hu, Di and Song,
		  Ruihua and Zhao, Wayne Xin and Jin, Qin and Lu, Zhiwu},
  title		= {TikTalk: A Video-Based Dialogue Dataset for Multi-Modal
		  Chitchat in Real World},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612425},
  doi		= {10.1145/3581783.3612425},
  abstract	= {To facilitate the research on intelligent and human-like
		  chatbots with multi-modal context, we introduce a new
		  video-based multi-modal dialogue dataset, called TikTalk.
		  We collect 38K videos from a popular video-sharing
		  platform, along with 367K conversations posted by users
		  beneath them. Users engage in spontaneous conversations
		  based on their multi-modal experiences from watching
		  videos, which helps recreate real-world chitchat context.
		  Compared to previous multi-modal dialogue datasets, the
		  richer context types in TikTalk lead to more diverse
		  conversations, but also increase the difficulty in
		  capturing human interests from intricate multi-modal
		  information to generate personalized responses. Moreover,
		  external knowledge is more frequently evoked in our
		  dataset. These facts reveal new challenges for multi-modal
		  dialogue models. We quantitatively demonstrate the
		  characteristics of TikTalk, propose a video-based
		  multi-modal chitchat task, and evaluate several dialogue
		  baselines. Experimental results indicate that the models
		  incorporating large language models (LLM) can generate more
		  diverse responses, while the model utilizing knowledge
		  graphs to introduce external knowledge performs the best
		  overall. Furthermore, no existing model can solve all the
		  above challenges well. There is still a large room for
		  future improvements, even for LLM with visual extensions.
		  Our dataset is available at
		  https://ruc-aimind.github.io/projects/TikTalk/.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {1303–1313},
  numpages	= {11},
  keywords	= {chitchat, dataset, multi-modal dialogue, real world},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@Article{	  10.1145/3576901,
  author	= {Sarkar, Souvika and Bijoy, Biddut Sarker and Saba, Syeda
		  Jannatus and Feng, Dongji and Mahajan, Yash and Amin,
		  Mohammad Ruhul and Islam, Sheikh Rabiul and Karmaker
		  (“Santu”), Shubhra Kanti},
  title		= {Ad-Hoc Monitoring of COVID-19 Global Research Trends for
		  Well-Informed Policy Making},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3576901},
  doi		= {10.1145/3576901},
  abstract	= {The COVID-19 pandemic has affected millions of people
		  worldwide with severe health, economic, social, and
		  political implications. Healthcare Policy Makers (HPMs) and
		  medical experts are at the core of responding to this
		  continuously evolving pandemic situation and are working
		  hard to contain the spread and severity of this relatively
		  unknown virus. Biomedical researchers are continually
		  discovering new information about this virus and
		  communicating the findings through scientific articles. As
		  such, it is crucial for HPMs and funding agencies to
		  monitor the COVID-19 research trend globally on a regular
		  basis. However, given the influx of biomedical research
		  articles, monitoring COVID-19 research trends has become
		  more challenging than ever, especially when HPMs want
		  on-demand guided search techniques with a set of topics of
		  interest in mind. Unfortunately, existing topic trend
		  modeling techniques are unable to serve this purpose as (1)
		  traditional topic models are unsupervised, and (2) HPMs in
		  different regions may have different topics of interest
		  that they want to track. &nbsp;&nbsp; To address this
		  problem, we introduce a novel computational task in this
		  article called Ad-Hoc Topic Tracking, which is essentially
		  a combination of zero-shot topic categorization and the
		  spatio-temporal analysis task. We then propose multiple
		  zero-shot classification methods to solve this task by
		  building on state-of-the-art language understanding
		  techniques. Next, we picked the best-performing method
		  based on its accuracy on a separate validation dataset and
		  then applied it to a corpus of recent biomedical research
		  articles to track COVID-19 research endeavors across the
		  globe using a spatio-temporal analysis. A demo website has
		  also been developed for HPMs to create custom
		  spatio-temporal visualizations of COVID-19 research trends.
		  The research outcomes demonstrate that the proposed
		  zero-shot classification methods can potentially facilitate
		  further research on this important subject matter. At the
		  same time, the spatio-temporal visualization tool will
		  greatly assist HPMs and funding agencies in making
		  well-informed policy decisions for advancing scientific
		  research efforts.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= feb,
  articleno	= {26},
  numpages	= {28},
  keywords	= {Topic models, zero-shot learning, COVID-19, policy making,
		  spatio-temporal analysis}
}

@InProceedings{	  10.1145/3404835.3463232,
  author	= {Zhang, Qi and Jia, Qinglin and Wang, Chuyuan and Li,
		  Jingjie and Wang, Zhaowei and He, Xiuqiang},
  title		= {AMM: Attentive Multi-field Matching for News
		  Recommendation},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3463232},
  doi		= {10.1145/3404835.3463232},
  abstract	= {Personalized news recommendation is a critical technology
		  to help users find interested news, and how to precisely
		  match users' interests and candidate news lies in the core
		  of news recommendation. Existing studies generally learn
		  user's interest vector by aggregating his/her browsed news
		  and then match it with the candidate news vector, which may
		  lose the textual semantic matching signals for
		  recommendation. In this paper, we propose an Attentive
		  Multi-field Matching (AMM) framework for news
		  recommendation which captures the semantic matching
		  representations between each browsed news and candidate
		  news, and then aggregates them as final user-news matching
		  signal. In addition, our method incorporates multi-field
		  information and designs a within-field and cross-field
		  matching mechanism, which leverages complementary
		  information from different fields (e.g., titles, abstracts
		  and bodies) and obtain the multi-field matching
		  representations. To achieve a comprehensive semantic
		  understanding, we employ the most popular language model
		  BERT to learn the matching representation of each
		  browsed-candidate news pair, and incorporate the attention
		  mechanism in aggregating procedure to characterize the
		  importance of each matching representation for the final
		  user-news matching signal. Experiments on the real world
		  datasets validate the effectiveness of AMM.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1588–1592},
  numpages	= {5},
  keywords	= {multi-field, news recommendation, semantic matching},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@Article{	  10.1145/3622933,
  author	= {Jia, Qi and Liu, Yizhu and Ren, Siyu and Zhu, Kenny Q.},
  title		= {Taxonomy of Abstractive Dialogue Summarization: Scenarios,
		  Approaches, and Future Directions},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3622933},
  doi		= {10.1145/3622933},
  abstract	= {Abstractive dialogue summarization generates a concise and
		  fluent summary covering the salient information in a
		  dialogue among two or more interlocutors. It has attracted
		  significant attention in recent years based on the massive
		  emergence of social communication platforms and an urgent
		  requirement for efficient dialogue information
		  understanding and digestion. Different from news or
		  articles in traditional document summarization, dialogues
		  bring unique characteristics and additional challenges,
		  including different language styles and formats, scattered
		  information, flexible discourse structures, and unclear
		  topic boundaries. This survey provides a comprehensive
		  investigation of existing work for abstractive dialogue
		  summarization from scenarios, approaches to evaluations. It
		  categorizes the task into two broad categories according to
		  the type of input dialogues, i.e., open-domain and
		  task-oriented, and presents a taxonomy of existing
		  techniques in three directions, namely, injecting dialogue
		  features, designing auxiliary training tasks, and using
		  additional data. A list of datasets under different
		  scenarios and widely accepted evaluation metrics are
		  summarized for completeness. After that, the trends of
		  scenarios and techniques are summarized, together with deep
		  insights into correlations between extensively exploited
		  features and different scenarios. Based on these analyses,
		  we recommend future directions, including more controlled
		  and complicated scenarios, technical innovations and
		  comparisons, publicly available datasets in special
		  domains, and so on.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {67},
  numpages	= {38},
  keywords	= {Dialogue summarization, dialogue context modeling,
		  abstractive summarization}
}

@Proceedings{	  10.1145/3586182,
  title		= {UIST '23 Adjunct: Adjunct Proceedings of the 36th Annual
		  ACM Symposium on User Interface Software and Technology},
  year		= {2023},
  isbn		= {9798400700965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {San Francisco, CA, USA}
}

@Article{	  10.1145/3554727,
  author	= {Dong, Chenhe and Li, Yinghui and Gong, Haifan and Chen,
		  Miaoxin and Li, Junxin and Shen, Ying and Yang, Min},
  title		= {A Survey of Natural Language Generation},
  year		= {2022},
  issue_date	= {August 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {8},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3554727},
  doi		= {10.1145/3554727},
  abstract	= {This article offers a comprehensive review of the research
		  on Natural Language Generation (NLG) over the past two
		  decades, especially in relation to data-to-text generation
		  and text-to-text generation deep learning methods, as well
		  as new applications of NLG technology. This survey aims to
		  (a) give the latest synthesis of deep learning research on
		  the NLG core tasks, as well as the architectures adopted in
		  the field; (b) detail meticulously and comprehensively
		  various NLG tasks and datasets, and draw attention to the
		  challenges in NLG evaluation, focusing on different
		  evaluation methods and their relationships; (c) highlight
		  some future emphasis and relatively recent research issues
		  that arise due to the increasing synergy between NLG and
		  other artificial intelligence areas, such as computer
		  vision, text, and computational creativity.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {173},
  numpages	= {38},
  keywords	= {Natural language generation, data-to-text generation,
		  text-to-text generation, deep learning, evaluation}
}

@InProceedings{	  10.1145/3442381.3449852,
  author	= {Liu, Qian and Geng, Xiubo and Lu, Jie and Jiang, Daxin},
  title		= {Pivot-based Candidate Retrieval for Cross-lingual Entity
		  Linking},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3449852},
  doi		= {10.1145/3442381.3449852},
  abstract	= {Entity candidate retrieval plays a critical role in
		  cross-lingual entity linking (XEL). In XEL, entity
		  candidate retrieval needs to retrieve a list of plausible
		  candidate entities from a large knowledge graph in a target
		  language given a piece of text in a sentence or question,
		  namely a mention, in a source language. Existing works
		  mainly fall into two categories: lexicon-based and
		  semantic-based approaches. The lexicon-based approach
		  usually creates cross-lingual and mention-entity lexicons,
		  which is effective but relies heavily on bilingual
		  resources (e.g. inter-language links in Wikipedia). The
		  semantic-based approach maps mentions and entities in
		  different languages to a unified embedding space, which
		  reduces dependence on large-scale bilingual dictionaries.
		  However, its effectiveness is limited by the representation
		  capacity of fixed-length vectors. In this paper, we propose
		  a pivot-based approach which inherits the advantages of the
		  aforementioned two approaches while avoiding their
		  limitations. It takes an intermediary set of plausible
		  target-language mentions as pivots to bridge the two types
		  of gaps: cross-lingual gap and mention-entity gap.
		  Specifically, it first converts mentions in the source
		  language into an intermediary set of plausible mentions in
		  the target language by cross-lingual semantic retrieval and
		  a selective mechanism, and then retrieves candidate
		  entities based on the generated mentions by lexical
		  retrieval. The proposed approach only relies on a small
		  bilingual word dictionary, and fully exploits the benefits
		  of both lexical and semantic matching. Experimental results
		  on two challenging cross-lingual entity linking datasets
		  spanning over 11 languages show that the pivot-based
		  approach outperforms both the lexicon-based and
		  semantic-based approach by a large margin.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {1076–1085},
  numpages	= {10},
  keywords	= {Information extraction, cross-lingual retrieval, entity
		  linking},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3578503.3583625,
  author	= {Sha, Alyssa Shuang and Nunes, Bernardo Pereira and Haller,
		  Armin},
  title		= {Link Topics from Q&amp;A Platforms using Wikidata: A Tool
		  for Cross-platform Hierarchical Classification},
  year		= {2023},
  isbn		= {9798400700897},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3578503.3583625},
  doi		= {10.1145/3578503.3583625},
  abstract	= {This paper proposes a novel rule-based topic
		  classification tool for questions on Q&amp;A platforms
		  mediated by the Wikidata ontology – an open and
		  accessible multilingual ontology curated by a large
		  community of online users. Q&amp;A platforms are important
		  sources of information on the Web and often appear as part
		  of Web search results. By adopting Wikidata taxonomic
		  relations as references, our tool can categories the Web
		  content from different platforms in a unified
		  coarse-to-fine mode based on their domain coverage. To
		  validate and demonstrate the potential applicability of our
		  tool, a set of use cases and experiments are carried out on
		  two popular Q&amp;A platforms – Zhihu and Quora, where
		  the impact of topic categories on question lifecycles is
		  explored. Furthermore, we compare our results with the
		  output generated by GPT-3 classifier. This tool sheds light
		  on how structured knowledge bases can enable data
		  interoperability and serve as a filtering functionality to
		  mitigate classification bias of OpenAI.},
  booktitle	= {Proceedings of the 15th ACM Web Science Conference 2023},
  pages		= {357–362},
  numpages	= {6},
  keywords	= {Entity Linking, Q&amp;A platforms, Topic classification,
		  Wikidata ontology},
  location	= {Austin, TX, USA},
  series	= {WebSci '23}
}

@InProceedings{	  10.1145/3442381.3450141,
  author	= {Daza, Daniel and Cochez, Michael and Groth, Paul},
  title		= {Inductive Entity Representations from Text via Link
		  Prediction},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450141},
  doi		= {10.1145/3442381.3450141},
  abstract	= {Knowledge Graphs (KG) are of vital importance for multiple
		  applications on the web, including information retrieval,
		  recommender systems, and metadata annotation. Regardless of
		  whether they are built manually by domain experts or with
		  automatic pipelines, KGs are often incomplete. To address
		  this problem, there is a large amount of work that proposes
		  using machine learning to complete these graphs by
		  predicting new links. Recent work has begun to explore the
		  use of textual descriptions available in knowledge graphs
		  to learn vector representations of entities in order to
		  preform link prediction. However, the extent to which these
		  representations learned for link prediction generalize to
		  other tasks is unclear. This is important given the cost of
		  learning such representations. Ideally, we would prefer
		  representations that do not need to be trained again when
		  transferring to a different task, while retaining
		  reasonable performance. Therefore, in this work, we propose
		  a holistic evaluation protocol for entity representations
		  learned via a link prediction objective. We consider the
		  inductive link prediction and entity classification tasks,
		  which involve entities not seen during training. We also
		  consider an information retrieval task for entity-oriented
		  search. We evaluate an architecture based on a pretrained
		  language model, that exhibits strong generalization to
		  entities not observed during training, and outperforms
		  related state-of-the-art methods (22% MRR improvement in
		  link prediction on average). We further provide evidence
		  that the learned representations transfer well to other
		  tasks without fine-tuning. In the entity classification
		  task we obtain an average improvement of 16% in accuracy
		  compared with baselines that also employ pre-trained
		  models. In the information retrieval task, we obtain
		  significant improvements of up to 8.8% in NDCG@10 for
		  natural language queries. We thus show that the learned
		  representations are not limited KG-specific tasks, and have
		  greater generalization properties than evaluated in
		  previous work.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {798–808},
  numpages	= {11},
  keywords	= {entity classification, entity representations, information
		  retrieval, knowledge graphs, link prediction},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Article{	  10.1145/3617680,
  author	= {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou,
		  Ming and Song, Dawei},
  title		= {A Survey of Controllable Text Generation Using
		  Transformer-based Pre-trained Language Models},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3617680},
  doi		= {10.1145/3617680},
  abstract	= {Controllable Text Generation (CTG) is an emerging area in
		  the field of natural language generation (NLG). It is
		  regarded as crucial for the development of advanced text
		  generation technologies that better meet the specific
		  constraints in practical applications. In recent years,
		  methods using large-scale pre-trained language models
		  (PLMs), in particular the widely used Transformer-based
		  PLMs, have become a new paradigm of NLG, allowing
		  generation of more diverse and fluent text. However, due to
		  the limited level of interpretability of deep neural
		  networks, the controllability of these methods needs to be
		  guaranteed. To this end, controllable text generation using
		  Transformer-based PLMs has become a rapidly growing yet
		  challenging new research hotspot. A diverse range of
		  approaches have emerged in the past 3 to 4 years, targeting
		  different CTG tasks that require different types of
		  controlled constraints. In this article, we present a
		  systematic critical review on the common tasks, main
		  approaches, and evaluation methods in this area. Finally,
		  we discuss the challenges that the field is facing, and put
		  forward various promising future directions. To the best of
		  our knowledge, this is the first survey article to
		  summarize the state-of-the-art CTG techniques from the
		  perspective of Transformer-based PLMs. We hope it can help
		  researchers and practitioners in the related fields to
		  quickly track the academic and technological frontier,
		  providing them with a landscape of the area and a roadmap
		  for future research.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {64},
  numpages	= {37},
  keywords	= {Controllable text generation, pre-trained language models,
		  Transformer, controllability, systematic review}
}

@InProceedings{	  10.1145/3404835.3463000,
  author	= {Li, Yanran and Li, Wenjie and Wang, Zhitao},
  title		= {Graph-Structured Context Understanding for
		  Knowledge-grounded Response Generation},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3463000},
  doi		= {10.1145/3404835.3463000},
  abstract	= {In this work, we establish a context graph from both
		  conversation utterances and external knowledge, and develop
		  a novel graph-based encoder to better understand the
		  conversation context. Specifically, the encoder fuses the
		  information in the context graph stage-by-stage and
		  provides global context-graph-aware representations of each
		  node in the graph to facilitate knowledge-grounded response
		  generation. On a large-scale conversation corpus, we
		  validate the effectiveness of the proposed approach and
		  demonstrate the benefit of knowledge in conversation
		  understanding.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1930–1934},
  numpages	= {5},
  keywords	= {dialogue systems, knowledge-grounded response generation},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@Article{	  10.1145/3609483,
  author	= {Moscato, Vincenzo and Postiglione, Marco and Sperl\'{\i},
		  Giancarlo},
  title		= {Few-shot Named Entity Recognition: Definition, Taxonomy
		  and Research Directions},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {5},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3609483},
  doi		= {10.1145/3609483},
  abstract	= {Recent years have seen an exponential growth (+98% in 2022
		  w.r.t. the previous year) of the number of research
		  articles in the few-shot learning field, which aims at
		  training machine learning models with extremely limited
		  available data. The research interest toward few-shot
		  learning systems for Named Entity Recognition (NER) is thus
		  at the same time increasing. NER consists in identifying
		  mentions of pre-defined entities from unstructured text,
		  and serves as a fundamental step in many downstream tasks,
		  such as the construction of Knowledge Graphs, or Question
		  Answering. The need for a NER system able to be trained
		  with few-annotated examples comes in all its urgency in
		  domains where the annotation process requires time,
		  knowledge and expertise (e.g., healthcare, finance, legal),
		  and in low-resource languages. In this survey, starting
		  from a clear definition and description of the few-shot NER
		  (FS-NER) problem, we take stock of the current
		  state-of-the-art and propose a taxonomy which divides
		  algorithms in two macro-categories according to the
		  underlying mechanisms: model-centric and data-centric. For
		  each category, we line-up works as a story to show how the
		  field is moving toward new research directions. Eventually,
		  techniques, limitations, and key aspects are deeply
		  analyzed to facilitate future studies.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  articleno	= {94},
  numpages	= {46},
  keywords	= {Few-shot learning, Named Entity Recognition}
}

@Article{	  10.1145/3446343,
  author	= {Bashar, Md Abul and Nayak, Richi},
  title		= {Active Learning for Effectively Fine-Tuning Transfer
		  Learning to Downstream Task},
  year		= {2021},
  issue_date	= {April 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {12},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3446343},
  doi		= {10.1145/3446343},
  abstract	= {Language model (LM) has become a common method of transfer
		  learning in Natural Language Processing (NLP) tasks when
		  working with small labeled datasets. An LM is pretrained
		  using an easily available large unlabelled text corpus and
		  is fine-tuned with the labelled data to apply to the target
		  (i.e., downstream) task. As an LM is designed to capture
		  the linguistic aspects of semantics, it can be biased to
		  linguistic features. We argue that exposing an LM model
		  during fine-tuning to instances that capture diverse
		  semantic aspects (e.g., topical, linguistic, semantic
		  relations) present in the dataset will improve its
		  performance on the underlying task. We propose a Mixed
		  Aspect Sampling (MAS) framework to sample instances that
		  capture different semantic aspects of the dataset and use
		  the ensemble classifier to improve the classification
		  performance. Experimental results show that MAS performs
		  better than random sampling as well as the state-of-the-art
		  active learning models to abuse detection tasks where it is
		  hard to collect the labelled data for building an accurate
		  classifier.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= feb,
  articleno	= {24},
  numpages	= {24},
  keywords	= {Misogynistic tweet, active learning, hate speech,
		  imbalanced dataset, topic model, transfer learning}
}

@InProceedings{	  10.1145/3587259.3627572,
  author	= {Rula, Anisa and D'Souza, Jennifer},
  title		= {Procedural Text Mining with Large Language Models},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627572},
  doi		= {10.1145/3587259.3627572},
  abstract	= {Recent advancements in the field of Natural Language
		  Processing, particularly the development of large-scale
		  language models that are pretrained on vast amounts of
		  knowledge, are creating novel opportunities within the
		  realm of Knowledge Engineering. In this paper, we
		  investigate the usage of large language models (LLMs) in
		  both zero-shot and in-context learning settings to tackle
		  the problem of extracting procedures from unstructured PDF
		  text in an incremental question-answering fashion. In
		  particular, we leverage the current state-of-the-art GPT-4
		  (Generative Pre-trained Transformer 4) model, accompanied
		  by two variations of in-context learning that involve an
		  ontology with definitions of procedures and steps and a
		  limited number of samples of few-shot learning. The
		  findings highlight both the promise of this approach and
		  the value of the in-context learning customisations. These
		  modifications have the potential to significantly address
		  the challenge of obtaining sufficient training data, a
		  hurdle often encountered in deep learning-based Natural
		  Language Processing techniques for procedure extraction.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {9–16},
  numpages	= {8},
  keywords	= {knowledge capture, knowledge representation},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@Proceedings{	  10.1145/3591196,
  title		= {C&amp;C '23: Proceedings of the 15th Conference on
		  Creativity and Cognition},
  year		= {2023},
  isbn		= {9798400701801},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, USA}
}

@InProceedings{	  10.1145/3584931.3606951,
  author	= {Lim, Gionnieve and Kim, Hyunwoo and Choi, Yoonseo and Li,
		  Toby Jia-Jun and Kulkarni, Chinmay and Subramonyam,
		  Hariharan and Seering, Joseph and Bernstein, Michael S. and
		  Zhang, Amy X. and Glassman, Elena L. and Perrault, Simon
		  and Kim, Juho},
  title		= {Designing for AI-Powered Social Computing Systems},
  year		= {2023},
  isbn		= {9798400701290},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584931.3606951},
  doi		= {10.1145/3584931.3606951},
  abstract	= {The CSCW community has been active in designing,
		  implementing, and evaluating novel social computing
		  systems. In recent years, there has been a rise in using AI
		  to empower social interactions and the capabilities of
		  these systems. While these implementations charge ahead of
		  the establishment of ethical and legal frameworks, it is
		  timely to reflect on the state of AI-powered social
		  computing systems and to identify new research agendas for
		  the community. This Special Interest Group aims to bring in
		  researchers and practitioners from different fields to
		  foster discussions on the key considerations and challenges
		  in designing for AI-powered social computing systems and to
		  promote opportunities for new research collaborations.},
  booktitle	= {Companion Publication of the 2023 Conference on Computer
		  Supported Cooperative Work and Social Computing},
  pages		= {572–575},
  numpages	= {4},
  keywords	= {artificial intelligence, design, human-AI interaction,
		  social computing systems},
  location	= {Minneapolis, MN, USA},
  series	= {CSCW '23 Companion}
}

@InProceedings{	  10.1145/3474085.3475545,
  author	= {Tian, Hongshuo and Xu, Ning and Liu, An-An and Yan,
		  Chenggang and Mao, Zhendong and Zhang, Quan and Zhang,
		  Yongdong},
  title		= {Mask and Predict: Multi-step Reasoning for Scene Graph
		  Generation},
  year		= {2021},
  isbn		= {9781450386517},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3474085.3475545},
  doi		= {10.1145/3474085.3475545},
  abstract	= {Scene Graph Generation (SGG) aims to parse the image as a
		  set of semantics, containing objects and their relations.
		  Currently, the SGG methods only stay at presenting the
		  intuitive detection in the image, such as the triplet "logo
		  on board". Intuitively, we humans can further refine these
		  intuitive detections as rational descriptions like "flower
		  painted on surfboard". However, most of existing methods
		  always formulate SGG as a straightforward task, only
		  limited by the manner of one-time prediction, which focuses
		  on a single-pass pipeline and predicts all the semantic.
		  Therefore, to handle this problem, we propose a novel
		  multi-step reasoning manner for SGG. Concretely, we break
		  SGG into two explicit learning stages, including intuitive
		  training stage (ITS) and rational training stage (RTS). In
		  the first stage, we follow the traditional SGG processing
		  to detect objects and relationships, yielding an intuitive
		  scene graph. In the second stage, we perform multi-step
		  reasoning to refine the intuitive scene graph. For each
		  step of reasoning, it consists of two kinds of operations:
		  mask and predict. According to primary predictions and
		  their confidences, we constantly select and mask the
		  low-confidence predictions, which features are optimized
		  and predicted again. After several iterations, all of
		  intuitive semantics will gradually tend to be revised with
		  high confidences, yielding a rational scene graph.
		  Extensive experiments on Visual Genome prove the
		  superiority of the proposed method. Additional ablation
		  studies and visualization cases further validate its
		  effectiveness.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Multimedia},
  pages		= {4128–4136},
  numpages	= {9},
  keywords	= {mask and predict, multi-step reasoning, scene graph},
  location	= {Virtual Event, China},
  series	= {MM '21}
}

@Article{	  10.1109/taslp.2021.3123885,
  author	= {Liao, Xianwen and Huang, Yongzhong and Wei, Yongzhuang and
		  Zhang, Chenhao and Wang, Fu and Wang, Yong},
  title		= {Efficient Estimate of Sentence's Representation Based on
		  the Difference Semantics Model},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3123885},
  doi		= {10.1109/TASLP.2021.3123885},
  abstract	= {Sentence representation is an important research hotspot
		  in natural language processing (NLP) since it can map the
		  semantics of sentences into semantics vectors, thereby
		  effectively solving complex semantics computing problems.
		  Recently, sentence representations are mainly obtained by
		  indirect means. Specifically, for sentence representations
		  obtained by unsupervised means, they are often calculated
		  by the weighted sum of embeddings of tokens in sentences;
		  for sentence representations obtained by self-supervised or
		  supervised means, they are often derived from intermediate
		  encodings of sentences in prediction tasks. For example,
		  Google's BERT and MUSE respectively use the embedding of
		  [CLS] in the next sentence prediction task and intermediate
		  encodings of sentences in the translation bridge task as
		  sentence representations. In this paper, we use the
		  observed semantics increment feature of sentences to
		  directly model the semantics function of sentences. To be
		  able to use the existing neural network language model to
		  approximate the semantics function, we first implement the
		  first-order Taylor expansion on the semantics function to
		  obtain a difference semantics model and then add it to BERT
		  as a subtask to perform self-supervised fine-tuning.
		  Finally, we get a new sentence representation model S-BERT.
		  S-BERT achieves the state-of-the-art performance on many
		  datasets in Chinese, English, and Vietnamese.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= oct,
  pages		= {3384–3399},
  numpages	= {16}
}

@InProceedings{	  10.1145/3584931.3611284,
  author	= {Boonprakong, Nattapat and He, Gaole and Gadiraju, Ujwal
		  and van Berkel, Niels and Wang, Danding and Chen, Si and
		  Liu, Jiqun and Tag, Benjamin and Goncalves, Jorge and
		  Dingler, Tilman},
  title		= {Workshop on Understanding and Mitigating Cognitive Biases
		  in Human-AI Collaboration},
  year		= {2023},
  isbn		= {9798400701290},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584931.3611284},
  doi		= {10.1145/3584931.3611284},
  abstract	= {AI systems are increasingly incorporated into human
		  decision-making. Yet, human decision-makers are often
		  affected by their cognitive biases. In critical settings,
		  such as medical diagnosis, criminal judgment, or
		  information consumption, these cognitive biases hinder
		  optimal decision outcomes, thereby resulting in dangerous
		  decisions and negative societal impact. The use of AI
		  systems can amplify and exacerbate cognitive biases in
		  their users. In this workshop, we seek to foster
		  discussions on ongoing research around cognitive biases in
		  human-AI collaboration and identify future research
		  directions to understand, quantify, and mitigate the
		  effects of cognitive biases. We will explore cognitive
		  biases appearing in various contexts of human-AI
		  collaboration: what can cause them?; how can we measure,
		  model, mitigate, and manage cognitive biases?; and how can
		  we utilise cognitive biases for the greater good? We will
		  reflect on workshop discussions to form a research
		  community around cognitive biases and bias-aware systems.},
  booktitle	= {Companion Publication of the 2023 Conference on Computer
		  Supported Cooperative Work and Social Computing},
  pages		= {512–517},
  numpages	= {6},
  keywords	= {Cognitive Bias, Debiasing, Human-AI Collaboration},
  location	= {Minneapolis, MN, USA},
  series	= {CSCW '23 Companion}
}

@InProceedings{	  10.1145/3581641.3584065,
  author	= {Pataranutaporn, Pat and Danry, Valdemar and Blanchard,
		  Lancelot and Thakral, Lavanay and Ohsugi, Naoki and Maes,
		  Pattie and Sra, Misha},
  title		= {Living Memories: AI-Generated Characters as Digital
		  Mementos},
  year		= {2023},
  isbn		= {9798400701061},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581641.3584065},
  doi		= {10.1145/3581641.3584065},
  abstract	= {Every human culture has developed practices and rituals
		  associated with remembering people of the past - be it for
		  mourning, cultural preservation, or learning about
		  historical events. In this paper, we present the concept of
		  “Living Memories”: interactive digital mementos that
		  are created from journals, letters and data that an
		  individual have left behind. Like an interactive
		  photograph, living memories can be talked to and asked
		  questions, making accessing the knowledge, attitudes and
		  past experiences of a person easily accessible. To
		  demonstrate our concept, we created an AI-based system for
		  generating living memories from any data source and
		  implemented living memories of the three historical figures
		  “Leonardo Da Vinci”, “Murasaki Shikibu”, and
		  “Captain Robert Scott”. As a second key contribution,
		  we present a novel metrics scheme for evaluating the
		  accuracy of living memory architectures and show the
		  accuracy of our pipeline to improve over baselines.
		  Finally, we compare the user experience and learning
		  effects of interacting with the living memory of Leonardo
		  Da Vinci to reading his journal. Our results show that
		  interacting with the living memory, in addition to simply
		  reading a journal, increases learning effectiveness and
		  motivation to learn about the character.},
  booktitle	= {Proceedings of the 28th International Conference on
		  Intelligent User Interfaces},
  pages		= {889–901},
  numpages	= {13},
  keywords	= {AI, AI-Generated Characters, Human-AI Interaction},
  location	= {Sydney, NSW, Australia},
  series	= {IUI '23}
}

@Proceedings{	  10.1145/3571884,
  title		= {CUI '23: Proceedings of the 5th International Conference
		  on Conversational User Interfaces},
  year		= {2023},
  isbn		= {9798400700149},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Eindhoven, Netherlands}
}

@InProceedings{	  10.1145/3442381.3450029,
  author	= {Yu, Bowen and Zhang, Zhenyu and Sheng, Jiawei and Liu,
		  Tingwen and Wang, Yubin and Wang, Yucheng and Wang, Bin},
  title		= {Semi-Open Information Extraction},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450029},
  doi		= {10.1145/3442381.3450029},
  abstract	= {Open Information Extraction (OIE), the task aimed at
		  discovering all textual facts organized in the form of
		  (subject, predicate, object) found within a sentence, has
		  gained much attention recently. However, in some
		  knowledge-driven applications such as question answering,
		  we often have a target entity and hope to obtain its
		  structured factual knowledge for better understanding,
		  instead of extracting all possible facts aimlessly from the
		  corpus. In this paper, we define a new task, namely
		  Semi-Open Information Extraction (SOIE), to address this
		  need. The goal of SOIE is to discover domain-independent
		  facts towards a particular entity from general and diverse
		  web text. To facilitate research on this new task, we
		  propose a large-scale human-annotated benchmark called
		  SOIED, consisting of 61,984 facts for 8,013 subject
		  entities annotated on 24,000 Chinese sentences collected
		  from the web search engine. In addition, we propose a novel
		  unified model called USE for this task. First, we introduce
		  subject-guided sequence as input to a pre-trained language
		  model and normalize the hidden representations conditioned
		  on the subject embedding to encode the sentence in a
		  subject-aware manner. Second, we decompose SOIE into three
		  uncoupled subtasks: predicate extraction, object
		  extraction, and boundary alignment. They can all be
		  formulated as the problem of table filling by forming a
		  two-dimensional tag table based on a task-specific tagging
		  scheme. Third, we introduce a collaborative learning
		  strategy that enables the interactive relations among
		  subtasks to be better exploited by explicitly exchanging
		  informative clues. Finally, we evaluate USE and several
		  strong baselines on our new dataset. Experimental results
		  demonstrate the advantages of the proposed method and
		  reveal insight for future improvement.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {1661–1672},
  numpages	= {12},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3587259.3627545,
  author	= {Theodoropoulos, Christos and Mulligan, Natalia and
		  Stappenbeck, Thaddeus and Bettencourt-Silva, Joao},
  title		= {Representation Learning for Person or Entity-Centric
		  Knowledge Graphs: An Application in Healthcare},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627545},
  doi		= {10.1145/3587259.3627545},
  abstract	= {Knowledge graphs (KGs) are a popular way to organise
		  information based on ontologies or schemas. Despite
		  advances in KGs, representing knowledge remains a
		  non-trivial task across industries and it is especially
		  challenging in the biomedical and healthcare domains due to
		  complex interdependent relations between entities,
		  heterogeneity, lack of standardization, and sparseness of
		  data. KGs are used to discover diagnoses or prioritize
		  genes relevant to disease, but they often rely on schemas
		  that are not centred around a node or entity of interest,
		  such as a person. Entity-centric KGs are relatively
		  unexplored but hold promise in representing important
		  facets connected to a central node and unlocking downstream
		  tasks beyond graph traversal and reasoning, such as
		  training graph neural networks (GNNs) for a wide range of
		  predictive tasks. This paper presents an end-to-end
		  representation learning framework to extract entity-centric
		  KGs from structured and unstructured data. We introduce a
		  star-shaped ontology to represent the multiple facets of a
		  person and use it to guide KG creation. Compact
		  representations of the graphs are created leveraging GNNs
		  and experiments are conducted using different levels of
		  heterogeneity or explicitness. A readmission prediction
		  task is used to evaluate the results of the proposed
		  framework, showing a stable system, robust to missing data,
		  that outperforms a range of baseline machine learning
		  classifiers. We highlight that this approach has several
		  potential applications across domains and is
		  open-sourced.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {225–233},
  numpages	= {9},
  keywords	= {Entity-Centric Knowledge Graphs, Graph Neural Networks,
		  Person-Centric Ontology, Representation Learning},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@Proceedings{	  10.1145/3581754,
  title		= {IUI '23 Companion: Companion Proceedings of the 28th
		  International Conference on Intelligent User Interfaces},
  year		= {2023},
  isbn		= {9798400701078},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sydney, NSW, Australia}
}

@Article{	  10.1145/3489142,
  author	= {Li, Qun and Xiao, Fu and Bhanu, Bir and Sheng, Biyun and
		  Hong, Richang},
  title		= {Inner Knowledge-based Img2Doc Scheme for Visual Question
		  Answering},
  year		= {2022},
  issue_date	= {August 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {3},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3489142},
  doi		= {10.1145/3489142},
  abstract	= {Visual Question Answering (VQA) is a research topic of
		  significant interest at the intersection of computer vision
		  and natural language understanding. Recent research
		  indicates that attributes and knowledge can effectively
		  improve performance for both image captioning and VQA. In
		  this article, an inner knowledge-based Img2Doc algorithm
		  for VQA is presented. The inner knowledge is characterized
		  as the inner attribute relationship in visual images. In
		  addition to using an attribute network for inner
		  knowledge-based image representation, VQA scheme is
		  associated with a question-guided Doc2Vec method for
		  question–answering. The attribute network generates inner
		  knowledge-based features for visual images, while a novel
		  question-guided Doc2Vec method aims at converting natural
		  language text to vector features. After the vector features
		  are extracted, they are combined with visual image features
		  into a classifier to provide an answer. Based on our model,
		  the VQA problem is resolved by textual question answering.
		  The experimental results demonstrate that the proposed
		  method achieves superior performance on multiple benchmark
		  datasets.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= mar,
  articleno	= {76},
  numpages	= {21},
  keywords	= {VQA, dense image captioning, Doc2Vec, inner
		  knowledge-based, attribute network}
}

@Article{	  10.1145/3624733,
  author	= {Deldjoo, Yashar and Nazary, Fatemeh and Ramisa, Arnau and
		  McAuley, Julian and Pellegrini, Giovanni and Bellogin,
		  Alejandro and Noia, Tommaso Di},
  title		= {A Review of Modern Fashion Recommender Systems},
  year		= {2023},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3624733},
  doi		= {10.1145/3624733},
  abstract	= {The textile and apparel industries have grown tremendously
		  over the past few years. Customers no longer have to visit
		  many stores, stand in long queues, or try on garments in
		  dressing rooms, as millions of products are now available
		  in online catalogs. However, given the plethora of options
		  available, an effective recommendation system is necessary
		  to properly sort, order, and communicate relevant product
		  material or information to users. Effective fashion
		  recommender systems (RSs) can have a noticeable impact on
		  billions of customers’ shopping experiences and increase
		  sales and revenues on the provider side.The goal of this
		  survey is to provide a review of RSs that operate in the
		  specific vertical domain of garment and fashion products.
		  We have identified the most pressing challenges in fashion
		  RS research and created a taxonomy that categorizes the
		  literature according to the objective they are trying to
		  accomplish (e.g., item or outfit recommendation, size
		  recommendation, and explainability, among others) and type
		  of side information (users, items, context). We have also
		  identified the most important evaluation goals and
		  perspectives (outfit generation, outfit recommendation,
		  pairing recommendation, and fill-in-the-blank outfit
		  compatibility prediction) and the most commonly used
		  datasets and evaluation metrics.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {87},
  numpages	= {37},
  keywords	= {Recommender systems, information retrieval, fashion
		  retail, machine learning, artificial intelligence, computer
		  vision, text mining, e-commerce}
}

@Proceedings{	  10.1145/3591106,
  title		= {ICMR '23: Proceedings of the 2023 ACM International
		  Conference on Multimedia Retrieval},
  year		= {2023},
  isbn		= {9798400701788},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Thessaloniki, Greece}
}

@Proceedings{	  10.1145/3616961,
  title		= {Mindtrek '23: Proceedings of the 26th International
		  Academic Mindtrek Conference},
  year		= {2023},
  isbn		= {9798400708749},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tampere, Finland}
}

@Article{	  10.1145/3465074.3465080,
  author	= {Talamadupula, Kartik},
  title		= {Applied AI matters: AI4Code: applying artificial
		  intelligence to source code},
  year		= {2021},
  issue_date	= {March 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {1},
  url		= {https://doi.org/10.1145/3465074.3465080},
  doi		= {10.1145/3465074.3465080},
  abstract	= {The marriage of Artificial Intelligence (AI) techniques to
		  problems surrounding the generation, maintenance, and use
		  of source code has come to the fore in recent years as an
		  important AI application area1. A large chunk of this
		  recent attention can be attributed to contemporaneous
		  advancements in Natural Language Processing (NLP)
		  techniques and sub-fields. The naturalness hypothesis,
		  which states that "software is a form of human
		  communication" and that code exhibits patterns that are
		  similar to (human) natural languages (Devanbu, 2015;
		  Hindle, Barr, Gabel, Su, &amp; Devanbu, 2016), has allowed
		  for the application of many of these NLP advances to
		  code-centric usecases. This development has contributed to
		  a spate of work in the community --- much of it captured in
		  a survey by Allamanis, Barr, Devanbu, and Sutton (2018)
		  that focuses on classifying these approaches by the type of
		  probabilistic model applied to source code.This increase in
		  the variety of AI techniques applied to source code has
		  found various manifestations in the industry at large. Code
		  and software form the backbone that underpins almost all
		  modern technical advancements: it is thus natural that
		  breakthroughs in this area should reflect in the emergence
		  of real world deployments.},
  journal	= {AI Matters},
  month		= jul,
  pages		= {18–20},
  numpages	= {3}
}

@InProceedings{	  10.1145/3485447.3512018,
  author	= {Truong, Quoc-Tuan and Zhao, Tong and Yuan, Changhe and Li,
		  Jin and Chan, Jim and Pantel, Soo-Min and Lauw, Hady W.},
  title		= {AmpSum: Adaptive Multiple-Product Summarization towards
		  Improving Recommendation Captions},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3512018},
  doi		= {10.1145/3485447.3512018},
  abstract	= {In e-commerce websites, multiple related product
		  recommendations are usually organized into “widgets”,
		  each given a name, as a recommendation caption, to describe
		  the products within. These recommendation captions are
		  usually manually crafted and generic in nature, making it
		  difficult to attach meaningful and informative names at
		  scale. As a result, the captions are inadequate in helping
		  customers to better understand the connection between the
		  multiple recommendations and make faster product discovery.
		  We propose an Adaptive Multiple-Product Summarization
		  framework (AmpSum) that automatically and adaptively
		  generates widget captions based on different recommended
		  products. The multiplicity of products to be summarized in
		  a widget caption is particularly novel. The lack of
		  well-developed labels motivates us to design a weakly
		  supervised learning approach with distant supervision to
		  bootstrap the model learning from pseudo labels, and then
		  fine-tune the model with a small amount of manual labels.
		  To validate the efficacy of this method, we conduct
		  extensive experiments on several product categories of
		  Amazon data. The results demonstrate that our proposed
		  framework consistently outperforms state-of-the-art
		  baselines over 9.47-29.14% on ROUGE and 27.31% on METEOR.
		  With case studies, we illustrate how AmpSum could
		  adaptively generate summarization based on different
		  product recommendations.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {2978–2988},
  numpages	= {11},
  keywords	= {Multiple-Product Summarization, Product Summarization,
		  Recommendation Captions},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3576840.3578330,
  author	= {El Zein, Dima and C\^{a}mara, Arthur and Da Costa Pereira,
		  C\'{e}lia and Tettamanzi, Andrea},
  title		= {RULKNE: Representing User Knowledge State in
		  Search-as-Learning with Named Entities},
  year		= {2023},
  isbn		= {9798400700354},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3576840.3578330},
  doi		= {10.1145/3576840.3578330},
  abstract	= {A reliable representation of the user’s knowledge state
		  during a learning search session is crucial to understand
		  their real information needs. When a search system is aware
		  of such a state, it can adapt the search results and
		  provide greater support for the user’s learning
		  objectives. A common practice to track the user’s
		  knowledge state is to consider the content of the documents
		  they read during their search session(s). However, most
		  current work ignores entity mentions in the documents,
		  which, when linked to knowledge graphs, can be a source of
		  valuable information regarding the user’s knowledge. To
		  fill this gap, we extend RULK—Representing User Knowledge
		  in Search-as-Learning—with entity linking capabilities.
		  The extended framework RULK represents and tracks user
		  knowledge as a collection of such entities. It eventually
		  estimates the user knowledge gain—learning outcome—by
		  measuring the similarity between the represented knowledge
		  and the learning objective. We show that our methods allow
		  for up to 10% improvements when estimating user knowledge
		  gains.},
  booktitle	= {Proceedings of the 2023 Conference on Human Information
		  Interaction and Retrieval},
  pages		= {388–393},
  numpages	= {6},
  keywords	= {Interactive IR, Named Entities, Retrieval system,
		  Search-As-Learning, User Knowledge},
  location	= {Austin, TX, USA},
  series	= {CHIIR '23}
}

@Article{	  10.1145/3575803,
  author	= {Di, Donglin and Song, Xianyang and Zhang, Weinan and
		  Zhang, Yue and Wang, Fanglin},
  title		= {Building Dialogue Understanding Models for Low-resource
		  Language Indonesian from Scratch},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3575803},
  doi		= {10.1145/3575803},
  abstract	= {Using off-the-shelf resources from resource-rich languages
		  to transfer knowledge to low-resource languages has
		  received a lot of attention. The requirements of enabling
		  the model to achieve the reliable performance, including
		  the scale of required annotated data and the effective
		  framework, are not well guided. To address the first
		  question, we empirically investigate the cost-effectiveness
		  of several methods for training intent classification and
		  slot-filling models from scratch in Indonesia (ID) using
		  English data. Confronting the second challenge, we propose
		  a Bi-Confidence-Frequency Cross-Lingual transfer framework
		  (BiCF), which consists of “BiCF Mixing”, “Latent
		  Space Refinement” and “Joint Decoder”, respectively,
		  to overcome the lack of low-resource language dialogue
		  data. BiCF Mixing based on the word-level alignment
		  strategy generates code-mixed data by utilizing the
		  importance-frequency and translating-confidence. Moreover,
		  Latent Space Refinement trains a new dialogue understanding
		  model using code-mixed data and word embedding models.
		  Joint Decoder based on Bidirectional LSTM (BiLSTM) and
		  Conditional Random Field (CRF) is used to obtain
		  experimental results of intent classification and
		  slot-filling. We also release a large-scale fine-labeled
		  Indonesia dialogue dataset (ID-WOZ1) and ID-BERT for
		  experiments. BiCF achieves 93.56% and 85.17% (F1 score) on
		  intent classification and slot filling, respectively.
		  Extensive experiments demonstrate that our framework
		  performs reliably and cost-efficiently on different scales
		  of manually annotated Indonesian data.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {105},
  numpages	= {20},
  keywords	= {Dialogue datasets, intent classification, slot-filling,
		  indonesian}
}

@InProceedings{	  10.1145/3571884.3597133,
  author	= {Addlesee, Angus and Damonte, Marco},
  title		= {Understanding and Answering Incomplete Questions},
  year		= {2023},
  isbn		= {9798400700149},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571884.3597133},
  doi		= {10.1145/3571884.3597133},
  abstract	= {Voice assistants interrupt people when they pause
		  mid-question, a frustrating interaction that requires the
		  full repetition of the entire question again. This impacts
		  all users, but particularly people with cognitive
		  impairments. In human-human conversation, these situations
		  are recovered naturally as people understand the words that
		  were uttered. In this paper we build answer pipelines which
		  parse incomplete questions and repair them following human
		  recovery strategies. We evaluated these pipelines on our
		  new corpus, SLUICE. It contains 21,000 interrupted
		  questions, from LC-QuAD 2.0 and QALD-9-plus, paired with
		  their underspecified SPARQL queries. Compared to a system
		  that is given the full question, our best partial
		  understanding pipeline answered only 0.77% fewer questions.
		  Results show that our pipeline correctly identifies what
		  information is required to provide an answer but is not yet
		  provided by the incomplete question. It also accurately
		  identifies where that missing information belongs in the
		  semantic structure of the question.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Conversational User Interfaces},
  articleno	= {10},
  numpages	= {9},
  keywords	= {accessibility, human agent interaction, knowledge base
		  question answering, semantic parsing, voice user
		  experience},
  location	= {Eindhoven, Netherlands},
  series	= {CUI '23}
}

@Proceedings{	  10.1145/3594778,
  title		= {GRADES-NDA '23: Proceedings of the 6th Joint Workshop on
		  Graph Data Management Experiences &amp; Systems (GRADES)
		  and Network Data Analytics (NDA)},
  year		= {2023},
  isbn		= {9798400702013},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {GRADES-NDA 2023 is the sixth joint meeting of the GRADES
		  and NDA workshops, which were each independently organized
		  at previous SIGMOD-PODS meetings, GRADES since 2013 and NDA
		  since 2016. The focus of the GRADES-NDA workshop is the
		  application areas, usage scenarios and open challenges in
		  managing largescale graph-shaped data. The workshop is a
		  forum for exchanging ideas and methods for mining,
		  querying, and learning with real-world network data,
		  developing new common understandings of the problems at
		  hand, sharing of data sets and benchmarks where applicable,
		  and leveraging existing knowledge from different
		  disciplines. GRADES-NDA aims to present technical
		  contributions inside graph, RDF, and other data management
		  systems on massive graphs.The purpose of this workshop is
		  to bring together researchers from academia, industry, and
		  government to create a forum for discussing recent advances
		  in large-scale graph data management and analytics systems,
		  as well as propose and discuss novel methods and techniques
		  towards addressing domain specific challenges and handling
		  noise in real-world graphs.},
  location	= {Seattle, WA, USA}
}

@Article{	  10.1145/3573204,
  author	= {Yao, Jing and Liu, Zheng and Yang, Junhan and Dou,
		  Zhicheng and Xie, Xing and Wen, Ji-Rong},
  title		= {CDSM: Cascaded Deep Semantic Matching on Textual Graphs
		  Leveraging Ad-hoc Neighbor Selection},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3573204},
  doi		= {10.1145/3573204},
  abstract	= {Deep semantic matching aims at discriminating the
		  relationship between documents based on deep neural
		  networks. In recent years, it becomes increasingly popular
		  to organize documents with a graph structure, then leverage
		  both the intrinsic document features and the extrinsic
		  neighbor features to derive discrimination. Most of the
		  existing works mainly care about how to utilize the
		  presented neighbors, whereas limited effort is made to
		  filter appropriate neighbors. We argue that the neighbor
		  features could be highly noisy and partially useful. Thus,
		  a lack of effective neighbor selection will not only incur
		  a great deal of unnecessary computation cost but also
		  restrict the matching accuracy severely. In this work, we
		  propose a novel framework, Cascaded Deep Semantic Matching
		  (CDSM), for accurate and efficient semantic matching on
		  textual graphs. CDSM is highlighted for its two-stage
		  workflow. In the first stage, a lightweight CNN-based
		  ad-hod neighbor selector is deployed to filter useful
		  neighbors for the matching task with a small computation
		  cost. We design both one-step and multi-step selection
		  methods. In the second stage, a high-capacity graph-based
		  matching network is employed to compute fine-grained
		  relevance scores based on the well-selected neighbors. It
		  is worth noting that CDSM is a generic framework which
		  accommodates most of the mainstream graph-based semantic
		  matching networks. The major challenge is how the selector
		  can learn to discriminate the neighbors’ usefulness which
		  has no explicit labels. To cope with this problem, we
		  design a weak-supervision strategy for optimization, where
		  we train the graph-based matching network at first and then
		  the ad-hoc neighbor selector is learned on top of the
		  annotations from the matching network. We conduct extensive
		  experiments with three large-scale datasets, showing that
		  CDSM notably improves the semantic matching accuracy and
		  efficiency thanks to the selection of high-quality
		  neighbors. The source code is released at
		  https://github.com/jingjyyao/CDSM.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= feb,
  articleno	= {32},
  numpages	= {24},
  keywords	= {Semantic matching, textual graph, neighbor selection}
}

@Article{	  10.1109/taslp.2023.3340610,
  author	= {Zhao, Yaru and Cheng, Bo and Huang, Yakun and Wan,
		  Zhiguo},
  title		= {FluGCF: A Fluent Dialogue Generation Model With Coherent
		  Concept Entity Flow},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3340610},
  doi		= {10.1109/TASLP.2023.3340610},
  abstract	= {The integration of external knowledge graphs into dialogue
		  systems effectively mitigates the generation of generic and
		  uninteresting responses. This approach, particularly the
		  explicit modeling of conversation flows from related
		  concept entities, facilitates the generation of
		  semantically rich and informative responses. However,
		  recent models guided by concept entity flows present two
		  primary limitations: (1) a limited semantic understanding
		  of the post message, which complicates the selection of
		  highly relevant 1-hop concept entities, and (2) an
		  inability to extract dynamic and diverse semantic relations
		  between the post message and 2-hop concept entities. To
		  address these issues, we introduce FluGCF, a novel model
		  that fluently generates dialogues with coherent guidance
		  from concept entity flows. FluGCF employs a ternary fusion
		  to explicitly model multi-hop concept entity flows using a
		  post-aware knowledge encoding mechanism. This mechanism
		  learns semantic concept entity features from both word and
		  sentence-level text features. Additionally, we design a
		  corresponding ternary decoding mechanism that dynamically
		  selects concept entities or words from the vocabulary to
		  enhance fluency and diversity in dialogue generation.
		  FluGCF, implemented in PyTorch, was extensively evaluated
		  on a large-scale dataset, revealing that it surpasses
		  baseline models, including the state-of-the-art
		  knowledge-aware model ConceptFlow, by nearly 15% in terms
		  of fluency. Furthermore, it demonstrated notable
		  enhancements in coherence, diversity and informativeness.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= dec,
  pages		= {853–867},
  numpages	= {15}
}

@Article{	  10.1145/3510030,
  author	= {Hu, Yang and Chapman, Adriane and Wen, Guihua and Hall,
		  Dame Wendy},
  title		= {What Can Knowledge Bring to Machine Learning?—A Survey
		  of Low-shot Learning for Structured Data},
  year		= {2022},
  issue_date	= {June 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {3},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3510030},
  doi		= {10.1145/3510030},
  abstract	= {Supervised machine learning has several drawbacks that
		  make it difficult to use in many situations. Drawbacks
		  include heavy reliance on massive training data, limited
		  generalizability, and poor expressiveness of high-level
		  semantics. Low-shot Learning attempts to address these
		  drawbacks. Low-shot learning allows the model to obtain
		  good predictive power with very little or no training data,
		  where structured knowledge plays a key role as a high-level
		  semantic representation of human. This article will review
		  the fundamental factors of low-shot learning technologies,
		  with a focus on the operation of structured knowledge under
		  different low-shot conditions. We also introduce other
		  techniques relevant to low-shot learning. Finally, we point
		  out the limitations of low-shot learning, the prospects and
		  gaps of industrial applications, and future research
		  directions.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= mar,
  articleno	= {48},
  numpages	= {45},
  keywords	= {Machine learning, low-shot learning, structured knowledge,
		  industrial applications, future directions}
}

@InProceedings{	  10.1145/3539618.3591667,
  author	= {Li, Na and Kteich, Hanane and Bouraoui, Zied and
		  Schockaert, Steven},
  title		= {Distilling Semantic Concept Embeddings from Contrastively
		  Fine-Tuned Language Models},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591667},
  doi		= {10.1145/3539618.3591667},
  abstract	= {Learning vectors that capture the meaning of concepts
		  remains a fundamental challenge. Somewhat surprisingly,
		  perhaps, pre-trained language models have thus far only
		  enabled modest improvements to the quality of such concept
		  embeddings. Current strategies for using language models
		  typically represent a concept by averaging the
		  contextualised representations of its mentions in some
		  corpus. This is potentially sub-optimal for at least two
		  reasons. First, contextualised word vectors have an unusual
		  geometry, which hampers downstream tasks. Second, concept
		  embeddings should capture the semantic properties of
		  concepts, whereas contextualised word vectors are also
		  affected by other factors. To address these issues, we
		  propose two contrastive learning strategies, based on the
		  view that whenever two sentences reveal similar properties,
		  the corresponding contextualised vectors should also be
		  similar. One strategy is fully unsupervised, estimating the
		  properties which are expressed in a sentence from the
		  neighbourhood structure of the contextualised word
		  embeddings. The second strategy instead relies on a distant
		  supervision signal from ConceptNet. Our experimental
		  results show that the resulting vectors substantially
		  outperform existing concept embeddings in predicting the
		  semantic properties of concepts, with the ConceptNet-based
		  strategy achieving the best results. These findings are
		  furthermore confirmed in a clustering task and in the
		  downstream task of ontology completion.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {216–226},
  numpages	= {11},
  keywords	= {commonsense knowledge, contrastive learning, language
		  models, word embedding},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Proceedings{	  10.1145/3587259,
  title		= {K-CAP '23: Proceedings of the 12th Knowledge Capture
		  Conference 2023},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the 12th ACM
		  International Conference on Knowledge Capture: K-CAP 2023,
		  held in person on December 5th - 7th in Pensacola, Florida,
		  US.Driven by the increasing demands for knowledge-based
		  applications and the unprecedented availability of
		  information from heterogeneous data sources, the study of
		  knowledge capture is of crucial importance. Knowledge
		  capture involves the extraction of useful knowledge from
		  vast and diverse data sources as well as its acquisition
		  directly from human experts.Nowadays knowledge is derived
		  from an increasingly diverse set of data resources that
		  differ with regard to their domain, format, quality,
		  coverage, specificity, viewpoint, bias, and most
		  importantly, consumers and producers of data. The
		  heterogeneity, amount and complexity of data allow us to
		  answer complex questions that could not be answered in
		  isolation, requiring the interaction of different
		  scientific fields and technologies. A goal of K-CAP is to
		  develop such synergies using systematic and rigorous
		  methodologies.The call for papers attracted 105 submissions
		  from all over the world, covering a diverse range of topics
		  spanning knowledge mining, large language models for
		  information extraction, neuro-symbolic approaches for
		  knowledge capture, knowledge engineering,
		  question-answering, knowledge graphs, natural language
		  processing, reasoning, entity linking, querying and
		  knowledge-based applications. From a competitive set of
		  high-quality submissions, we accepted 27 long research
		  papers, 5 short papers, and 1 vision paper. The
		  high-quality program is divided into 7 research sessions,
		  in addition to 3 tutorials reflecting novel topics of
		  interest in Knowledge Capture.We encourage everyone to
		  attend the keynote talks that we have planned for K-CAP
		  2023. The highly anticipated talks by Dr. Robert R. Hoffman
		  (Florida Institute for Human and Machine Cognition) and Dr.
		  Jane Pinelis (Johns Hopkins University Applied Physics
		  Laboratory) will guide us to a better understanding of the
		  future of knowledge capture and explainable, resilient AI
		  ecosystems, as they become commonplace in real world
		  applications.},
  location	= {Pensacola, FL, USA}
}

@InProceedings{	  10.1145/3442381.3449838,
  author	= {Shao, Huajie and Wang, Jun and Lin, Haohong and Zhang,
		  Xuezhou and Zhang, Aston and Ji, Heng and Abdelzaher,
		  Tarek},
  title		= {Controllable and Diverse Text Generation in E-commerce},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3449838},
  doi		= {10.1145/3442381.3449838},
  abstract	= {In E-commerce, a key challenge in text generation is to
		  find a good trade-off between word diversity and accuracy
		  (relevance) in order to make generated text appear more
		  natural and human-like. In order to improve the relevance
		  of generated results, conditional text generators were
		  developed that use input keywords or attributes to produce
		  the corresponding text. Prior work, however, do not finely
		  control the diversity of automatically generated sentences.
		  For example, it does not control the order of keywords to
		  put more relevant ones first. Moreover, it does not
		  explicitly control the balance between diversity and
		  accuracy. To remedy these problems, we propose a
		  fine-grained controllable generative model,
		  called&nbsp;Apex, that uses an algorithm borrowed from
		  automatic control (namely, a variant of the proportional,
		  integral, and derivative (PID) controller) to precisely
		  manipulate the diversity/accuracy trade-off of generated
		  text. The algorithm is injected into a Conditional
		  Variational Autoencoder (CVAE), allowing Apex to control
		  both (i) the order of keywords in the generated sentences
		  (conditioned on the input keywords and their order), and
		  (ii) the trade-off between diversity and accuracy.
		  Evaluation results on real world datasets&nbsp;1 show that
		  the proposed method outperforms existing generative models
		  in terms of diversity and relevance. Moreover, it achieves
		  about 97% accuracy in the control of the order of keywords.
		  Apex is currently deployed to generate production
		  descriptions and item recommendation reasons in Taobao2,
		  the largest E-commerce platform in China. The A/B
		  production test results show that our method improves
		  click-through rate (CTR) by 13.17% compared to the existing
		  method for production descriptions. For item recommendation
		  reason, it is able to increase CTR by 6.89% and 1.42%
		  compared to user reviews and top-K item recommendation
		  without reviews, respectively.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {2392–2401},
  numpages	= {10},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3564746.3587001,
  author	= {Adatrao, Naga Sai Krishna and Gadireddy, Gowtham Reddy and
		  Noh, Jiho},
  title		= {A Survey on Conversational Search and Applications in
		  Biomedicine},
  year		= {2023},
  isbn		= {9781450399210},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3564746.3587001},
  doi		= {10.1145/3564746.3587001},
  abstract	= {This paper aims to provide a radical rundown on
		  Conversational Search (ConvSearch), an approach to enhance
		  the information retrieval (IR) method where users engage in
		  a dialogue for the information-seeking tasks. In this
		  survey, we predominantly focused on the human interactive
		  characteristics of the ConvSearch systems, highlighting the
		  operations of the action modules, likely the retrieval
		  system, question-answering, and recommender system. We
		  labeled various ConvSearch research problems in knowledge
		  bases, natural language processing, and dialogue management
		  systems with action modules. We further categorized the
		  framework to ConvSearch, and the application is directed
		  toward biomedical and healthcare fields for the utilization
		  of clinical social technology. Finally, we conclude by
		  talking through the challenges and issues of ConvSearch,
		  particularly in Bio-Medicine. Our main aim is to provide an
		  integrated and unified vision of the ConvSearch components
		  from different fields, which benefit the
		  information-seeking process in healthcare systems.},
  booktitle	= {Proceedings of the 2023 ACM Southeast Conference},
  pages		= {78–88},
  numpages	= {11},
  keywords	= {information retrieval, conversational search, question
		  answering, knowledge base, dialogue management systems,
		  recommender systems, generative language models, biomedical
		  convsearch, privacy concerns},
  location	= {Virtual Event, USA},
  series	= {ACMSE '23}
}

@Proceedings{	  10.1145/3611380,
  title		= {MMAsia '23 Workshops: Proceedings of the 5th ACM
		  International Conference on Multimedia in Asia Workshops},
  year		= {2023},
  isbn		= {9798400703263},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tainan, Taiwan}
}

@Proceedings{	  10.1145/3600211,
  title		= {AIES '23: Proceedings of the 2023 AAAI/ACM Conference on
		  AI, Ethics, and Society},
  year		= {2023},
  isbn		= {9798400702310},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Montr\'{e}al, QC, Canada}
}

@Proceedings{	  10.1145/3579051,
  title		= {IJCKG '22: Proceedings of the 11th International Joint
		  Conference on Knowledge Graphs},
  year		= {2022},
  isbn		= {9781450399876},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Article{	  10.1145/3439816,
  author	= {Guo, Bin and Wang, Hao and Ding, Yasan and Wu, Wei and
		  Hao, Shaoyang and Sun, Yueqi and Yu, Zhiwen},
  title		= {Conditional Text Generation for Harmonious Human-Machine
		  Interaction},
  year		= {2021},
  issue_date	= {April 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {12},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3439816},
  doi		= {10.1145/3439816},
  abstract	= {In recent years, with the development of deep learning,
		  text-generation technology has undergone great changes and
		  provided many kinds of services for human beings, such as
		  restaurant reservation and daily communication. The
		  automatically generated text is becoming more and more
		  fluent so researchers begin to consider more
		  anthropomorphic text-generation technology, that is, the
		  conditional text generation, including emotional text
		  generation, personalized text generation, and so on.
		  Conditional Text Generation (CTG) has thus become a
		  research hotspot. As a promising research field, we find
		  that much attention has been paid to exploring it.
		  Therefore, we aim to give a comprehensive review of the new
		  research trends of CTG. We first summarize several key
		  techniques and illustrate the technical evolution route in
		  the field of neural text generation, based on the concept
		  model of CTG. We further make an investigation of existing
		  CTG fields and propose several general learning models for
		  CTG. Finally, we discuss the open issues and promising
		  research directions of CTG.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= feb,
  articleno	= {14},
  numpages	= {50},
  keywords	= {Human-computer interaction, conditional text generation,
		  deep learning, dialog systems, personalization}
}

@InProceedings{	  10.1145/3534678.3539443,
  author	= {Huang, Jiaxin and Meng, Yu and Han, Jiawei},
  title		= {Few-Shot Fine-Grained Entity Typing with Automatic Label
		  Interpretation and Instance Generation},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539443},
  doi		= {10.1145/3534678.3539443},
  abstract	= {We study the problem of few-shot Fine-grained Entity
		  Typing (FET), where only a few annotated entity mentions
		  with contexts are given for each entity type. Recently,
		  prompt-based tuning has demonstrated superior performance
		  to standard fine-tuning in few-shot scenarios by
		  formulating the entity type classification task as a
		  ''fill-in-the-blank'' problem. This allows effective
		  utilization of the strong language modeling capability of
		  Pre-trained Language Models (PLMs). Despite the success of
		  current prompt-based tuning approaches, two major
		  challenges remain: (1) the verbalizer in prompts is either
		  manually designed or constructed from external knowledge
		  bases, without considering the target corpus and label
		  hierarchy information, and (2) current approaches mainly
		  utilize the representation power of PLMs, but have not
		  explored their generation power acquired through extensive
		  general-domain pre-training. In this work, we propose a
		  novel framework for few-shot FET consisting of two modules:
		  (1) an entity type label interpretation module
		  automatically learns to relate type labels to the
		  vocabulary by jointly leveraging few-shot instances and the
		  label hierarchy, and (2) a type-based contextualized
		  instance generator produces new instances based on given
		  instances to enlarge the training set for better
		  generalization. On three benchmark datasets, our model
		  outperforms existing methods by significant margins.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {605–614},
  numpages	= {10},
  keywords	= {entity typing, few-shot learning, prompt-based learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3583780.3614961,
  author	= {Li, Qi},
  title		= {Harnessing the Power of Pre-trained Vision-Language Models
		  for Efficient Medical Report Generation},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614961},
  doi		= {10.1145/3583780.3614961},
  abstract	= {Medical images are commonly used in clinical practice. But
		  the need for diagnosis and reporting from image-based
		  examinations far excels the current medical capacity.
		  Automatic Medical Report Generation (MRG) can help to ease
		  the burden of radiologists. Vision-Language Pre-training
		  (VLP) has received tremendous success on various tasks,
		  therefore it is naturally expected that MRG can harvest
		  from this rapid advancement. However, directly applying
		  existing VLP models in the medical domain is impracticable
		  due to their data-hungry nature, the need for aligning
		  different modalities, prohibitive training time, exorbitant
		  hardware barrier, and the challenge of open-ended text
		  generation. To address these problems, we propose MedEPT, a
		  parameter-efficient approach for MRG that can utilize
		  ever-ignored image-only datasets. It employs
		  parameter-efficient tuning (PET) for VLP adaption to
		  mitigate inefficiency in fine-tuning time and hardware.
		  MedEPT also employs MRGPID to augment and expand adaption
		  datasets by synthesizing meaningful text for image-only
		  datasets. We perform a systematic evaluation of our method.
		  Empirical results show that we obtain a better performance
		  than the state-of-the-art method while using less than 10%
		  trainable parameters and not more than 30% training time
		  than ever before.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1308–1317},
  numpages	= {10},
  keywords	= {large language models, medical report generation,
		  pre-trained vision-language models},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3539597.3572720,
  author	= {Zhu, Chenguang and Xu, Yichong and Ren, Xiang and Lin,
		  Bill Yuchen and Jiang, Meng and Yu, Wenhao},
  title		= {Knowledge-Augmented Methods for Natural Language
		  Processing},
  year		= {2023},
  isbn		= {9781450394079},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539597.3572720},
  doi		= {10.1145/3539597.3572720},
  abstract	= {Knowledge in NLP has been a rising trend especially after
		  the advent of large-scale pre-trained models. Knowledge is
		  critical to equip statistics-based models with common
		  sense, logic and other external information. In this
		  tutorial, we will introduce recent state-of-the-art works
		  in applying knowledge in language understanding, language
		  generation and commonsense reasoning.},
  booktitle	= {Proceedings of the Sixteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1228–1231},
  numpages	= {4},
  keywords	= {commonsense reasoning, knowledge-augmented methods,
		  language generation, natural language understanding},
  location	= {Singapore, Singapore},
  series	= {WSDM '23}
}

@Article{	  10.1145/3626763,
  author	= {Fan, Wenfei and Han, Ziyan and Ren, Weilong and Wang, Ding
		  and Wang, Yaoshu and Xie, Min and Yan, Mengyi},
  title		= {Splitting Tuples of Mismatched Entities},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {4},
  url		= {https://doi.org/10.1145/3626763},
  doi		= {10.1145/3626763},
  abstract	= {There has been a host of work on entity resolution (ER),
		  to identify tuples that refer to the same entity. This
		  paper studies the inverse of ER, to identify tuples to
		  which distinct real-world entities are matched by mistake,
		  and split such tuples into a set of tuples, one for each
		  entity. We formulate the tuple splitting problem. We
		  propose a scheme to decide what tuples to split and what
		  tuples to correct without splitting, fix errors/assign
		  attribute values to the split tuples, and impute missing
		  values. The scheme introduces a class of rules, which embed
		  predicates for aligning entities across relations and
		  knowledge graphs G, assessing correlation between
		  attributes, and extracting data from G. It unifies logic
		  deduction, correlation models, and data extraction by
		  chasing the data with the rules. We train machine learning
		  models to assess attribute correlation and predict missing
		  values. We develop algorithms for the tuple splitting
		  scheme. Using real-life data, we empirically verify that
		  the scheme is efficient and accurate, with F-measure 0.92
		  on average.},
  journal	= {Proc. ACM Manag. Data},
  month		= dec,
  articleno	= {269},
  numpages	= {29},
  keywords	= {data quality, entity resolution, tuple splitting}
}

@Proceedings{	  10.1145/3628454,
  title		= {IAIT '23: Proceedings of the 13th International Conference
		  on Advances in Information Technology},
  year		= {2023},
  isbn		= {9798400708497},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bangkok, Thailand}
}

@Proceedings{	  10.1145/3627915,
  title		= {CSAE '23: Proceedings of the 7th International Conference
		  on Computer Science and Application Engineering},
  year		= {2023},
  isbn		= {9798400700590},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, China}
}

@Article{	  10.14778/3626292.3626294,
  author	= {Arora, Simran and Yang, Brandon and Eyuboglu, Sabri and
		  Narayan, Avanika and Hojel, Andrew and Trummer, Immanuel
		  and R\'{e}, Christopher},
  title		= {Language Models Enable Simple Systems for Generating
		  Structured Views of Heterogeneous Data Lakes},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {2},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3626292.3626294},
  doi		= {10.14778/3626292.3626294},
  abstract	= {A long standing goal in the data management community is
		  developing systems that input documents and output
		  queryable tables without user effort. Given the sheer
		  variety of potential documents, state-of-the art systems
		  make simplifying assumptions and use domain specific
		  training. In this work, we ask whether we can maintain
		  generality by using the in-context learning abilities of
		  large language models (LLMs). We propose and evaluate
		  Evaporate, a prototype system powered by LLMs. We identify
		  two strategies for implementing this system: prompt the LLM
		  to directly extract values from documents or prompt the LLM
		  to synthesize code that performs the extraction. Our
		  evaluations show a cost-quality tradeoff between these two
		  approaches. Code synthesis is cheap, but far less accurate
		  than directly processing each document with the LLM. To
		  improve quality while maintaining low cost, we propose an
		  extended implementation, Evaporate-Code+, which achieves
		  better quality than direct extraction. Our insight is to
		  generate many candidate functions and ensemble their
		  extractions using weak supervision. Evaporate-Code+
		  outperforms the state-of-the art systems using a sublinear
		  pass over the documents with the LLM. This equates to a
		  110X reduction in the number of documents the LLM needs to
		  process across our 16 real-world evaluation settings.},
  journal	= {Proc. VLDB Endow.},
  month		= oct,
  pages		= {92–105},
  numpages	= {14}
}

@Article{	  10.1145/3590773,
  author	= {Becattini, Federico and Bongini, Pietro and Bulla, Luana
		  and Bimbo, Alberto Del and Marinucci, Ludovica and
		  Mongiov\`{\i}, Misael and Presutti, Valentina},
  title		= {VISCOUNTH: A Large-scale Multilingual Visual Question
		  Answering Dataset for Cultural Heritage},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {6},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3590773},
  doi		= {10.1145/3590773},
  abstract	= {Visual question answering has recently been settled as a
		  fundamental multi-modal reasoning task of artificial
		  intelligence that allows users to get information about
		  visual content by asking questions in natural language. In
		  the cultural heritage domain, this task can contribute to
		  assisting visitors in museums and cultural sites, thus
		  increasing engagement. However, the development of visual
		  question answering models for cultural heritage is
		  prevented by the lack of suitable large-scale datasets. To
		  meet this demand, we built a large-scale heterogeneous and
		  multilingual (Italian and English) dataset for cultural
		  heritage that comprises approximately 500K Italian cultural
		  assets and 6.5M question-answer pairs. We propose a novel
		  formulation of the task that requires reasoning over both
		  the visual content and an associated natural language
		  description, and present baselines for this task. Results
		  show that the current state of the art is reasonably
		  effective but still far from satisfactory; therefore,
		  further research in this area is recommended. Nonetheless,
		  we also present a holistic baseline to address visual and
		  contextual questions and foster future research on the
		  topic.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= jul,
  articleno	= {193},
  numpages	= {20},
  keywords	= {Visual question answering, cultural heritage}
}

@InProceedings{	  10.1145/3597638.3615650,
  author	= {McDonnell, Emma J. and Mack, Kelly Avery and Gerling,
		  Kathrin and Spiel, Katta and Bennett, Cynthia L. and
		  Brewer, Robin N. and Williams, Rua Mae and Tigwell, Garreth
		  W.},
  title		= {Tackling the Lack of a Practical Guide in
		  Disability-Centered Research},
  year		= {2023},
  isbn		= {9798400702204},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3597638.3615650},
  doi		= {10.1145/3597638.3615650},
  abstract	= {Accessibility research strives to develop technology that
		  is useful for disabled people, but the research processes
		  that we engage in do not always center disabled people in a
		  way that allows us to shape artifacts so that they benefit
		  disabled communities. In this workshop, we want to address
		  core questions that are relevant in this context: How can
		  research questions be defined in a way that shares power
		  between research teams and technology users? How should
		  research processes be designed to be broadly accessible for
		  disabled people? And what are equitable ways of summarizing
		  and sharing research findings in a way that allows disabled
		  communities to critically appraise findings with us?
		  Through discussion among all attendees, we want to develop
		  a practical guide in disability-centered research that will
		  be made available and further developed as a community
		  resource when engaging in accessibility research.},
  booktitle	= {Proceedings of the 25th International ACM SIGACCESS
		  Conference on Computers and Accessibility},
  articleno	= {106},
  numpages	= {5},
  keywords	= {Access, Disability Justice, Research Methods},
  location	= {New York, NY, USA},
  series	= {ASSETS '23}
}

@Article{	  10.1145/3588911,
  author	= {Omar, Reham and Dhall, Ishika and Kalnis, Panos and
		  Mansour, Essam},
  title		= {A Universal Question-Answering Platform for Knowledge
		  Graphs},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {1},
  url		= {https://doi.org/10.1145/3588911},
  doi		= {10.1145/3588911},
  abstract	= {Knowledge from diverse application domains is organized as
		  knowledge graphs (KGs) that are stored in RDF engines
		  accessible in the web via SPARQL endpoints. Expressing a
		  well-formed SPARQL query requires information about the
		  graph structure and the exact URIs of its components, which
		  is impractical for the average user. Question answering
		  (QA) systems assist by translating natural language
		  questions to SPARQL. Existing QA systems are typically
		  based on application-specific human-curated rules, or
		  require prior information, expensive pre-processing and
		  model adaptation for each targeted KG. Therefore, they are
		  hard to generalize to a broad set of applications and KGs.
		  In this paper, we propose KGQAn, a universal QA system that
		  does not need to be tailored to each target KG. Instead of
		  curated rules, KGQAn introduces a novel formalization of
		  question understanding as a text generation problem to
		  convert a question into an intermediate abstract
		  representation via a neural sequence-to-sequence model. We
		  also develop a just-in-time linker that maps at query time
		  the abstract representation to a SPARQL query for a
		  specific KG, using only the publicly accessible APIs and
		  the existing indices of the RDF store, without requiring
		  any pre-processing. Our experiments with several real KGs
		  demonstrate that KGQAn is easily deployed and outperforms
		  by a large margin the state-of-the-art in terms of quality
		  of answers and processing time, especially for arbitrary
		  KGs, unseen during the training.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {57},
  numpages	= {25},
  keywords	= {RDF, just-in-time entity and relation linking, knowledge
		  graphs, natural language question answering, seq2seq
		  models}
}

@InProceedings{	  10.1145/3583780.3614758,
  author	= {Yang, Kailai and Zhang, Tianlin and Ji, Shaoxiong and
		  Ananiadou, Sophia},
  title		= {A Bipartite Graph is All We Need for Enhancing Emotional
		  Reasoning with Commonsense Knowledge},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614758},
  doi		= {10.1145/3583780.3614758},
  abstract	= {The context-aware emotional reasoning ability of AI
		  systems, especially in conversations, is of vital
		  importance in applications such as online opinion mining
		  from social media and empathetic dialogue systems. Due to
		  the implicit nature of conveying emotions in many
		  scenarios, commonsense knowledge is widely utilized to
		  enrich utterance semantics and enhance conversation
		  modeling. However, most previous knowledge infusion methods
		  perform empirical knowledge filtering and design highly
		  customized architectures for knowledge interaction with the
		  utterances, which can discard useful knowledge aspects and
		  limit their generalizability to different knowledge
		  sources. Based on these observations, we propose a
		  Bipartite Heterogeneous Graph (BHG) method for enhancing
		  emotional reasoning with commonsense knowledge. In BHG, the
		  extracted context-aware utterance representations and
		  knowledge representations are modeled as heterogeneous
		  nodes. Two more knowledge aggregation node types are
		  proposed to perform automatic knowledge filtering and
		  interaction. BHG-based knowledge infusion can be directly
		  generalized to multi-type and multi-grained knowledge
		  sources. In addition, we propose a Multi-dimensional
		  Heterogeneous Graph Transformer (MHGT) to perform graph
		  reasoning, which can retain unchanged feature spaces and
		  unequal dimensions for heterogeneous node types during
		  inference to prevent unnecessary loss of information.
		  Experiments show that BHG-based methods significantly
		  outperform state-of-the-art knowledge infusion methods and
		  show generalized knowledge infusion ability with higher
		  efficiency. Further analysis proves that previous empirical
		  knowledge filtering methods do not guarantee to provide the
		  most useful knowledge information. Our code is available
		  at: https://github.com/SteveKGYang/BHG.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2917–2927},
  numpages	= {11},
  keywords	= {bipartite heterogeneous graph, casual emotion entailment,
		  emotion recognition in conversations, knowledge infusion},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3610591.3616429,
  author	= {Bhardwaj, Purav and Sra, Misha},
  title		= {Ghost in the Machine : Discourses with AI},
  year		= {2023},
  isbn		= {9798400703201},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3610591.3616429},
  doi		= {10.1145/3610591.3616429},
  abstract	= {AI systems analyze vast amounts of data, uncover patterns,
		  and make decisions - emulating a semblance of intelligence
		  despite lacking qualia and embodiment that form the basis
		  of the human condition. In this paper, we expound on "Ghost
		  in the Machine", an interactive installation that delves
		  into our pervasive tendency to anthropomorphize AI,
		  ascribing human-like qualities, intentions, and even
		  consciousness. Participants engage in dialogue with the AI
		  as it collaboratively materializes the AI's thoughts in
		  moving image and generative sound. The installation
		  attempts to forge embodiment for an amorphous AI, revealing
		  errors in its comprehension, represented by the metaphor of
		  hallucinations.},
  booktitle	= {SIGGRAPH Asia 2023 Art Papers},
  articleno	= {6},
  numpages	= {6},
  keywords	= {Anthropomorphism, Artificial Intelligence, Simulation},
  location	= {Sydney, NSW, Australia},
  series	= {SA '23}
}

@Article{	  10.1145/3505245,
  author	= {Gruetzemacher, Ross and Paradice, David},
  title		= {Deep Transfer Learning &amp; Beyond: Transformer Language
		  Models in Information Systems Research},
  year		= {2022},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {10s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3505245},
  doi		= {10.1145/3505245},
  abstract	= {AI is widely thought to be poised to transform business,
		  yet current perceptions of the scope of this transformation
		  may be myopic. Recent progress in natural language
		  processing involving transformer language models (TLMs)
		  offers a potential avenue for AI-driven business and
		  societal transformation that is beyond the scope of what
		  most currently foresee. We review this recent progress as
		  well as recent literature utilizing text mining in top IS
		  journals to develop an outline for how future IS research
		  can benefit from these new techniques. Our review of
		  existing IS literature reveals that suboptimal text mining
		  techniques are prevalent and that the more advanced TLMs
		  could be applied to enhance and increase IS research
		  involving text data, and to enable new IS research topics,
		  thus creating more value for the research community. This
		  is possible because these techniques make it easier to
		  develop very powerful custom systems and their performance
		  is superior to existing methods for a wide range of tasks
		  and applications. Further, multilingual language models
		  make possible higher quality text analytics for research in
		  multiple languages. We also identify new avenues for IS
		  research, like language user interfaces, that may offer
		  even greater potential for future IS research.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {204},
  numpages	= {35},
  keywords	= {Natural language processing, text mining, artificial
		  intelligence, deep learning, transfer learning, language
		  models}
}

@InProceedings{	  10.1145/3568294.3580129,
  author	= {Pramanick, Pradip and Sarkar, Chayan},
  title		= {Utilizing Prior Knowledge to Improve Automatic Speech
		  Recognition in Human-Robot Interactive Scenarios},
  year		= {2023},
  isbn		= {9781450399708},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3568294.3580129},
  doi		= {10.1145/3568294.3580129},
  abstract	= {The prolificacy of human-robot interaction not only
		  depends on a robot's ability to understand the intent and
		  content of the human utterance but also gets impacted by
		  the automatic speech recognition (ASR) system. Modern ASR
		  can provide highly accurate (grammatically and
		  syntactically) translation. Yet, the general purpose ASR
		  often misses out on the semantics of the translation by
		  incorrect word prediction due to open-vocabulary modeling.
		  ASR inaccuracy can have significant repercussions as this
		  can lead to a completely different action by the robot in
		  the real world. Can any prior knowledge be helpful in such
		  a scenario? In this work, we explore how prior knowledge
		  can be utilized in ASR decoding. Using our experiments, we
		  demonstrate how our system can significantly improve ASR
		  translation for robotic task instruction.},
  booktitle	= {Companion of the 2023 ACM/IEEE International Conference on
		  Human-Robot Interaction},
  pages		= {471–475},
  numpages	= {5},
  keywords	= {asr, cognitive robot, embodied agent, hri, robotics
		  knowledge},
  location	= {Stockholm, Sweden},
  series	= {HRI '23}
}

@Article{	  10.1145/3575666,
  author	= {Greengard, Samuel},
  title		= {Computational Linguistics Finds its Voice},
  year		= {2023},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {66},
  number	= {2},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3575666},
  doi		= {10.1145/3575666},
  abstract	= {Advances in artificial intelligence permit computers to
		  converse with humans in seemingly realistic ways.},
  journal	= {Commun. ACM},
  month		= jan,
  pages		= {18–20},
  numpages	= {3}
}

@Article{	  10.1145/3527546.3527568,
  author	= {Ghosal, Tirthankar and Al-Khatib, Khalid and Hou, Yufang
		  and de Waard, Anita and Freitag, Dayne},
  title		= {Report on the 1st workshop on argumentation knowledge
		  graphs (ArgKG 2021) at AKBC 2021},
  year		= {2022},
  issue_date	= {December 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {2},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3527546.3527568},
  doi		= {10.1145/3527546.3527568},
  abstract	= {The first workshop on Argumentation Knowledge Graphs
		  (ArgKG) was held virtually at the Automated Knowledge Base
		  Construction (AKBC 2021) conference on October 7, 2021.
		  ArgKG @ AKBC 2021 brought together the Computational
		  Argumentation and Knowledge Graphs communities, aiming to
		  promote cross-pollination of ideas and encourage
		  discussions and collaborations between the two communities.
		  This paper describes the workshop and compiles several of
		  its findings and insights.Date: 7 October, 2021.Website:
		  https://argkg21.argmining.org.},
  journal	= {SIGIR Forum},
  month		= mar,
  articleno	= {19},
  numpages	= {12}
}

@InProceedings{	  10.1145/3501409.3501595,
  author	= {Zhao, Di and Jiang, Shuai and Wang, Wei and Zhang, Jing
		  and Luan, Rui-Peng},
  title		= {Domain Named Entity Recognition and Applications in Test
		  and Evaluation},
  year		= {2022},
  isbn		= {9781450384322},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3501409.3501595},
  doi		= {10.1145/3501409.3501595},
  abstract	= {A great amount of information in Test and Evaluation
		  (T&amp;E) is presented in the form of multi-source
		  heterogeneous data such as performance test, combat trial
		  and during-service assessment. Despite the existence of
		  numerous and well-versed Domain Named Entity Recognition
		  (DNER) methods in the general field, it still remains
		  scarcely resourced. In this paper we survey novel methods
		  that have recently been introduced for such DNER tasks. In
		  addition, we construct the dataset for further NER tasks in
		  the field of Test and Evaluation. Finally, our work lays
		  the cornerstone for the development of subsequent NER in
		  this field.},
  booktitle	= {Proceedings of the 2021 5th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {1043–1049},
  numpages	= {7},
  keywords	= {DNER, NER, Test and Evaluation},
  location	= {Xiamen, China},
  series	= {EITCE '21}
}

@InProceedings{	  10.1145/3591106.3592227,
  author	= {Adjali, Omar and Grimal, Paul and Ferret, Olivier and
		  Ghannay, Sahar and Le Borgne, Herv\'{e}},
  title		= {Explicit Knowledge Integration for Knowledge-Aware Visual
		  Question Answering about Named Entities},
  year		= {2023},
  isbn		= {9798400701788},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3591106.3592227},
  doi		= {10.1145/3591106.3592227},
  abstract	= {Recent years have shown unprecedented growth of interest
		  in Vision-Language related tasks, with the need to address
		  the inherent challenges of integrating linguistic and
		  visual information to solve real-world applications. Such a
		  typical task is Visual Question Answering (VQA), which aims
		  to answer questions about visual content. The limitations
		  of the VQA task in terms of question redundancy and poor
		  linguistic variability encouraged researchers to propose
		  Knowledge-aware Visual Question Answering tasks as a
		  natural extension of VQA. In this paper, we tackle the
		  KVQAE (Knowledge-based Visual Question Answering about
		  named Entities) task, which proposes to answer questions
		  about named entities defined in a knowledge base and
		  grounded in visual content. In particular, besides the
		  textual and visual information, we propose to leverage the
		  structural information extracted from syntactic dependency
		  trees and external knowledge graphs to help answer
		  questions about a large spectrum of entities of various
		  types. Thus, by combining contextual and graph-based
		  representations using Graph Convolutional Networks (GCNs),
		  we are able to learn meaningful embeddings for Information
		  Retrieval tasks. Experiments on the ViQuAE public dataset
		  show how our approach improves the state-of-the-art
		  baselines while demonstrating the interest of injecting
		  external knowledge to enhance multimodal information
		  retrieval.},
  booktitle	= {Proceedings of the 2023 ACM International Conference on
		  Multimedia Retrieval},
  pages		= {29–38},
  numpages	= {10},
  keywords	= {Knowledge injection, Multimedia retrieval},
  location	= {Thessaloniki, Greece},
  series	= {ICMR '23}
}

@InProceedings{	  10.1145/3581641.3584057,
  author	= {Jung, Jeesu and Seo, Hyein and Jung, Sangkeun and Chung,
		  Riwoo and Ryu, Hwijung and Chang, Du-Seong},
  title		= {Interactive User Interface for Dialogue Summarization},
  year		= {2023},
  isbn		= {9798400701061},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581641.3584057},
  doi		= {10.1145/3581641.3584057},
  abstract	= {Summarization is one of the important tasks of natural
		  language processing used to distill information. Recently,
		  the sequence-to-sequence method was applied, in a general
		  manner, to summarization tasks. The problem is that a large
		  amount of information must be pre-trained for a specific
		  domain, and information other than input statements cannot
		  be utilized. To compensate for this shortcoming,
		  controllable summarization has recently been in the
		  spotlight. We introduced three properties into controllable
		  summarization: 1) a new human-machine communication input
		  format, 2) a robust constraint-sensitive summarization
		  method for these formats, and 3) a practical interactive
		  summarization interface available to the user. Experiments
		  on the Wizard-of-Wikipedia dataset show that applying this
		  input format and the constraint-sensitive method enhances
		  summarization performance compared to the typical method. A
		  user study shows that the interactive summarization
		  interface is practical and that participants are evaluating
		  it positively.},
  booktitle	= {Proceedings of the 28th International Conference on
		  Intelligent User Interfaces},
  pages		= {934–957},
  numpages	= {24},
  keywords	= {Dialogue summarization, constraint-sensitive generation,
		  neural networks, text tagging},
  location	= {Sydney, NSW, Australia},
  series	= {IUI '23}
}

@InProceedings{	  10.1145/3583780.3614860,
  author	= {Tang, Zee Hen and Yeh, Mi-Yen},
  title		= {EAGLE: Enhance Target-Oriented Dialogs by Global Planning
		  and Topic Flow Integration},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614860},
  doi		= {10.1145/3583780.3614860},
  abstract	= {In this study, we propose a novel model EAGLE for
		  target-oriented dialogue generation. Without relying on any
		  knowledge graphs, our method integrates the global planning
		  strategy in both topic path generation and response
		  generation given the initial and target topics. EAGLE
		  comprises three components: a topic path sampling strategy,
		  a topic flow generator, and a global planner. Our approach
		  confers a number of advantages: EAGLE is robust to the
		  target that has never appeared in the training data set and
		  able to plan the topic flow globally. The topic path
		  sampling strategy samples topic paths based on two
		  predefined rules and use the sampled paths to train the
		  topic path generator. The topic flow generator then applies
		  a non-autoregressive method to generate intermediate topics
		  that link the initial and target topics smoothly. In
		  addition, the global planner is a response generator that
		  generates a response based on the future topic sequence and
		  conversation history, enabling it to plan how to transition
		  to future topics smoothly. Our experimental results
		  demonstrate that EAGLE produces more coherent responses and
		  smoother transitions than state-of-the-art baselines, with
		  an overall success rate improvement of approximately 25%
		  and an average smoothness score improvement of 10% in both
		  offline and human evaluations.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2402–2411},
  numpages	= {10},
  keywords	= {conversation generation, global planning, target-oriented,
		  topic transition},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3582768.3582786,
  author	= {Dao, An Tuan and Aizawa, Akiko and Matsumoto, Yuji},
  title		= {Named Entity Recognition on COVID-19 Scientific Papers},
  year		= {2023},
  isbn		= {9781450397629},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3582768.3582786},
  doi		= {10.1145/3582768.3582786},
  abstract	= {Text mining techniques, especially named entity
		  recognition (NER), play a vital role in supporting
		  researchers for keeping track of hundred thousand of papers
		  on COVID-19 related literature. Although a few research has
		  been performed NER on COVID-19 scientific papers, very
		  little is currently known concerning the behaviors of
		  current entity recognition models in this new domain.
		  Therefore, this ongoing study attempts to analyze current
		  NER models’ performance and limitations on the CORD-19
		  dataset. By examining three NER models, this study showed
		  that NER performance is improved with the similarity
		  between the testing and pretraining data. When there are
		  little manually annotated resources for COVID-19 NER exist,
		  our analysis suggested that for training purposes,
		  enhancing the dictionary for seed annotation is effective
		  (not necessarily requiring costly human annotation).},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {26–30},
  numpages	= {5},
  keywords	= {COVID-19, coronavirus, named entity recognition},
  location	= {Bangkok, Thailand},
  series	= {NLPIR '22}
}

@Article{	  10.1145/3564275,
  author	= {van der Linden, Sanne and Sevastjanova, Rita and Funk,
		  Mathias and El-Assady, Mennatallah},
  title		= {MediCoSpace: Visual Decision-Support for Doctor-Patient
		  Consultations using Medical Concept Spaces from EHRs},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3564275},
  doi		= {10.1145/3564275},
  abstract	= {Healthcare systems are under pressure from an aging
		  population, rising costs, and increasingly complex
		  conditions and treatments. Although data are determined to
		  play a bigger role in how doctors diagnose and prescribe
		  treatments, they struggle due to a lack of time and an
		  abundance of structured and unstructured information. To
		  address this challenge, we introduce MediCoSpace, a visual
		  decision-support tool for more efficient doctor-patient
		  consultations. The tool links patient reports to past and
		  present diagnoses, diseases, drugs, and treatments, both
		  for the current patient and other patients in comparable
		  situations. MediCoSpace uses textual medical data,
		  deep-learning supported text analysis and concept spaces to
		  facilitate a visual discovery process. The tool is
		  evaluated by five medical doctors. The results show that
		  MediCoSpace facilitates a promising, yet complex way to
		  discover unlikely relations and thus suggests a path toward
		  the development of interactive visual tools to provide
		  physicians with more holistic diagnoses and personalized,
		  dynamic treatments for patients.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= jan,
  articleno	= {15},
  numpages	= {20},
  keywords	= {Visual analytics, natural language processing, interaction
		  design, electronic health records}
}

@Article{	  10.1145/3606699,
  author	= {Pich\'{e}, Dominique and Font, Ludovic and Zouaq, Amal and
		  Gagnon, Michel},
  title		= {Comparing Heuristic Rules and Masked Language Models for
		  Entity Alignment in the Literature Domain},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {3},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3606699},
  doi		= {10.1145/3606699},
  abstract	= {The cultural world offers a staggering amount of rich and
		  varied metadata on cultural heritage, accumulated by
		  governmental, academic, and commercial players. However,
		  the variety of involved institutions means that the data
		  are stored in as many complex and often incompatible models
		  and standards, which limits its availability and
		  explorability by the greater public. The adoption of Linked
		  Open Data technologies allows a strong interlinking of
		  these various databases as well as external connections
		  with existing knowledge bases. However, as they often
		  contain references to the same entities, the delicate issue
		  of entity alignment becomes the central challenge,
		  especially in the absence or scarcity of unique global
		  identifiers. To tackle this issue, we explored two
		  approaches, one based on a set of heuristic rules and one
		  based on masked language models, or masked language models
		  (MLMs). We compare these two approaches, as well as
		  different variations of MLMs, including some models trained
		  on a different language, and various levels of data
		  cleaning and labeling. Our results show that heuristics are
		  a solid approach but also that MLM-based entity alignment
		  obtains better performance coupled with the fact that it is
		  robust to the data format and does not require any form of
		  data preprocessing, which was not the case of the heuristic
		  approach in our experiments.},
  journal	= {J. Comput. Cult. Herit.},
  month		= aug,
  articleno	= {62},
  numpages	= {18},
  keywords	= {Linked open data, entity matching, masked language models,
		  cultural heritage, literature}
}

@Article{	  10.1145/3556538,
  author	= {Benedetto, Luca and Cremonesi, Paolo and Caines, Andrew
		  and Buttery, Paula and Cappelli, Andrea and Giussani,
		  Andrea and Turrin, Roberto},
  title		= {A Survey on Recent Approaches to Question Difficulty
		  Estimation from Text},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3556538},
  doi		= {10.1145/3556538},
  abstract	= {Question Difficulty Estimation from Text (QDET) is the
		  application of Natural Language Processing techniques to
		  the estimation of a value, either numerical or categorical,
		  which represents the difficulty of questions in educational
		  settings. We give an introduction to the field, build a
		  taxonomy based on question characteristics, and present the
		  various approaches that have been proposed in recent years,
		  outlining opportunities for further research. This survey
		  provides an introduction for researchers and practitioners
		  into the domain of question difficulty estimation from text
		  and acts as a point of reference about recent research in
		  this topic to date.},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  articleno	= {178},
  numpages	= {37},
  keywords	= {Question difficulty estimation, question calibration,
		  student assessment}
}

@InProceedings{	  10.1145/3604237.3626862,
  author	= {Chung, Andy and Tanaka-Ishii, Kumiko},
  title		= {Modeling Momentum Spillover with Economic Links Discovered
		  from Financial Documents},
  year		= {2023},
  isbn		= {9798400702402},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604237.3626862},
  doi		= {10.1145/3604237.3626862},
  abstract	= {Momentum spillover is a market anomaly well-acknowledged
		  in finance literature. This paper proposes using novel
		  economic links discovered from financial documents as a
		  momentum spillover channel, followed by modeling with graph
		  attention networks. These text-based economic links are
		  constructed using contextual embeddings extracted with
		  pre-trained language models from various sections of
		  company annual reports and earnings call transcripts. We
		  examine the effectiveness of our proposed methods based on
		  point-in-time S&amp;P500 constituents from 2010/01/01 to
		  2022/12/31 in the US stock market. We compare our proposed
		  model against the mean aggregator of peer firms’ momentum
		  baseline and Monte Carlo experiments based on randomized
		  nodes or edges. Our results show that our proposed graph
		  neural network model significantly outperforms the peer
		  firm’s momentum aggregation baseline. Furthermore,
		  economic links discovered in some sections of company
		  annual reports and earnings call transcripts are useful for
		  modeling momentum spillover. In particular, the economic
		  link constructed from management discussion and analysis
		  from earnings call transcripts outperforms the industry
		  link, which represents the well-acknowledged industry
		  momentum.},
  booktitle	= {Proceedings of the Fourth ACM International Conference on
		  AI in Finance},
  pages		= {490–497},
  numpages	= {8},
  keywords	= {Economic links, graph neural networks, inattention, large
		  language models, momentum},
  location	= {Brooklyn, NY, USA},
  series	= {ICAIF '23}
}

@InProceedings{	  10.1145/3543507.3583387,
  author	= {Zhang, Zhenyu and Yu, Bowen and Liu, Tingwen and Liu,
		  Tianyun and Wang, Yubin and Guo, Li},
  title		= {Learning Structural Co-occurrences for Structured Web Data
		  Extraction in Low-Resource Settings},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583387},
  doi		= {10.1145/3543507.3583387},
  abstract	= {Extracting structured information from all manner of
		  webpages is an important problem with the potential to
		  automate many real-world applications. Recent work has
		  shown the effectiveness of leveraging DOM trees and
		  pre-trained language models to describe and encode
		  webpages. However, they typically optimize the model to
		  learn the semantic co-occurrence of elements and labels in
		  the same webpage, thus their effectiveness depends on
		  sufficient labeled data, which is labor-intensive. In this
		  paper, we further observe structural co-occurrences in
		  different webpages of the same website: the same position
		  in the DOM tree usually plays the same semantic role, and
		  the DOM nodes in this position also share similar surface
		  forms. Motivated by this, we propose a novel method,
		  Structor, to effectively incorporate the structural
		  co-occurrences over DOM tree and surface form into
		  pre-trained language models. Such structural co-occurrences
		  help the model learn the task better under low-resource
		  settings, and we study two challenging experimental
		  scenarios: website-level low-resource setting and
		  webpage-level low-resource setting, to evaluate our
		  approach. Extensive experiments on the public SWDE dataset
		  show that Structor significantly outperforms the
		  state-of-the-art models in both settings, and even achieves
		  three times the performance of the strong baseline model in
		  the case of extreme lack of training data.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1683–1692},
  numpages	= {10},
  keywords	= {low-resource setting, regular expression, structural
		  co-occurrence, web information extraction},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3503161.3551610,
  author	= {Ramesh, Raksha and Anand, Vishal and Chen, Zifan and Dong,
		  Yifei and Chen, Yun and Lin, Ching-Yung},
  title		= {Leveraging Text Representation and Face-head Tracking for
		  Long-form Multimodal Semantic Relation Understanding},
  year		= {2022},
  isbn		= {9781450392037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503161.3551610},
  doi		= {10.1145/3503161.3551610},
  abstract	= {In the intricate problem of understanding long-form
		  multi-modal inputs, few key-aspects in scene-understanding
		  and dialogue-and-discourse are often overlooked. In this
		  paper, we investigate two such key-aspects for better
		  semantic and relational understanding - (i).
		  head-object-tracking in addition to usual face-tracking,
		  and (ii). fusing scene-to-text representation with external
		  common-sense knowledge-base for effective mapping to
		  sub-tasks of interest. The usage of head-tracking
		  especially helps with enriching sparse entity mapping to
		  inter-entity conversation interactions. These methods are
		  guided by natural language supervision on visual models,
		  and perform well for interaction and sentiment
		  understanding tasks.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Multimedia},
  pages		= {7215–7219},
  numpages	= {5},
  keywords	= {dialogue and discourse, intent detection, knowledge
		  graphs, language models, natural language processing,
		  object tracking, slot filling, speaker diarization},
  location	= {Lisboa, Portugal},
  series	= {MM '22}
}

@Article{	  10.1109/taslp.2023.3275028,
  author	= {Deng, Shumin and Yang, Jiacheng and Ye, Hongbin and Tan,
		  Chuanqi and Chen, Mosha and Huang, Songfang and Huang, Fei
		  and Chen, Huajun and Zhang, Ningyu},
  title		= {LOGEN: Few-Shot Logical Knowledge-Conditioned Text
		  Generation With Self-Training},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3275028},
  doi		= {10.1109/TASLP.2023.3275028},
  abstract	= {Natural language generation from structured data mainly
		  focuses on surface-level descriptions, suffering from
		  uncontrollable content selection and low fidelity. Previous
		  works leverage logical forms to facilitate logical
		  knowledge-conditioned text generation. Though achieving
		  remarkable progress, they are data-hungry, which makes the
		  adoption for real-world applications challenging with
		  limited data. To this end, this paper proposes a unified
		  framework for logical knowledge-conditioned text generation
		  in the few-shot setting. With only a few seeds logical
		  forms (e.g., 20/100 shot), our approach leverages
		  self-training and samples pseudo logical forms based on
		  content and structure consistency. Experimental results
		  demonstrate that our approach can obtain better few-shot
		  performance than baselines.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= may,
  pages		= {2124–2133},
  numpages	= {10}
}

@InProceedings{	  10.1145/3511047.3537659,
  author	= {Bolioli, Andrea and Bosca, Alessio and Damiano, Rossana
		  and Lieto, Antonio and Striani, Manuel},
  title		= {A complementary account to emotion extraction and
		  classification in cultural heritage based on the
		  Plutchik’s theory},
  year		= {2022},
  isbn		= {9781450392327},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511047.3537659},
  doi		= {10.1145/3511047.3537659},
  abstract	= {The paper presents a combined approach to knowledge-based
		  emotion attribution and classification of cultural items
		  employed in the H2020 project SPICE. In particular, we show
		  a preliminary experimentation conducted on a selection of
		  items contributed by the GAM Museum in Turin (Galleria di
		  Arte Moderna), pointing out how different language-based
		  approaches to emotion categorization (used in the systems
		  Sophia and DEGARI respectively) can be powerfully combined
		  to cope with both coverage and extended affective
		  attributions. Interestingly, both approaches are based on
		  an ontology of the Plutchik’s theory of emotions.},
  booktitle	= {Adjunct Proceedings of the 30th ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {374–382},
  numpages	= {9},
  keywords	= {Affective Content Aggregation, Commonsense Reasoning,
		  Description Logics},
  location	= {Barcelona, Spain},
  series	= {UMAP '22 Adjunct}
}

@Article{	  10.1145/3617892,
  author	= {Niu, Yanrui and Liang, Chao and Lu, Ankang and Huang,
		  Baojin and Wang, Zhongyuan and Guo, Jiahao},
  title		= {Person-action Instance Search in Story Videos: An
		  Experimental Study},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3617892},
  doi		= {10.1145/3617892},
  abstract	= {Person-Action instance search (P-A INS) aims to retrieve
		  the instances of a specific person doing a specific action,
		  which appears in the 2019–2021 INS tasks of the
		  world-famous TREC Video Retrieval Evaluation (TRECVID).
		  Most of the top-ranking solutions can be summarized with a
		  Division-Fusion-Optimization (DFO) framework, in which
		  person and action recognition scores are obtained
		  separately, then fused, and, optionally, further optimized
		  to generate the final ranking. However, TRECVID only
		  evaluates the final ranking results, ignoring the effects
		  of intermediate steps and their implementation methods. We
		  argue that conducting the fine-grained evaluations of
		  intermediate steps of DFO framework will (1) provide a
		  quantitative analysis of the different methods’
		  performance in intermediate steps; (2) find out better
		  design choices that contribute to improving retrieval
		  performance; and (3) inspire new ideas for future research
		  from the limitation analysis of current techniques.
		  Particularly, we propose an indirect evaluation method
		  motivated by the leave-one-out strategy, which finds an
		  optimal solution surpassing the champion teams in
		  2020–2021 INS tasks. Moreover, to validate the
		  generalizability and robustness of the proposed solution
		  under various scenarios, we specifically construct a new
		  large-scale P-A INS dataset and conduct comparative
		  experiments with both the leading NIST TRECVID INS solution
		  and the state-of-the-art P-A INS method. Finally, we
		  discuss the limitations of our evaluation work and suggest
		  future research directions.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= nov,
  articleno	= {46},
  numpages	= {34},
  keywords	= {Movie video, composite concepts, person-action instance
		  search}
}

@Proceedings{	  10.1145/3610591,
  title		= {SA '23: SIGGRAPH Asia 2023 Art Papers},
  year		= {2023},
  isbn		= {9798400703201},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sydney, NSW, Australia}
}

@Proceedings{	  10.1145/3615886,
  title		= {GeoAI '23: Proceedings of the 6th ACM SIGSPATIAL
		  International Workshop on AI for Geographic Knowledge
		  Discovery},
  year		= {2023},
  isbn		= {9798400703485},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Emerging advances from artificial intelligence, hardware
		  accelerators, and data processing architectures continue to
		  reach the geospatial information sciences, with a
		  transformative impact in many societal challenges. Recent
		  breakthroughs in deep learning have brought forward an
		  automated capability to learn hierarchical representational
		  features from massive and complex data, including text,
		  images, and videos. In tandem, rapid innovations in sensing
		  technologies are supporting the collection of geospatial
		  data in even higher resolution and throughput, supporting
		  the observation, mapping, and analysis of different
		  events/phenomena over the earth's surface with
		  unprecedented detail. Combined, these developments are
		  offering potential for breakthroughs in geographic
		  knowledge discovery, impacting decision making in areas
		  such as humanitarian mapping, intelligent transport
		  systems, urban expansion analysis, health data analysis and
		  epidemiology, the study of climate change, handling natural
		  disasters, and the general monitoring of the Earth's
		  surface.},
  location	= {Hamburg, Germany}
}

@InProceedings{	  10.1145/3623462.3623465,
  author	= {Wehmeier, Colter and Artopoulos, Georgios},
  title		= {MetaFraming: A Methodology for Democratizing Heritage
		  Interpretation Through Wiki Surveys},
  year		= {2023},
  isbn		= {9798400708367},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3623462.3623465},
  doi		= {10.1145/3623462.3623465},
  abstract	= {Recent developments in the Digital Humanities reveal how
		  traditional survey methods, when applied to the study of
		  cultural heritage, often struggle to encapsulate the
		  intricate social dynamics of our interactions with built
		  environments and artefacts. Despite the allure of digital
		  tools promising scalability, nonlinearity, and increased
		  engagement, their fit for heritage interpretation remains
		  an open question. To address this gap, we introduce
		  MetaFraming—a contribution to participatory methodology
		  designed to leverage computational social science tools
		  such as artificial intelligence and wiki surveys, towards
		  inclusive and democratic approaches in heritage
		  interpretation. MetaFraming enables researchers to
		  transform extensive preliminary research notes into a
		  metadata-rich, semantically structured dataset using an AI
		  processing pipeline, thereby modelling diverse perspectives
		  on heritage artefacts. Following manual refinement, this
		  dataset serves as the initial ’seed’ state for a wiki
		  survey (a user-editable, collaborative survey). Such a
		  survey enables the crowd to rank propositions, comment, and
		  contribute new ideas. Notably, participant input itself
		  contains metadata, allowing for a subsequent automated
		  pipeline to reconstruct the context of actions such as
		  comments. This secondary process provides rich insights
		  into recommendations, specific user/actor experiences,
		  group interests, and the complex relationships between
		  them. Through a design-research framework, we apply
		  MetaFraming to a case study in architectural heritage: our
		  artefact of study is Le Corbusier’s renowned Unit\'{e}
		  d’habitation (1952), a seminal prototype for social
		  housing and urbanism in post-war France. This exploration
		  enables us to contrast our novel web-based survey method
		  with traditional approaches, thereby highlighting new
		  opportunities for computer-aided collaboration in heritage
		  interpretation. By fostering reflective exploration of
		  built environments and societal legacies, our work
		  contributes to the growing discourse on digital
		  technologies in cultural heritage, advocating for
		  interdisciplinary research and dialogue.},
  booktitle	= {Proceedings of the 20th International Conference on
		  Culture and Computer Science: Code and Materiality},
  articleno	= {4},
  numpages	= {9},
  keywords	= {MetaFraming, Modern Architectural Heritage, Participatory
		  Heritage, Wiki Surveys},
  location	= {Lisbon, Portugal},
  series	= {KUI '23}
}

@InProceedings{	  10.1145/3573428.3573743,
  author	= {Xu, Liangbin and Ji, Baiyang},
  title		= {Industry Classification Algorithm Based on Improved BERT
		  Model},
  year		= {2023},
  isbn		= {9781450397148},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3573428.3573743},
  doi		= {10.1145/3573428.3573743},
  abstract	= {With the rapid development of the economy, many
		  enterprises are derived, and different enterprises have
		  different economic activities, and the description of
		  economic activities is usually in the form of short texts.
		  The latest National Standard of the People's Republic of
		  China - Classification of National Economic Industries is
		  divided into 1,381 categories according to categories,
		  major categories, medium categories and minor categories,
		  and industry classification often relies on human
		  experience, which takes a long time and the work is more
		  mechanical. This paper aims to reduce the time and labor
		  costs consumed by traditional industry classification
		  methods. This paper draws on short text classification
		  algorithm, convolutional neural network algorithm, and
		  considers the particularity of different industries. An
		  industry classification algorithm based on the improved
		  BERT model is proposed. Based on the BERT model, the
		  algorithm combines the convolutional neural network and the
		  three-channel model to analyze the main business
		  description of the listed company from the three levels of
		  words, words and concepts, so as to determine the industry
		  attribution. Taking the main products of listed companies
		  screened in Shanghai Stock Exchange and Oriental Fortune
		  Net in the past three years as a data set, the experimental
		  results show that this method is excellent for industry
		  classification.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {1790–1794},
  numpages	= {5},
  keywords	= {BERT, Industry classification, convolutional neural
		  network, short text classification, three-channel model},
  location	= {Xiamen, China},
  series	= {EITCE '22}
}

@Proceedings{	  10.1145/3615887,
  title		= {GeoHumanities '23: Proceedings of the 7th ACM SIGSPATIAL
		  International Workshop on Geospatial Humanities},
  year		= {2023},
  isbn		= {9798400703492},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The 7th International Workshop on Geospatial Humanities
		  (GeoHumanities 2023) was held, together with the 31st ACM
		  SIGSPATIAL International Conference on Advances in
		  Geographic Information Systems, in Hamburg, Germany. This
		  workshop has been a regular venue for computational
		  research at the cutting edge of spatial data creation,
		  curation, analysis, visualization, and interpretation in
		  the humanities. It brings together researchers and
		  practitioners from computer science, the geographical
		  information sciences, and the humanities, whose work
		  combines humanistic questions with computational spatial
		  methods. The GeoHumanities series of workshops supports
		  interdisciplinary, and often collaborative, research that
		  makes novel contributions to both the humanities and the
		  sciences.},
  location	= {Hamburg, Germany}
}

@Proceedings{	  10.1145/3603163,
  title		= {HT '23: Proceedings of the 34th ACM Conference on
		  Hypertext and Social Media},
  year		= {2023},
  isbn		= {9798400702327},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rome, Italy}
}

@InProceedings{	  10.1145/3583780.3614936,
  author	= {Guo, Hao and Zeng, Weixin and Tang, Jiuyang and Zhao,
		  Xiang},
  title		= {Interpretable Fake News Detection with Graph Evidence},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614936},
  doi		= {10.1145/3583780.3614936},
  abstract	= {Automatic detection of fake news has received widespread
		  attentions over recent years. A pile of efforts has been
		  put forward to address the problem with high accuracy,
		  while most of them lack convincing explanations, making it
		  difficult to curb the continued spread of false news in
		  real-life cases. Although some models leverage external
		  resources to provide preliminary interpretability, such
		  external signals are not always available. To fill in this
		  gap, in this work, we put forward an interpretable fake
		  news detection model IKA by making use of the historical
		  evidence in the form of graphs. Specifically, we establish
		  both positive and negative evidence graphs by collecting
		  the signals from the historical news, i.e., training data.
		  Then, given a piece of news to be detected, in addition to
		  the common features used for detecting false news, we
		  compare the news and evidence graphs to generate both the
		  matching vector and the related graph evidence for
		  explaining the prediction. We conduct extensive experiments
		  on both Chinese and English datasets. The experiment
		  results show that the detection accuracy of IKA exceeds the
		  state-of-the-art approaches and IKA can provide useful
		  explanations for the prediction results. Besides, IKA is
		  general and can be applied on other models to improve their
		  interpretability.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {659–668},
  numpages	= {10},
  keywords	= {explainable machine learning, fake news detection, social
		  media},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3511808.3557506,
  author	= {Huang, Chao and Xia, Lianghao and Wang, Xiang and He,
		  Xiangnan and Yin, Dawei},
  title		= {Self-Supervised Learning for Recommendation},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557506},
  doi		= {10.1145/3511808.3557506},
  abstract	= {Recommender systems are playing an increasingly critical
		  role to alleviate information overload and satisfy users'
		  information seeking requirements in a wide spectrum of
		  online platforms. However, the ubiquity of data sparsity
		  and noise notably limits the representation capacity of
		  existing recommender systems to learn high-quality user
		  (item) embeddings. Inspired by recent advances of
		  self-supervised learning (SSL) techniques, SSL-based
		  representation learning models benefit a variety of
		  recommendation domains. Such methods have achieved new
		  levels of performance while reducing the dependence on
		  observed supervision labels in diverse recommendation
		  tasks. In this tutorial, we aim to provide a systemic
		  review of state-of-the-art SSL-based recommender systems.
		  To be specific, we summarize and categorize existing work
		  of SSL-based recommender systems in terms of recommendation
		  scenarios. For each type of recommendation task, the
		  corresponding challenges and methods will be presented in a
		  comprehensive way. Finally, some future directions and open
		  questions will be raised to inspire more investigation on
		  this important research line.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {5136–5139},
  numpages	= {4},
  keywords	= {collaborative filtering, contrastive learning, graph
		  neural networks, recommender system, self-supervised
		  learning},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InBook{	  10.1145/3581906.3581922,
  author	= {Punjani, Dharmen and Tsalapati, Eleni},
  title		= {Question Answering Engines for Geospatial Knowledge
		  Graphs},
  year		= {2023},
  isbn		= {9798400707407},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  url		= {https://doi.org/10.1145/3581906.3581922},
  booktitle	= {Geospatial Data Science: A Hands-on Approach for Building
		  Geospatial Applications Using Linked Data Technologies},
  pages		= {257–282},
  numpages	= {26}
}

@InProceedings{	  10.1145/3599957.3606249,
  author	= {Ahn, Sung-Yoon and Lee, Sang-Woong},
  title		= {BERT-based classification of fungi protein sequences with
		  multiple GO labels},
  year		= {2023},
  isbn		= {9798400702280},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3599957.3606249},
  doi		= {10.1145/3599957.3606249},
  abstract	= {Due to the increase of reported fungi-related diseases, it
		  has come to many health organizations concern that there
		  may be possible highly contagious fungi that may cause yet
		  another pandemic. Though the likelihood of such is low,
		  research is needed to grasp the understanding of unknown
		  fungi. Identifying and figuring out the traits of unknown
		  fungi through in vitro and in vivo experiments take time
		  and resources. In silico methods yield faster results with
		  a slight drop in accuracy. Modern in silico approaches
		  utilizing deep learning, allow for faster and more accurate
		  classifications. In this study, we perform the
		  classification of one or more gene ontologies of fungi
		  protein sequences. We collected open-source protein
		  sequences from UniProt and applied an algorithm to label
		  the sequences with their gene ontologies. We use ProtBERT
		  with additional layers to give classification results to
		  all the different gene ontologies. Experimental results
		  reveal that when classifying with the top 5 most frequent
		  gene ontologies, the model was able to yield 0.7915 for
		  F1-score, 0.7073 for MCC, and 0.8865 for AuROC. With the
		  top 10 most frequent gene ontologies it yielded 0.6490 for
		  F1-score, 0.6836 for MCC, and 0.7653 for AuROC.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Research in Adaptive and Convergent Systems},
  articleno	= {28},
  numpages	= {4},
  keywords	= {BERT, Fungi, Gene Ontology},
  location	= {Gdansk, Poland},
  series	= {RACS '23}
}

@Article{	  10.1145/3571726,
  author	= {Rahman, Md Rayhanur and Hezaveh, Rezvan Mahdavi and
		  Williams, Laurie},
  title		= {What Are the Attackers Doing Now? Automating Cyberthreat
		  Intelligence Extraction from Text on Pace with the Changing
		  Threat Landscape: A Survey},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {12},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3571726},
  doi		= {10.1145/3571726},
  abstract	= {Cybersecurity researchers have contributed to the
		  automated extraction of CTI from textual sources, such as
		  threat reports and online articles describing cyberattack
		  strategies, procedures, and tools. The goal of this article
		  is to aid cybersecurity researchers in understanding the
		  current techniques used for cyberthreat intelligence
		  extraction from text through a survey of relevant studies
		  in the literature. Our work finds 11 types of extraction
		  purposes and 7 types of textual sources for CTI extraction.
		  We observe the technical challenges associated with
		  obtaining available clean and labeled data for replication,
		  validation, and further extension of the studies. We
		  advocate for building upon the current CTI extraction work
		  to help cybersecurity practitioners with proactive
		  decision-making such as in threat prioritization and
		  mitigation strategy formulation to utilize knowledge from
		  past cybersecurity incidents.},
  journal	= {ACM Comput. Surv.},
  month		= mar,
  articleno	= {241},
  numpages	= {36},
  keywords	= {Cyberthreat intelligence, CTI extraction, CTI mining, IoC
		  extraction, TTPs extraction, attack pattern extraction,
		  threat reports, tactical threat intelligence, technical
		  threat intelligence}
}

@InProceedings{	  10.1145/3551349.3556929,
  author	= {Liu, Zixi and Feng, Yang and Yin, Yining and Sun, Jingyu
		  and Chen, Zhenyu and Xu, Baowen},
  title		= {QATest: A Uniform Fuzzing Framework for Question Answering
		  Systems},
  year		= {2023},
  isbn		= {9781450394758},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3551349.3556929},
  doi		= {10.1145/3551349.3556929},
  abstract	= {The tremendous advancements in deep learning techniques
		  have empowered question answering(QA) systems with the
		  capability of dealing with various tasks. Many commercial
		  QA systems, such as Siri, Google Home, and Alexa, have been
		  deployed to assist people in different daily activities.
		  However, modern QA systems are often designed to deal with
		  different topics and task formats, which makes both the
		  test collection and labeling tasks difficult and thus
		  threats their quality. To alleviate this challenge, in this
		  paper, we design and implement a fuzzing framework for QA
		  systems, namely QATest, based on the metamorphic testing
		  theory. It provides the first uniform solution to generate
		  tests with oracle information automatically for various QA
		  systems, such as machine reading comprehension, open-domain
		  QA, and QA on knowledge bases. To further improve testing
		  efficiency and generate more tests detecting erroneous
		  behaviors, we design N-Gram coverage and perplexity
		  priority based on the features of the question data to
		  guide the generation process. To evaluate the performance
		  of QATest, we experiment with it on four QA systems that
		  are designed for different tasks. The experiment results
		  show that the tests generated by QATest detect hundreds of
		  erroneous behaviors of QA systems efficiently. Also, the
		  results confirm that the testing criteria can improve test
		  diversity and fuzzing efficiency.},
  booktitle	= {Proceedings of the 37th IEEE/ACM International Conference
		  on Automated Software Engineering},
  articleno	= {81},
  numpages	= {12},
  keywords	= {automated testing, fuzz testing, natural language
		  processing, question answering systems},
  location	= {Rochester, MI, USA},
  series	= {ASE '22}
}

@Article{	  10.1145/3629168,
  author	= {Ao, Xiang and Luo, Ling and Wang, Xiting and Yang, Zhao
		  and Chen, Jiun-Hung and Qiao, Ying and He, Qing and Xie,
		  Xing},
  title		= {Put Your Voice on Stage: Personalized Headline Generation
		  for News Articles},
  year		= {2023},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {3},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3629168},
  doi		= {10.1145/3629168},
  abstract	= {In this article, we study the problem of personalized news
		  headline generation, which aims to produce not only concise
		  and fact-consistent titles for news articles but also
		  decorate these titles as personalized irresistible reading
		  invitations by incorporating readers’ preferences. We
		  propose an approach named PNG&nbsp;(Personalized News
		  headline Generator) by utilizing distant supervision in
		  readers’ past click behaviors to resolve. First, user
		  preference representations are learned through a
		  knowledge-aware user encoder that comprehensively captures
		  the genuine, sequential, and flash interests of users
		  reflected in their historical clicked news. Then, a
		  user-perturbed pointer-generator network is devised to
		  accomplish the headline generation in which the learned
		  user representations implicitly affect the word prediction.
		  The proposed model is optimized by reinforcement learning
		  solvers where indicators on factual, personalized, and
		  linguistic aspects of the generated headline are regarded
		  as rewards. Extensive experiments are conducted on the
		  real-world dataset PENS,1 which is a large-scale benchmark
		  collected from Microsoft News. Both the quantitative and
		  qualitative results validate the effectiveness of our
		  approach.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= dec,
  articleno	= {54},
  numpages	= {20},
  keywords	= {News headline generation, user modeling, personalization}
}

@InProceedings{	  10.1145/3524458.3547242,
  author	= {Colagrossi, Marco and Consoli, Sergio and Panella,
		  Francesco and Barbaglia, Luca},
  title		= {Tracking socio-economic activities in European countries
		  with unconventional data},
  year		= {2022},
  isbn		= {9781450392846},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3524458.3547242},
  doi		= {10.1145/3524458.3547242},
  abstract	= {This contribution shows our ongoing work aimed at
		  monitoring societal issues and economic activities (e.g.,
		  industrial production, unemployment, loneliness, cultural
		  participation) across EU member states mining
		  unconventional data sources to complement official
		  statistics. Considered unconventional data sources include
		  the Global Dataset of Events, Language and Tone (GDELT),
		  Google Search data, and Dow Jones Data, News and Analytics
		  (DNA). We show an early experiment aiming at nowcasting
		  unemployment in Germany, Spain, France, and Italy,
		  demonstrating the added value of these data both for
		  scholars and policymakers.},
  booktitle	= {Proceedings of the 2022 ACM Conference on Information
		  Technology for Social Good},
  pages		= {323–330},
  numpages	= {8},
  keywords	= {alternative (big) datasets, social media, text analysis},
  location	= {Limassol, Cyprus},
  series	= {GoodIT '22}
}

@InProceedings{	  10.1145/3459637.3482222,
  author	= {Zhou, Yichao and Jiang, Jyun-Yu and Chen, Xiusi and Wang,
		  Wei},
  title		= {#StayHome or #Marathon? Social Media Enhanced Pandemic
		  Surveillance on Spatial-temporal Dynamic Graphs},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482222},
  doi		= {10.1145/3459637.3482222},
  abstract	= {COVID-19 has caused lasting damage to almost every domain
		  in public health, society, and economy. To monitor the
		  pandemic trend, existing studies rely on the aggregation of
		  traditional statistical models and epidemic spread theory.
		  In other words, historical statistics of COVID-19, as well
		  as the population mobility data, become the essential
		  knowledge for monitoring the pandemic trend. However, these
		  solutions can barely provide precise prediction and
		  satisfactory explanations on the long-term disease
		  surveillance while the ubiquitous social media resources
		  can be the key enabler for solving this problem. For
		  example, serious discussions may occur on social media
		  before and after some breaking events take place. To take
		  advantage of the social media data, we propose a novel
		  framework, Social Media enhAnced pandemic suRveillance
		  Technique (SMART), which is composed of two modules: (i)
		  information extraction module to construct heterogeneous
		  knowledge graphs based on the extracted events and
		  relationships among them; (ii) time series prediction
		  module to provide both short-term and long-term forecasts
		  of the confirmed cases and fatality at the state-level in
		  the United States and to discover risk factors for COVID-19
		  interventions. Extensive experiments show that our method
		  largely outperforms the state-of-the-art baselines by 7.3%
		  and 7.4% in confirmed case/fatality prediction,
		  respectively.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2738–2748},
  numpages	= {11},
  keywords	= {information extraction, social media mining, time series
		  prediction},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.1145/3580480,
  author	= {Du, Kelvin and Xing, Frank and Cambria, Erik},
  title		= {Incorporating Multiple Knowledge Sources for Targeted
		  Aspect-based Financial Sentiment Analysis},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {3},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3580480},
  doi		= {10.1145/3580480},
  abstract	= {Combining symbolic and subsymbolic methods has become a
		  promising strategy as research tasks in AI grow
		  increasingly complicated and require higher levels of
		  understanding. Targeted Aspect-based Financial Sentiment
		  Analysis (TABFSA) is an example of such complicated tasks,
		  as it involves processes like information extraction,
		  information specification, and domain adaptation. However,
		  little is known about the design principles of such hybrid
		  models leveraging external lexical knowledge. To fill this
		  gap, we define anterior, parallel, and posterior knowledge
		  integration and propose incorporating multiple lexical
		  knowledge sources strategically into the fine-tuning
		  process of pre-trained transformer models for TABFSA.
		  Experiments on the Financial Opinion mining and Question
		  Answering challenge (FiQA) Task 1 and SemEval 2017 Task 5
		  datasets show that the knowledge-enabled models
		  systematically improve upon their plain deep learning
		  counterparts, and some outperform state-of-the-art results
		  reported in terms of aspect sentiment analysis error. We
		  discover that parallel knowledge integration is the most
		  effective and domain-specific lexical knowledge is more
		  important according to our ablation analysis.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= jun,
  articleno	= {23},
  numpages	= {24},
  keywords	= {Financial sentiment analysis, neural networks, knowledge
		  enabled system, deep learning, transformer models}
}

@InProceedings{	  10.1145/3583780.3614870,
  author	= {Tiwari, Abhisek and Saha, Anisha and Saha, Sriparna and
		  Bhattacharyya, Pushpak and Dhar, Minakshi},
  title		= {Experience and Evidence are the eyes of an excellent
		  summarizer! Towards Knowledge Infused Multi-modal Clinical
		  Conversation Summarization},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614870},
  doi		= {10.1145/3583780.3614870},
  abstract	= {With the advancement of telemedicine, both researchers and
		  medical practitioners are working hand-in-hand to develop
		  various techniques to automate various medical operations,
		  such as diagnosis report generation. In this paper, we
		  first present a multi-modal clinical conversation summary
		  generation task that takes a clinician-patient interaction
		  (both textual and visual information) and generates a
		  succinct synopsis of the conversation. We propose a
		  knowledge-infused, multi-modal, multi-tasking medical
		  domain identification and clinical conversation summary
		  generation (MM-CliConSummation) framework. It leverages an
		  adapter to infuse knowledge and visual features and unify
		  the fused feature vector using a gated mechanism.
		  Furthermore, we developed a multi-modal, multi-intent
		  clinical conversation summarization corpus annotated with
		  intent, symptom, and summary. The extensive set of
		  experiments, both quantitatively and qualitatively, led to
		  the following findings: (a) critical significance of
		  visuals, (b) more precise and medical entity preserving
		  summary with additional knowledge infusion, and (c) a
		  correlation between medical department identification and
		  clinical synopsis generation. Furthermore, the dataset and
		  source code are available at
		  https://github.com/NLP-RL/MM-CliConSummation},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2452–2461},
  numpages	= {10},
  keywords	= {multimodal infusion, multimodal medical dialogue
		  summarization, online counselling, text generation},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3543507.3583330,
  author	= {Zhang, Rongjunchen and Wu, Tingmin and Chen, Xiao and Wen,
		  Sheng and Nepal, Surya and Paris, Cecile and Xiang, Yang},
  title		= {Dynalogue: A Transformer-Based Dialogue System with
		  Dynamic Attention},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583330},
  doi		= {10.1145/3543507.3583330},
  abstract	= {Businesses face a range of cyber risks, both external
		  threats and internal vulnerabilities that continue to
		  evolve over time. As cyber attacks continue to increase in
		  complexity and sophistication, more organisations will
		  experience them. For this reason, it is important that
		  organisations seek timely consultancy from cyber
		  professionals so that they can respond to and recover from
		  cyber attacks as quickly as possible. However, huge surges
		  in cyber attacks have long left cyber professionals short
		  of what is required to cover the security needs. This
		  problem is getting worse when an increasing number of
		  people choose to work from home during the pandemic because
		  this situation usually yields extra communication cost. In
		  this paper, we propose to develop a cybersecurity-oriented
		  dialogue system, called Dynalogue1, which can provide
		  consultancy online as a cyber professional. For the first
		  time, Dynalogue provides a promising solution to mitigate
		  the need for cyber professionals via automatically
		  generating problem-targeted conversions to victims of cyber
		  attacks. In spite of many dialogue systems developed in the
		  past, Dynalogue provides a distinct capability of handling
		  long and complicated sentences that are common in
		  cybersecurity-related conversations. It is challenging to
		  have this capability because limited memory in dialogue
		  systems can be hard to accommodate sufficient key
		  information of long sentences. To overcome this challenge,
		  Dynalogue utilises an attention mechanism that dynamically
		  captures key semantics within a sentence instead of using
		  fix window to cut off the sentence. To evaluate Dynalogue,
		  we collect 67K real-world conversations (0.6M utterances)
		  from Bleeping Computer2, which is one of the most popular
		  cybersecurity consultancy websites in the world. The
		  results suggest that Dynalogue outperforms all the existing
		  dialogue systems with 1% ∼ 9% improvements on all
		  different metrics. We further run Dynalogue on the public
		  dataset WikiHow to validate its compatibility in other
		  domains where conversations are also long and complicated.
		  Dynalogue also outperforms all the other methods with at
		  most 2.4% improvement.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1604–1615},
  numpages	= {12},
  keywords	= {Cybersecurity conversation dataset, Dynamic attention,
		  Generation-based dialogue system, Retrieval-based dialogue
		  system, Transformer},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@Proceedings{	  10.1145/3586183,
  title		= {UIST '23: Proceedings of the 36th Annual ACM Symposium on
		  User Interface Software and Technology},
  year		= {2023},
  isbn		= {9798400701320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {San Francisco, CA, USA}
}

@Article{	  10.1109/taslp.2023.3254166,
  author	= {Su, Xinxin and Huang, Zhen and Zhao, Yunxiang and Chen,
		  Yifan and Dou, Yong and Pan, Hengyue},
  title		= {Recent Trends in Deep Learning Based Textual Emotion Cause
		  Extraction},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3254166},
  doi		= {10.1109/TASLP.2023.3254166},
  abstract	= {Emotion Cause Extraction Field (ECEF) focuses on the cause
		  that triggers an emotion in a document. Traditional ECEF
		  aims to extract the cause based on a given emotion while
		  recent ECEF focuses more on extracting both the emotion and
		  its corresponding cause. ECEF has attracted a lot of
		  attention due to the significant developments in deep
		  learning techniques, especially machine reading
		  comprehension and neural network-based information
		  retrieval. However, a comprehensive review of existing
		  approaches and recent trends in ECEF is lacking. In this
		  paper, we present a thorough survey to summarise existing
		  methods for ECEF, including those for Emotion Cause
		  Extraction (ECE), Emotion Cause Pair Extraction (ECPE), and
		  Conversational Emotion Cause Extraction Field (CECEF). We
		  also detail the widely used public datasets and discuss the
		  limitations and prospects of existing methods in ECEF.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jul,
  pages		= {2765–2786},
  numpages	= {22}
}

@Article{	  10.1145/3473939,
  author	= {Ding, Haoran and Luo, Xiao},
  title		= {Attention-based Unsupervised Keyphrase Extraction and
		  Phrase Graph for COVID-19 Medical Literature Retrieval},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {1},
  url		= {https://doi.org/10.1145/3473939},
  doi		= {10.1145/3473939},
  abstract	= {Searching, reading, and finding information from the
		  massive medical text collections are challenging. A typical
		  biomedical search engine is not feasible to navigate each
		  article to find critical information or keyphrases.
		  Moreover, few tools provide a visualization of the relevant
		  phrases to the query. However, there is a need to extract
		  the keyphrases from each document for indexing and
		  efficient search. The transformer-based neural
		  networks—BERT has been used for various natural language
		  processing tasks. The built-in self-attention mechanism can
		  capture the associations between words and phrases in a
		  sentence. This research investigates whether the
		  self-attentions can be utilized to extract keyphrases from
		  a document in an unsupervised manner and identify relevancy
		  between phrases to construct a query relevancy phrase graph
		  to visualize the search corpus phrases on their relevancy
		  and importance. The comparison with six baseline methods
		  shows that the self-attention-based unsupervised keyphrase
		  extraction works well on a medical literature dataset. This
		  unsupervised keyphrase extraction model can also be applied
		  to other text data. The query relevancy graph model is
		  applied to the COVID-19 literature dataset and to
		  demonstrate that the attention-based phrase graph can
		  successfully identify the medical phrases relevant to the
		  query terms.},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= oct,
  articleno	= {12},
  numpages	= {16},
  keywords	= {Keyphrase extraction, deep learning, medical information
		  retrieval, COVID-19}
}

@Article{	  10.1145/3520082,
  author	= {Shang, Yu-Ming and Huang, Heyan and Sun, Xin and Wei, Wei
		  and Mao, Xian-Ling},
  title		= {Learning Relation Ties with a Force-Directed Graph in
		  Distant Supervised Relation Extraction},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3520082},
  doi		= {10.1145/3520082},
  abstract	= {Relation ties, defined as the correlation and mutual
		  exclusion between different relations, are critical for
		  distant supervised relation extraction. Previous studies
		  usually obtain this property by greedily learning the local
		  connections between relations. However, they are
		  essentially limited because of failing to capture the
		  global topology structure of relation ties and may easily
		  fall into a locally optimal solution. To address this
		  issue, we propose a novel force-directed graph to
		  comprehensively learn relation ties. Specifically, we first
		  construct a graph according to the global co-occurrence of
		  all relations. Then, we borrow the idea of Coulomb’s law
		  from physics and introduce the concept of attractive force
		  and repulsive force into this graph to learn correlation
		  and mutual exclusion between relations. Finally, the
		  obtained relation representations are applied as an
		  inter-dependent relation classifier. Extensive experimental
		  results demonstrate that our method is capable of modeling
		  global correlation and mutual exclusion between relations,
		  and outperforms the state-of-the-art baselines. In
		  addition, the proposed force-directed graph can be used as
		  a module to augment existing relation extraction systems
		  and improve their performance.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {10},
  numpages	= {23},
  keywords	= {Distant supervision, relation extraction, relation ties,
		  force-directed graph}
}

@InProceedings{	  10.1145/3485447.3511945,
  author	= {Liu, Xiao and Hong, Haoyun and Wang, Xinghao and Chen,
		  Zeyi and Kharlamov, Evgeny and Dong, Yuxiao and Tang, Jie},
  title		= {SelfKG: Self-Supervised Entity Alignment in Knowledge
		  Graphs},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511945},
  doi		= {10.1145/3485447.3511945},
  abstract	= {Entity alignment, aiming to identify equivalent entities
		  across different knowledge graphs (KGs), is a fundamental
		  problem for constructing Web-scale KGs. Over the course of
		  its development, the label supervision has been considered
		  necessary for accurate alignments. Inspired by the recent
		  progress of self-supervised learning, we explore the extent
		  to which we can get rid of supervision for entity
		  alignment. Commonly, the label information (positive entity
		  pairs) is used to supervise the process of pulling the
		  aligned entities in each positive pair closer. However, our
		  theoretical analysis suggests that the learning of entity
		  alignment can actually benefit more from pushing unlabeled
		  negative pairs far away from each other than pulling
		  labeled positive pairs close. By leveraging this discovery,
		  we develop the self-supervised learning objective for
		  entity alignment. We present SelfKG with efficient
		  strategies to optimize this objective for aligning entities
		  without label supervision. Extensive experiments on
		  benchmark datasets demonstrate that SelfKG &nbsp;without
		  supervision can match or achieve comparable results with
		  state-of-the-art supervised baselines. The performance of
		  SelfKG suggests that self-supervised learning offers great
		  potential for entity alignment in KGs. The code and data
		  are available at https://github.com/THUDM/SelfKG.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {860–870},
  numpages	= {11},
  keywords	= {Entity Alignment, Knowledge Graphs, Self-Supervised
		  Learning},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3578741.3578752,
  author	= {Yang, Wenchuan and Gu, Tianyu and Sui, Runqi},
  title		= {A Faster Method For Generating Chinese Text
		  Summaries-Combining Extractive Summarization And
		  Abstractive Summarization},
  year		= {2023},
  isbn		= {9781450399067},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3578741.3578752},
  doi		= {10.1145/3578741.3578752},
  abstract	= {Extractive summarization and generative summarization are
		  the two main ways to generate
		  summarization.However,previous work treats both of them as
		  two independent subtasks.In this paper,we obtain new
		  summarization by combining extractive summarization and
		  generative summarization.This method extracts the key
		  information of the article firstly,and then generates the
		  summarization of the extracted information.The experimental
		  result shows that this method can significantly improve the
		  quality of the generative text compared with extractive
		  summarization,and can significantly improve the generative
		  speed compared with generative summarization.},
  booktitle	= {Proceedings of the 2022 5th International Conference on
		  Machine Learning and Natural Language Processing},
  pages		= {54–58},
  numpages	= {5},
  keywords	= {Abstractive Summarization,Generative Summarization,T5
		  Model,Nezha Model,Recall-Oriented Understudy for Gisting
		  Evaluation},
  location	= {Sanya, China},
  series	= {MLNLP '22}
}

@InProceedings{	  10.1145/3587828.3587838,
  author	= {He, Jun and Chen, Jing and Peng, Li and Sun, Bo and Zhang,
		  Huiying},
  title		= {Question Difficulty Prediction with External Knowledge},
  year		= {2023},
  isbn		= {9781450398589},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587828.3587838},
  doi		= {10.1145/3587828.3587838},
  abstract	= {The difficulty of test questions is an important indicator
		  for educational examination and recommendation of
		  personalized learning resources. Its evaluation mainly
		  depends on the experience of experts, which is subjective.
		  In recent years, question difficulty prediction (QDP) using
		  neural networks has attracted more and more attention.
		  Although these methods improve the QDP efficiency, it works
		  ill for questions involving abstract concepts, such as
		  numerical calculation, date, and questions whose answers
		  require background knowledge. Therefore, we propose a
		  difficulty prediction model based on rich knowledge fusion
		  (RKF+), which solves the problem that the difficulty
		  prediction models cannot obtain conceptual knowledge and
		  background knowledge. The key is to introduce the
		  attentional mechanism with a sentry vector, which can
		  dynamically obtain the text representation and external
		  knowledge representation of test questions. To further
		  fusion the acquired external knowledge, our model added a
		  bi-interaction layer. Finally, the validity of this model
		  is verified on three different datasets. Besides, the
		  importance of attentional mechanism and external knowledge
		  representation is further analyzed by ablation experiment.
		  In addition, based on a real English reading comprehension
		  test dataset, we explore the influence of two kinds of
		  external knowledge on the question difficulty prediction
		  model.},
  booktitle	= {Proceedings of the 2023 12th International Conference on
		  Software and Computer Applications},
  pages		= {59–64},
  numpages	= {6},
  keywords	= {External Knowledge, Natural Language Processing, Problem
		  Difficulty Prediction},
  location	= {Kuantan, Malaysia},
  series	= {ICSCA '23}
}

@InProceedings{	  10.1145/3572549.3572629,
  author	= {Zhao, Gang and Yi, Jiarong and Chu, Jie and Zhang, Yinan
		  and Yin, Jianghua},
  title		= {Design and Implementation of the Teacher-student Dialogue
		  Automatic Speech Recognition Tool Based on Classroom Verbal
		  Characteristics},
  year		= {2023},
  isbn		= {9781450397766},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3572549.3572629},
  doi		= {10.1145/3572549.3572629},
  abstract	= {Speech behavior is a tool for teachers and students to
		  express and communicate in the classroom, and analyzing the
		  content of classroom teacher-student dialogue is one of the
		  key basic technologies to break through the automatic
		  analysis of teaching videos. However, the current discourse
		  research tools rely on the universal speech recognition
		  cloud platform, and there are problems such as confusion
		  between teacher-student role and unclear discourse
		  boundaries for classroom teacher-student dialogue
		  recognition. Therefore, this paper designs and develops
		  teacher-student dialogue recognition tools after classifing
		  and encoding the auditory information data and summarizing
		  the general rules of speech characteristics in classroom
		  teaching videos. To achieve the purpose of analyzing
		  classroom teaching videos, education researchers can use
		  the tool to intercept teaching videos, obtain boundary
		  points of teacher-student dialogue, and recognize their
		  speech content. Experimental results show that the tool has
		  stable performance, which can quickly and accurately
		  process audio signals, segment and cluster teacher and
		  student voiceprint features. Finally, it produce structured
		  text containing the time point of the discourse boundary,
		  the teacher-student role label and the discourse content,
		  which will provide data support for analyzing further
		  classroom teaching video behavior.},
  booktitle	= {Proceedings of the 14th International Conference on
		  Education Technology and Computers},
  pages		= {503–508},
  numpages	= {6},
  keywords	= {Classroom verbal characteristics, Speaker diarization,
		  Speech recognition, Teacher-student dialogue, Teaching
		  video},
  location	= {Barcelona, Spain},
  series	= {ICETC '22}
}

@Article{	  10.1145/3597307,
  author	= {Navigli, Roberto and Conia, Simone and Ross, Bj\"{o}rn},
  title		= {Biases in Large Language Models: Origins, Inventory, and
		  Discussion},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2},
  issn		= {1936-1955},
  url		= {https://doi.org/10.1145/3597307},
  doi		= {10.1145/3597307},
  abstract	= {In this article, we introduce and discuss the pervasive
		  issue of bias in the large language models that are
		  currently at the core of mainstream approaches to Natural
		  Language Processing (NLP). We first introduce data
		  selection bias, that is, the bias caused by the choice of
		  texts that make up a training corpus. Then, we survey the
		  different types of social bias evidenced in the text
		  generated by language models trained on such corpora,
		  ranging from gender to age, from sexual orientation to
		  ethnicity, and from religion to culture. We conclude with
		  directions focused on measuring, reducing, and tackling the
		  aforementioned types of bias.},
  journal	= {J. Data and Information Quality},
  month		= jun,
  articleno	= {10},
  numpages	= {21},
  keywords	= {Bias in NLP, language models}
}

@InProceedings{	  10.1145/3543873.3587657,
  author	= {Sahijwani, Harshita and Dhole, Kaustubh and Purwar, Ankur
		  and Vasudevan, Venugopal and Agichtein, Eugene},
  title		= {Contextual Response Interpretation for Automated
		  Structured Interviews: A Case Study in Market Research},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587657},
  doi		= {10.1145/3543873.3587657},
  abstract	= {Structured interviews are used in many settings,
		  importantly in market research on topics such as brand
		  perception, customer habits, or preferences, which are
		  critical to product development, marketing, and e-commerce
		  at large. Such interviews generally consist of a series of
		  questions that are asked to a participant. These interviews
		  are typically conducted by skilled interviewers, who
		  interpret the responses from the participants and can adapt
		  the interview accordingly. Using automated conversational
		  agents to conduct such interviews would enable reaching a
		  much larger and potentially more diverse group of
		  participants than currently possible. However, the
		  technical challenges involved in building such a
		  conversational system are relatively unexplored. To learn
		  more about these challenges, we convert a market research
		  multiple-choice questionnaire to a conversational format
		  and conduct a user study. We address the key task of
		  conducting structured interviews, namely interpreting the
		  participant’s response, for example, by matching it to
		  one or more predefined options. Our findings can be applied
		  to improve response interpretation for the information
		  elicitation phase of conversational recommender systems.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {886–891},
  numpages	= {6},
  keywords	= {conversational preference elicitation, conversational
		  recommender systems, intent prediction},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@InProceedings{	  10.1145/3503162.3503166,
  author	= {Ramnani, Roshni and Sengupta, Shubhashis},
  title		= {From Opinion Mining to Improvement Mining :
		  Understanding Product Improvements from User Reviews},
  year		= {2022},
  isbn		= {9781450395960},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503162.3503166},
  doi		= {10.1145/3503162.3503166},
  abstract	= {A valuable trove of information exists for product(s) or
		  services online via user opinions like detailed reviews
		  provided by customers on popular e-commerce websites. Users
		  express their individual opinions in the form of overall
		  product/service experiences, which may include explicit
		  positive/negative feedback, preferences, concerns, and
		  suggestions for the future. Such information can be
		  valuable to product/service owners in helping them
		  understand the improvement(s) that must be made to a
		  particular product or service. The primary focus of opinion
		  mining has been on understanding positive and negative
		  aspects within the review effectively. Limited emphasis has
		  been placed on finer topics like user suggestions or
		  conflicting information from users. In this work, we
		  describe a method to extract possible product / service
		  improvements from opinionated text in the form of
		  non-conflicting negative feedback, user tips,
		  recommendations, product usage details, feature
		  suggestions, and specific complaints.},
  booktitle	= {Proceedings of the 13th Annual Meeting of the Forum for
		  Information Retrieval Evaluation},
  pages		= {52–57},
  numpages	= {6},
  keywords	= {Information Extraction, Opinion Mining, Suggestion
		  Mining},
  location	= {Virtual Event, India},
  series	= {FIRE '21}
}

@InProceedings{	  10.1145/3583780.3615128,
  author	= {Lee, Eric W. and Ho, Joyce C.},
  title		= {PGB: A PubMed Graph Benchmark for Heterogeneous Network
		  Representation Learning},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615128},
  doi		= {10.1145/3583780.3615128},
  abstract	= {There has been rapid growth in biomedical literature, yet
		  capturing the heterogeneity of the bibliographic
		  information of these articles remains relatively
		  understudied. Graph neural networks have gained popularity,
		  however, they may not fully capture the information
		  available in the PubMed database, a biomedical literature
		  repository containing over 33 million articles. We
		  introduce PubMed Graph Benchmark (PGB), a new benchmark
		  dataset for evaluating heterogeneous graph representations.
		  PGB is one of the largest heterogeneous networks to date
		  and aggregates the rich metadata into a unified source
		  including abstract, authors, citations, keywords, and the
		  associated keyword hierarchy. The benchmark contains an
		  evaluation task of 21 systematic review topics, an
		  essential knowledge translation tool.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5331–5335},
  numpages	= {5},
  keywords	= {heterogeneous information network, network embedding,
		  pubmed benchmark, systematic review},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3583780.3615258,
  author	= {Ghosh, Madhusudan and Ganguly, Debasis and Basuchowdhuri,
		  Partha and Naskar, Sudip Kumar},
  title		= {Extracting Methodology Components from AI Research Papers:
		  A Data-driven Factored Sequence Labeling Approach},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615258},
  doi		= {10.1145/3583780.3615258},
  abstract	= {Extraction of methodology component names from scientific
		  articles is a challenging task due to the diversified
		  contexts around the occurrences of these entities, and the
		  different levels of granularity and containment
		  relationships exhibited by these entities. We hypothesize
		  that standard sequence labeling approaches may not
		  adequately model the dependence of methodology name
		  mentions with their contexts, due to the problems of their
		  large, fast evolving, and domain-specific vocabulary. As a
		  solution, we propose a factored approach, where the
		  mention-context dependencies are represented in a more
		  fine-grained manner, thus allowing the model parameters to
		  better adjust to the different characteristic patterns
		  inherent within the data. In particular, we experiment with
		  two variants of this factored approach - one that uses the
		  per-entity category information derived from an ontology,
		  and the other that makes use of the topology of the
		  sentence embedding space to infer a category for each
		  entity constituting that sentence. We demonstrate that both
		  these factored variants of SciBERT outperform their
		  non-factored counterpart, a state-of-the-art model for
		  scientific concept extraction.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3897–3901},
  numpages	= {5},
  keywords	= {clustering, factored modelling, information extraction,
		  scientific literature},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Proceedings{	  10.1145/3565387,
  title		= {CSAE '22: Proceedings of the 6th International Conference
		  on Computer Science and Application Engineering},
  year		= {2022},
  isbn		= {9781450396004},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, China}
}

@Article{	  10.1109/taslp.2023.3265205,
  author	= {Li, Jijie and Shuang, Kai and Guo, Jinyu and Shi, Zengyi
		  and Wang, Hongman},
  title		= {Enhancing Semantic Relation Classification With Shortest
		  Dependency Path Reasoning},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3265205},
  doi		= {10.1109/TASLP.2023.3265205},
  abstract	= {Relation Classification (RC) is a basic and essential task
		  of Natural Language Processing. Existing RC methods can be
		  classified into two categories: sequence-based methods and
		  dependency-based methods. Sequence-based methods identify
		  the target relation based on the overall semantics of the
		  whole sentence, which will inevitably introduce noisy
		  features. Dependency-based methods extract indicative
		  word-level features from the Shortest Dependency Path (SDP)
		  between given entities and attempt to establish a
		  statistical association between the words and the target
		  relations. This pattern relatively eliminates the influence
		  of noisy features and achieves a robust performance on long
		  sentences. Nevertheless, we observe that majority of
		  relation classification processes involve complex semantic
		  reasoning which is hard to be achieved based on the
		  word-level statistical association. To solve this problem,
		  we categorize all relations into atomic relations and
		  composed-relations. The atomic relations are the basic
		  relations that can be identified based on the word-level
		  features, while the composed-relation requires to be
		  deducted from multiple atomic relations. Correspondingly,
		  we propose the &lt;bold&gt;At&lt;/bold&gt;omic Relation
		  &lt;bold&gt;E&lt;/bold&gt;ncoding and
		  &lt;bold&gt;R&lt;/bold&gt;easoning
		  &lt;bold&gt;M&lt;/bold&gt;odel (ATERM). In the atomic
		  relation encoding stage, ATERM groups the word-level
		  features and encodes multiple atomic relations in parallel.
		  In the atomic relation reasoning stage, ATERM establishes
		  the atomic relation chain where relation-level features are
		  extracted to identify composed-relations. Experiments show
		  that our method achieves state-of-the-art results on the
		  three most popular relation classification datasets –
		  TACRED, TACRED-Revisit, and SemEval 2010 task 8 with
		  significant improvements.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= apr,
  pages		= {1550–1560},
  numpages	= {11}
}

@InProceedings{	  10.1145/3459637.3482377,
  author	= {Weller, Tobias and Acosta, Maribel},
  title		= {Predicting Instance Type Assertions in Knowledge Graphs
		  Using Stochastic Neural Networks},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482377},
  doi		= {10.1145/3459637.3482377},
  abstract	= {Instance type information is particularly relevant to
		  perform reasoning and obtain further information about
		  entities in knowledge graphs (KGs). However, during
		  automated or pay-as-you-go KG construction processes,
		  instance types might be incomplete or missing in some
		  entities. Previous work focused mostly on representing
		  entities and relations as embeddings based on the
		  statements in the KG. While the computed embeddings encode
		  semantic descriptions and preserve the relationship between
		  the entities, the focus of these methods is often not on
		  predicting schema knowledge, but on predicting missing
		  statements between instances for completing the KG. To fill
		  this gap, we propose an approach that first learns a KG
		  representation suitable for predicting instance type
		  assertions. Then, our solution implements a neural network
		  architecture to predict instance types based on the learned
		  representation. Results show that our representations of
		  entities are much more separable with respect to their
		  associations with classes in the KG, compared to existing
		  methods. For this reason, the performance of predicting
		  instance types on a large number of KGs, in particular on
		  cross-domain KGs with a high variety of classes, is
		  significantly better in terms of F1-score than previous
		  work.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2111–2118},
  numpages	= {8},
  keywords	= {entity classification, entity type prediction, knowledge
		  graphs, stochastic networks},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.1109/taslp.2023.3325973,
  author	= {Wang, Siyuan and Wei, Zhongyu and Xu, Jiarong and Li,
		  Taishan and Fan, Zhihao},
  title		= {Unifying Structure Reasoning and Language Pre-Training for
		  Complex Reasoning Tasks},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3325973},
  doi		= {10.1109/TASLP.2023.3325973},
  abstract	= {Recent pre-trained language models (PLMs) equipped with
		  foundation reasoning skills have shown remarkable
		  performance on downstream complex tasks. However, the
		  significant structure reasoning skill has been rarely
		  studied, which involves modeling implicit structure
		  information within the text and performing explicit logical
		  reasoning over them to deduce the conclusion. This paper
		  proposes a unified learning framework that combines
		  explicit structure reasoning and language pre-training to
		  endow PLMs with the structure reasoning skill. It first
		  identifies several elementary structures within contexts to
		  construct structured queries and performs step-by-step
		  reasoning along the queries to identify the answer entity.
		  The fusion of textual semantics and structure reasoning is
		  achieved by using contextual representations learned by
		  PLMs to initialize the representation space of structures,
		  and performing stepwise reasoning on this semantic
		  representation space. Experimental results on four datasets
		  demonstrate that the proposed model achieves significant
		  improvements in complex reasoning tasks involving diverse
		  structures, and shows transferability to downstream tasks
		  with limited training data and effectiveness for complex
		  reasoning of KGs modality.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= oct,
  pages		= {1586–1595},
  numpages	= {10}
}

@Proceedings{	  10.1145/3487553,
  title		= {WWW '22: Companion Proceedings of the Web Conference
		  2022},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, Lyon, France}
}

@Proceedings{	  10.1145/3582515,
  title		= {GoodIT '23: Proceedings of the 2023 ACM Conference on
		  Information Technology for Social Good},
  year		= {2023},
  isbn		= {9798400701160},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3604571,
  title		= {Asian CHI '23: Proceedings of the Asian HCI Symposium
		  2023},
  year		= {2023},
  isbn		= {9798400707612},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Online, Indonesia}
}

@InProceedings{	  10.1145/3539618.3591957,
  author	= {Ghosh, Sreyan and Tyagi, Utkarsh and Kumar, Sonal and
		  Manocha, Dinesh},
  title		= {BioAug: Conditional Generation based Data Augmentation for
		  Low-Resource Biomedical NER},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591957},
  doi		= {10.1145/3539618.3591957},
  abstract	= {Biomedical Named Entity Recognition (BioNER) is the
		  fundamental task of identifying named entities from
		  biomedical text. However, BioNER suffers from severe data
		  scarcity and lacks high-quality labeled data due to the
		  highly specialized and expert knowledge required for
		  annotation. Though data augmentation has shown to be highly
		  effective for low-resource NER in general, existing data
		  augmentation techniques fail to produce factual and diverse
		  augmentations for BioNER. In this paper, we present BioAug,
		  a novel data augmentation framework for low-resource
		  BioNER. BioAug, built on BART, is trained to solve a novel
		  text reconstruction task based on selective masking and
		  knowledge augmentation. Post training, we perform
		  conditional generation and generate diverse augmentations
		  conditioning BioAug on selectively corrupted text similar
		  to the training stage. We demonstrate the effectiveness of
		  BioAug on 5 benchmark BioNER datasets and show that BioAug
		  outperforms all our baselines by a significant margin
		  (1.5%-21.5% absolute improvement) and is able to generate
		  augmentations that are both more factual and diverse. Code:
		  https://github.com/Sreyan88/BioAug.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1853–1858},
  numpages	= {6},
  keywords	= {biomedical, information extraction, named entity
		  recognition},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3580219.3580250,
  author	= {Sun, Zhi and Zhang, Zhiyong and Zhang, Ling and Chen,
		  Jianfeng},
  title		= {A Novel Approach for Associating IP Addresses with
		  Organizations using Deep Learning},
  year		= {2023},
  isbn		= {9781450397513},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580219.3580250},
  doi		= {10.1145/3580219.3580250},
  abstract	= {The cyberspace is a philosophy and computer in the field
		  of an abstract concept, refers to the computer and computer
		  networks in virtual reality. Cyber threats are becoming
		  more sophisticated, and new attack techniques have the high
		  potential to damage computers or networks. Hence, it is
		  necessary to analyze the correlation between cyberspace
		  entities. It is good for discovering potential unknown
		  threats or even APT attacks. Due to cyberspace has many
		  elements, such as IP addresses, domain names,
		  organizations, and personal accounts, it is difficult to
		  analyze the association relationship of entities
		  automatically, especially to quickly discover the
		  organization to which the IP address belongs. To find out
		  the relationship between the IP address and organization,
		  this paper proposes a novel approach by using deep
		  learning. The method is designed based on an improved
		  TF-IDF algorithm and the pre-trained deep learning mode.
		  The experiment results show that the proposed approach has
		  promising results with less complexity, which is beneficial
		  to using two weight factors of entity ranking. Meanwhile, t
		  the results show that the method has strong robustness and
		  parallelism on the dataset.},
  booktitle	= {Proceedings of the 7th International Conference on Control
		  Engineering and Artificial Intelligence},
  pages		= {173–177},
  numpages	= {5},
  keywords	= {cyberspace security, cyber threat intelligence, entity
		  identification, deep learning},
  location	= {Sanya, China},
  series	= {CCEAI '23}
}

@InProceedings{	  10.1145/3511808.3557275,
  author	= {Howard, Phillip and Ma, Arden and Lal, Vasudev and Simoes,
		  Ana Paula and Korat, Daniel and Pereg, Oren and Wasserblat,
		  Moshe and Singer, Gadi},
  title		= {Cross-Domain Aspect Extraction using Transformers
		  Augmented with Knowledge Graphs},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557275},
  doi		= {10.1145/3511808.3557275},
  abstract	= {The extraction of aspect terms is a critical step in
		  fine-grained sentiment analysis of text. Existing
		  approaches for this task have yielded impressive results
		  when the training and testing data are from the same
		  domain. However, these methods show a drastic decrease in
		  performance when applied to cross-domain settings where the
		  domain of the testing data differs from that of the
		  training data. To address this lack of extensibility and
		  robustness, we propose a novel approach for automatically
		  constructing domain-specific knowledge graphs that contain
		  information relevant to the identification of aspect terms.
		  We introduce a methodology for injecting information from
		  these knowledge graphs into Transformer models, including
		  two alternative mechanisms for knowledge insertion: via
		  query enrichment and via manipulation of attention
		  patterns. We demonstrate state-of-the-art performance on
		  benchmark datasets for cross-domain aspect term extraction
		  using our approach and investigate how the amount of
		  external knowledge available to the Transformer impacts
		  model performance.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {780–790},
  numpages	= {11},
  keywords	= {aspect extraction, aspect-based sentiment analysis,
		  knowledge graphs, knowledge injection, transformers},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3447548.3467438,
  author	= {Zhang, Jiawen and Zhu, Jiaqi and Yang, Yi and Shi, Wandong
		  and Zhang, Congcong and Wang, Hongan},
  title		= {Knowledge-Enhanced Domain Adaptation in Few-Shot Relation
		  Classification},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467438},
  doi		= {10.1145/3447548.3467438},
  abstract	= {Relation classification (RC) is an important task in
		  knowledge extraction from texts, while data-driven
		  approaches, although achieving high performance, heavily
		  rely on a large amount of annotated training data.
		  Recently, many few-shot RC models have been proposed and
		  yielded promising results in general domain datasets, but
		  when adapting to a specific domain, such as medicine, the
		  performance drops dramatically. In this paper, we propose a
		  Knowledge-Enhanced Few-shot RC model for the Domain
		  Adaptation task (KEFDA), which incorporates general and
		  domain-specific knowledge graphs (KGs) to the RC model to
		  improve its domain adaptability. With the help of
		  concept-level KGs, the model can better understand the
		  semantics of texts and easily summarize the global
		  semantics of relation types from only a few instances. To
		  be more important, as a kind of meta-information, the
		  manner of utilizing KGs can be transferred from existing
		  tasks to new tasks, even across domains. Specifically, we
		  design a knowledge-enhanced prototypical network to conduct
		  instance matching, and a relation-meta learning network for
		  implicit relation matching. The two scoring functions are
		  combined to infer the relation type of a new instance.
		  Experimental results on the Domain Adaptation Challenge in
		  the FewRel 2.0 benchmark demonstrate that our approach
		  significantly outperforms the state-of-the-art models (by
		  6.63% on average).},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {2183–2191},
  numpages	= {9},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@Proceedings{	  10.1145/3562007,
  title		= {CCRIS '22: Proceedings of the 2022 3rd International
		  Conference on Control, Robotics and Intelligent System},
  year		= {2022},
  isbn		= {9781450396851},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, China}
}

@InProceedings{	  10.1145/3543507.3583449,
  author	= {Yadav, Shweta and Cobeli, undefinedtefan and Caragea,
		  Cornelia},
  title		= {Towards Understanding Consumer Healthcare Questions on the
		  Web with Semantically Enhanced Contrastive Learning},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583449},
  doi		= {10.1145/3543507.3583449},
  abstract	= {In recent years, seeking health information on the web has
		  become a preferred way for healthcare consumers to support
		  their information needs. Generally, healthcare consumers
		  use long and detailed questions with several peripheral
		  details to express their healthcare concerns, contributing
		  to natural language understanding challenges. One way to
		  address this challenge is by summarizing the questions.
		  However, most of the existing abstractive summarization
		  systems generate impeccably fluent yet factually incorrect
		  summaries. In this paper, we present a
		  semantically-enhanced contrastive learning-based framework
		  for generating abstractive question summaries that are
		  faithful and factually correct. We devised multiple
		  strategies based on question semantics to generate the
		  erroneous (negative) summaries, such that the model has the
		  understanding of plausible and incorrect perturbations of
		  the original summary. Our extensive experimental results on
		  two benchmark consumer health question summarization
		  datasets confirm the effectiveness of our proposed method
		  by achieving state-of-the-art performance and generating
		  factually correct and fluent summaries, as measured by
		  human evaluation.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1773–1783},
  numpages	= {11},
  keywords	= {abstractive summarization, consumer healthcare question
		  understanding, contrastive learning},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3594315.3594359,
  author	= {Dai, Chenquan and Zhuang, Xiaobin and Cai, Jiaxin},
  title		= {A Survey on Deep Learning for Chinese Medical Named Entity
		  Recognition},
  year		= {2023},
  isbn		= {9781450399029},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3594315.3594359},
  doi		= {10.1145/3594315.3594359},
  abstract	= {At present, how to make full use of medical and health
		  data for exploration and analysis to better support
		  clinical decision-making faces many challenges. This paper
		  aims to summarize and analyze the methods and research
		  status of Chinese medical named entity recognition, and
		  understand the research progress of named entity
		  recognition technology in Chinese electronic medical record
		  text. This paper conducts literature research from multiple
		  perspectives, such as the basic concepts of electronic
		  medical records and named entity recognition, the
		  acquisition of corpus datasets and the named entity
		  recognition algorithm. The research progress of Chinese
		  electronic medical record named entity recognition in
		  recent years is reviewed, and the development trend of
		  electronic medical record named entity recognition in the
		  future Chinese is analyzed.},
  booktitle	= {Proceedings of the 2023 9th International Conference on
		  Computing and Artificial Intelligence},
  pages		= {472–476},
  numpages	= {5},
  location	= {Tianjin, China},
  series	= {ICCAI '23}
}

@InProceedings{	  10.1145/3503161.3547948,
  author	= {Chen, Zhihong and Li, Guanbin and Wan, Xiang},
  title		= {Align, Reason and Learn: Enhancing Medical
		  Vision-and-Language Pre-training with Knowledge},
  year		= {2022},
  isbn		= {9781450392037},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503161.3547948},
  doi		= {10.1145/3503161.3547948},
  abstract	= {Medical vision-and-language pre-training (Med-VLP) has
		  received considerable attention owing to its applicability
		  to extracting generic vision-and-language representations
		  from medical images and texts. Most existing methods mainly
		  contain three elements: uni-modal encoders (i.e., a vision
		  encoder and a language encoder), a multi-modal fusion
		  module, and pretext tasks, with few studies considering the
		  importance of medical domain expert knowledge and
		  explicitly exploiting such knowledge to facilitate Med-VLP.
		  Although there exist knowledge-enhanced vision-and-language
		  pre-training (VLP) methods in the general domain, most
		  require off-the-shelf toolkits (e.g., object detectors and
		  scene graph parsers), which are unavailable in the medical
		  domain. In this paper, we propose a systematic and
		  effective approach to enhance Med-VLP by structured medical
		  knowledge from three perspectives. First, considering
		  knowledge can be regarded as the intermediate medium
		  between vision and language, we align the representations
		  of the vision encoder and the language encoder through
		  knowledge. Second, we inject knowledge into the multi-modal
		  fusion model to enable the model to perform reasoning using
		  knowledge as the supplementation of the input image and
		  text. Third, we guide the model to put emphasis on the most
		  critical information in images and texts by designing
		  knowledge-induced pretext tasks. To perform a comprehensive
		  evaluation and facilitate further research, we construct a
		  medical vision-and-language benchmark including three
		  tasks. Experimental results illustrate the effectiveness of
		  our approach, where state-of-the-art performance is
		  achieved on all downstream tasks. Further analyses explore
		  the effects of different components of our approach and
		  various settings of pre-training.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Multimedia},
  pages		= {5152–5161},
  numpages	= {10},
  keywords	= {knowledge-enhanced learning, medical analysis, multi-modal
		  pre-training, vision-and-language},
  location	= {Lisboa, Portugal},
  series	= {MM '22}
}

@Proceedings{	  10.1145/3589132,
  title		= {SIGSPATIAL '23: Proceedings of the 31st ACM International
		  Conference on Advances in Geographic Information Systems},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The conference started as a series of workshops and
		  symposia back in 1993 with the aim of promoting
		  interdisciplinary discussions among researchers,
		  developers, users, and practitioners and fostering research
		  in all aspects of geographic information systems,
		  especially in relation to novel systems based on geospatial
		  data and knowledge. It continues to provide a forum for
		  original research contributions covering all conceptual,
		  design and implementation aspects of geospatial data
		  ranging from applications, user interfaces and
		  visualization, to data storage, query processing, indexing,
		  machine learning and data mining. The conference is the
		  premier annual event of the ACM Special Interest Group on
		  Spatial Information (ACM SIGSPATIAL).},
  location	= {Hamburg, Germany}
}

@InProceedings{	  10.1145/3501409.3501580,
  author	= {Hu, Hongwei and Yin, Meijuan and Liu, Xiaonan},
  title		= {A Relation-Oriented Method for Joint Entity and Relation
		  Extraction Based on Neural Network},
  year		= {2022},
  isbn		= {9781450384322},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3501409.3501580},
  doi		= {10.1145/3501409.3501580},
  abstract	= {Entity and relation extraction is a basic task of
		  information extraction in natural language processing. At
		  present, Entity and relation extraction based on artificial
		  intelligence has been widely studied, but most methods
		  adopt the idea of identifying the entity first and then
		  extracting the relation, which suffers from the problems of
		  overlapping triple and entity redundancy. We propose a
		  Relation-Oriented method for Joint Entity and Relation
		  Extraction (ROJER), which first extracts the relation types
		  contained in the text through the relation extraction
		  module, then the pre-extracted relation types are
		  integrated into the entity recognition module, reduce the
		  focus on irrelevant entities and avoid extracting redundant
		  entities. Then the entity pairs corresponding to the
		  extracted relation types are identified to tackle the
		  overlapping triple problem and finally extract all the
		  relation triples. Experiments on the DuIE dataset show that
		  the F1 score of ROJER reaches 78.4%, which is 1.2% higher
		  than the CasRel model, confirming the validity of our
		  method.},
  booktitle	= {Proceedings of the 2021 5th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {951–955},
  numpages	= {5},
  keywords	= {Artificial intelligence, Entity redundancy, Information
		  extraction, Overlapping triple, Relation extraction},
  location	= {Xiamen, China},
  series	= {EITCE '21}
}

@Proceedings{	  10.1145/3584931,
  title		= {CSCW '23 Companion: Companion Publication of the 2023
		  Conference on Computer Supported Cooperative Work and
		  Social Computing},
  year		= {2023},
  isbn		= {9798400701290},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Minneapolis, MN, USA}
}

@Article{	  10.14778/3611540.3611636,
  author	= {Dong, Xin Luna},
  title		= {Generations of Knowledge Graphs: The Crazy Ideas and the
		  Business Impact},
  year		= {2023},
  issue_date	= {August 2023},
  publisher	= {VLDB Endowment},
  volume	= {16},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3611540.3611636},
  doi		= {10.14778/3611540.3611636},
  abstract	= {Knowledge Graphs (KGs) have been used to support a wide
		  range of applications, from web search to personal
		  assistant. In this paper, we describe three generations of
		  knowledge graphs: entity-based KGs, which have been
		  supporting general search and question answering (e.g., at
		  Google and Bing); text-rich KGs, which have been supporting
		  search and recommendations for products, bio-informatics,
		  etc. (e.g., at Amazon and Alibaba); and the emerging
		  integration of KGs and LLMs, which we call dual neural KGs.
		  We describe the characteristics of each generation of KGs,
		  the crazy ideas behind the scenes in constructing such KGs,
		  and the techniques developed over time to enable industry
		  impact. In addition, we use KGs as examples to demonstrate
		  a recipe to evolve research ideas from innovations to
		  production practice, and then to the next level of
		  innovations, to advance both science and business.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {4130–4137},
  numpages	= {8}
}

@Proceedings{	  10.1145/3590003,
  title		= {CACML '23: Proceedings of the 2023 2nd Asia Conference on
		  Algorithms, Computing and Machine Learning},
  year		= {2023},
  isbn		= {9781450399449},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shanghai, China}
}

@InProceedings{	  10.1145/3477495.3531957,
  author	= {Xu, Chen and Li, Piji and Wang, Wei and Yang, Haoran and
		  Wang, Siyun and Xiao, Chuangbai},
  title		= {COSPLAY: Concept Set Guided Personalized Dialogue
		  Generation Across Both Party Personas},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531957},
  doi		= {10.1145/3477495.3531957},
  abstract	= {Maintaining a consistent persona is essential for building
		  a human-like conversational model. However, the lack of
		  attention to the partner makes the model more egocentric:
		  they tend to show their persona by all means such as
		  twisting the topic stiffly, pulling the conversation to
		  their own interests regardless, and rambling their persona
		  with little curiosity to the partner. In this work, we
		  propose COSPLAY(COncept Set guided PersonaLized dialogue
		  generation Across both partY personas) that considers both
		  parties as a "team": expressing self-persona while keeping
		  curiosity toward the partner, leading responses around
		  mutual personas, and finding the common ground.
		  Specifically, we first represent self-persona, partner
		  persona and mutual dialogue all in the concept sets. Then,
		  we propose the Concept Set framework with a suite of
		  knowledge-enhanced operations to process them such as set
		  algebras, set expansion, and set distance. Based on these
		  operations as medium, we train the model by utilizing 1)
		  concepts of both party personas, 2) concept relationship
		  between them, and 3) their relationship to the future
		  dialogue. Extensive experiments on a large public dataset,
		  Persona-Chat, demonstrate that our model outperforms
		  state-of-the-art baselines for generating less egocentric,
		  more human-like, and higher quality responses in both
		  automatic and human evaluations.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {201–211},
  numpages	= {11},
  keywords	= {common ground modeling, knowledge concept set, mutual
		  benefit, personalized dialogue generation, reinforcement
		  learning.},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3544548.3581026,
  author	= {Deng, Wesley Hanwen and Guo, Boyuan and Devrio, Alicia and
		  Shen, Hong and Eslami, Motahhare and Holstein, Kenneth},
  title		= {Understanding Practices, Challenges, and Opportunities for
		  User-Engaged Algorithm Auditing in Industry Practice},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3581026},
  doi		= {10.1145/3544548.3581026},
  abstract	= {Recent years have seen growing interest among both
		  researchers and practitioners in user-engaged approaches to
		  algorithm auditing, which directly engage users in
		  detecting problematic behaviors in algorithmic systems.
		  However, we know little about industry practitioners’
		  current practices and challenges around user-engaged
		  auditing, nor what opportunities exist for them to better
		  leverage such approaches in practice. To investigate, we
		  conducted a series of interviews and iterative co-design
		  activities with practitioners who employ user-engaged
		  auditing approaches in their work. Our findings reveal
		  several challenges practitioners face in appropriately
		  recruiting and incentivizing user auditors, scaffolding
		  user audits, and deriving actionable insights from
		  user-engaged audit reports. Furthermore, practitioners
		  shared organizational obstacles to user-engaged auditing,
		  surfacing a complex relationship between practitioners and
		  user auditors. Based on these findings, we discuss
		  opportunities for future HCI research to help realize the
		  potential (and mitigate risks) of user-engaged auditing in
		  industry practice.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {377},
  numpages	= {18},
  keywords	= {bias, fairness, industry practitioners, responsible AI,
		  user-engaged algorithm auditing},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@InProceedings{	  10.1145/3573428.3573599,
  author	= {Zhang, Luyi and Li, Ren and Xiao, Qiao},
  title		= {A Prompt-based Few-shot Machine Reading Comprehension
		  Model for Intelligent Bridge Management},
  year		= {2023},
  isbn		= {9781450397148},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3573428.3573599},
  doi		= {10.1145/3573428.3573599},
  abstract	= {Bridge inspection reports are an important data source in
		  the bridge management process, and they contain a large
		  amount of fine-grained information. However, the research
		  on machine reading comprehension (MRC) methods for this
		  field is insufficient, and annotating large scale
		  domain-specific corpus is time-consuming. This paper
		  presented a novel prompt-based few-shot MRC approach for
		  intelligent bridge management. The proposed model uses the
		  pretrained model MacBERT as backbone. The prompt templates
		  are designed based on some domain-specific heuristic rules.
		  The experimental results show that our model outperforms
		  the baseline models in different few-shot settings. The
		  proposed model can provide technical support for the
		  construction of automatic question answering system in the
		  field of bridge management.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {946–950},
  numpages	= {5},
  keywords	= {Bridge inspection, Few-shot, Machine reading
		  comprehension, Prompt},
  location	= {Xiamen, China},
  series	= {EITCE '22}
}

@Article{	  10.1145/3597299,
  author	= {Roy, Prasenjeet and Kundu, Suman},
  title		= {Review on Query-focused Multi-document Summarization
		  (QMDS) with Comparative Analysis},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3597299},
  doi		= {10.1145/3597299},
  abstract	= {The problem of query-focused multi-document summarization
		  (QMDS) is to generate a summary from multiple source
		  documents on identical/similar topics based on the query
		  submitted by the users. This article provides a systematic
		  review of the literature of QMDS. The research works are
		  classified into six major categories based on the
		  summarization methodologies used. Different techniques used
		  for finding query-relevant summaries for different
		  algorithms under each of the six major groups are reported.
		  Further, 17 evaluation metrics used for evaluating
		  algorithms for text summaries against the human-curated
		  summaries are compiled here in this article. Extensive
		  experiments are performed on eight different datasets.
		  Comparative results of nine methodologies, each
		  representing one of the six different groups, are
		  presented. Seven different evaluation metrics are used in
		  the comparative study. It is observed that DL- and ML-based
		  QMDS methods perform. better in comparison to the other
		  methods.},
  journal	= {ACM Comput. Surv.},
  month		= aug,
  articleno	= {5},
  numpages	= {38},
  keywords	= {Query focused multi-document summarization, query
		  relevance}
}

@InProceedings{	  10.1145/3539618.3591849,
  author	= {Van Gysel, Christophe},
  title		= {Modeling Spoken Information Queries for Virtual
		  Assistants: Open Problems, Challenges and Opportunities},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591849},
  doi		= {10.1145/3539618.3591849},
  abstract	= {Virtual assistants are becoming increasingly important
		  speech-driven Information Retrieval platforms that assist
		  users with various tasks. We discuss open problems and
		  challenges with respect to modeling spoken information
		  queries for virtual assistants, and list opportunities
		  where Information Retrieval methods and research can be
		  applied to improve the quality of virtual assistant speech
		  recognition. We discuss how query domain classification,
		  knowledge graphs and user interaction data, and query
		  personalization can be helpful to improve the accurate
		  recognition of spoken information domain queries. Finally,
		  we also provide a brief overview of current problems and
		  challenges in speech recognition.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3335–3338},
  numpages	= {4},
  keywords	= {automated speech recognition, query log analysis, virtual
		  assistants},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3544549.3585699,
  author	= {Xu, Erqian and Wang, Hecong and Bai, Zhen},
  title		= {Engage AI and Child in Explanatory Dialogue on Commonsense
		  Reasoning},
  year		= {2023},
  isbn		= {9781450394222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544549.3585699},
  doi		= {10.1145/3544549.3585699},
  abstract	= {Human-level commonsense reasoning capability is vital for
		  human-AI interaction, enabling AI to understand,
		  anticipate, and respond to human’s thoughts, feelings,
		  and behaviors. Despite the recent advancements in AI
		  commonsense reasoning due to generative language models, a
		  young child is often more rational than state-of-the-art
		  AIs in terms of commonsense reasoning. The field of
		  cognitive science, child development, and explainable AI
		  have long recognized the importance of explanations for
		  sharing knowledge and resolving contradictions. We,
		  therefore, raise the question: can AIs leverage the power
		  of explanations to learn human-level commonsense reasoning?
		  More specifically, can explanatory dialogue with children
		  help AIs to develop commonsense reasoning capabilities? As
		  a first step in this line of research, we aim to engage
		  children in explanatory dialogue with AIs during story
		  reading. We present our novel explanatory dialogue
		  interface based on a state-of-the-art multi-step
		  commonsense reasoning engine and discuss our upcoming pilot
		  study.},
  booktitle	= {Extended Abstracts of the 2023 CHI Conference on Human
		  Factors in Computing Systems},
  articleno	= {99},
  numpages	= {8},
  keywords	= {Commonsense Reasoning, Explainable AI, Human-AI
		  Collaboration},
  location	= {Hamburg, Germany},
  series	= {CHI EA '23}
}

@InProceedings{	  10.1145/3582768.3582772,
  author	= {Losing, Viktor and Eggert, Julian},
  title		= {Extraction of Common Physical Properties of Everyday
		  Objects from Structured Sources},
  year		= {2023},
  isbn		= {9781450397629},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3582768.3582772},
  doi		= {10.1145/3582768.3582772},
  abstract	= {Commonsense knowledge is essential for the reasoning of AI
		  systems, particularly in the context of action planning for
		  robots. The focus of this paper is on common-sense object
		  properties, which are especially useful to restrict the
		  search space of planning algorithms. Popular sources for
		  such knowledge are commonsense knowledge bases that provide
		  the information in a structured form. However, the utility
		  of the provided object-property pairs is limited as they
		  can be simply incorrect, subjective, unspecific, or relate
		  only to a narrow context. In this paper, we suggest a
		  methodology to create a highly accurate dataset of object
		  properties that are related to common physical attributes.
		  The approach is based on filtering non-physical properties
		  within commonsense knowledge bases and improving the
		  accuracy of the remaining object-property pairs based on
		  supervised machine learning using annotated data. Thereby,
		  we evaluate different types of features and models and
		  significantly increase the correctness of object-property
		  pairs compared to the original sources.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {164–168},
  numpages	= {5},
  keywords	= {knowledge bases, neural networks, transformer model},
  location	= {Bangkok, Thailand},
  series	= {NLPIR '22}
}

@InProceedings{	  10.1145/3539597.3570478,
  author	= {Su, Xing and Yang, Jian and Wu, Jia and Zhang, Yuchen},
  title		= {Mining User-aware Multi-relations for Fake News Detection
		  in Large Scale Online Social Networks},
  year		= {2023},
  isbn		= {9781450394079},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539597.3570478},
  doi		= {10.1145/3539597.3570478},
  abstract	= {Users' involvement in creating and propagating news is a
		  vital aspect of fake news detection in online social
		  networks. Intuitively, credible users are more likely to
		  share trustworthy news, while untrusted users have a higher
		  probability of spreading untrustworthy news. In this paper,
		  we construct a dual-layer graph (i.e., news layer and user
		  layer) to extract multi-relations of news and users in
		  social networks to derive rich information for detecting
		  fake news. Based on the dual-layer graph, we propose a fake
		  news detection model Us-DeFake. It learns the propagation
		  features of news in the news layer and the interaction
		  features of users in the user layer. Through the
		  inter-layer in the graph, Us-DeFake fuses the user signals
		  that contain credibility information into the news
		  features, to provide distinctive user-aware embeddings of
		  news for fake news detection. The training process conducts
		  on multiple dual-layer subgraphs obtained by a graph
		  sampler to scale Us-DeFake in large scale social networks.
		  Extensive experiments on real-world datasets illustrate the
		  superiority of Us-DeFake which outperforms all baselines,
		  and the users' credibility signals learned by interaction
		  relation can notably improve the performance of our
		  model.},
  booktitle	= {Proceedings of the Sixteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {51–59},
  numpages	= {9},
  keywords	= {fake news detection, large scale social networks,
		  multi-relations},
  location	= {Singapore, Singapore},
  series	= {WSDM '23}
}

@InProceedings{	  10.1145/3550356.3561570,
  author	= {Ibrahimi, Ilirian and Moudilos, Dimitris},
  title		= {Towards model reuse in low-code development platforms
		  based on knowledge graphs},
  year		= {2022},
  isbn		= {9781450394673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3550356.3561570},
  doi		= {10.1145/3550356.3561570},
  abstract	= {Low-code Development Platforms (LCDP) are applications to
		  provide fast full-stack application development. These
		  platforms rely on models as their core artifacts in order
		  to reduce the complexity of software development. Despite
		  the growing need and importance of using models in low-code
		  platforms, there is still limited support for model
		  discovery and reuse, which leads to unnecessary repetitive
		  work. Therefore, facilities for automated discovery and
		  recommendation of relevant models and model fragments are
		  desired. A prerequisite for producing relevant
		  recommendations for adding new features to a model is a
		  model repository equipped with an efficient query mechanism
		  in combination with algorithms for processing the retrieved
		  data. In this paper, we present an approach that enables
		  model recommendations by initially converting and merging
		  heterogeneous models into a homogeneous graph, which serves
		  as the repository of our approach. Afterwards, the approach
		  queries the repository for relevant matches and uses
		  N-grams to produce recommendations for models under
		  development. The current approach is demonstrated on the
		  zAppDev LCDP, but conceptually it can be integrated on any
		  LCDP and can be applied for the reuse of any graph-based
		  model. We evaluated our approach by reconstructing four
		  different models that do not exist in our repository with
		  the support of our approach.},
  booktitle	= {Proceedings of the 25th International Conference on Model
		  Driven Engineering Languages and Systems: Companion
		  Proceedings},
  pages		= {826–836},
  numpages	= {11},
  keywords	= {MDE, N-grams, RDF, graph-repository, low-code platform,
		  model reuse, recommendation system},
  location	= {Montreal, Quebec, Canada},
  series	= {MODELS '22}
}

@Proceedings{	  10.1145/3604237,
  title		= {ICAIF '23: Proceedings of the Fourth ACM International
		  Conference on AI in Finance},
  year		= {2023},
  isbn		= {9798400702402},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Brooklyn, NY, USA}
}

@InProceedings{	  10.1145/3477495.3532003,
  author	= {Alghanmi, Israa and Espinosa-Anke, Luis and Schockaert,
		  Steven},
  title		= {Interpreting Patient Descriptions using Distantly
		  Supervised Similar Case Retrieval},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3532003},
  doi		= {10.1145/3477495.3532003},
  abstract	= {Biomedical natural language processing often involves the
		  interpretation of patient descriptions, for instance for
		  diagnosis or for recommending treatments. Current methods,
		  based on biomedical language models, have been found to
		  struggle with such tasks. Moreover, retrieval augmented
		  strategies have only had limited success, as it is rare to
		  find sentences which express the exact type of knowledge
		  that is needed for interpreting a given patient
		  description. For this reason, rather than attempting to
		  retrieve explicit medical knowledge, we instead propose to
		  rely on a nearest neighbour strategy. First, we retrieve
		  text passages that are similar to the given patient
		  description, and are thus likely to describe patients in
		  similar situations, while also mentioning some hypothesis
		  (e.g. a possible diagnosis of the patient). We then judge
		  the likelihood of the hypothesis based on the similarity of
		  the retrieved passages. Identifying similar cases is
		  challenging, however, as descriptions of similar patients
		  may superficially look rather different, among others
		  because they often contain an abundance of irrelevant
		  details. To address this challenge, we propose a strategy
		  that relies on a distantly supervised cross-encoder.
		  Despite its conceptual simplicity, we find this strategy to
		  be effective in practice.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {460–470},
  numpages	= {11},
  keywords	= {biomedical nlp, distant supervision, similar case
		  retrieval},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3477495.3531944,
  author	= {Chatterjee, Shubham and Dietz, Laura},
  title		= {BERT-ER: Query-specific BERT Entity Representations for
		  Entity Ranking},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531944},
  doi		= {10.1145/3477495.3531944},
  abstract	= {Entity-oriented search systems often learn vector
		  representations of entities via the introductory paragraph
		  from the Wikipedia page of the entity. As such
		  representations are the same for every query, our
		  hypothesis is that the representations are not ideal for IR
		  tasks. In this work, we present BERT Entity Representations
		  (BERT-ER) which are query-specific vector representations
		  of entities obtained from text that describes how an entity
		  is relevant for a query. Using BERT-ER in a downstream
		  entity ranking system, we achieve a performance improvement
		  of 13-42% (Mean Average Precision) over a system that uses
		  the BERT embedding of the introductory paragraph from
		  Wikipedia on two large-scale test collections. Our approach
		  also outperforms entity ranking systems using entity
		  embeddings from Wikipedia2Vec, ERNIE, and E-BERT. We show
		  that our entity ranking system using BERT-ER can increase
		  precision at the top of the ranking by promoting relevant
		  entities to the top. With this work, we release our BERT
		  models and query-specific entity embeddings fine-tuned for
		  the entity ranking task.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1466–1477},
  numpages	= {12},
  keywords	= {bert, entity ranking, query-specific entity
		  representations},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@Article{	  10.1145/3524618,
  author	= {Song, Yaguang and Yang, Xiaoshan and Xu, Changsheng},
  title		= {Self-supervised Calorie-aware Heterogeneous Graph Networks
		  for Food Recommendation},
  year		= {2023},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {1s},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3524618},
  doi		= {10.1145/3524618},
  abstract	= {With the rapid development of online recipe sharing
		  platforms, food recommendation is emerging as an important
		  application. Although recent studies have made great
		  progress on food recommendation, they have two shortcomings
		  that are likely to affect the recommendation performance.
		  (1) The relations between ingredients are not considered,
		  which may lead to sub-optimal representations of recipes
		  and further result in the neglect of the user’s
		  personalized ingredient combination preference. (2)
		  Existing methods do not consider the impact of users’
		  preferences on calories in users’ food decision-making
		  process. In this article, we propose a Self-supervised
		  Calorie-aware Heterogeneous Graph Network (SCHGN) to model
		  the relations between ingredients and incorporate calories
		  of food simultaneously. Specifically, we first incorporate
		  users, recipes, ingredients, and calories into a
		  heterogeneous graph and explicitly present the complex
		  relations among them with directed edges. Then, we explore
		  the co-occurrence relation of ingredients in different
		  recipes via self-supervised ingredient prediction. To
		  capture users’ dynamic preferences on calories of food,
		  we learn calorie-aware user representations by hierarchical
		  message passing and compute a comprehensive user-guided
		  recipe representation by attention mechanism. The final
		  food recommendation is accomplished based on the similarity
		  between a user’s calorie-aware representation and the
		  user-guided representation of a recipe. Extensive
		  experiment results on benchmark datasets demonstrate the
		  effectiveness of the proposed method.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= feb,
  articleno	= {27},
  numpages	= {23},
  keywords	= {Food recommendation, recipe calories, heterogeneous graph,
		  self-supervised learning}
}

@Article{	  10.1145/3588940,
  author	= {Fan, Wenfei and Fu, Wenzhi and Jin, Ruochun and Liu,
		  Muyang and Lu, Ping and Tian, Chao},
  title		= {Making It Tractable to Catch Duplicates and Conflicts in
		  Graphs},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {1},
  url		= {https://doi.org/10.1145/3588940},
  doi		= {10.1145/3588940},
  abstract	= {This paper proposes an approach for entity resolution (ER)
		  and conflict resolution (CR) in large-scale graphs. It is
		  based on a class of Graph Cleaning Rules (GCRs), which
		  support the primitives of relational data cleaning rules,
		  and may embed machine learning classifiers as predicates.
		  As opposed to previous graph rules, GCRs are defined with a
		  dual graph pattern to accommodate irregular structures of
		  schemaless graphs, and adopt patterns of a star form to
		  reduce the complexity. We show that the satisfiability,
		  implication and validation problems are all in polynomial
		  time (PTIME) for GCRs, as opposed to the intractability of
		  these classical problems for previous graph dependencies.
		  We develop a parallel algorithm to discover GCRs by
		  combining the generations of patterns and predicates, and a
		  parallel PTIME algorithm for "deep" ER and CR by
		  recursively applying the mined GCRs. We show that these
		  algorithms guarantee to reduce runtime when more processors
		  are used. Using real-life and synthetic graphs, we
		  experimentally verify that rule discovery and error
		  detection with GCRs are substantially faster than with
		  previous graph dependencies, with improved accuracy.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {86},
  numpages	= {28},
  keywords	= {conflict resolution, entity resolution, graph cleaning
		  rules}
}

@InProceedings{	  10.1145/3614008.3614053,
  author	= {Gong, Jibing and Fang, Xiaohan and Wang, Chenglong and Ju,
		  Jingxin and Bao, Yanghao and Zhang, Jin and Xu, Jianjun},
  title		= {Author Name Disambiguation based on Capsule Network via
		  Semantic and Structural Features},
  year		= {2023},
  isbn		= {9798400707575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3614008.3614053},
  doi		= {10.1145/3614008.3614053},
  abstract	= {Author Name Disambiguation (AND) is a crucial task in the
		  knowledge engineering of the bibliography. In academic
		  search systems, author name ambiguity is a common
		  phenomenon caused by different authors with the same name
		  and leads to that author name can not be used to reliably
		  identify all scholar authors. In recent researches, one
		  papers’ attributes are often used to learn its
		  representation as feature. However, most existing methods
		  ignore to extract deep features and potential relationship
		  among papers. To address the problem, we propose a novel
		  model named Author Name Disambiguation based on Capsule
		  Network via Semantic and Structural Features (ADSSF). ADSSF
		  uses both supervised and unsupervised methods to learn the
		  representation of papers. First, we present a new
		  Capsule-Networks-based feature extraction model which can
		  mine deep features and potential relationship. And then, in
		  the representation learning of papers, ADFFS fuses the
		  semantic and structural features of papers by multi-task
		  learning. Finally, a clustering method is leveraged to
		  correctly cluster authors. Experimental results on the
		  AMiner datasets demonstrate that the ADSSF outperforms the
		  state-of-the-art baselines.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Signal Processing and Machine Learning},
  pages		= {293–300},
  numpages	= {8},
  keywords	= {Author Name Disambiguation, Capsule Network, Knowledge
		  Engineering, Semantic and Structural Feature Fusion},
  location	= {Tianjin, China},
  series	= {SPML '23}
}

@InProceedings{	  10.1145/3459637.3482273,
  author	= {Ye, Muchao and Cui, Suhan and Wang, Yaqing and Luo, Junyu
		  and Xiao, Cao and Ma, Fenglong},
  title		= {MedRetriever: Target-Driven Interpretable Health Risk
		  Prediction via Retrieving Unstructured Medical Text},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482273},
  doi		= {10.1145/3459637.3482273},
  abstract	= {The broad adoption of electronic health record (EHR)
		  systems and the advances of deep learning technology have
		  motivated the development of health risk prediction models,
		  which mainly depend on the expressiveness and temporal
		  modeling capacity of deep neural networks (DNNs) to improve
		  prediction performance. Some further augment the prediction
		  by using external knowledge, however, a great deal of EHR
		  information inevitably loses during the knowledge mapping.
		  In addition, prediction made by existing models usually
		  lacks reliable interpretation, which undermines their
		  reliability in guiding clinical decision-making. To solve
		  these challenges, we propose MedRetriever, an effective and
		  flexible framework that leverages unstructured medical text
		  collected from authoritative websites to augment health
		  risk prediction as well as to provide understandable
		  interpretation. Besides, MedRetriever explicitly takes the
		  target disease documents into consideration, which provide
		  key guidance for the model to learn in a target-driven
		  direction, i.e., from the target disease to the input EHR.
		  To specify, MedRetriever can flexibly choose its backbone
		  from major predictive models to learn the EHR embedding for
		  each visit. After that, the EHR embedding and features of
		  target disease documents are aggregated into a query by
		  self-attention to retrieve highly relevant text segments
		  from the medical text pool, which is stored in the
		  dynamically updated text memory. Finally, the comprehensive
		  EHR embedding and the text memory are used for prediction
		  and interpretation. We evaluate MedRetriever against nine
		  state-of-the-art approaches across three real-world EHR
		  datasets, which consistently achieves the best performance
		  in AUC and recall metrics and outperforms the best baseline
		  by at least 4.8% in recall on three test datasets.
		  Furthermore, we conduct case studies to show the
		  easy-to-understand interpretation by MedRetriever.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2414–2423},
  numpages	= {10},
  keywords	= {data mining, electronic health records, external
		  knowledge, health risk prediction},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3459637.3481991,
  author	= {Jie, Fei and Huang, Yanxiang and Bai, Qiangwei and Wu,
		  Xindong},
  title		= {HAO Unity: A Graph-based System for Unifying Heterogeneous
		  Data},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3481991},
  doi		= {10.1145/3459637.3481991},
  abstract	= {Many real-world applications have to face the problem of
		  diversity in data formats and semantics. Currently, how to
		  deal with heterogeneous data effectively is still a big
		  challenge. With the rise of knowledge graphs, more and more
		  applications are built upon graph-like data models, which
		  benefit from flexible schemas and convenient support for
		  relationship queries. We propose a graph-based unifying
		  system for heterogeneous data unification, which helps to
		  (1) transform data in many other formats into graphs, or
		  conversely, from graph to other formats, (2) integrate
		  graph data based on HAO intelligence, which achieves schema
		  integration and entity consolidation, and (3) explore data
		  at different levels via querying the integrated graphs. In
		  this paper, we introduce the overall system architecture,
		  explain in detail the implementation, and display the usage
		  in two practical scenarios.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4725–4729},
  numpages	= {5},
  keywords	= {data exploration, data integration, data unification,
		  graph database},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3510003.3510129,
  author	= {Liu, Yalin and Lin, Jinfeng and Anuyah, Oghenemaro and
		  Metoyer, Ronald and Cleland-Huang, Jane},
  title		= {Generating and visualizing trace link explanations},
  year		= {2022},
  isbn		= {9781450392211},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3510003.3510129},
  doi		= {10.1145/3510003.3510129},
  abstract	= {Recent breakthroughs in deep-learning (DL) approaches have
		  resulted in the dynamic generation of trace links that are
		  far more accurate than was previously possible. However,
		  DL-generated links lack clear explanations, and therefore
		  non-experts in the domain can find it difficult to
		  understand the underlying semantics of the link, making it
		  hard for them to evaluate the link's correctness or
		  suitability for a specific software engineering task. In
		  this paper we present a novel NLP pipeline for generating
		  and visualizing trace link explanations. Our approach
		  identifies domain-specific concepts, retrieves a corpus of
		  concept-related sentences, mines concept definitions and
		  usage examples, and identifies relations between
		  cross-artifact concepts in order to explain the links. It
		  applies a post-processing step to prioritize the most
		  likely acronyms and definitions and to eliminate
		  non-relevant ones. We evaluate our approach using project
		  artifacts from three different domains of interstellar
		  telescopes, positive train control, and electronic
		  healthcare systems, and then report coverage, correctness,
		  and potential utility of the generated definitions. We
		  design and utilize an explanation interface which leverages
		  concept definitions and relations to visualize and explain
		  trace link rationales, and we report results from a user
		  study that was conducted to evaluate the effectiveness of
		  the explanation interface. Results show that the
		  explanations presented in the interface helped non-experts
		  to understand the underlying semantics of a trace link and
		  improved their ability to vet the correctness of the
		  link.},
  booktitle	= {Proceedings of the 44th International Conference on
		  Software Engineering},
  pages		= {1033–1044},
  numpages	= {12},
  keywords	= {concept mining, explanation interface, software
		  traceability},
  location	= {Pittsburgh, Pennsylvania},
  series	= {ICSE '22}
}

@Article{	  10.1145/3464377,
  author	= {Ma, Longxuan and Li, Mingda and Zhang, Wei-Nan and Li,
		  Jiapeng and Liu, Ting},
  title		= {Unstructured Text Enhanced Open-Domain Dialogue System: A
		  Systematic Survey},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3464377},
  doi		= {10.1145/3464377},
  abstract	= {Incorporating external knowledge into dialogue generation
		  has been proven to benefit the performance of an
		  open-domain Dialogue System (DS), such as generating
		  informative or stylized responses, controlling conversation
		  topics. In this article, we study the open-domain DS that
		  uses unstructured text as external knowledge sources
		  (Unstructured Text Enhanced Dialogue System (UTEDS)). The
		  existence of unstructured text entails distinctions between
		  UTEDS and traditional data-driven DS and we aim at
		  analyzing these differences. We first give the definition
		  of the UTEDS related concepts, then summarize the recently
		  released datasets and models. We categorize UTEDS into
		  Retrieval and Generative models and introduce them from the
		  perspective of model components. The retrieval models
		  consist of Fusion, Matching, and Ranking modules, while the
		  generative models comprise Dialogue and Knowledge Encoding,
		  Knowledge Selection (KS), and Response Generation modules.
		  We further summarize the evaluation methods utilized in
		  UTEDS and analyze the current models’ performance. At
		  last, we discuss the future development trends of UTEDS,
		  hoping to inspire new research in this field.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= sep,
  articleno	= {9},
  numpages	= {44},
  keywords	= {Unstructured text, knowledge grounded, knowledge
		  selection, open-domain dialogue}
}

@InProceedings{	  10.1145/3544548.3580948,
  author	= {Wang, Sitong and Petridis, Savvas and Kwon, Taeahn and Ma,
		  Xiaojuan and Chilton, Lydia B},
  title		= {PopBlends: Strategies for Conceptual Blending with Large
		  Language Models},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3580948},
  doi		= {10.1145/3544548.3580948},
  abstract	= {Pop culture is an important aspect of communication. On
		  social media people often post pop culture reference images
		  that connect an event, product or other entity to a pop
		  culture domain. Creating these images is a creative
		  challenge that requires finding a conceptual connection
		  between the users’ topic and a pop culture domain. In
		  cognitive theory, this task is called conceptual blending.
		  We present a system called PopBlends that automatically
		  suggests conceptual blends. The system explores three
		  approaches that involve both traditional knowledge
		  extraction methods and large language models. Our
		  annotation study shows that all three methods provide
		  connections with similar accuracy, but with very different
		  characteristics. Our user study shows that people found
		  twice as many blend suggestions as they did without the
		  system, and with half the mental demand. We discuss the
		  advantages of combining large language models with
		  knowledge bases for supporting divergent and convergent
		  thinking.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {435},
  numpages	= {19},
  keywords	= {applications of large language models, creativity support
		  tools, natural language processing},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@Proceedings{	  10.1145/3622896,
  title		= {CCRIS '23: Proceedings of the 2023 4th International
		  Conference on Control, Robotics and Intelligent System},
  year		= {2023},
  isbn		= {9798400708190},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guangzhou, China}
}

@Article{	  10.1145/3476464,
  author	= {Manogaran, Gunasekaran and Qudrat-Ullah, Hassan and Xin,
		  Qin},
  title		= {Introduction to the Special Issue on Deep Structured
		  Learning for Natural Language Processing, Part 3},
  year		= {2021},
  issue_date	= {September 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3476464},
  doi		= {10.1145/3476464},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= sep,
  articleno	= {72e},
  numpages	= {3}
}

@Article{	  10.1145/3611641,
  author	= {Demartini, Gianluca and Roitero, Kevin and Mizzaro,
		  Stefano},
  title		= {Data Bias Management},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {67},
  number	= {1},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3611641},
  doi		= {10.1145/3611641},
  abstract	= {Envisioning a unique approach toward bias and fairness
		  research.},
  journal	= {Commun. ACM},
  month		= dec,
  pages		= {28–32},
  numpages	= {5}
}

@InProceedings{	  10.1145/3511808.3557607,
  author	= {Jing, Zhiwen and Zhao, Ziliang and Feng, Yang and Ma,
		  Xaochen and Wu, Nan and Kang, Shengqiao and Yang, Cheng and
		  Zhang, Yujia and Guo, Hao},
  title		= {GReS: Graphical Cross-domain Recommendation for Supply
		  Chain Platform},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557607},
  doi		= {10.1145/3511808.3557607},
  abstract	= {Supply Chain Platforms (SCPs) provide downstream
		  industries with raw materials. Compared with traditional
		  e-commerce platforms, data in SCPs is more sparse due to
		  limited user interests. To tackle the data sparsity
		  problem, one can apply Cross-Domain Recommendation (CDR) to
		  improve the recommendation performance of the target domain
		  with the source domain information. However, applying CDR
		  to SCPs directly ignores hierarchical structures of
		  commodities in SCPs, which reduce recommendation
		  performance. In this paper, we take the catering platform
		  as an example and propose GReS, a graphical CDR model. The
		  model first constructs a tree-shaped graph to represent the
		  hierarchy of different nodes of dishes and ingredients, and
		  then applies our proposed Tree2vec method combining GCN and
		  BERT models to embed the graph for recommendations.
		  Experimental results show that GReS significantly
		  outperforms state-of-the-art methods in CDR for SCPs.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4094–4098},
  numpages	= {5},
  keywords	= {cross-domain recommendation, supply chain platform},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@Article{	  10.1109/taslp.2022.3221045,
  author	= {Xie, Jiayuan and Fang, Wenhao and Huang, Qingbao and Cai,
		  Yi and Wang, Tao},
  title		= {Enhancing Paraphrase Question Generation With Prior
		  Knowledge},
  year		= {2022},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3221045},
  doi		= {10.1109/TASLP.2022.3221045},
  abstract	= {Paraphrase question generation (PQG) aims to rewrite a
		  given original question to a new paraphrase question, where
		  the paraphrase question needs to have the same expressed
		  meaning as the original question, but have a difference in
		  expression form. Existing methods on PQG mainly focus on
		  synonym substitution or word order adjustment based on the
		  original question. However, rewriting based on the
		  word-level may not guarantee the difference between
		  paraphrase questions and original questions. In this paper,
		  we propose a knowledge-aware paraphrase question generation
		  model. Our model first employs a knowledge extractor to
		  extract the prior knowledge related to the original
		  question from the knowledge base. Then an attention
		  mechanism and a gate mechanism are introduced in our model
		  to selectively utilize the extracted prior knowledge for
		  rewriting, which helps to expand the content of the
		  generated question to maximize the difference.
		  Additionally, we use a discriminator module to promote the
		  generated paraphrase to be semantically close to the
		  original question and the ground truth. Specifically, the
		  loss function of the discriminator penalizes the excessive
		  distance between the representation of the paraphrase
		  question and the ground truth. Extensive experiments on the
		  Quora dataset show that the proposed model outperforms the
		  baselines. Further, our model is applied to the SQuAD
		  dataset, which proves the generalization ability of our
		  model in the existing QA dataset.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= dec,
  pages		= {1464–1475},
  numpages	= {12}
}

@Book{		  10.1145/3581906,
  editor	= {Koubarakis, Manolis},
  title		= {Geospatial Data Science: A Hands-on Approach for Building
		  Geospatial Applications Using Linked Data Technologies},
  year		= {2023},
  isbn		= {9798400707407},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  volume	= {51},
  abstract	= {This introductory textbook teaches the simple development
		  of geospatial applications based on the principles and
		  software tools of geospatial data science. It introduces a
		  new generation of geospatial technologies that have emerged
		  from the development of the Semantic Web and the Linked
		  Data paradigm, and shows how data scientists can use them
		  to build environmental applications easily.Geospatial data
		  science is the science of collecting, organizing,
		  analyzing, and visualizing geospatial data. Since around
		  2010, there has been extensive work in the area of
		  geospatial data science using semantic technologies and
		  linked data, from researchers in the areas of the Semantic
		  Web, Geospatial Databases and Geoinformatics. The main
		  results of this research have been the publication of the
		  OGC standard GeoSPARQL and the implementation of a number
		  of linked data tools supporting this standard. Up to now,
		  there has been no textbook that enables someone to teach
		  this material to undergraduate or graduate students.The
		  material of the book is developed in a tutorial style and
		  it is appropriate for an introductory course on the
		  subject. This can be an advanced undergraduate course or a
		  graduate course offered by Computer Science or GIS faculty.
		  It is a hands-on approach and every chapter contains
		  exercises that help students master the material.The book
		  is accompanied by a Web site: where solutions to some of
		  the exercises are given together with supplementary
		  material such as datasets and code. Most of the material in
		  the book has been tried in the “Knowledge Technologies”
		  course taught by the editor in the Department of
		  Informatics and Telecommunications of the National and
		  Kapodistrian University of Athens since 2012.}
}

@InProceedings{	  10.1145/3560071.3560086,
  author	= {Zhang, Kunli and Zhang, Chenghao and Ye, Yajuan and Zan,
		  Hongying and Liu, Xiaomei},
  title		= {Named Entity Recognition in Electronic Medical Records
		  Based on Transfer Learning},
  year		= {2022},
  isbn		= {9781450397087},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3560071.3560086},
  doi		= {10.1145/3560071.3560086},
  abstract	= {Named entity recognition is the first step in clinical
		  electronic medical record text mining, which is significant
		  for clinical decision support and personalized medicine.
		  However, the lack of annotated electronic medical record
		  datasets limits the application of pre-trained language
		  models and deep neural networks in this field. To alleviate
		  the problem of data scarcity, we propose
		  T-RoBERTa-BiLSTM-CRF, a transfer learning-based electronic
		  medical record entity recognition model, which aggregates
		  the characteristics of medical data from different sources
		  and uses a small amount of electronic medical record data
		  as target data for further training. Compared with existing
		  models, our approach can model medical entities more
		  effectively, and the extensive comparative experiments on
		  the CCKS 2019 and DEMRC datasets show the effectiveness of
		  our approach.},
  booktitle	= {Proceedings of the 2022 International Conference on
		  Intelligent Medicine and Health},
  pages		= {91–98},
  numpages	= {8},
  keywords	= {Electronic Medical Record, Named Entity Recognition,
		  Transfer Learning},
  location	= {Xiamen, China},
  series	= {ICIMH '22}
}

@Proceedings{	  10.1145/3615335,
  title		= {SIGDOC '23: Proceedings of the 41st ACM International
		  Conference on Design of Communication},
  year		= {2023},
  isbn		= {9798400703362},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Orlando, FL, USA}
}

@InProceedings{	  10.1145/3404835.3464922,
  author	= {Li, Feng-Lin and Zhao, Zhongzhou and Lu, Qin and Lin,
		  Xuming and Chen, Hehong and Chen, Bo and Pu, Liming and
		  Zhang, Jiashuo and Sun, Fu and Liu, Xikai and Xie, Liqun
		  and Huang, Qi and Zhang, Ji and Chen, Haiqing},
  title		= {AliMe Avatar: Multi-modal Content Production and
		  Presentation for Live-streaming E-commerce},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3464922},
  doi		= {10.1145/3404835.3464922},
  abstract	= {We present AliMe Avatar, a Vtuber designed for
		  live-streaming sales in the E-commerce field. To support
		  the emerging live shopping mode, the core of our digitial
		  avatar is to enable customers to understand products and
		  encourage customers to purchase in a virtual broadcasting
		  room. Based on computer graphics &amp; vision, natural
		  language processing, and speech recognition &amp;
		  synthesis, our AI avatar is able to offer three kinds of
		  key capabilities: custom appearance, product broadcasting,
		  and multi-modal interaction. Currently, it has been
		  launched online in the Taobao app, broadcasts 700+ hours
		  and serves hundreds of thousands of customers per day. In
		  this paper, we mainly focus on the product broadcasting
		  part, demonstrate the system, present the underlying
		  techniques, and share our experience in dealing with
		  live-streaming E-commerce.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2635–2636},
  numpages	= {2},
  keywords	= {Vtuber, content production, multi-modality, visual
		  presentation},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@Article{	  10.1109/taslp.2022.3161157,
  author	= {Mao, Qianren and Li, Jianxin and Peng, Hao and He, Shizhu
		  and Wang, Lihong and Yu, Philip S. and Wang, Zheng},
  title		= {Fact-Driven Abstractive Summarization by Utilizing
		  Multi-Granular Multi-Relational Knowledge},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3161157},
  doi		= {10.1109/TASLP.2022.3161157},
  abstract	= {Abstractive summarization generates a concise summary to
		  capture the key ideas of the source text. This task
		  underpins important applications like information
		  retrieval, document comprehension, and event tracking.
		  While much progress has been achieved, state-of-the-art
		  summarization approaches often fail to generate
		  high-quality summaries to reproduce factual details
		  accurately. One of the key limitations of existing
		  solutions is that they are primarily concerned about
		  extracting facts from the source text but overlook other
		  crucial factual information, such as the related time,
		  locations, reasons, consequences, purposes, participants
		  and involved parties. Furthermore, the current
		  summarization frameworks are inadequate in modeling the
		  complex semantic relations among facts and the
		  corresponding factual information, leaving much room for
		  improvement. This paper presents
		  &lt;sc&gt;FFSum&lt;/sc&gt;, a novel summarization framework
		  for exploiting multi-grained factual information to improve
		  text summarization. To this end, &lt;sc&gt;FFSum&lt;/sc&gt;
		  constructs an individual fine-grained factual graph with
		  multiple relations among facts and the corresponding
		  factual information. It employs a fact-driven graph
		  attention network to integrate multi-granular factual
		  representations at the encoding stage. It then uses a
		  hybrid pointer network to retrieve factual pieces from the
		  graph for the summary generation. We evaluate the
		  &lt;sc&gt;FFSum&lt;/sc&gt; by applying it to two real-world
		  datasets. Experimental results show that the
		  &lt;sc&gt;FFSum&lt;/sc&gt; consistently outperforms a
		  state-of-the-art approach across evaluation datasets.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {1665–1678},
  numpages	= {14}
}

@Article{	  10.1145/3627824,
  author	= {Xu, Ronghui and Huang, Weiming and Zhao, Jun and Chen,
		  Meng and Nie, Liqiang},
  title		= {A Spatial and Adversarial Representation Learning Approach
		  for Land Use Classification with POIs},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {6},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3627824},
  doi		= {10.1145/3627824},
  abstract	= {Points-of-interests (POIs) have been proven to be
		  indicative for sensing urban land use in numerous studies.
		  However, recent progress mainly relies on spatial
		  co-occurrence patterns among POI categories, which falls
		  short in utilizing the rich semantic information embodied
		  in POI hierarchical categories and in sensing the spatial
		  distribution patterns of POIs at an individual zonal scale.
		  In this context, we present a spatial and adversarial
		  representation learning approach (SARL) for predicting land
		  use of urban zones with POIs. SARL deeply mines the
		  information from POIs from both spatial and categorical
		  perspectives. Specifically, we first utilize a
		  convolutional neural network to sense the spatial
		  distribution patterns of POIs in each urban zone. We then
		  leverage an autoencoder and an adversarial learning
		  strategy to mine the POI categorical information in all
		  hierarchical levels, which emphasizes the prominent and
		  definitive POIs while preserves the overall POI
		  hierarchical structures in each zone. Finally, we fuse
		  these information from the two perspectives via a Wide
		  &amp; Deep network and carry out land use prediction with
		  the fused embeddings. We conduct comprehensive experiments
		  to validate the effectiveness of SARL in four European
		  cities with real-world data. The results demonstrate that
		  SARL substantially outperforms several competitive
		  baselines.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= nov,
  articleno	= {114},
  numpages	= {25},
  keywords	= {Land use classification, urban zone embedding, POI spatial
		  distribution, POI categorical hierarchy, adversarial
		  learning}
}

@InProceedings{	  10.1145/3611450.3611477,
  author	= {Liu, Haitao and Song, Jihua and Peng, Weiming},
  title		= {An Evidential Classifier with Multiple Pre-trained
		  Language Models for Nested Named Entity Recognition},
  year		= {2023},
  isbn		= {9798400707605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3611450.3611477},
  doi		= {10.1145/3611450.3611477},
  abstract	= {Nested named entity recognition (NER) is an important and
		  challenging task in information extraction. One effective
		  approach is to detect regions in sentences that are later
		  classified by neural networks. Since pre-trained language
		  models (PLMs) were proposed, nested NER models have
		  benefited a lot from them. However, it is common that only
		  one PLM is utilized for a given model, and the performance
		  varies with different PLMs. We note that there exist some
		  conflicting predictions which lead to the final variation.
		  Thus, there is still room for investigation as to whether a
		  model could achieve even better performance by conducting a
		  comprehensive analysis of results from various PLMs. In
		  this paper, we propose an evidential classifier with
		  multiple PLMs for nested NER. First, the well-known deep
		  exhaustive model is trained separately with different PLMs,
		  whose predictions are then treated as pieces of evidence
		  that can be represented in the framework of Dempster-Shafer
		  theory. Finally, the pooled evidence is obtained using a
		  combination rule, based on which the inference is
		  performed. Experiments are conducted on the GENIA dataset,
		  and detailed analysis demonstrates the merits of our
		  model.},
  booktitle	= {Proceedings of the 2023 3rd International Conference on
		  Artificial Intelligence, Automation and Algorithms},
  pages		= {181–185},
  numpages	= {5},
  keywords	= {Dempster-Shafer theory, information fusion, nested named
		  entity recognition, pre-trained language models},
  location	= {Beijing, China},
  series	= {AI2A '23}
}

@Article{	  10.1109/tcbb.2021.3135844,
  author	= {Sun, Yi and Wang, Jian and Lin, Hongfei and Zhang, Yijia
		  and Yang, Zhihao},
  title		= {Knowledge Guided Attention and Graph Convolutional
		  Networks for Chemical-Disease Relation Extraction},
  year		= {2022},
  issue_date	= {Jan.-Feb. 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2021.3135844},
  doi		= {10.1109/TCBB.2021.3135844},
  abstract	= {The automatic extraction of the chemical-disease relation
		  (CDR) from the text becomes critical because it takes a lot
		  of time and effort to extract valuable CDR manually.
		  Studies have shown that prior knowledge from the biomedical
		  knowledge base is important for relation extraction. The
		  method of combining deep learning models with prior
		  knowledge is worthy of our study. In this paper, we propose
		  a new model called Knowledge Guided Attention and Graph
		  Convolutional Networks (KGAGN) for CDR extraction. First,
		  to make full advantage of domain knowledge, we train entity
		  embedding as a feature representation of input sequence,
		  and relation embedding to capture weighted contextual
		  information further through the attention mechanism. Then,
		  to make full advantage of syntactic dependency information
		  in cross-sentence CDR extraction, we construct
		  document-level syntactic dependency graphs and encode them
		  using a graph convolution network (GCN). Finally, the
		  chemical-induced disease (CID) relation is extracted by
		  using weighted context features and long-range dependency
		  features both of which contain additional knowledge
		  information We evaluated our model on the CDR dataset
		  published by the BioCreative-V community and achieves an
		  F1-score of 73.3%, surpassing other state-of-the-art
		  methods. the code implemented by PyTorch 1.7.0 deep
		  learning library can be downloaded from Github:
		  &lt;uri&gt;https://github.com/sunyi123/cdr&lt;/uri&gt;.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= dec,
  pages		= {489–499},
  numpages	= {11}
}

@InProceedings{	  10.1145/3487553.3524674,
  author	= {Alam, Mehwish and Iana, Andreea and Grote, Alexander and
		  Ludwig, Katharina and M\"{u}ller, Philipp and Paulheim,
		  Heiko},
  title		= {Towards Analyzing the Bias of News Recommender Systems
		  Using Sentiment and Stance Detection},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524674},
  doi		= {10.1145/3487553.3524674},
  abstract	= {News recommender systems are used by online news providers
		  to alleviate information overload and to provide
		  personalized content to users. However, algorithmic news
		  curation has been hypothesized to create filter bubbles and
		  to intensify users’ selective exposure, potentially
		  increasing their vulnerability to polarized opinions and
		  fake news. In this paper, we show how information on news
		  items’ stance and sentiment can be utilized to analyze
		  and quantify the extent to which recommender systems suffer
		  from biases. To that end, we have annotated a German news
		  corpus on the topic of migration using stance detection and
		  sentiment analysis. In an experimental evaluation with four
		  different recommender systems, our results show a slight
		  tendency of all four models for recommending articles with
		  negative sentiments and stances against the topic of
		  refugees and migration. Moreover, we observed a positive
		  correlation between the sentiment and stance bias of the
		  text-based recommenders and the preexisting user bias,
		  which indicates that these systems amplify users’
		  opinions and decrease the diversity of recommended news.
		  The knowledge-aware model appears to be the least prone to
		  such biases, at the cost of predictive accuracy.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {448–457},
  numpages	= {10},
  keywords	= {German news articles, echo chambers, filter bubbles, news
		  recommendation, polarization, sentiment analysis, stance
		  detection},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Article{	  10.1613/jair.1.14318,
  author	= {Aksoy, Meltem and Yanik, Seda and Amasyali, Mehmet Fatih},
  title		= {Reviewer Assignment Problem: A Systematic Review of the
		  Literature},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {76},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.14318},
  doi		= {10.1613/jair.1.14318},
  abstract	= {Appropriate reviewer assignment significantly impacts the
		  quality of proposal evaluation, as accurate and fair
		  reviews are contingent on their assignment to relevant
		  reviewers. The crucial task of assigning reviewers to
		  submitted proposals is the starting point of the review
		  process and is also known as the reviewer assignment
		  problem (RAP). Due to the obvious restrictions of manual
		  assignment, journal editors, conference organizers, and
		  grant managers demand automatic reviewer assignment
		  approaches. Many studies have proposed assignment solutions
		  in response to the demand for automated procedures since
		  1992. The primary objective of this survey paper is to
		  provide scholars and practitioners with a comprehensive
		  overview of available research on the RAP. To achieve this
		  goal, this article presents an in-depth systematic review
		  of 103 publications in the field of reviewer assignment
		  published in the past three decades and available in the
		  Web of Science, Scopus, ScienceDirect, Google Scholar, and
		  Semantic Scholar databases. This review paper classified
		  and discussed the RAP approaches into two broad categories
		  and numerous subcategories based on their underlying
		  techniques. Furthermore, potential future research
		  directions for each category are presented. This survey
		  shows that the research on the RAP is becoming more
		  significant and that more effort is required to develop new
		  approaches and a framework.},
  journal	= {J. Artif. Int. Res.},
  month		= may,
  numpages	= {67}
}

@Article{	  10.1145/3571730,
  author	= {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu,
		  Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and
		  Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  title		= {Survey of Hallucination in Natural Language Generation},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {12},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3571730},
  doi		= {10.1145/3571730},
  abstract	= {Natural Language Generation (NLG) has improved
		  exponentially in recent years thanks to the development of
		  sequence-to-sequence deep learning technologies such as
		  Transformer-based language models. This advancement has led
		  to more fluent and coherent NLG, leading to improved
		  development in downstream tasks such as abstractive
		  summarization, dialogue generation, and data-to-text
		  generation. However, it is also apparent that deep learning
		  based generation is prone to hallucinate unintended text,
		  which degrades the system performance and fails to meet
		  user expectations in many real-world scenarios. To address
		  this issue, many studies have been presented in measuring
		  and mitigating hallucinated texts, but these have never
		  been reviewed in a comprehensive manner before.In this
		  survey, we thus provide a broad overview of the research
		  progress and challenges in the hallucination problem in
		  NLG. The survey is organized into two parts: (1) a general
		  overview of metrics, mitigation methods, and future
		  directions, and (2) an overview of task-specific research
		  progress on hallucinations in the following downstream
		  tasks, namely abstractive summarization, dialogue
		  generation, generative question answering, data-to-text
		  generation, and machine translation. This survey serves to
		  facilitate collaborative efforts among researchers in
		  tackling the challenge of hallucinated texts in NLG.},
  journal	= {ACM Comput. Surv.},
  month		= mar,
  articleno	= {248},
  numpages	= {38},
  keywords	= {Hallucination, intrinsic hallucination, extrinsic
		  hallucination, faithfulness in NLG, factuality in NLG,
		  consistency in NLG}
}

@InProceedings{	  10.1145/3526113.3545631,
  author	= {Kaur, Harmanpreet and Downey, Doug and Singh, Amanpreet
		  and Cheng, Evie Yu-Yen and Weld, Daniel and Bragg,
		  Jonathan},
  title		= {FeedLens: Polymorphic Lenses for Personalizing Exploratory
		  Search over Knowledge Graphs},
  year		= {2022},
  isbn		= {9781450393201},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3526113.3545631},
  doi		= {10.1145/3526113.3545631},
  abstract	= {The vast scale and open-ended nature of knowledge graphs
		  (KGs) make exploratory search over them cognitively
		  demanding for users. We introduce a new technique,
		  polymorphic lenses, that improves exploratory search over a
		  KG by obtaining new leverage from the existing preference
		  models that KG-based systems maintain for recommending
		  content. The approach is based on a simple but powerful
		  observation: in a KG, preference models can be re-targeted
		  to recommend not only entities of a single base entity type
		  (e.g., papers in the scientific literature KG, products in
		  an e-commerce KG), but also all other types (e.g., authors,
		  conferences, institutions; sellers, buyers). We implement
		  our technique in a novel system,&nbsp;FeedLens, which is
		  built over&nbsp;Semantic Scholar, a production system for
		  navigating the scientific literature KG.&nbsp;FeedLens
		  reuses the existing preference models on&nbsp;Semantic
		  Scholar—people’s curated research feeds—as lenses for
		  exploratory search. Semantic Scholar users can curate
		  multiple feeds/lenses for different topics of interest,
		  e.g., one for human-centered AI and another for document
		  embeddings. Although these lenses are defined in terms of
		  papers, FeedLens re-purposes them to also guide search over
		  authors, institutions, venues, etc. Our system design is
		  based on feedback from intended users via two pilot surveys
		  (n = 17 and n = 13, respectively). We compare&nbsp;FeedLens
		  and&nbsp;Semantic Scholar via a third (within-subjects)
		  user study (n = 15) and find that&nbsp;FeedLens increases
		  user engagement while reducing the cognitive effort
		  required to complete a short literature review task. Our
		  qualitative results also highlight people’s preference
		  for this more effective exploratory search experience
		  enabled by&nbsp;FeedLens.},
  booktitle	= {Proceedings of the 35th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {97},
  numpages	= {15},
  keywords	= {Exploratory search, Interaction techniques, Knowledge
		  graphs, Recommender systems, System design, User study},
  location	= {Bend, OR, USA},
  series	= {UIST '22}
}

@InProceedings{	  10.1145/3581783.3611784,
  author	= {Lu, Jinda and Wang, Shuo and Zhang, Xinyu and Hao, Yanbin
		  and He, Xiangnan},
  title		= {Semantic-based Selection, Synthesis, and Supervision for
		  Few-shot Learning},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3611784},
  doi		= {10.1145/3581783.3611784},
  abstract	= {Few-shot learning (FSL) is designed to explore the
		  distribution of novel categories from a few samples. It is
		  a challenging task since the classifier is usually
		  susceptible to over-fitting when learning from limited
		  training samples. To alleviate this phenomenon, a common
		  solution is to achieve more training samples using a
		  generic generation strategy in visual space. However, there
		  are some limitations to this solution. It is because a
		  feature extractor trained on base samples (known knowledge)
		  tends to focus on the textures and structures of the
		  objects it learns, which is inadequate for describing novel
		  samples. To solve these issues, we introduce semantics and
		  propose a Semantic-based Selection, Synthesis, and S
		  upervision (4S) method, where semantics provide more
		  diverse and informative supervision for recognizing novel
		  objects. Specifically, we first utilize semantic knowledge
		  to explore the correlation of categories in the textual
		  space and select base categories related to the given novel
		  category. This process can improve the efficiency of
		  subsequent operations (synthesis and supervision). Then, we
		  analyze the semantic knowledge to hallucinate the training
		  samples by selectively synthesizing the contents from base
		  and support samples. This operation not only increases the
		  number of training samples but also takes advantage of the
		  contents of the base categories to enhance the description
		  of support samples. Finally, we also employ semantic
		  knowledge as both soft and hard supervision to enrich the
		  supervision for the fine-tuning procedure. Empirical
		  studies on four FSL benchmarks demonstrate the
		  effectiveness of 4S.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {3569–3578},
  numpages	= {10},
  keywords	= {data synthesis, few-shot learning, semantic supervision},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3539618.3591843,
  author	= {Hu, Sen and Yang, Changlin and Wang, Junjie and Liu, Siye
		  and Xu, Teng and Zhang, Wangshu and Zheng, Jing},
  title		= {A Data-centric Solution to Improve Online Performance of
		  Customer Service Bots},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591843},
  doi		= {10.1145/3539618.3591843},
  abstract	= {The online performance of customer service bots is often
		  less than satisfactory because of the gap between limited
		  training data and real-world user questions. As a
		  straightforward way to improve online performance, model
		  iteration and re-deployment are time consuming and
		  labor-intensive, and therefore difficult to sustain. To fix
		  badcases and improve online performance of chatbots in a
		  timely and continuous manner, we propose a data-centric
		  solution consisting of three main modules: badcase
		  detection, bad case correction, and answer extraction. By
		  making full use of online model signals, implicit user
		  feedback and artificial customer service log, the proposed
		  solution can fix online badcases automatically. Our
		  solution has been deployed and bringing consistently
		  positive impacts for hundreds of customer service bots used
		  by Alipay app.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3305–3309},
  numpages	= {5},
  keywords	= {continuous improvement, customer service bots,
		  data-centric AI},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3534678.3539344,
  author	= {Datta, Debanjan and Chen, Feng and Ramakrishnan, Naren},
  title		= {Framing Algorithmic Recourse for Anomaly Detection},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539344},
  doi		= {10.1145/3534678.3539344},
  abstract	= {The problem of algorithmic recourse has been explored for
		  supervised machine learning models, to provide more
		  interpretable, transparent and robust outcomes from
		  decision support systems. An unexplored area is that of
		  algorithmic recourse for anomaly detection, specifically
		  for tabular data with only discrete feature values. Here
		  the problem is to present a set of counterfactuals that are
		  deemed normal by the underlying anomaly detection model so
		  that applications can utilize this information for
		  explanation purposes or to recommend countermeasures. We
		  present an approach-Context preserving Algorithmic Recourse
		  for Anomalies in Tabular data(CARAT), that is effective,
		  scalable, and agnostic to the underlying anomaly detection
		  model. CARAT uses a transformer based encoder-decoder model
		  to explain an anomaly by finding features with low
		  likelihood. Subsequently semantically coherent
		  counterfactuals are generated by modifying the highlighted
		  features, using the overall context of features in the
		  anomalous instance(s). Extensive experiments help
		  demonstrate the efficacy of CARAT.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {283–293},
  numpages	= {11},
  keywords	= {algorithmic recourse, anomaly detection, deep learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@Article{	  10.14778/3523210.3523224,
  author	= {Fan, Wenfei and Fu, Wenzhi and Jin, Ruochun and Lu, Ping
		  and Tian, Chao},
  title		= {Discovering association rules from big graphs},
  year		= {2022},
  issue_date	= {March 2022},
  publisher	= {VLDB Endowment},
  volume	= {15},
  number	= {7},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3523210.3523224},
  doi		= {10.14778/3523210.3523224},
  abstract	= {This paper tackles two challenges to discovery of graph
		  rules. Existing discovery methods often (a) return an
		  excessive number of rules, and (b) do not scale with large
		  graphs given the intractability of the discovery problem.
		  We propose an application-driven strategy to cut back rules
		  and data that are irrelevant to users' interests, by
		  training a machine learning (ML) model to identify data
		  pertaining to a given application. Moreover, we introduce a
		  sampling method to reduce a big graph G to a set H of small
		  sample graphs. Given expected support and recall bounds,
		  the method is able to deduce samples in H and mine rules
		  from H to satisfy the bounds in the entire G. As proof of
		  concept, we develop an algorithm to discover Graph
		  Association Rules (GARs), which are a combination of graph
		  patterns and attribute dependencies, and may embed ML
		  classifiers as predicates. We show that the algorithm is
		  parallelly scalable, i.e., it guarantees to reduce runtime
		  when more machines are used. We experimentally verify that
		  the method is able to discover rules with recall above 91%
		  when using sample ratio 10%, with speedup of 61 times.},
  journal	= {Proc. VLDB Endow.},
  month		= mar,
  pages		= {1479–1492},
  numpages	= {14}
}

@Article{	  10.1145/3481608,
  author	= {Han, Xu and Zhang, Zhengyan and Liu, Zhiyuan},
  title		= {Knowledgeable machine learning for natural language
		  processing},
  year		= {2021},
  issue_date	= {November 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {64},
  number	= {11},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3481608},
  doi		= {10.1145/3481608},
  journal	= {Commun. ACM},
  month		= oct,
  pages		= {50–51},
  numpages	= {2}
}

@InProceedings{	  10.1145/3488560.3498393,
  author	= {Wu, Sixing and Wang, Minghui and Li, Ying and Zhang, Dawei
		  and Wu, Zhonghai},
  title		= {Improving the Applicability of Knowledge-Enhanced Dialogue
		  Generation Systems by Using Heterogeneous Knowledge from
		  Multiple Sources},
  year		= {2022},
  isbn		= {9781450391320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3488560.3498393},
  doi		= {10.1145/3488560.3498393},
  abstract	= {Traditional conversational systems can only access the
		  given query during the response generation, leading to
		  meaningless responses. To this end, researchers proposed to
		  enhance dialogue generation by integrating external
		  knowledge. Although such methods have achieved remarkable
		  gains, the use of only single-source knowledge often makes
		  existing knowledge-enhanced methods degenerate into
		  traditional models in real scenarios because of the
		  insufficient knowledge coverage of single-source knowledge.
		  To improve the applicability of knowledge-enhanced methods,
		  we propose two novel frameworks to use heterogeneous
		  knowledge from multiple sources. We first propose an
		  MHKD-Seq2Seq framework, which can use different
		  heterogeneous knowledge by identifying abstract-level
		  knowledge behaviors; meanwhile, a Diffuse-Aggregate scheme
		  is used to process multiple knowledge simultaneously and
		  produce a unified result. The next framework MHKD-ARPLM can
		  leverage the advantages of pretrained language models with
		  Knowledge Linearization techniques. In experiments, we
		  collected dialogues from previously open-released datasets
		  and built a multi-source knowledge-aligned dataset
		  TriKE-Weibo, which involves three knowledge sources:
		  commonsense, texts, and infobox tables. Extensive
		  evaluations demonstrate the performance leadership of our
		  approaches against competitive baseline models.},
  booktitle	= {Proceedings of the Fifteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {1149–1157},
  numpages	= {9},
  keywords	= {knowledge-enhanced dialogue generation, multi-source
		  knowledge},
  location	= {Virtual Event, AZ, USA},
  series	= {WSDM '22}
}

@InProceedings{	  10.5555/3535850.3536091,
  author	= {Singh, Ishika and Singh, Gargi and Modi, Ashutosh},
  title		= {Pre-trained Language Models as Prior Knowledge for Playing
		  Text-based Games},
  year		= {2022},
  isbn		= {9781450392136},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Recently, text world games have been proposed to enable
		  artificial agents to understand and reason about real-world
		  scenarios. These text-based games are challenging for
		  artificial agents, as it requires an understanding of and
		  interaction using natural language in a partially
		  observable environment. Past approaches have paid less
		  attention to the language understanding capability of the
		  proposed agents. In this paper, we improve the semantic
		  understanding of the agent by proposing a simple RL with LM
		  framework where we use transformer-based language models
		  with Deep RL models. Overall, our proposed approach
		  outperforms on 4 games out of the 14 text-based games,
		  while performing comparable to the state-of-the-art models
		  on the remaining games.},
  booktitle	= {Proceedings of the 21st International Conference on
		  Autonomous Agents and Multiagent Systems},
  pages		= {1729–1731},
  numpages	= {3},
  keywords	= {NLP, interactive fiction games, reinforcement learning},
  location	= {Virtual Event, New Zealand},
  series	= {AAMAS '22}
}

@InProceedings{	  10.1145/3477495.3531668,
  author	= {La Cava, Lucio and Simeri, Andrea and Tagarelli, Andrea},
  title		= {LawNet-Viz: A Web-based System to Visually Explore
		  Networks of Law Article References},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531668},
  doi		= {10.1145/3477495.3531668},
  abstract	= {We present LawNet-Viz, a web-based tool for the modeling,
		  analysis and visualization of law reference networks
		  extracted from a statute law corpus. LawNet-Viz is designed
		  to support legal research tasks and help legal
		  professionals as well as laymen visually exploring the
		  article connections built upon the explicit law references
		  detected in the article contents. To demonstrate
		  LawNet-Viz, we show its application to the Italian Civil
		  Code (ICC), which exploits a recent BERT-based model
		  fine-tuned on the ICC. LawNet-Viz is a system prototype
		  that is planned for product development.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3300–3305},
  numpages	= {6},
  keywords	= {artificial intelligence and law, deep language models, law
		  article citation networks, network analysis and
		  visualization},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3604915.3608801,
  author	= {Yang, Boming and Liu, Dairui and Suzumura, Toyotaro and
		  Dong, Ruihai and Li, Irene},
  title		= {✨ Going Beyond Local: Global Graph-Enhanced Personalized
		  News Recommendations},
  year		= {2023},
  isbn		= {9798400702419},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604915.3608801},
  doi		= {10.1145/3604915.3608801},
  abstract	= {Precisely recommending candidate news articles to users
		  has always been a core challenge for personalized news
		  recommendation systems. Most recent works primarily focus
		  on using advanced natural language processing techniques to
		  extract semantic information from rich textual data,
		  employing content-based methods derived from local
		  historical news. However, this approach lacks a global
		  perspective, failing to account for users’ hidden
		  motivations and behaviors beyond semantic information. To
		  address this challenge, we propose a novel model called
		  GLORY (Global-LOcal news Recommendation sYstem), which
		  combines global representations learned from other users
		  with local representations to enhance personalized
		  recommendation systems. We accomplish this by constructing
		  a Global-aware Historical News Encoder, which includes a
		  global news graph and employs gated graph neural networks
		  to enrich news representations, thereby fusing historical
		  news representations by a historical news aggregator.
		  Similarly, we extend this approach to a Global Candidate
		  News Encoder, utilizing a global entity graph and a
		  candidate news aggregator to enhance candidate news
		  representation. Evaluation results on two public news
		  datasets demonstrate that our method outperforms existing
		  approaches. Furthermore, our model offers more diverse
		  recommendations1.},
  booktitle	= {Proceedings of the 17th ACM Conference on Recommender
		  Systems},
  pages		= {24–34},
  numpages	= {11},
  keywords	= {Graph Neural Network, News Modeling, News Recommendation},
  location	= {Singapore, Singapore},
  series	= {RecSys '23}
}

@Proceedings{	  10.1145/3587828,
  title		= {ICSCA '23: Proceedings of the 2023 12th International
		  Conference on Software and Computer Applications},
  year		= {2023},
  isbn		= {9781450398589},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Kuantan, Malaysia}
}

@Proceedings{	  10.1145/3544549,
  title		= {CHI EA '23: Extended Abstracts of the 2023 CHI Conference
		  on Human Factors in Computing Systems},
  year		= {2023},
  isbn		= {9781450394222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hamburg, Germany}
}

@Article{	  10.1145/3588314,
  author	= {Yan, Jinghui and Zong, Chengqing and Xu, Jinan},
  title		= {Combination of Loss-based Active Learning and
		  Semi-supervised Learning for Recognizing Entities in
		  Chinese Electronic Medical Records},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3588314},
  doi		= {10.1145/3588314},
  abstract	= {The recognition of entities in an electronic medical
		  record (EMR) is especially important to downstream tasks,
		  such as clinical entity normalization and medical dialogue
		  understanding. However, in the medical professional field,
		  training a high-quality named entity recognition system
		  always requires large-scale annotated datasets, which are
		  highly expensive to obtain. In this article, to lower the
		  cost of data annotation and maximizing the use of unlabeled
		  data, we propose a hybrid approach to recognizing the
		  entities in Chinese electronic medical record, which is in
		  combination of loss-based active learning and
		  semi-supervised learning. Specifically, we adopted a
		  dynamic balance strategy to dynamically balance the minimum
		  loss predicted by a named entity recognition decoder and a
		  loss prediction module at different stages in the process.
		  Experimental results demonstrated our proposed
		  framework’s effectiveness and efficiency, achieving
		  higher performances than existing approaches on Chinese EMR
		  entity recognition datasets under limited labeling
		  resources.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {123},
  numpages	= {19},
  keywords	= {Electronic medical record, loss-based active learning,
		  dynamic balance strategy, semi-supervised learning}
}

@InProceedings{	  10.1145/3584371.3613008,
  author	= {Zhang, Wenlong and Zeng, Kangping and Yang, Xinming and
		  Shi, Tian and Wang, Ping},
  title		= {Text-to-ESQ: A Two-Stage Controllable Approach for
		  Efficient Retrieval of Vaccine Adverse Events from NoSQL
		  Database},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3613008},
  doi		= {10.1145/3584371.3613008},
  abstract	= {The Vaccine Adverse Event Reporting System (VAERS)
		  contains detailed reports of adverse events following
		  vaccine administration. However, efficiently and accurately
		  searching for specific information from VAERS poses
		  significant challenges, especially for medical experts.
		  Natural language querying (NLQ) methods tackle the
		  challenge by translating the input questions into
		  executable queries, allowing for the exploration of complex
		  databases with large amounts of information. Most existing
		  studies focus on the relational database and solve the
		  Text-to-SQL task. However, the capability of full-text for
		  Text-to-SQL is greatly limited by the data structures and
		  functionality of the SQL databases. In addition, the
		  potential of natural language querying has not been
		  comprehensively explored in the healthcare domain. To
		  overcome these limitations, we investigate the potential of
		  NoSQL databases, specifically Elasticsearch, and forge a
		  new research direction for NLQ, which we refer to as
		  Text-to-ESQ generation. This exploration requires us to
		  re-design various aspects of NLQ, such as the target
		  application and the advantages of NoSQL database. In our
		  approach, we develop a two-stage controllable (TSC)
		  framework consisting of a question-to-question (Q2Q)
		  translation module and an ESQ condition extraction (ECE)
		  module. These modules are carefully designed to efficiently
		  retrieve information from the VEARS data stored in a NoSQL
		  database. Additionally, we construct a dedicated
		  question-ESQ pair dataset called VAERSESQ, to support the
		  task in the healthcare domain. Extensive experiments were
		  conducted on the VAERSESQ dataset to evaluate the proposed
		  methods. The results, both quantitative and qualitative,
		  demonstrate the accuracy and efficiency of our approach in
		  generating queries for NoSQL databases, thus enabling
		  efficient retrieval of VEARS data.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {54},
  numpages	= {10},
  keywords	= {natural language querying, question translation,
		  text-to-ESQ, VAERS, elasticsearch query},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1145/3485447.3511998,
  author	= {Chen, Xiang and Zhang, Ningyu and Xie, Xin and Deng,
		  Shumin and Yao, Yunzhi and Tan, Chuanqi and Huang, Fei and
		  Si, Luo and Chen, Huajun},
  title		= {KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic
		  Optimization for Relation Extraction},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511998},
  doi		= {10.1145/3485447.3511998},
  abstract	= {Recently, prompt-tuning has achieved promising results for
		  specific few-shot classification tasks. The core idea of
		  prompt-tuning is to insert text pieces (i.e., templates)
		  into the input and transform a classification task into a
		  masked language modeling problem. However, for relation
		  extraction, determining an appropriate prompt template
		  requires domain expertise, and it is cumbersome and
		  time-consuming to obtain a suitable label word.
		  Furthermore, there exists abundant semantic and prior
		  knowledge among the relation labels that cannot be ignored.
		  To this end, we focus on incorporating knowledge among
		  relation labels into prompt-tuning for relation extraction
		  and propose a Knowledge-aware Prompt-tuning approach with
		  synergistic optimization (KnowPrompt). Specifically, we
		  inject latent knowledge contained in relation labels into
		  prompt construction with learnable virtual type words and
		  answer words. Then, we synergistically optimize their
		  representation with structured constraints. Extensive
		  experimental results on five datasets with standard and
		  low-resource settings demonstrate the effectiveness of our
		  approach. Our code and datasets are available in GitHub1
		  for reproducibility.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {2778–2788},
  numpages	= {11},
  keywords	= {Knowledge-aware, Prompt-tuning, Relation Extraction},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3459637.3482111,
  author	= {Lin, Xuming and Cui, Shaobo and Zhao, Zhongzhou and Zhou,
		  Wei and Zhang, Ji and Chen, Haiqing},
  title		= {GGP: A Graph-based Grouping Planner for Explicit Control
		  of Long Text Generation},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482111},
  doi		= {10.1145/3459637.3482111},
  abstract	= {Existing data-driven methods can well handle short text
		  generation. However, when applied to the long-text
		  generation scenarios such as story generation or
		  advertising text generation in the commercial scenario,
		  these methods may generate illogical and uncontrollable
		  texts. To address these aforementioned issues, we propose a
		  graph-based grouping planner~(GGP) following the idea of
		  first-plan-then-generate. Specifically, given a collection
		  of key phrases, GGP firstly encodes these phrases into a
		  instance-level sequential representation and a corpus-level
		  graph-based representation separately. With these two
		  synergic representations, we then regroup these phrases
		  into a fine-grained plan, based on which we generate the
		  final long text. We conduct our experiments on three long
		  text generation datasets and the experimental results
		  reveal that GGP significantly outperforms baselines, which
		  proves that GGP can control the long text generation with
		  knowing how to say and in what order.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3253–3257},
  numpages	= {5},
  keywords	= {copy mechanism, graph neural networks, long text
		  generation, planning based data-to-text},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.14778/3611479.3611533,
  author	= {Eltabakh, Mohamed Y. and Kunjir, Mayuresh and Elmagarmid,
		  Ahmed K. and Ahmad, Mohammad Shahmeer},
  title		= {Cross Modal Data Discovery over Structured and
		  Unstructured Data Lakes},
  year		= {2023},
  issue_date	= {July 2023},
  publisher	= {VLDB Endowment},
  volume	= {16},
  number	= {11},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3611479.3611533},
  doi		= {10.14778/3611479.3611533},
  abstract	= {Organizations are collecting increasingly large amounts of
		  data for data-driven decision making. These data are often
		  dumped into a centralized repository, e.g., a data lake,
		  consisting of thousands of structured and unstructured
		  datasets. Perversely, such mixture makes the problem of
		  discovering tables or documents that are relevant to a
		  user's query very challenging. Despite the recent efforts
		  in data discovery, the problem remains widely open
		  especially in the two fronts of (1) discovering
		  relationships and relatedness across structured and
		  unstructured datasets-where existing techniques suffer from
		  either scalability, being customized for a specific problem
		  type (e.g., entity matching or data integration), or
		  demolishing the structural properties on its way, and (2)
		  developing a holistic system for integrating various
		  similarity measurements and sketches in an effective way to
		  boost the discovery accuracy.In this paper, we propose a
		  new data discovery system, named CMDL, for addressing these
		  two limitations. CMDL supports the data discovery process
		  over both structured and unstructured data while retaining
		  the structural properties of tables. As a result, CMDL is
		  the only system to date that empowers end-users to
		  seamlessly pipeline the discovery tasks across the two
		  modalities. We propose a novel multi-modal embedding
		  representation that captures the similarities between text
		  documents and tabular columns. The model training relies on
		  labeled datasets generated though weak supervision, and
		  thus the system is domain agnostic and easily
		  generalizable. We evaluate CMDL on three real-world data
		  lakes with diverse applications and show that our system is
		  significantly more effective for cross-modality discovery
		  compared to the search-based baseline techniques. Moreover,
		  CMDL is more accurate and robust to different data types
		  and distributions compared to the state-of-the-art systems
		  that are limited to only the structured datasets.},
  journal	= {Proc. VLDB Endow.},
  month		= jul,
  pages		= {3377–3390},
  numpages	= {14}
}

@InProceedings{	  10.1145/3539618.3591884,
  author	= {Guo, Shuyu and Zhang, Shuo and Sun, Weiwei and Ren,
		  Pengjie and Chen, Zhumin and Ren, Zhaochun},
  title		= {Towards Explainable Conversational Recommender Systems},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591884},
  doi		= {10.1145/3539618.3591884},
  abstract	= {Explanations in conventional recommender systems have
		  demonstrated benefits in helping the user understand the
		  rationality of the recommendations and improving the
		  system's efficiency, transparency, and trustworthiness. In
		  the conversational environment, multiple contextualized
		  explanations need to be generated, which poses further
		  challenges for explanations. To better measure
		  explainability in CRS, we propose ten evaluation
		  perspectives based on the concepts from conventional
		  recommender systems together with the characteristics of
		  CRS. We assess five existing CRS benchmark datasets using
		  these metrics and observe the necessity of improving the
		  explanation quality of CRS. To achieve this, we conduct
		  manual and automatic approaches to extend these dialogues
		  and construct a new CRS dataset, namely Explainable
		  Recommendation Dialogues (E-ReDial). It includes 756
		  dialogues with over 2,000 high-quality rewritten
		  explanations. We compare two baseline approaches to perform
		  explanation generation based on E-ReDial. Experimental
		  results suggest that models trained on E-ReDial can
		  significantly improve explainability while introducing
		  knowledge into the models can further improve the
		  performance. GPT-3 in the in-context learning setting can
		  generate more realistic and diverse movie descriptions. In
		  contrast, T5 training on E-Redial can better generate clear
		  reasons for recommendations based on user preferences.
		  E-ReDial is available at
		  https://github.com/Superbooming/E-ReDial.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2786–2795},
  numpages	= {10},
  keywords	= {conversational information access, conversational
		  recommendation, explainable recommendation},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3486635.3491068,
  author	= {Gurav, Rutuja and De, Debraj and Thakur, Gautam and Fan,
		  Junchuan},
  title		= {Conflation of Geospatial POI Data and Ground-level Imagery
		  via Link Prediction on Joint Semantic Graph},
  year		= {2021},
  isbn		= {9781450391207},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3486635.3491068},
  doi		= {10.1145/3486635.3491068},
  abstract	= {With the proliferation of smartphone cameras and social
		  networks, we have rich, multi-modal data about points of
		  interest (POIs) - like cultural landmarks, institutions,
		  businesses, etc. - within a given areas of interest (AOI)
		  (e.g., a county, city or a neighborhood) available to us.
		  Data conflation across multiple modalities of data sources
		  is one of the key challenges in maintaining a geographical
		  information system (GIS) which accumulate data about POIs.
		  Given POI data from nine different sources, and
		  ground-level geo-tagged and scene-captioned images from two
		  different image hosting platforms, in this work we explore
		  the application of graph neural networks (GNNs) to perform
		  data conflation, while leveraging a natural graph structure
		  evident in geospatial data. The preliminary results
		  demonstrate the capacity of a GNN operation to learn
		  distributions of entity (POIs and images) features, coupled
		  with topological structure of entity's local neighborhood
		  in a semantic nearest neighbor graph, in order to predict
		  links between a pair of entities.},
  booktitle	= {Proceedings of the 4th ACM SIGSPATIAL International
		  Workshop on AI for Geographic Knowledge Discovery},
  pages		= {5–8},
  numpages	= {4},
  keywords	= {Areas of Interest (AOI), POI configuration, Points of
		  Interest (POI), data conflation, graph neural network,
		  ground-level imagery, semantic space, word embedding},
  location	= {Beijing, China},
  series	= {GEOAI '21}
}

@Proceedings{	  10.1145/3603765,
  title		= {ICISDM '23: Proceedings of the 2023 7th International
		  Conference on Information System and Data Mining},
  year		= {2023},
  isbn		= {9798400700637},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Atlanta, USA}
}

@Proceedings{	  10.1145/3582768,
  title		= {NLPIR '22: Proceedings of the 2022 6th International
		  Conference on Natural Language Processing and Information
		  Retrieval},
  year		= {2022},
  isbn		= {9781450397629},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bangkok, Thailand}
}

@InProceedings{	  10.1145/3539618.3591740,
  author	= {Zamani, Hamed and Bendersky, Michael},
  title		= {Multivariate Representation Learning for Information
		  Retrieval},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591740},
  doi		= {10.1145/3539618.3591740},
  abstract	= {Dense retrieval models use bi-encoder network
		  architectures for learning query and document
		  representations. These representations are often in the
		  form of a vector representation and their similarities are
		  often computed using the dot product function. In this
		  paper, we propose a new representation learning framework
		  for dense retrieval. Instead of learning a vector for each
		  query and document, our framework learns a multivariate
		  distribution and uses negative multivariate KL divergence
		  to compute the similarity between distributions. For
		  simplicity and efficiency reasons, we assume that the
		  distributions are multivariate normals and then train large
		  language models to produce mean and variance vectors for
		  these distributions. We provide a theoretical foundation
		  for the proposed framework and show that it can be
		  seamlessly integrated into the existing approximate nearest
		  neighbor algorithms to perform retrieval efficiently. We
		  conduct an extensive suite of experiments on a wide range
		  of datasets, and demonstrate significant improvements
		  compared to competitive dense retrieval models.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {163–173},
  numpages	= {11},
  keywords	= {approximate nearest neighbor search, dense retrieval,
		  learning to rank, neural information retrieval},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3543873.3587540,
  author	= {Naik, Riya},
  title		= {Multi-turn mediated solutions for Conversational
		  Artificial Intelligent systems leveraging graph-based
		  techniques},
  year		= {2023},
  isbn		= {9781450394192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543873.3587540},
  doi		= {10.1145/3543873.3587540},
  abstract	= {The current era is dominated by intelligent Question
		  Answering (QA) systems that can instantly answer almost all
		  their questions, saving users search time and increasing
		  the throughput and precision in the applied domain. A vast
		  amount of work is being carried out in QA systems to
		  deliver better content satisfying users’ information
		  needs [2]. Since QA systems are ascending the cycle of
		  emerging technologies, there are potential research gaps
		  that can be explored. QA systems form a significant part of
		  Conversational Artificial Intelligent systems giving rise
		  to a new research pathway, i.e., Conversational Question
		  Answering (CQA) systems [32]. We propose to design and
		  develop a CQA system leveraging Hypergraph-based
		  techniques. The approach focuses on the multi-turn
		  conversation and multi-context to gauge users’ exact
		  information needs and deliver better answers. We further
		  aim to address "supporting evidence-based retrieval" for
		  fact-based responsible answer generation. Since the QA
		  system requires a large amount of data and processing, we
		  also intend to investigate hardware performance for
		  effective system utilization.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2023},
  pages		= {586–590},
  numpages	= {5},
  keywords	= {Contextual Embeddings, Conversational Artificial
		  Intelligence, Evidence-based retrieval, Graph-based models,
		  Question Answering},
  location	= {Austin, TX, USA},
  series	= {WWW '23 Companion}
}

@Article{	  10.1109/tcbb.2022.3161032,
  author	= {Bai, Jun and Yin, Chuantao and Zhang, Jianfei and Wang,
		  Yanmeng and Dong, Yi and Rong, Wenge and Xiong, Zhang},
  title		= {Adversarial Knowledge Distillation Based Biomedical
		  Factoid Question Answering},
  year		= {2022},
  issue_date	= {Jan.-Feb. 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3161032},
  doi		= {10.1109/TCBB.2022.3161032},
  abstract	= {Biomedical factoid question answering is an essential
		  application for biomedical information sharing. Recently,
		  neural network based approaches have shown remarkable
		  performance for this task. However, due to the scarcity of
		  annotated data which requires intensive knowledge of
		  expertise, training a robust model on limited-scale
		  biomedical datasets remains a challenge. Previous works
		  solve this problem by introducing useful knowledge. It is
		  found that the interaction between question and answer
		  (QA-interaction) is also a kind of knowledge which could
		  help extract answer accurately. This research develops a
		  knowledge distillation framework for biomedical factoid
		  question answering, in which a teacher model as the
		  knowledge source of QA-interaction is designed to enhance
		  the student model. In addition, to further alleviate the
		  problem of limited-scale dataset, a novel adversarial
		  knowledge distillation technique is proposed to robustly
		  distill the knowledge from teacher model to student model
		  by constructing perturbed examples as additional training
		  data. By forcing the student model to mimic the predicted
		  distributions of teacher model on both original examples
		  and perturbed examples, the knowledge of QA-interaction can
		  be learned by student model. We evaluate the proposed
		  framework on the widely used BioASQ datasets, and
		  experimental results have shown the proposed method’s
		  promising potential.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= mar,
  pages		= {106–118},
  numpages	= {13}
}

@InProceedings{	  10.1145/3512527.3531401,
  author	= {Geng, Minghao and Zhao, Qingjie},
  title		= {Improve Image Captioning by Modeling Dynamic Scene Graph
		  Extension},
  year		= {2022},
  isbn		= {9781450392389},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3512527.3531401},
  doi		= {10.1145/3512527.3531401},
  abstract	= {Recently, scene graph generation methods have been used in
		  image captioning to encode the objects and their
		  relationships in the encoder-decoder framework, where the
		  decoder selects part of the graph nodes as input for word
		  inference. However, current methods attend to scene graph
		  relying on ambiguous language information, neglecting the
		  strong connections between scene graph nodes. In this
		  paper, we propose a Scene Graph Extension (SGE)
		  architecture to model the dynamic scene graph extension
		  using the partly generated sentence. Our model first uses
		  the generated words and previous attention results of scene
		  graph nodes to make up a partial scene graph. Then we
		  choose objects or relationships that has close connection
		  with the generated graph to infer the next word. Our SGE is
		  appealing in view that it is pluggable to any scene graph
		  based image captioning method. We conduct the extensive
		  experiments on MSCOCO dataset. The results shows that the
		  proposed SGE significantly outperforms the baselines,
		  resulting in a state-of-the-art performance under most
		  metrics.},
  booktitle	= {Proceedings of the 2022 International Conference on
		  Multimedia Retrieval},
  pages		= {398–406},
  numpages	= {9},
  keywords	= {image captioning, image representation, language
		  generation, scene graph},
  location	= {Newark, NJ, USA},
  series	= {ICMR '22}
}

@InProceedings{	  10.1145/3477495.3531742,
  author	= {Plum, Alistair and Ranasinghe, Tharindu and Jones, Spencer
		  and Orasan, Constantin and Mitkov, Ruslan},
  title		= {Biographical Semi-Supervised Relation Extraction Dataset},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531742},
  doi		= {10.1145/3477495.3531742},
  abstract	= {Extracting biographical information from online documents
		  is a popular research topic among the information
		  extraction (IE) community. Various natural language
		  processing (NLP) techniques such as text classification,
		  text summarisation and relation extraction are commonly
		  used to achieve this. Among these techniques, RE is the
		  most common since it can be directly used to build
		  biographical knowledge graphs. RE is usually framed as a
		  supervised machine learning (ML) problem, where ML models
		  are trained on annotated datasets. However, there are few
		  annotated datasets for RE since the annotation process can
		  be costly and time-consuming. To address this, we
		  developedBiographical, the first semi-supervised dataset
		  for RE. The dataset, which is aimed towards digital
		  humanities (DH) and historical research, is automatically
		  compiled by aligning sentences from Wikipedia articles with
		  matching structured data from sources including Pantheon
		  and Wikidata. By exploiting the structure of Wikipedia
		  articles and robust named entity recognition (NER), we
		  match information with relatively high precision in order
		  to compile annotated relation pairs for ten different
		  relations that are important in the DH domain. Furthermore,
		  we demonstrate the effectiveness of the dataset by training
		  a state-of-the-art neural model to classify relation pairs,
		  and evaluate it on a manually annotated gold standard
		  set.Biographical is primarily aimed at training neural
		  models for RE within the domain of digital humanities and
		  history, but as we discuss at the end of this paper, it can
		  be useful for other purposes as well.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3121–3130},
  numpages	= {10},
  keywords	= {biographical information extraction, relation extraction,
		  transformers},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3604915.3608866,
  author	= {Spillo, Giuseppe},
  title		= {Knowledge-Aware Recommender Systems based on Multi-Modal
		  Information Sources},
  year		= {2023},
  isbn		= {9798400702419},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604915.3608866},
  doi		= {10.1145/3604915.3608866},
  abstract	= {The last few years showed a growing interest in the design
		  and development of Knowledge-Aware Recommender Systems
		  (KARSs). This is mainly due to their capability in encoding
		  and exploiting several data sources, both structured (such
		  as knowledge graphs) and unstructured (such as plain text).
		  Nowadays, a lot of models at the state-of-the-art in KARSs
		  use deep learning, enabling them to exploit large amounts
		  of information, including knowledge graphs (KGs), user
		  reviews, plain text, and multimedia content (pictures,
		  audio, videos). In my Ph.D. I will follow this research
		  trend and I will explore and study techniques for designing
		  KARSs leveraging representations learnt from multi-modal
		  information sources, in order to provide users with fair,
		  accurate, and explainable recommendations.},
  booktitle	= {Proceedings of the 17th ACM Conference on Recommender
		  Systems},
  pages		= {1312–1317},
  numpages	= {6},
  keywords	= {graph neural networks, knowledge aware recommender
		  systems, knowledge graphs, multimedia content embedding,
		  word embedding},
  location	= {Singapore, Singapore},
  series	= {RecSys '23}
}

@InProceedings{	  10.1145/3442381.3449991,
  author	= {Yan, Rui and Liao, Weiheng and Cui, Jianwei and Zhang,
		  Hailei and Hu, Yichuan and Zhao, Dongyan},
  title		= {Multilingual COVID-QA: Learning towards Global Information
		  Sharing via Web Question Answering in Multiple Languages},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3449991},
  doi		= {10.1145/3442381.3449991},
  abstract	= {Since late December 2019, it has been reported an outbreak
		  of atypical pneumonia, now known as COVID-19 caused by the
		  novel coronavirus. Cases have spread to more than 200
		  countries and regions internationally. World Health
		  Organization (WHO) officially declares the coronavirus
		  outbreak a pandemic and the public health emergency has
		  caused world-wide impact to daily lives: people are advised
		  to keep social distance, in-person events have been moved
		  online, and some function facilitates have been
		  locked-down. Alternatively, the Web becomes an active venue
		  for people to share information. With respect to the
		  on-going topic, people continuously post questions online
		  and seek for answers. Yet, sharing global information
		  conveyed in different languages is challenging because the
		  language barrier is intrinsically unfriendly to monolingual
		  speakers. In this paper, we propose a multilingual COVID-QA
		  model to answer people’s questions in their own languages
		  while the model is able to absorb knowledge from other
		  languages. Another challenge is that in most cases, the
		  information to share does not have parallel data in
		  multiple languages. To this end, we propose a novel
		  framework which incorporates (unsupervised) translation
		  alignment to learn as pseudo-parallel data. Then we train
		  multilingual question-answering mapping and generation. We
		  demonstrate the effectiveness of our proposed approach
		  compared against a series of competitive baselines. In this
		  way, we make it easier to share global information across
		  the language barriers, and hopefully we contribute to the
		  battle against COVID-19.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {2590–2600},
  numpages	= {11},
  keywords	= {Web question and answering (Web QA), multilingual text
		  generation, response to COVID-19 pandemic},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Proceedings{	  10.1145/3568199,
  title		= {MLMI '22: Proceedings of the 2022 5th International
		  Conference on Machine Learning and Machine Intelligence},
  year		= {2022},
  isbn		= {9781450397551},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Proceedings{	  10.1145/3632314,
  title		= {ISIA '23: Proceedings of the 2023 International Conference
		  on Intelligent Sensing and Industrial Automation},
  year		= {2023},
  isbn		= {9798400709401},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, China}
}

@Article{	  10.1109/taslp.2023.3310879,
  author	= {Chang, Hongyang and Xu, Hongfei and van Genabith, Josef
		  and Xiong, Deyi and Zan, Hongying},
  title		= {JoinER-BART: Joint Entity and Relation Extraction With
		  Constrained Decoding, Representation Reuse and Fusion},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3310879},
  doi		= {10.1109/TASLP.2023.3310879},
  abstract	= {Joint Entity and Relation Extraction (JERE) is an
		  important research direction in Information Extraction
		  (IE). Given the surprising performance with fine-tuning of
		  pre-trained BERT in a wide range of NLP tasks, nowadays
		  most studies for JERE are based on the BERT model. Rather
		  than predicting a simple tag for each word, these
		  approaches are usually forced to design complex tagging
		  schemes, as they may have to extract entity-relation pairs
		  which may overlap with others from the same sequence of
		  word representations in a sentence. Recently,
		  sequence-to-sequence (seq2seq) pre-trained BART models show
		  better performance than BERT models in many NLP tasks.
		  Importantly, a seq2seq BART model can simply generate
		  sequences of (many) entity-relation triplets with its
		  decoder, rather than just tag input words. In this article,
		  we present a new generative JERE framework based on
		  pre-trained BART. Different from the basic seq2seq BART
		  architecture: 1) our framework employs a constrained
		  classifier which only predicts either a token of the input
		  sentence or a relation in each decoding step, and 2) we
		  reuse representations from the pre-trained BART encoder in
		  the classifier instead of a newly trained weight matrix, as
		  this better utilizes the knowledge of the pre-trained model
		  and context-aware representations for classification, and
		  empirically leads to better performance. In our experiments
		  on the widely studied NYT and WebNLG datasets, we show that
		  our approach outperforms previous studies and establishes a
		  new state-of-the-art (92.91 and 91.37 F1 respectively in
		  exact match evaluation).},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {3603–3616},
  numpages	= {14}
}

@InProceedings{	  10.1145/3582935.3583030,
  author	= {Tang, Xilang and Xie, Xiaoyue and Cui, Lijie and Xu, Xiao
		  and Wang, Jianhao},
  title		= {Fault Knowledge Acquisition of Aircraft Based on Event
		  Extraction Technology},
  year		= {2023},
  isbn		= {9781450396806},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3582935.3583030},
  doi		= {10.1145/3582935.3583030},
  abstract	= {In order to make full use of aircraft fault information of
		  total life cycle and construct a knowledge base for
		  intelligent fault diagnosis, this paper preliminarily
		  explored extracting fault knowledge from massive
		  unstructured texts by using the event extraction
		  technology. According to the business requirements of fault
		  diagnosis, we defined the elements of fault events,
		  including trigger words, and fault time, argument roles
		  such fault time, fault occasion, fault unit, signal
		  indicate. To extract above elements, three models,
		  including fault event identification model, trigger
		  extraction model and Argument extraction model is developed
		  in this paper. The results show that this method is
		  effective.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Information Technologies and Electrical Engineering},
  pages		= {566–571},
  numpages	= {6},
  location	= {Changsha, China},
  series	= {ICITEE '22}
}

@Proceedings{	  10.1145/3604915,
  title		= {RecSys '23: Proceedings of the 17th ACM Conference on
		  Recommender Systems},
  year		= {2023},
  isbn		= {9798400702419},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Singapore, Singapore}
}

@InProceedings{	  10.1145/3459637.3482276,
  author	= {Ai, Qingyao and Narayanan.R, Lakshmi},
  title		= {Model-agnostic vs. Model-intrinsic Interpretability for
		  Explainable Product Search},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482276},
  doi		= {10.1145/3459637.3482276},
  abstract	= {Product retrieval systems have served as the main entry
		  for customers to discover and purchase products online.
		  With increasing concerns on the transparency and
		  accountability of AI systems, studies on explainable
		  information retrieval has received more and more attention
		  in the research community. Interestingly, in the domain of
		  e-commerce, despite the extensive studies on explainable
		  product recommendation, the studies of explainable product
		  search is still in an early stage. In this paper, we study
		  how to construct effective explainable product search by
		  comparing model-agnostic explanation paradigms with
		  model-intrinsic paradigms and analyzing the important
		  factors that determine the performance of product search
		  explanations. We propose an explainable product search
		  model with model-intrinsic interpretability and conduct
		  crowdsourcing to compare it with the state-of-the-art
		  explainable product search model with model-agnostic
		  interpretability. We observe that both paradigms have their
		  own advantages and the effectiveness of search explanations
		  on different properties are affected by different factors.
		  For example, explanation fidelity is more important for
		  user's overall satisfaction on the system while explanation
		  novelty may be more useful in attracting user purchases.
		  These findings could have important implications for the
		  future studies and design of explainable product search
		  engines.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {5–15},
  numpages	= {11},
  keywords	= {attention mechanism, product search, search explanation},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.1109/taslp.2023.3278185,
  author	= {Liu, Yuanzhi and He, Min and Yang, Qingqing and Jeon,
		  Gwanggil},
  title		= {An Unsupervised Framework With Attention Mechanism and
		  Embedding Perturbed Encoder for Non-Parallel Text Sentiment
		  Style Transfer},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3278185},
  doi		= {10.1109/TASLP.2023.3278185},
  abstract	= {Text sentiment style transfer aims to extract the
		  sentiment words from a sentence and transfer them into
		  another expected sentiment style while retaining the
		  original sentence's content. However, previous works have
		  not achieved satisfactory performance on the text sentiment
		  style transfer task, especially for non-parallel text. In
		  this article, a novel framework with the attention
		  mechanism and embedding perturbed encoder is proposed to
		  improve the performance of non-parallel text sentiment
		  style transfer. Firstly, the reverse attention mechanism is
		  adopted to disentangle the sentiment style information from
		  the latent representation. And then an embedding perturbed
		  encoder is designed to append an adjustable noise to the
		  embedding space to make the latent representation more
		  semantic. Finally, the attention mechanism is introduced to
		  give different weights for generated words during the
		  decoding process, so that the model can focus on those
		  high-weight words to enhance the quality of sentiment style
		  transfer. Experiments on the corpora of Yelp and IMDB
		  demonstrate that the suggested framework outperforms
		  previous works on the aspects of sentiment style transfer
		  accuracy, content preservation and language fluency.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= may,
  pages		= {2134–2144},
  numpages	= {11}
}

@Proceedings{	  10.1145/3579895,
  title		= {ICNCC '22: Proceedings of the 2022 11th International
		  Conference on Networks, Communication and Computing},
  year		= {2022},
  isbn		= {9781450398039},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@InProceedings{	  10.1145/3442381.3450126,
  author	= {Zhang, Zhihan and Geng, Xiubo and Qin, Tao and Wu, Yunfang
		  and Jiang, Daxin},
  title		= {Knowledge-Aware Procedural Text Understanding with
		  Multi-Stage Training},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450126},
  doi		= {10.1145/3442381.3450126},
  abstract	= {Procedural text describes dynamic state changes during a
		  step-by-step natural process (e.g., photosynthesis). In
		  this work, we focus on the task of procedural text
		  understanding, which aims to comprehend such documents and
		  track entities’ states and locations during a process.
		  Although recent approaches have achieved substantial
		  progress, their results are far behind human performance.
		  Two challenges, the difficulty of commonsense reasoning and
		  data insufficiency, still remain unsolved, which require
		  the incorporation of external knowledge bases. Previous
		  works on external knowledge injection usually rely on noisy
		  web mining tools and heuristic rules with limited
		  applicable scenarios. In this paper, we propose a novel
		  KnOwledge-Aware proceduraL text understAnding (KoaLa)
		  model, which effectively leverages multiple forms of
		  external knowledge in this task. Specifically, we retrieve
		  informative knowledge triples from ConceptNet and perform
		  knowledge-aware reasoning while tracking the entities.
		  Besides, we employ a multi-stage training schema which
		  fine-tunes the BERT model over unlabeled data collected
		  from Wikipedia before further fine-tuning it on the final
		  model. Experimental results on two procedural text
		  datasets, ProPara and Recipes, verify the effectiveness of
		  the proposed methods, in which our model achieves
		  state-of-the-art performance in comparison to various
		  baselines.1},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {3512–3523},
  numpages	= {12},
  keywords	= {Entity Tracking, Knowledge-Aware Reasoning, Multi-Stage
		  Training, Procedural Text Understanding},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@InProceedings{	  10.1145/3539618.3591673,
  author	= {Chen, Xiaolin and Song, Xuemeng and Wei, Yinwei and Nie,
		  Liqiang and Chua, Tat-Seng},
  title		= {Dual Semantic Knowledge Composed Multimodal Dialog
		  Systems},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591673},
  doi		= {10.1145/3539618.3591673},
  abstract	= {Textual response generation is an essential task for
		  multimodal task-oriented dialog systems. Although existing
		  studies have achieved fruitful progress, they still suffer
		  from two critical limitations: 1) focusing on the attribute
		  knowledge but ignoring the relation knowledge that can
		  reveal the correlations between different entities and
		  hence promote the response generation, and 2)only
		  conducting the cross-entropy loss based output-level
		  supervision but lacking the representation-level
		  regularization. To address these limitations, we devise a
		  novel multimodal task-oriented dialog system (named
		  MDS-S2). Specifically, MDS-S2 first simultaneously acquires
		  the context related attribute and relation knowledge from
		  the knowledge base, whereby the non-intuitive relation
		  knowledge is extracted by the n-hop graph walk. Thereafter,
		  considering that the attribute knowledge and relation
		  knowledge can benefit the responding to different levels of
		  questions, we design a multi-level knowledge composition
		  module in MDS-S^2 to obtain the latent composed response
		  representation. Moreover, we devise a set of latent query
		  variables to distill the semantic information from the
		  composed response representation and the ground truth
		  response representation, respectively, and thus conduct the
		  representation-level semantic regularization. Extensive
		  experiments on a public dataset have verified the
		  superiority of our proposed MDS-S2. We have released the
		  codes and parameters to facilitate the research
		  community.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1518–1527},
  numpages	= {10},
  keywords	= {dual semantic knowledge, multimodal task-oriented dialog
		  systems, representation-level regularization},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3584871.3584872,
  author	= {Patel, Manali and Jariwala, Krupa and Chattopadhyay,
		  Chiranjoy},
  title		= {Deep Learning techniques for stock market forecasting:
		  Recent trends and challenges},
  year		= {2023},
  isbn		= {9781450398237},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584871.3584872},
  doi		= {10.1145/3584871.3584872},
  abstract	= {Stock market forecasting has been a very intensive area of
		  research in recent years due to the highly uncertain and
		  volatile nature of stock data which makes this task
		  challenging. By accurately predicting a particular stock's
		  price investors can gain maximum profit out of their
		  investment. With the great success of Deep Learning methods
		  in various domains, it has attracted the research community
		  to apply these models for financial domain also. These DL
		  methods have been proven to achieve better accuracy and
		  predictions compared to econometric and traditional ML
		  methods. This work reviews recent papers according to
		  various Deep Learning models which included: Artificial
		  Neural Networks, Convolution Neural Networks, Sequence to
		  Sequence models, Generative Adversarial Networks, Graph
		  Neural Networks and Transformers applied for stock market
		  forecasting. Furthermore this work also reviews datasets,
		  features, evaluation parameters and results of various
		  methods. From the analysis done on various DL models we
		  found that Graph Neural Networks and Transformer models
		  have potential to interpret dynamic and non-linear patterns
		  of financial time series data with greater accuracy. In
		  addition to this, correlation among various stock indices
		  and investors sentiment along with historical data has
		  great influence on the prediction accuracy. We also
		  identified the benchmark datasets for stock market
		  forecasting based on market capitalization value of an
		  economy. The aim of this paper is to provide insight into
		  most recent work done in the finance domain and identify
		  future directions for more accurate predictions.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Software Engineering and Information Management},
  pages		= {1–11},
  numpages	= {11},
  keywords	= {Corporate relationship, Deep Learning, Graph Neural
		  Networks, Sentiment analysis, Stock market forecasting,
		  Transformers},
  location	= {Palmerston North, New Zealand},
  series	= {ICSIM '23}
}

@InProceedings{	  10.1145/3487553.3524701,
  author	= {Cuffy, Clint and French, Evan and Fehrmann, Sophia and
		  McInnes, Bridget T.},
  title		= {Exploring Representations for Singular and Multi-Concept
		  Relations for Biomedical Named Entity Normalization},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524701},
  doi		= {10.1145/3487553.3524701},
  abstract	= {Since the rise of the COVID-19 pandemic, peer-reviewed
		  biomedical repositories have experienced a surge in
		  chemical and disease related queries. These queries have a
		  wide variety of naming conventions and nomenclatures from
		  trademark and generic, to chemical composition mentions.
		  Normalizing or disambiguating these mentions within texts
		  provides researchers and data-curators with more relevant
		  articles returned by their search query. Named entity
		  normalization aims to automate this disambiguation process
		  by linking entity mentions onto their appropriate candidate
		  concepts within a biomedical knowledge base or ontology. We
		  explore several term embedding aggregation techniques in
		  addition to how the term’s context affects evaluation
		  performance. We also evaluate our embedding approaches for
		  normalizing term instances containing one or many relations
		  within unstructured texts.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {823–832},
  numpages	= {10},
  keywords	= {MeSH identifier, concept linking, concept mapping, concept
		  normalization, concept unique identifier, datasets, entity
		  linking, entity normalization, named entity disambiguation,
		  named entity linking, named entity normalization, neural
		  networks, transformer, word embeddings},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3511808.3557422,
  author	= {Genest, Pierre-Yves and Portier, Pierre-Edouard and
		  Egyed-Zsigmond, El\"{o}d and Goix, Laurent-Walter},
  title		= {PromptORE - A Novel Approach Towards Fully Unsupervised
		  Relation Extraction},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557422},
  doi		= {10.1145/3511808.3557422},
  abstract	= {Unsupervised Relation Extraction (RE) aims to identify
		  relations between entities in text, without having access
		  to labeled data during training. This setting is
		  particularly relevant for domain specific RE where no
		  annotated dataset is available and for open-domain RE where
		  the types of relations are a priori unknown. Although
		  recent approaches achieve promising results, they heavily
		  depend on hyperparameters whose tuning would most often
		  require labeled data. To mitigate the reliance on
		  hyperparameters, we propose PromptORE, a "Prompt-based Open
		  Relation Extraction" model. We adapt the novel
		  prompt-tuning paradigm to work in an unsupervised setting,
		  and use it to embed sentences expressing a relation. We
		  then cluster these embeddings to discover candidate
		  relations, and we experiment different strategies to
		  automatically estimate an adequate number of clusters. To
		  the best of our knowledge, PromptORE is the first
		  unsupervised RE model that does not need hyperparameter
		  tuning. Results on three general and specific domain
		  datasets show that PromptORE consistently outperforms
		  state-of-the-art models with a relative gain of more than
		  40% in B3, V-measure and ARI. Qualitative analysis also
		  indicates PromptORE's ability to identify semantically
		  coherent clusters that are very close to true relations.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {561–571},
  numpages	= {11},
  keywords	= {natural language processing, open relation extraction,
		  prompt-tuning, unsupervised relation extraction},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3523227.3546769,
  author	= {Ning, Lin and Chien, Steve and Song, Shuang and Chen, Mei
		  and Xue, Yunqi and Berlowitz, Devora},
  title		= {EANA: Reducing Privacy Risk on Large-scale Recommendation
		  Models},
  year		= {2022},
  isbn		= {9781450392785},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3523227.3546769},
  doi		= {10.1145/3523227.3546769},
  abstract	= {Embedding-based deep neural networks (DNNs) are widely
		  used in large-scale recommendation systems.
		  Differentially-private stochastic gradient descent (DP-SGD)
		  provides a way to enable personalized experiences while
		  preserving user privacy by injecting noise into every model
		  parameter during the training process. However, it is
		  challenging to apply DP-SGD to large-scale embedding-based
		  DNNs due to its effect on training speed. This happens
		  because the noise added by DP-SGD causes normally sparse
		  gradients to become dense, introducing a large
		  communication overhead between workers and parameter
		  servers in a typical distributed training framework. This
		  paper proposes embedding-aware noise addition (EANA) to
		  mitigate the communication overhead, making training a
		  large-scale embedding-based DNN possible. We examine the
		  privacy benefit of EANA both analytically and empirically
		  using secret sharer techniques. We demonstrate that
		  training with EANA can achieve reasonable model precision
		  while providing good practical privacy protection as
		  measured by the secret sharer tests. Experiments on a
		  real-world, large-scale dataset and model show that EANA is
		  much faster than standard DP-SGD, improving the training
		  speed by 54X and unblocking the training of a large-scale
		  embedding-based DNN with reduced privacy risk.},
  booktitle	= {Proceedings of the 16th ACM Conference on Recommender
		  Systems},
  pages		= {399–407},
  numpages	= {9},
  keywords	= {embedding-based deep neural networks, large-scale,
		  privacy, recommendation system, secret sharer},
  location	= {Seattle, WA, USA},
  series	= {RecSys '22}
}

@InProceedings{	  10.1145/3487664.3487701,
  author	= {Deka, Pritam and Jurek-Loughrey, Anna and Deepak},
  title		= {Unsupervised Keyword Combination Query Generation from
		  Online Health Related Content for Evidence-Based Fact
		  Checking},
  year		= {2022},
  isbn		= {9781450395564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487664.3487701},
  doi		= {10.1145/3487664.3487701},
  abstract	= {False information in the domain of online health related
		  articles is of great concern, which can be witnessed in the
		  current pandemic situation of Covid-19. It is markedly
		  different from fake news in the political context as health
		  information should be evaluated against the most recent and
		  reliable medical resources such as scholarly repositories.
		  However, one of the challenges with such an approach is the
		  retrieval of the pertinent resources. In this work, we
		  formulate a new unsupervised task of generating queries
		  using keywords extracted from a health-related article
		  which can be further applied to retrieve relevant
		  authoritative and reliable medical content from scholarly
		  repositories to assess the article’s veracity. We propose
		  a three-step approach for it and illustrate that our method
		  is able to generate effective queries. We also curate a new
		  dataset to aid the evaluation for this task which will be
		  made available upon request.},
  booktitle	= {The 23rd International Conference on Information
		  Integration and Web Intelligence},
  pages		= {267–277},
  numpages	= {11},
  keywords	= {health misinformation, information retrieval, keyword
		  extraction, query generation},
  location	= {Linz, Austria},
  series	= {iiWAS2021}
}

@InProceedings{	  10.1145/3459637.3482166,
  author	= {F\"{a}rber, Michael and Leisinger, Ann-Kathrin},
  title		= {Recommending Datasets for Scientific Problem
		  Descriptions},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482166},
  doi		= {10.1145/3459637.3482166},
  abstract	= {The steadily rising number of datasets is making it
		  increasingly difficult for researchers and practitioners to
		  be aware of all datasets, particularly of the most relevant
		  datasets for a given research problem. To this end, dataset
		  search engines have been proposed. However, they are based
		  on user's keywords and, thus, have difficulty determining
		  precisely fitting datasets for complex research problems.
		  In this paper, we propose a system that recommends suitable
		  datasets based on a given research problem description. The
		  recommendation task is designed as a domain-specific text
		  classification task. As shown in a comprehensive offline
		  evaluation using various state-of-the-art models, as well
		  as 88,000 paper abstracts and 265,000 citation contexts as
		  research problem descriptions, we obtain an F1-score of
		  0.75. In an additional user study, we show that users in
		  real-world settings are 88% satisfied in all test cases. We
		  therefore see promising future directions for dataset
		  recommendation.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3014–3018},
  numpages	= {5},
  keywords	= {datasets, machine learning, recommendation, text
		  classification},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3460210.3493547,
  author	= {Cadorel, Lucie and Blanchi, Alicia and Tettamanzi, Andrea
		  G. B.},
  title		= {Geospatial Knowledge in Housing Advertisements: Capturing
		  and Extracting Spatial Information from Text},
  year		= {2021},
  isbn		= {9781450384575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460210.3493547},
  doi		= {10.1145/3460210.3493547},
  abstract	= {Information of the geographical and spatial type is found
		  in numerous text documents and constitutes a very
		  challenging target for extraction. Geoparsing applications
		  have been developed to extract geographic terms. However,
		  off-the-shelf Named Entity Recognition (NER) models are
		  mainly designed for Toponym recognition and are very
		  sensitive to language specificity. In this paper, we
		  propose a workflow to first extract geographic and spatial
		  entities based on a BiLSTM-CRF architecture with a
		  concatenation of several text representations. We also
		  propose a Relation Extraction module, particularly aimed at
		  spatial relationships extraction, to build a structured
		  Geospatial knowledge base. We demonstrate our pipeline by
		  applying it to the case of French housing advertisements,
		  which generally provide information about a property's
		  location and neighbourhood. Our results show that the
		  workflow tackles French language and the variability and
		  irregularity of housing advertisements, generalizes
		  Geoparsing to all geographic and spatial terms, and
		  successfully retrieves most of the relationships between
		  entities from the text.},
  booktitle	= {Proceedings of the 11th Knowledge Capture Conference},
  pages		= {41–48},
  numpages	= {8},
  keywords	= {geographical knowledge, information extraction, named
		  entity recognition, relation extraction, text mining},
  location	= {Virtual Event, USA},
  series	= {K-CAP '21}
}

@Article{	  10.1145/3476415.3476428,
  author	= {Metzler, Donald and Tay, Yi and Bahri, Dara and Najork,
		  Marc},
  title		= {Rethinking search: making domain experts out of
		  dilettantes},
  year		= {2021},
  issue_date	= {June 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {1},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3476415.3476428},
  doi		= {10.1145/3476415.3476428},
  abstract	= {When experiencing an information need, users want to
		  engage with a domain expert, but often turn to an
		  information retrieval system, such as a search engine,
		  instead. Classical information retrieval systems do not
		  answer information needs directly, but instead provide
		  references to (hopefully authoritative) answers. Successful
		  question answering systems offer a limited corpus created
		  on-demand by human experts, which is neither timely nor
		  scalable. Pre-trained language models, by contrast, are
		  capable of directly generating prose that may be responsive
		  to an information need, but at present they are dilettantes
		  rather than domain experts - they do not have a true
		  understanding of the world, they are prone to
		  hallucinating, and crucially they are incapable of
		  justifying their utterances by referring to supporting
		  documents in the corpus they were trained over. This paper
		  examines how ideas from classical information retrieval and
		  pre-trained language models can be synthesized and evolved
		  into systems that truly deliver on the promise of domain
		  expert advice.},
  journal	= {SIGIR Forum},
  month		= jul,
  articleno	= {13},
  numpages	= {27}
}

@InProceedings{	  10.1145/3544549.3585755,
  author	= {Kernan Freire, Samuel and Wang, Chaofan and Ruiz-Arenas,
		  Santiago and Niforatos, Evangelos},
  title		= {Tacit Knowledge Elicitation for Shop-floor Workers with an
		  Intelligent Assistant},
  year		= {2023},
  isbn		= {9781450394222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544549.3585755},
  doi		= {10.1145/3544549.3585755},
  abstract	= {Many industries face the challenge of capturing workers’
		  knowledge to share it, particularly tacit knowledge. The
		  operation of complex systems such as a manufacturing line
		  is knowledge-intensive. Considering this knowledge’s
		  breadth and dynamic nature, existing knowledge-sharing
		  solutions are inefficient and resource intensive.
		  Conversational user interfaces are an efficient way to
		  convey information that mimics how humans share knowledge;
		  however, we know little about how to design them
		  specifically for knowledge sharing, especially regarding
		  tacit knowledge. In this work, we present an intelligent
		  assistant that we have developed to support the elicitation
		  of tacit knowledge from workers through systematic
		  reflection. The system can interact with workers by voice
		  or text and generate visualizations of shop floor data to
		  support reflective prompts.},
  booktitle	= {Extended Abstracts of the 2023 CHI Conference on Human
		  Factors in Computing Systems},
  articleno	= {266},
  numpages	= {7},
  keywords	= {chatbots, human-centered AI, industry 5.0, intelligent
		  assistant, knowledge sharing, systematic reflection, tacit
		  knowledge},
  location	= {Hamburg, Germany},
  series	= {CHI EA '23}
}

@Article{	  10.1145/3582900.3582915,
  author	= {Piscopo, Alessandro and Inel, Oana and Vrijenhoek, Sanne
		  and Millecamp, Martijn and Balog, Krisztian},
  title		= {Report on the 1st Workshop on Measuring the Quality of
		  Explanations in Recommender Systems (QUARE 2022) at SIGIR
		  2022},
  year		= {2023},
  issue_date	= {December 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {2},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3582900.3582915},
  doi		= {10.1145/3582900.3582915},
  abstract	= {Explainable recommenders are systems that explain why an
		  item is recommended, in addition to suggesting relevant
		  items to the users of the system. Although explanations are
		  known to be able to significantly affect a user's
		  decision-making process, significant gaps remain concerning
		  methodologies to evaluate them. This hinders
		  cross-comparison between explainable recommendation
		  approaches and is one of the issues hampering the
		  widespread adoption of explanations in industry settings.
		  The goal of QUARE '22 was to promote discussion upon future
		  research and practice directions around evaluation
		  methodologies for explanations in recommender systems. To
		  that end, we brought together researchers and practitioners
		  from academia and industry in a half-day event, co-located
		  with SIGIR 2022. The workshop's program included two
		  keynote talks, three sessions of technical paper
		  presentations in the form of lightning talks followed by
		  panel discussions, and a final plenary discussion session.
		  Although the area of explanations for recommender systems
		  is still in its early stages, QUARE saw the participation
		  of researchers and practitioners from several fields,
		  laying the groundwork for the creation of a community
		  around this topic and indicating promising directions for
		  future research and development.Date: 15 July,
		  2022.Website:
		  https://sites.google.com/view/quare-2022/home.},
  journal	= {SIGIR Forum},
  month		= jan,
  articleno	= {11},
  numpages	= {16}
}

@Article{	  10.1109/taslp.2022.3202123,
  author	= {Hong, Ruixin and Zhang, Hongming and Yu, Xintong and
		  Zhang, Changshui},
  title		= {Learning Event Extraction From a Few Guideline Examples},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3202123},
  doi		= {10.1109/TASLP.2022.3202123},
  abstract	= {Existing fully supervised event extraction models achieve
		  advanced performance with large-scale labeled data.
		  However, when new event types emerge and annotations are
		  scarce, it is hard for the supervised models to master the
		  new types with limited annotations. In contrast, humans can
		  learn to understand new event types with only a few
		  examples in the event extraction guideline. In this paper,
		  we work on a challenging yet more realistic setting, the
		  few-example event extraction. It requires models to learn
		  event extraction with only a few sentences in guidelines as
		  training data, so that we do not need to collect
		  large-scale annotations each time when new event types
		  emerge. As models tend to overfit when trained with only a
		  few examples, we propose knowledge-guided data augmentation
		  to generate valid and diverse sentences from the guideline
		  examples. To help models better leverage the augmented
		  data, we add a consistency regularization to guarantee
		  consistent representations between the augmented sentences
		  and the original ones. Experiments on the standard
		  benchmark ACE-2005 indicate that our method can extract
		  event triggers and arguments effectively with only a few
		  guideline examples.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {2955–2967},
  numpages	= {13}
}

@InProceedings{	  10.1145/3437963.3441781,
  author	= {Su, Lixin and Zhang, Ruqing and Guo, Jiafeng and Fan,
		  Yixing and Chen, Jiangui and Lan, Yanyan and Cheng, Xueqi},
  title		= {Beyond Relevance: Trustworthy Answer Selection via
		  Consensus Verification},
  year		= {2021},
  isbn		= {9781450382977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3437963.3441781},
  doi		= {10.1145/3437963.3441781},
  abstract	= {Community Question Answering (CQA) sites such as Yahoo!
		  Answers and Baidu Knows have emerged as rich knowledge
		  resources for information seekers. However, answers posted
		  to CQA sites often vary a lot in their qualities. User
		  votes from the community may partially reflect the overall
		  quality of the answer, but they are often missing. Hence,
		  automatic selection of "good'' answers becomes a practical
		  research problem that will help us manage the quality of
		  accumulated knowledge. Without loss of generality, a good
		  answer should deliver not only relevant but also
		  trustworthy information that can help resolve the
		  information needs of the posted question, but the latter
		  has received less investigation in the past. In this paper,
		  we propose a novel matching-verification framework for
		  automatic answer selection. The matching component assesses
		  the relevance of a candidate answer to a given question as
		  conventional QA methods. The major enhancement is the
		  verification component, which aims to leverage the wisdom
		  of crowds, e.g., some big information repository, for
		  trustworthiness measurement. Given a question, we take the
		  top retrieved results from the information repository as
		  the supporting evidences to distill the consensus
		  representation. A major challenge is that there is no
		  guarantee that one can always obtain reliable consensus
		  from the wisdom of crowds for a question due to the noisy
		  nature and the limitation of the existing search
		  technology.Therefore, we decompose the trustworthiness
		  measurement into two parts, i.e., a verification score
		  which measures the consistency between a candidate answer
		  and the consensus representation, and a confidence score
		  which measures the reliability of the consensus itself.
		  Empirical studies on three real-world CQA data collections,
		  i.e. YahooQA, QuoraQA and AmazonQA, show that our approach
		  can significantly outperform the state-of-the-art methods
		  on the answer selection task.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {562–570},
  numpages	= {9},
  keywords	= {answer selection, answer verification, trustworthy},
  location	= {Virtual Event, Israel},
  series	= {WSDM '21}
}

@Proceedings{	  10.1145/3625704,
  title		= {ICEMT '23: Proceedings of the 7th International Conference
		  on Education and Multimedia Technology},
  year		= {2023},
  isbn		= {9798400709142},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tokyo, Japan}
}

@InProceedings{	  10.1145/3459637.3481930,
  author	= {Sun, Fu and Li, Feng-Lin and Wang, Ruize and Chen,
		  Qianglong and Cheng, Xingyi and Zhang, Ji},
  title		= {K-AID: Enhancing Pre-trained Language Models with Domain
		  Knowledge for Question Answering},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3481930},
  doi		= {10.1145/3459637.3481930},
  abstract	= {Knowledge enhanced pre-trained language models (K-PLMs)
		  are shown to be effective for many public tasks in the
		  literature, but few of them have been successfully applied
		  in practice. To address this problem, we propose K-AID, a
		  systematic approach that includes a low-cost knowledge
		  acquisition process for acquiring domain knowledge, an
		  effective knowledge infusion module for improving model
		  performance, and a knowledge distillation component for
		  reducing the model size and deploying K-PLMs on
		  resource-restricted devices (e.g., CPU) for real-world
		  application. Importantly, instead of capturing entity
		  knowledge like the majority of existing K-PLMs, our
		  approach captures relational knowledge, which contributes
		  to better improving sentence-level text classification and
		  text matching tasks that play a key role in question
		  answering (QA). We conducted a set of experiments on five
		  text classification tasks and three text matching tasks
		  from three domains, namely E-commerce, Government, and
		  Film&amp;TV, and performed online A/B tests in E-commerce.
		  Experimental results show that our approach is able to
		  achieve substantial improvement on sentence-level question
		  answering tasks and bring beneficial business value in
		  industrial settings.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4125–4134},
  numpages	= {10},
  keywords	= {domain knowledge, knowledge infusion, pre-trained language
		  models, question answering},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Proceedings{	  10.1145/3579654,
  title		= {ACAI '22: Proceedings of the 2022 5th International
		  Conference on Algorithms, Computing and Artificial
		  Intelligence},
  year		= {2022},
  isbn		= {9781450398336},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sanya, China}
}

@InProceedings{	  10.1145/3531146.3533099,
  author	= {Wang, Angelina and Barocas, Solon and Laird, Kristen and
		  Wallach, Hanna},
  title		= {Measuring Representational Harms in Image Captioning},
  year		= {2022},
  isbn		= {9781450393522},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3531146.3533099},
  doi		= {10.1145/3531146.3533099},
  abstract	= {Previous work has largely considered the fairness of image
		  captioning systems through the underspecified lens of
		  “bias.” In contrast, we present a set of techniques for
		  measuring five types of representational harms, as well as
		  the resulting measurements obtained for two of the most
		  popular image captioning datasets using a state-of-the-art
		  image captioning system. Our goal was not to audit this
		  image captioning system, but rather to develop normatively
		  grounded measurement techniques, in turn providing an
		  opportunity to reflect on the many challenges involved. We
		  propose multiple measurement techniques for each type of
		  harm. We argue that by doing so, we are better able to
		  capture the multi-faceted nature of each type of harm, in
		  turn improving the (collective) validity of the resulting
		  measurements. Throughout, we discuss the assumptions
		  underlying our measurement approach and point out when they
		  do not hold.},
  booktitle	= {Proceedings of the 2022 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {324–335},
  numpages	= {12},
  keywords	= {fairness measurement, harm propagation, image captioning},
  location	= {Seoul, Republic of Korea},
  series	= {FAccT '22}
}

@InProceedings{	  10.1145/3534678.3539249,
  author	= {Sun, Mingchen and Zhou, Kaixiong and He, Xin and Wang,
		  Ying and Wang, Xin},
  title		= {GPPT: Graph Pre-training and Prompt Tuning to Generalize
		  Graph Neural Networks},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539249},
  doi		= {10.1145/3534678.3539249},
  abstract	= {Despite the promising representation learning of graph
		  neural networks (GNNs), the supervised training of GNNs
		  notoriously requires large amounts of labeled data from
		  each application. An effective solution is to apply the
		  transfer learning in graph: using easily accessible
		  information to pre-train GNNs, and fine-tuning them to
		  optimize the downstream task with only a few labels.
		  Recently, many efforts have been paid to design the
		  self-supervised pretext tasks, and encode the universal
		  graph knowledge among the various applications. However,
		  they rarely notice the inherent training objective gap
		  between the pretext and downstream tasks. This significant
		  gap often requires costly fine-tuning for adapting the
		  pre-trained model to downstream problem, which prevents the
		  efficient elicitation of pre-trained knowledge and then
		  results in poor results. Even worse, the naive pre-training
		  strategy usually deteriorates the downstream task, and
		  damages the reliability of transfer learning in graph data.
		  To bridge the task gap, we propose a novel transfer
		  learning paradigm to generalize GNNs, namely graph
		  pre-training and prompt tuning (GPPT). Specifically, we
		  first adopt the masked edge prediction, the most simplest
		  and popular pretext task, to pre-train GNNs. Based on the
		  pre-trained model, we propose the graph prompting function
		  to modify the standalone node into a token pair, and
		  reformulate the downstream node classification looking the
		  same as edge prediction. The token pair is consisted of
		  candidate label class and node entity. Therefore, the
		  pre-trained GNNs could be applied without tedious
		  fine-tuning to evaluate the linking probability of token
		  pair, and produce the node classification decision. The
		  extensive experiments on eight benchmark datasets
		  demonstrate the superiority of GPPT, delivering an average
		  improvement of 4.29% in few-shot graph analysis and
		  accelerating the model convergence up to 4.32X. The code is
		  available in: https://github.com/MingChen-Sun/GPPT.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1717–1727},
  numpages	= {11},
  keywords	= {graph neural networks, pre-training, prompt tuning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3565387.3565396,
  author	= {Chen, Yang and Xu, Chunyan and Zhang, Tong and Li,
		  Guangyu},
  title		= {Complementary Random Walk: A New Perspective on Graph
		  Embedding},
  year		= {2022},
  isbn		= {9781450396004},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3565387.3565396},
  doi		= {10.1145/3565387.3565396},
  abstract	= {Random-walk based graph embedding algorithms like DeepWalk
		  and Node2Vec are widely used to learn distinguishable
		  representations of the nodes in a network. These methods
		  treat different walks starting from every node as sentences
		  in language to learn latent representations. However, nodes
		  in a unique walking sequence often appear repeatedly. This
		  situation results in the latent representations obtained by
		  the aforementioned algorithms cannot capture the
		  relationship between unconnected nodes, which have similar
		  node features and graph topology structures. In this paper,
		  we propose Complementary Random Walk (CRW) to solve this
		  problem and embed the nodes in a network to obtain more
		  robust low-dimensional vectors. By conducting a K-means
		  clustering algorithm to cluster different features
		  extracted from the graph, we can supply the original random
		  walk with many other walking sequences, which consist of
		  different unconnected nodes. And those nodes are sampled
		  from the same cluster based on graph features, such as node
		  degree, motif features, and so on. Our experiments achieve
		  comparable or superior performance compared with other
		  methods, validating the effectiveness of CRW.},
  booktitle	= {Proceedings of the 6th International Conference on
		  Computer Science and Application Engineering},
  articleno	= {9},
  numpages	= {5},
  keywords	= {Clustering, Graph Representation Learning, Random Walk},
  location	= {Virtual Event, China},
  series	= {CSAE '22}
}

@InProceedings{	  10.1145/3584371.3613001,
  author	= {Wang, Zifeng and Xiao, Cao and Sun, Jimeng},
  title		= {SPOT: Sequential Predictive Modeling of Clinical Trial
		  Outcome with Meta-Learning},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3613001},
  doi		= {10.1145/3584371.3613001},
  abstract	= {Clinical trials are essential to drug development but
		  time-consuming, costly, and prone to failure. Accurate
		  trial outcome prediction based on historical trial data
		  promises better trial investment decisions and more trial
		  success. Existing trial outcome prediction models were not
		  designed to model the relations among similar trials,
		  capture the progression of features and designs of similar
		  trials, or address the skewness of trial data which causes
		  inferior performance for less common trials.To fill the gap
		  and provide accurate trial outcome prediction, we propose
		  Sequential Predictive mOdeling of clinical Trial outcome
		  (SPOT) that first identifies trial topics to cluster the
		  multisourced trial data into relevant trial topics. It then
		  generates trial embeddings and organizes them by topic and
		  time to create clinical trial sequences. With the
		  consideration of each trial sequence as a task, it uses a
		  meta-learning strategy to achieve a point where the model
		  can rapidly adapt to new tasks with minimal updates. In
		  particular, the topic discovery module enables a deeper
		  understanding of the underlying structure of the data,
		  while sequential learning captures the evolution of trial
		  designs and outcomes. This results in predictions that are
		  not only more accurate but also more interpretable, taking
		  into account the temporal patterns and unique
		  characteristics of each trial topic. We demonstrate that
		  SPOT wins over the prior methods by a significant margin on
		  trial outcome benchmark data: with a 21.5% lift on phase I,
		  an 8.9% lift on phase II, and a 5.5% lift on phase III
		  trials in the metric of the area under precision-recall
		  curve (PR-AUC). Code is available at
		  https://github.com/RyanWangZf/PyTrial.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {53},
  numpages	= {11},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1145/3584376.3584478,
  author	= {Ge, Junwei and Qin, Zhixiang and Fang, Yiqiu},
  title		= {A Document-level Event Extraction Method Based on
		  Mogrifier LSTM},
  year		= {2023},
  isbn		= {9781450398343},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584376.3584478},
  doi		= {10.1145/3584376.3584478},
  abstract	= {Element dispersion is a difficulty for document-level
		  event extraction. While classic LSTM lacks the capability
		  to interact between input and context while collecting long
		  sequence features, previous document-level event extraction
		  utilizes the entire document as input, leaving the sequence
		  features of the document devoid of deeper contextual
		  information. This paper suggests a document-level event
		  extraction strategy based on Mogrifier LSTM to solve this
		  issue. We divide the text into multiple paragraphs and then
		  input each paragraph individually into the Mogrifier LSTM.
		  To increase the context modeling capability of lengthy
		  sequence text, the upgraded LSTM will allow the input of
		  the present moment and the output of the preceding moment
		  to be computed several times initially. Then an attention
		  mechanism is introduced to capture the internal correlation
		  of each paragraph and integrate the contextual semantics of
		  each paragraph. Finally, sequence annotation is used to
		  extract dispersed event elements and match event types.
		  According to the experimental results on Chinese financial
		  dataset, the method in this paper can effectively solve the
		  problems of loss of depth information and scattered
		  theoretical elements of long sequence features of
		  documents, and improve the effectiveness of document-level
		  event extraction.},
  booktitle	= {Proceedings of the 2022 4th International Conference on
		  Robotics, Intelligent Control and Artificial Intelligence},
  pages		= {571–576},
  numpages	= {6},
  location	= {Dongguan, China},
  series	= {RICAI '22}
}

@InProceedings{	  10.1145/3511808.3557198,
  author	= {Balalau, Oana and Ebel, Simon and Galizzi, Th\'{e}o and
		  Manolescu, Ioana and Massonnat, Quentin and Deiana, Antoine
		  and Gautreau, Emilie and Krempf, Antoine and Pontillon,
		  Thomas and Roux, G\'{e}rald and Yakin, Joanna},
  title		= {Statistical Claim Checking: StatCheck in Action},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557198},
  doi		= {10.1145/3511808.3557198},
  abstract	= {To strengthen public trust and counter disinformation,
		  computational fact-checking, leveraging digital data
		  sources, attracts interest from the journalists and the
		  computer science community. A particular class of
		  interesting data sources is statistics, that is, numerical
		  data compiled mostly by governments, administrations, and
		  international organizations. Statistics typically are
		  multidimensional datasets, where multiple dimensions
		  characterize one value, and the dimensions may be organized
		  in a hierarchy.We developed StatCheck, a fact-checking
		  system specialized in French. The technical novelty of
		  StatCheck is twofold: (i) we focus on multidimensional,
		  complex-structure statistics, which have received little
		  attention so far, despite their practical importance; and
		  (ii) novel statistical claim extraction modules for French,
		  an area where few resources exist. We will demonstrate our
		  system on large statistic datasets (hundreds of millions of
		  facts), including the complete INSEE (French) and Eurostat
		  (European Union) datasets.More information about
		  StatCheckis available online at:
		  https://team.inria.fr/cedar/projects/statcheck/.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4798–4802},
  numpages	= {5},
  keywords	= {data warehouses, fact-checking, multidimensional data,
		  natural language processing},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3583780.3615101,
  author	= {Zhang, Xiyuan and Chowdhury, Ranak Roy and Zhang, Jiayun
		  and Hong, Dezhi and Gupta, Rajesh K. and Shang, Jingbo},
  title		= {Unleashing the Power of Shared Label Structures for Human
		  Activity Recognition},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615101},
  doi		= {10.1145/3583780.3615101},
  abstract	= {Current human activity recognition (HAR) techniques regard
		  activity labels as integer class IDs without explicitly
		  modeling the semantics of class labels. We observe that
		  different activity names often have shared structures. For
		  example, "open door" and "open fridge" both have "open" as
		  the action; "kicking soccer ball" and "playing tennis ball"
		  both have "ball" as the object. Such shared structures in
		  label names can be translated to the similarity in sensory
		  data and modeling common structures would help uncover
		  knowledge across different activities, especially for
		  activities with limited samples. In this paper, we propose
		  SHARE, a HAR framework that takes into account shared
		  structures of label names for different activities. To
		  exploit the shared structures, SHARE comprises an encoder
		  for extracting features from input sensory time series and
		  a decoder for generating label names as a token sequence.
		  We also propose three label augmentation techniques to help
		  the model more effectively capture semantic structures
		  across activities, including a basic token-level
		  augmentation, and two enhanced embedding-level and
		  sequence-level augmentations utilizing the capabilities of
		  pre-trained models. SHARE outperforms state-of-the-art HAR
		  models in extensive experiments on seven HAR benchmark
		  datasets. We also evaluate in few-shot learning and label
		  imbalance settings and observe even more significant
		  performance gap.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3340–3350},
  numpages	= {11},
  keywords	= {human activity recognition, label name semantics, natural
		  language processing, time series classification},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3603607.3613481,
  author	= {Atzenbeck, Claus and Brooker, Sam and Ro\ss{}ner, Daniel},
  title		= {Storytelling Machines},
  year		= {2023},
  isbn		= {9798400702396},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603607.3613481},
  doi		= {10.1145/3603607.3613481},
  abstract	= {We are entering a period of unprecedented collaboration
		  between authors and computers, where artificial
		  intelligence in particular seems likely to act increasingly
		  in a co-authoring capacity. Automated or procedural
		  storytelling represents one exciting avenue of research. By
		  entering prompts and parameters into an AI text generator
		  like ChatGPT, authors could leverage an enormous textual
		  corpus to generate a “new” work that appears to have
		  been authored by a human. This paper proposes an
		  alternative platform, one more reflective of the
		  collaborative and organic creative process. Approached as a
		  tool for augmentation, Mother showcases the potential for
		  spatial hypertext to work alongside the author.},
  booktitle	= {Proceedings of the 6th Workshop on Human Factors in
		  Hypertext},
  articleno	= {4},
  numpages	= {9},
  keywords	= {Mother, education, hypertext, linguistics, recommender
		  system, spatial hypertext, storytelling, tropes},
  location	= {Rome, Italy},
  series	= {HUMAN '23}
}

@Article{	  10.1145/3543826,
  author	= {Demir, Seniz},
  title		= {Turkish Data-to-Text Generation Using Sequence-to-Sequence
		  Neural Networks},
  year		= {2022},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3543826},
  doi		= {10.1145/3543826},
  abstract	= {End-to-end data-driven approaches lead to rapid
		  development of language generation and dialogue systems.
		  Despite the need for large amounts of well-organized data,
		  these approaches jointly learn multiple components of the
		  traditional generation pipeline without requiring costly
		  human intervention. End-to-end approaches also enable the
		  use of loosely aligned parallel datasets in system
		  development by relaxing the degree of semantic
		  correspondences between training data representations and
		  text spans. However, their potential in Turkish language
		  generation has not yet been fully exploited. In this work,
		  we apply sequence-to-sequence (Seq2Seq) neural models to
		  Turkish data-to-text generation where the input data given
		  in the form of a meaning representation is verbalized. We
		  explore encoder-decoder architectures with attention
		  mechanism in unidirectional, bidirectional, and stacked
		  recurrent neural network (RNN) models. Our models generate
		  one-sentence biographies and dining venue descriptions
		  using a crowdsourced dataset where all field value pairs
		  that appear in meaning representations are fully captured
		  in reference sentences. To support this work, we also
		  explore the performances of our models on a more
		  challenging dataset, where the content of a meaning
		  representation is too large to fit into a single sentence,
		  and hence content selection and surface realization need to
		  be learned jointly. This dataset is retrieved by coupling
		  introductory sentences of person-related Turkish Wikipedia
		  articles with their contained infobox tables. Our empirical
		  experiments on both datasets demonstrate that Seq2Seq
		  models are capable of generating coherent and fluent
		  biographies and venue descriptions from field value pairs.
		  We argue that the wealth of knowledge residing in our
		  datasets and the insights obtained from this study hold the
		  potential to give rise to the development of new end-to-end
		  generation approaches for Turkish and other morphologically
		  rich languages.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= dec,
  articleno	= {37},
  numpages	= {27},
  keywords	= {Data-to-text generation, sequence-to-sequence model,
		  Turkish, Wikipedia}
}

@InProceedings{	  10.1145/3485447.3511949,
  author	= {Fan, Lu and Li, Qimai and Liu, Bo and Wu, Xiao-Ming and
		  Zhang, Xiaotong and Lv, Fuyu and Lin, Guli and Li, Sen and
		  Jin, Taiwei and Yang, Keping},
  title		= {Modeling User Behavior with Graph Convolution for
		  Personalized Product Search},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511949},
  doi		= {10.1145/3485447.3511949},
  abstract	= {User preference modeling is a vital yet challenging
		  problem in personalized product search. In recent years,
		  latent space based methods have achieved state-of-the-art
		  performance by jointly learning semantic representations of
		  products, users, and text tokens. However, existing methods
		  are limited in their ability to model user preferences.
		  They typically represent users by the products they visited
		  in a short span of time using attentive models and lack the
		  ability to exploit relational information such as
		  user-product interactions or item co-occurrence relations.
		  In this work, we propose to address the limitations of
		  prior arts by exploring local and global user behavior
		  patterns on a user successive behavior graph, which is
		  constructed by utilizing short-term actions of all users.
		  To capture implicit user preference signals and
		  collaborative patterns, we use an efficient jumping graph
		  convolution to explore high-order relations to enrich
		  product representations for user preference modeling. Our
		  approach can be seamlessly integrated with existing latent
		  space based methods and be potentially applied in any
		  product retrieval method that uses purchase history to
		  model user preferences. Extensive experiments on eight
		  Amazon benchmarks demonstrate the effectiveness and
		  potential of our approach. The source code is available at
		  https://github.com/floatSDSDS/SBG .},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {203–212},
  numpages	= {10},
  keywords	= {Graph Convolution, Personalized Product Search, User
		  Preference Modeling},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3586183.3606715,
  author	= {Zaidi, Ali and Turbeville, Kelsey and Ivan\v{c}i\'{c},
		  Kristijan and Moss, Jason and Gutierrez Villalobos, Jenny
		  and Sagar, Aravind and Li, Huiying and Mehra, Charu and Li,
		  Sixuan and Hutchins, Scott and Kumar, Ranjitha},
  title		= {Learning Custom Experience Ontologies via Embedding-based
		  Feedback Loops},
  year		= {2023},
  isbn		= {9798400701320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3586183.3606715},
  doi		= {10.1145/3586183.3606715},
  abstract	= {Organizations increasingly rely on behavioral analytics
		  tools like Google Analytics to monitor their digital
		  experiences. Making sense of the data these tools capture,
		  however, requires manual event tagging and filtering —
		  often a tedious process. Prior approaches have trained
		  machine learning models to automatically tag interaction
		  data, but draw from fixed digital experience vocabularies
		  which cannot be easily augmented or customized. This paper
		  introduces a novel machine learning interaction pattern
		  that generates customized tag predictions for
		  organizations. The approach employs a general user
		  experience word embedding to bootstrap an initial set of
		  predictions, which can then be refined and customized by
		  users to adapt the underlying vector space, iteratively
		  improving the quality of future predictions. The paper
		  presents a needfinding study that grounds the design
		  choices of the system, and describes a real-world
		  deployment as part of UserTesting.com that demonstrates the
		  efficacy of the approach.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {111},
  numpages	= {13},
  keywords	= {Sankey diagrams, UX research, clickstream analytics,
		  sequence alignment, usability testing},
  location	= {San Francisco, CA, USA},
  series	= {UIST '23}
}

@InProceedings{	  10.1145/3477495.3531678,
  author	= {Dong, Yuyang and Oyamada, Masafumi},
  title		= {Table Enrichment System for Machine Learning},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531678},
  doi		= {10.1145/3477495.3531678},
  abstract	= {Data scientists are constantly facing the problem of how
		  to improve prediction accuracy with insufficient tabular
		  data. We propose a table enrichment system that enriches a
		  query table by adding external attributes (columns) from
		  data lakes and improves the accuracy of machine learning
		  predictive models. Our system has four stages, join row
		  search, task-related table selection, row and column
		  alignment, and feature selection and evaluation, to
		  efficiently create an enriched table for a given query
		  table and a specified machine learning task. We demonstrate
		  our system with a web UI to show the use cases of table
		  enrichment.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3267–3271},
  numpages	= {5},
  keywords	= {machine learning, table augmentation, table discovery,
		  table enrichment},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3511808.3557313,
  author	= {Xu, Tianyu and Hua, Wen and Qu, Jianfeng and Li, Zhixu and
		  Xu, Jiajie and Liu, An and Zhao, Lei},
  title		= {Evidence-aware Document-level Relation Extraction},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557313},
  doi		= {10.1145/3511808.3557313},
  abstract	= {Document-level Relation Extraction (RE) is a promising
		  task aiming at identifying relations of multiple entity
		  pairs in a document. However, in most cases, a relational
		  fact can be expressed enough via a small subset of
		  sentences from the document, namely evidence sentence.
		  Moreover, there often exist strong semantic correlations
		  between evidence sentences that collaborate together to
		  describe a specific relation. To address these challenges,
		  we propose a novel evidence-aware model for document-level
		  RE. Particularly, we formulate evidence sentence selection
		  as a sequential decision problem through a crafted
		  reinforcement learning mechanism. Considering the explosive
		  search space of our agent, an efficient path searching
		  strategy is executed on the converted document graph to
		  heuristically obtain hopeful sentences and feed them to
		  reinforcement learning. Finally, each entity pair owns a
		  customized-filtered document for further inferring the
		  relation between them. We conduct various experiments on
		  two document-level RE benchmarks and achieve a remarkable
		  improvement over previous competitive baselines, verifying
		  the effectiveness of our method.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2311–2320},
  numpages	= {10},
  keywords	= {document-level relation extraction, evidence extraction,
		  reinforcement learning},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3411764.3445368,
  author	= {Pinhanez, Claudio Santos and Candello, Heloisa and
		  Cavalin, Paulo and Pichiliani, Mauro Carlos and Appel, Ana
		  Paula and Alves Ribeiro, Victor Henrique and Nogima, Julio
		  and de Bayser, Maira and Guerra, Melina and Ferreira,
		  Henrique and Malfatti, Gabriel},
  title		= {Integrating Machine Learning Data with Symbolic Knowledge
		  from Collaboration Practices of Curators to Improve
		  Conversational Systems},
  year		= {2021},
  isbn		= {9781450380966},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3411764.3445368},
  doi		= {10.1145/3411764.3445368},
  abstract	= {This paper describes how machine learning training data
		  and symbolic knowledge from curators of conversational
		  systems can be used together to improve the accuracy of
		  those systems and to enable better curatorial tools. This
		  is done in the context of a real-world practice of curators
		  of conversational systems who often embed
		  taxonomically-structured meta-knowledge into their
		  documentation. The paper provides evidence that the
		  practice is quite common among curators, that is used as
		  part of their collaborative practices, and that the
		  embedded knowledge can be mined by algorithms. Further,
		  this meta-knowledge can be integrated, using neuro-symbolic
		  algorithms, to the machine learning-based conversational
		  system, to improve its run-time accuracy and to enable
		  tools to support curatorial tasks. Those results point
		  towards new ways of designing development tools which
		  explore an integrated use of code and documentation by
		  machines.},
  booktitle	= {Proceedings of the 2021 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {104},
  numpages	= {13},
  keywords	= {Conversational Systems, Curatorial Practices.,
		  Documentation, Neuro-symbolic Systems},
  location	= {Yokohama, Japan},
  series	= {CHI '21}
}

@InProceedings{	  10.1145/3580305.3599375,
  author	= {Liu, Jiayu and Huang, Zhenya and Ma, Zhiyuan and Liu, Qi
		  and Chen, Enhong and Su, Tianhuang and Liu, Haifeng},
  title		= {Guiding Mathematical Reasoning via Mastering Commonsense
		  Formula Knowledge},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599375},
  doi		= {10.1145/3580305.3599375},
  abstract	= {Math formulas (e.g., "distance = speed X time'') serve as
		  one of the fundamental commonsense knowledge in human
		  cognition, where humans naturally acquire and manipulate
		  them in logical thinking for mathematical reasoning
		  problems. However, existing reasoning models mainly focus
		  on learning heuristic linguistics or patterns to generate
		  answers, but do not pay enough attention on learning with
		  such formula knowledge. Thus, they are not transparent
		  (thus uninterpretable) in terms of understanding and
		  grasping basic mathematical logic. In this paper, to
		  promote a step forward in the domain, we first construct
		  two datasets (Math23K-F and MAWPS-F) with precise
		  annotations of formula usage in each reasoning step for
		  math word problems. Especially, our datasets are refined on
		  the benchmark datasets, and thus ensure the generality and
		  comparability for relevant research. Then, we propose a
		  novel Formula-mastered Solver (FOMAS) with the guidance of
		  mastering formula knowledge to solve the problems.
		  Specifically, we establish FOMAS with two systems drawing
		  insight from the dual process theory, including a Knowledge
		  System and a Reasoning System, to learn and apply formula
		  knowledge, respectively. The Knowledge System accumulates
		  the math formulas, where we propose a novel pretraining
		  manner to mimic how humans grasp the mathematical logic
		  behind them. Then, in the Reasoning System, we develop
		  elaborate formula-guided symbol prediction and goal
		  generation methods that retrieve the necessary formula
		  knowledge from Knowledge System to improve both reasoning
		  accuracy and interpretability. It organically simulates how
		  humans conduct complex reasoning under the explicit
		  instruction of math formulas. Experimental results prove
		  that FOMAS has a stronger reasoning ability and achieves a
		  more interpretable reasoning process, which verifies the
		  necessity of introducing formula knowledge transparently.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1477–1488},
  numpages	= {12},
  keywords	= {knowledge representation and reasoning, math word
		  problem},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@Article{	  10.1109/taslp.2023.3331149,
  author	= {Zhu, Tiantian and Qin, Yang and Feng, Ming and Chen,
		  Qingcai and Hu, Baotian and Xiang, Yang},
  title		= {BioPRO: Context-Infused Prompt Learning for Biomedical
		  Entity Linking},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3331149},
  doi		= {10.1109/TASLP.2023.3331149},
  abstract	= {Recent research tends to address the biomedical entity
		  linking problem in a unified framework solely based on
		  surface form matching between mentions and entities.
		  Specifically, these methods focus on addressing the
		  &lt;italic&gt;variety&lt;/italic&gt; challenge of the
		  heterogeneous naming of biomedical concepts. Yet, the
		  &lt;italic&gt;ambiguity&lt;/italic&gt; challenge that the
		  same word under different contexts can be used to refer to
		  distinct concepts is usually ignored. To address this
		  challenge, we propose BioPRO, a two-stage entity linking
		  algorithm to enhance the biomedical entity representations
		  based on context-infused prompt learning. The first stage
		  includes a coarse-grained retrieval from a representation
		  space defined by a bi-encoder that independently embeds the
		  mention and entity's surface forms. Unlike previous
		  one-model-fits-all systems, each candidate is then
		  re-ranked with a fine-grained encoder based on
		  prompt-tuning that sufficiently stimulates knowledge in
		  contextual information of mentions and entities.
		  Furthermore, the trained fine-grained encoder can be
		  utilized to generate deep representations of bio-entities
		  and boost candidate retrieval in the first stage. Extensive
		  experiments show that our model achieves promising
		  performance improvements compared with several
		  state-of-the-art (SOTA) techniques on 4 biomedical corpora.
		  We also observe by cases that the proposed context-infused
		  prompt-tuning strategy is effective in solving both the
		  &lt;italic&gt;variety&lt;/italic&gt; and
		  &lt;italic&gt;ambiguity&lt;/italic&gt; challenges in the
		  linking task.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {374–385},
  numpages	= {12}
}

@Article{	  10.1109/tcbb.2022.3157630,
  author	= {Chai, Zhaoying and Jin, Han and Shi, Shenghui and Zhan,
		  Siyan and Zhuo, Lin and Yang, Yu and Lian, Qi},
  title		= {Noise Reduction Learning Based on XLNet-CRF for Biomedical
		  Named Entity Recognition},
  year		= {2022},
  issue_date	= {Jan.-Feb. 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3157630},
  doi		= {10.1109/TCBB.2022.3157630},
  abstract	= {In recent years, Biomedical Named Entity Recognition
		  (BioNER) systems have mainly been based on deep neural
		  networks, which are used to extract information from the
		  rapidly expanding biomedical literature. Long-distance
		  context autoencoding language models based on transformers
		  have recently been employed for BioNER with great success.
		  However, noise interference exists in the process of
		  pre-training and fine-tuning, and there is no effective
		  decoder for label dependency. Current models have many
		  aspects in need of improvement for better performance. We
		  propose two kinds of noise reduction models, Shared Labels
		  and Dynamic Splicing, based on XLNet encoding which is a
		  permutation language pre-training model and decoding by
		  Conditional Random Field (CRF). By testing 15 biomedical
		  named entity recognition datasets, the two models improved
		  the average F1-score by 1.504 and 1.48, respectively, and
		  state-of-the-art performance was achieved on 7 of them.
		  Further analysis proves the effectiveness of the two models
		  and the improvement of the recognition effect of CRF, and
		  suggests the applicable scope of the models according to
		  different data characteristics.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= mar,
  pages		= {595–605},
  numpages	= {11}
}

@InProceedings{	  10.1145/3534678.3539215,
  author	= {Chen, Changyu and Wang, Xiting and Yi, Xiaoyuan and Wu,
		  Fangzhao and Xie, Xing and Yan, Rui},
  title		= {Personalized Chit-Chat Generation for Recommendation Using
		  External Chat Corpora},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539215},
  doi		= {10.1145/3534678.3539215},
  abstract	= {Chit-chat has been shown effective in engaging users in
		  human-computer interaction. We find with a user study that
		  generating appropriate chit-chat for news articles can help
		  expand user interest and increase the probability that a
		  user reads a recommended news article. Based on this
		  observation, we propose a method to generate personalized
		  chit-chat for news recommendation. Different from existing
		  methods for personalized text generation, our method only
		  requires an external chat corpus obtained from an online
		  forum, which can be disconnected from the recommendation
		  dataset from both the user and item (news) perspectives.
		  This is achieved by designing a weak supervision method for
		  estimating users' personalized interest in a chit-chat post
		  by transferring knowledge learned by a news recommendation
		  model. Based on the method for estimating user interest, a
		  reinforcement learning framework is proposed to generate
		  personalized chit-chat. Extensive experiments, including
		  the automatic offline evaluation and user studies,
		  demonstrate the effectiveness of our method.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2721–2731},
  numpages	= {11},
  keywords	= {chit-chat, news recommendation, personalized text
		  generation, reinforcement learning},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3523286.3524517,
  author	= {Zhang, Sizhou and Chen, Zhihong and Liu, Dejian and Lv,
		  Qing},
  title		= {Building Structured Patient Follow-up Records from Chinese
		  Medical Records via Deep Learning},
  year		= {2022},
  isbn		= {9781450395755},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3523286.3524517},
  doi		= {10.1145/3523286.3524517},
  abstract	= {Employing deep learning (DL) method to process and analyze
		  Chinese medical records to build patient follow-up records
		  (PFRs) has been a very valuable task. In recent years, the
		  identification and classification of clinical terms in
		  electronic medical records has received increased
		  attention. However, electronic medical records are
		  difficult to access because of their exceedingly high
		  privacy, so it has become more feasible to extract
		  information from paper medical records. This study proposed
		  a DL approach that extract text information from the
		  pre-processed images of Chinese medical records by optical
		  character recognition (OCR) model base on CRNN first, and
		  then identify the clinical entities using named entity
		  recognition (NER) model based on BERT-CRF. The experimental
		  results of this study demonstrate that the proposed method
		  achieves precision over 75%, which is more than 90% for
		  some specific entities. In addition, the proposed method
		  can be extended as a universal approach to other diseases
		  that require the establishment of the structured patient
		  follow-up records (PFRs).},
  booktitle	= {Proceedings of the 2022 2nd International Conference on
		  Bioinformatics and Intelligent Computing},
  pages		= {65–71},
  numpages	= {7},
  keywords	= {Named Entity Recognition, Optical Character Recognition,
		  Patient Follow-up Records},
  location	= {Harbin, China},
  series	= {BIC '22}
}

@InProceedings{	  10.1145/3594536.3595168,
  author	= {Habba, Eliya and Keydar, Renana and Bareket, Dan and
		  Stanovsky, Gabriel},
  title		= {The Perfect Victim: Computational Analysis of Judicial
		  Attitudes towards Victims of Sexual Violence},
  year		= {2023},
  isbn		= {9798400701979},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3594536.3595168},
  doi		= {10.1145/3594536.3595168},
  abstract	= {We develop computational models to analyze court
		  statements in order to assess judicial attitudes toward
		  victims of sexual violence in the Israeli court system. The
		  study examines the resonance of "rape myths" in the
		  criminal justice system's response to sex crimes, in
		  particular in judicial assessment of victim's credibility.
		  We begin by formulating an ontology for evaluating judicial
		  attitudes toward victim's credibility, with eight ordinal
		  labels and binary categorizations. Second, we curate a
		  manually annotated dataset for judicial assessments of
		  victim's credibility in the Hebrew language, as well as a
		  model that can extract credibility labels from court cases.
		  The dataset consists of 855 verdict decision documents in
		  sexual assault cases from 1990-2021, annotated with the
		  help of legal experts and trained law students. The model
		  uses a combined approach of syntactic and latent structures
		  to find sentences that convey the judge's attitude towards
		  the victim and classify them according to the credibility
		  label set. Our ontology, data, and models will be made
		  available upon request, in the hope they spur future
		  progress in this judicial important task.},
  booktitle	= {Proceedings of the Nineteenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {111–120},
  numpages	= {10},
  keywords	= {Judicial decision making, Rape myths, Sexual violence,
		  Witness credibility},
  location	= {Braga, Portugal},
  series	= {ICAIL '23}
}

@InProceedings{	  10.1145/3459637.3482401,
  author	= {Shi, Shaoyun and Ma, Weizhi and Wang, Zhen and Zhang, Min
		  and Fang, Kun and Xu, Jingfang and Liu, Yiqun and Ma,
		  Shaoping},
  title		= {WG4Rec: Modeling Textual Content with Word Graph for News
		  Recommendation},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482401},
  doi		= {10.1145/3459637.3482401},
  abstract	= {News recommendation plays an indispensable role in
		  acquiring daily news for users. Previous studies make great
		  efforts to model high-order feature interactions between
		  users and items, where various neural models are applied
		  (e.g., RNN, GNN). However, we find that seldom efforts are
		  made to get better representations for news. Most previous
		  methods simply adopt pre-trained word embeddings to
		  represent news and also suffer from cold-start users. In
		  this work, we propose a new textual content representation
		  method by building a word graph for recommendation, which
		  is named WG4Rec. Three types of word associations are
		  adopted in WG4Rec for content representation and user
		  preference modeling, namely: 1)semantically-similar
		  according to pre-trained word vectors, 2)co-occurrence in
		  documents, and 3)co-click by users across documents. As
		  extra information can be unified by adding nodes/edges to
		  the word graph easily, WG4Rec is flexible to make use of
		  cross-platform and cross-domain context for recommendation
		  to alleviate the cold-start issue. To the best of our
		  knowledge, it is the first attempt that using these
		  relationships for news recommendation to better model
		  textual content and adopt cross-platform information.
		  Experimental results on two large-scale real-world datasets
		  show that WG4Rec significantly outperforms state-of-the-art
		  algorithms, especially for cold users in the online
		  environment. Besides, WG4Rec achieves better performances
		  when cross-platform information is utilized.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1651–1660},
  numpages	= {10},
  keywords	= {neural networks, news recommendation, word graph},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3477495.3531746,
  author	= {Chen, Xiang and Li, Lei and Zhang, Ningyu and Tan, Chuanqi
		  and Huang, Fei and Si, Luo and Chen, Huajun},
  title		= {Relation Extraction as Open-book Examination:
		  Retrieval-enhanced Prompt Tuning},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531746},
  doi		= {10.1145/3477495.3531746},
  abstract	= {Pre-trained language models have contributed significantly
		  to relation extraction by demonstrating remarkable few-shot
		  learning abilities. However, prompt tuning methods for
		  relation extraction may still fail to generalize to those
		  rare or hard patterns. Note that the previous parametric
		  learning paradigm can be viewed as memorization regarding
		  training data as a book and inference as the close-book
		  test. Those long-tailed or hard patterns can hardly be
		  memorized in parameters given few-shot instances. To this
		  end, we regard RE as an open-book examination and propose a
		  new semiparametric paradigm of retrieval-enhanced prompt
		  tuning for relation extraction. We construct an open-book
		  datastore for retrieval regarding prompt-based instance
		  representations and corresponding relation labels as
		  memorized key-value pairs. During inference, the model can
		  infer relations by linearly interpolating the base output
		  of PLM with the non-parametric nearest neighbor
		  distribution over the datastore. In this way, our model not
		  only infers relation through knowledge stored in the
		  weights during training but also assists decision-making by
		  unwinding and querying examples in the open-book datastore.
		  Extensive experiments on benchmark datasets show that our
		  method can achieve state-of-the-art in both standard
		  supervised and few-shot settings},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2443–2448},
  numpages	= {6},
  keywords	= {few-shot learning, prompt tuning, relation extraction},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@Article{	  10.1109/tcbb.2023.3292883,
  author	= {Zhu, Xinyu and Lu, Weiming},
  title		= {Multi-Label Classification With Dual Tail-Node
		  Augmentation for Drug Repositioning},
  year		= {2023},
  issue_date	= {Sept.-Oct. 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {5},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2023.3292883},
  doi		= {10.1109/TCBB.2023.3292883},
  abstract	= {Due to the lengthy and costly process of new drug
		  discovery, increasing attention has been paid to drug
		  repositioning, i.e., identifying new drug-disease
		  associations. Current machine learning methods for drug
		  repositioning mainly leverage matrix factorization or graph
		  neural networks, and have achieved impressive performance.
		  However, they often suffer from insufficient training
		  labels of inter-domain associations, while ignore the
		  intra-domain associations. Moreover, they often neglect the
		  importance of tail nodes that have few known associations,
		  which limits their effectiveness in drug repositioning. In
		  this paper, we propose a novel multi-label classification
		  model with dual
		  &lt;bold&gt;T&lt;/bold&gt;ail-&lt;bold&gt;N&lt;/bold&gt;ode
		  Augmentation for &lt;bold&gt;D&lt;/bold&gt;rug
		  &lt;bold&gt;R&lt;/bold&gt;epositioning (TNA-DR). We
		  incorporate disease-disease similarity and drug-drug
		  similarity information into
		  &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic
		  xlink:href="lu-ieq1-3292883.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-nearest
		  neighbor (&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic
		  xlink:href="lu-ieq2-3292883.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;NN)
		  augmentation module and contrastive augmentation module,
		  respectively, which effectively complements the weak
		  supervision of drug-disease associations. Furthermore,
		  before employing the two augmentation modules, we filter
		  the nodes by their degrees, so that the two modules are
		  only applied to tail nodes. We conduct 10-fold cross
		  validation experiments on four different real-world
		  datasets, and our model achieves the state-of-the-art
		  performance on all the four datasets. We also demonstrate
		  our model's capability of identifying drug candidates for
		  new diseases and discovering potential new links between
		  existing drugs and diseases.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jul,
  pages		= {3068–3079},
  numpages	= {12}
}

@Article{	  10.14778/3447689.3447703,
  author	= {Tata, Sandeep and Potti, Navneet and Wendt, James B. and
		  Costa, Lauro Beltr\~{a}o and Najork, Marc and Gunel, Beliz},
  title		= {Glean: structured extractions from templatic documents},
  year		= {2021},
  issue_date	= {February 2021},
  publisher	= {VLDB Endowment},
  volume	= {14},
  number	= {6},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3447689.3447703},
  doi		= {10.14778/3447689.3447703},
  abstract	= {Extracting structured information from templatic documents
		  is an important problem with the potential to automate many
		  real-world business workflows such as payment, procurement,
		  and payroll. The core challenge is that such documents can
		  be laid out in virtually infinitely different ways. A good
		  solution to this problem is one that generalizes well not
		  only to known templates such as invoices from a known
		  vendor, but also to unseen ones.We developed a system
		  called Glean to tackle this problem. Given a target schema
		  for a document type and some labeled documents of that
		  type, Glean uses machine learning to automatically extract
		  structured information from other documents of that type.
		  In this paper, we describe the overall architecture of
		  Glean, and discuss three key data management challenges :
		  1) managing the quality of ground truth data, 2) generating
		  training data for the machine learning model using labeled
		  documents, and 3) building tools that help a developer
		  rapidly build and improve a model for a given document
		  type. Through empirical studies on a real-world dataset, we
		  show that these data management techniques allow us to
		  train a model that is over 5 F1 points better than the
		  exact same model architecture without the techniques we
		  describe. We argue that for such information-extraction
		  problems, designing abstractions that carefully manage the
		  training data is at least as important as choosing a good
		  model architecture.},
  journal	= {Proc. VLDB Endow.},
  month		= feb,
  pages		= {997–1005},
  numpages	= {9}
}

@InProceedings{	  10.1145/3397481.3450655,
  author	= {Sovrano, Francesco and Vitali, Fabio},
  title		= {From Philosophy to Interfaces: an Explanatory Method and a
		  Tool Inspired by Achinstein’s Theory of Explanation},
  year		= {2021},
  isbn		= {9781450380171},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397481.3450655},
  doi		= {10.1145/3397481.3450655},
  abstract	= {We propose a new method for explanations in Artificial
		  Intelligence (AI) and a tool to test its expressive power
		  within a user interface. In order to bridge the gap between
		  philosophy and human-computer interfaces, we show a new
		  approach for the generation of interactive explanations
		  based on a sophisticated pipeline of AI algorithms for
		  structuring natural language documents into knowledge
		  graphs, answering questions effectively and satisfactorily.
		  Among the mainstream philosophical theories of explanation
		  we identified one that in our view is more easily
		  applicable as a practical model for user-centric tools:
		  Achinstein’s Theory of Explanation. With this work we aim
		  to prove that the theory proposed by Achinstein can be
		  actually adapted for being implemented into a concrete
		  software application, as an interactive process answering
		  questions. To this end we found a way to handle the generic
		  (archetypal) questions that implicitly characterise an
		  explanatory processes as preliminary overviews rather than
		  as answers to explicit questions, as commonly understood.
		  To show the expressive power of this approach we designed
		  and implemented a pipeline of AI algorithms for the
		  generation of interactive explanations under the form of
		  overviews, focusing on this aspect of explanations rather
		  than on existing interfaces and presentation logic layers
		  for question answering. Accordingly, through the
		  identification of a minimal set of archetypal questions it
		  is possible to create a generator of explanatory overviews
		  that is generic enough to significantly ease the
		  acquisition of knowledge by humans, regardless of the
		  specificities of the users outside of a minimum set of very
		  broad requirements (e.g. people able to read and understand
		  English and capable of performing basic common-sense
		  reasoning). We tested our hypothesis on a well-known
		  XAI-powered credit approval system by IBM, comparing CEM, a
		  static explanatory tool for post-hoc explanations, with an
		  extension we developed adding interactive explanations
		  based on our model. The results of the user study,
		  involving more than 100 participants, showed that our
		  proposed solution produced a statistically relevant
		  improvement on effectiveness (U=931.0, p=0.036) over the
		  baseline, thus giving evidence in favour of our theory.},
  booktitle	= {Proceedings of the 26th International Conference on
		  Intelligent User Interfaces},
  pages		= {81–91},
  numpages	= {11},
  keywords	= {Education and learning-related technologies, ExplanatorY
		  Artificial Intelligence (YAI), Methods for explanations},
  location	= {College Station, TX, USA},
  series	= {IUI '21}
}

@InProceedings{	  10.1145/3477495.3532080,
  author	= {Lin, Li and Cao, Yixin and Huang, Lifu and Li, Shu'Ang and
		  Hu, Xuming and Wen, Lijie and Wang, Jianmin},
  title		= {What Makes the Story Forward? Inferring Commonsense
		  Explanations as Prompts for Future Event Generation},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3532080},
  doi		= {10.1145/3477495.3532080},
  abstract	= {Prediction over event sequences is critical for many
		  real-world applications in Information Retrieval and
		  Natural Language Processing. Future Event Generation (FEG)
		  is a challenging task in event sequence prediction because
		  it requires not only fluent text generation but also
		  commonsense reasoning to maintain the logical coherence of
		  the entire event story. In this paper, we propose a novel
		  explainable FEG framework, Coep. It highlights and
		  integrates two types of event knowledge, sequential
		  knowledge of direct event-event relations and inferential
		  knowledge that reflects the intermediate character
		  psychology between events, such as intents, causes,
		  reactions, which intrinsically pushes the story forward. To
		  alleviate the knowledge forgetting issue, we design two
		  modules, IM and GM, for each type of knowledge, which are
		  combined via prompt tuning. First, IM focuses on
		  understanding inferential knowledge to generate commonsense
		  explanations and provide a soft prompt vector for GM. We
		  also design a contrastive discriminator for better
		  generalization ability. Second, GM generates future events
		  by modeling direct sequential knowledge with the guidance
		  of IM. Automatic and human evaluation demonstrate that our
		  approach can generate more coherent, specific, and logical
		  future events.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1098–1109},
  numpages	= {12},
  keywords	= {commonsense reasoning, contrastive training, textual event
		  generation},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@Article{	  10.1109/taslp.2021.3074014,
  author	= {Xu, Kun and Wu, Han and Song, Linfeng and Zhang, Haisong
		  and Song, Linqi and Yu, Dong},
  title		= {Conversational Semantic Role Labeling},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3074014},
  doi		= {10.1109/TASLP.2021.3074014},
  abstract	= {Semantic role labeling (SRL) aims to extract the arguments
		  for each predicate in an input sentence. Traditional SRL
		  can fail to analyze dialogues because it only works on
		  every single sentence, while ellipsis and anaphora
		  frequently occur in dialogues. To address this problem, we
		  propose the conversational SRL task, where an argument can
		  be the dialogue participants, a phrase in the dialogue
		  history or the current sentence. As the existing SRL
		  datasets are in the sentence level, we manually annotate
		  semantic roles for 3000 chit-chat dialogues (27198
		  sentences) to boost the research in this direction.
		  Experiments show that while traditional SRL systems (even
		  with the help of coreference resolution or rewriting)
		  perform poorly for analyzing dialogues, modeling dialogue
		  histories and participants greatly helps the performance,
		  indicating that adapting SRL to conversations is very
		  promising for universal dialogue understanding. Our initial
		  study by applying CSRL to two mainstream conversational
		  tasks, dialogue response generation and dialogue context
		  rewriting, also confirms the usefulness of CSRL.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= apr,
  pages		= {2465–2475},
  numpages	= {11}
}

@InProceedings{	  10.1145/3611643.3616258,
  author	= {Cao, Jialun and Lu, Yaojie and Wen, Ming and Cheung,
		  Shing-Chi},
  title		= {Testing Coreference Resolution Systems without Labeled
		  Test Sets},
  year		= {2023},
  isbn		= {9798400703270},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3611643.3616258},
  doi		= {10.1145/3611643.3616258},
  abstract	= {Coreference resolution (CR) is a task to resolve different
		  expressions (e.g., named entities, pronouns) that refer to
		  the same real-world en- tity/event. It is a core natural
		  language processing (NLP) component that underlies and
		  empowers major downstream NLP applications such as machine
		  translation, chatbots, and question-answering. De- spite
		  its broad impact, the problem of testing CR systems has
		  rarely been studied. A major difficulty is the shortage of
		  a labeled dataset for testing. While it is possible to feed
		  arbitrary sentences as test inputs to a CR system, a test
		  oracle that captures their expected test outputs
		  (coreference relations) is hard to define automatically. To
		  address the challenge, we propose Crest, an automated
		  testing methodology for CR systems. Crest uses constituency
		  and depen- dency relations to construct pairs of test
		  inputs subject to the same coreference. These relations can
		  be leveraged to define the meta- morphic relation for
		  metamorphic testing. We compare Crest with five
		  state-of-the-art test generation baselines on two popular
		  CR systems, and apply them to generate tests from 1,000
		  sentences randomly sampled from CoNLL-2012, a popular
		  dataset for corefer- ence resolution. Experimental results
		  show that Crest outperforms baselines significantly. The
		  issues reported by Crest are all true positives (i.e., 100%
		  precision), compared with 63% to 75% achieved by the
		  baselines.},
  booktitle	= {Proceedings of the 31st ACM Joint European Software
		  Engineering Conference and Symposium on the Foundations of
		  Software Engineering},
  pages		= {107–119},
  numpages	= {13},
  keywords	= {Coreference resolution testing, Metamorphic testing,
		  SE4AI},
  location	= {San Francisco, CA, USA},
  series	= {ESEC/FSE 2023}
}

@Article{	  10.1145/3519265,
  author	= {Sovrano, Francesco and Vitali, Fabio},
  title		= {Generating User-Centred Explanations via Illocutionary
		  Question Answering: From Philosophy to Interfaces},
  year		= {2022},
  issue_date	= {December 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {12},
  number	= {4},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3519265},
  doi		= {10.1145/3519265},
  abstract	= {We propose a new method for generating explanations with
		  Artificial Intelligence (AI) and a tool to test its
		  expressive power within a user interface. In order to
		  bridge the gap between philosophy and human-computer
		  interfaces, we show a new approach for the generation of
		  interactive explanations based on a sophisticated pipeline
		  of AI algorithms for structuring natural language documents
		  into knowledge graphs, answering questions effectively and
		  satisfactorily. With this work, we aim to prove that the
		  philosophical theory of explanations presented by
		  Achinstein can be actually adapted for being implemented
		  into a concrete software application, as an interactive and
		  illocutionary process of answering questions. Specifically,
		  our contribution is an approach to frame illocution in a
		  computer-friendly way, to achieve user-centrality with
		  statistical question answering. Indeed, we frame the
		  illocution of an explanatory process as that mechanism
		  responsible for anticipating the needs of the explainee in
		  the form of unposed, implicit, archetypal questions, hence
		  improving the user-centrality of the underlying explanatory
		  process. Therefore, we hypothesise that if an explanatory
		  process is an illocutionary act of providing content-giving
		  answers to questions, and illocution is as we defined it,
		  the more explicit and implicit questions can be answered by
		  an explanatory tool, the more usable (as per ISO 9241-210)
		  its explanations. We tested our hypothesis with a
		  user-study involving more than 60 participants, on two
		  XAI-based systems, one for credit approval (finance) and
		  one for heart disease prediction (healthcare). The results
		  showed that increasing the illocutionary power of an
		  explanatory tool can produce statistically significant
		  improvements (hence with a P value lower than .05) on
		  effectiveness. This, combined with a visible alignment
		  between the increments in effectiveness and satisfaction,
		  suggests that our understanding of illocution can be
		  correct, giving evidence in favour of our theory.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= nov,
  articleno	= {26},
  numpages	= {32},
  keywords	= {Methods for explanations, education and learning-related
		  technologies, explanatory artificial intelligence (YAI)}
}

@InProceedings{	  10.1145/3539618.3591880,
  author	= {Zhang, Haochen and Korikov, Anton and Farinneya, Parsa and
		  Abdollah Pour, Mohammad Mahdi and Bharadwaj, Manasa and
		  Pesaranghader, Ali and Huang, Xi Yu and Lok, Yi Xin and
		  Wang, Zhaoqi and Jones, Nathan and Sanner, Scott},
  title		= {Recipe-MPR: A Test Collection for Evaluating Multi-aspect
		  Preference-based Natural Language Retrieval},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591880},
  doi		= {10.1145/3539618.3591880},
  abstract	= {The rise of interactive recommendation assistants has led
		  to a novel domain of natural language (NL) recommendation
		  that would benefit from improved multi-aspect reasoning to
		  retrieve relevant items based on NL statements of
		  preference. Such preference statements often involve
		  multiple aspects, e.g., "I would like meat lasagna but I'm
		  watching my weight". Unfortunately, progress in this domain
		  is slowed by the lack of annotated data. To address this
		  gap, we curate a novel dataset which captures logical
		  reasoning over multi-aspect, NL preference-based queries
		  and a set of multiple-choice, multi-aspect item
		  descriptions. We focus on the recipe domain in which
		  multi-aspect preferences are often encountered due to the
		  complexity of the human diet. The goal of publishing our
		  dataset is to provide a benchmark for joint progress in
		  three key areas: 1) structured, multi-aspect NL reasoning
		  with a variety of properties (e.g., level of specificity,
		  presence of negation, and the need for commonsense,
		  analogical, and/or temporal inference), 2) the ability of
		  recommender systems to respond to NL preference utterances,
		  and 3) explainable NL recommendation facilitated by aspect
		  extraction and reasoning. We perform experiments using a
		  variety of methods (sparse and dense retrieval, zero- and
		  few-shot reasoning with large language models) in two
		  settings: a monolithic setting which uses the full query
		  and an aspect-based setting which isolates individual query
		  aspects and aggregates the results. GPT-3 results in much
		  stronger performance than other methods with 73% zero-shot
		  accuracy and 83% few-shot accuracy in the monolithic
		  setting. Aspect-based GPT-3, which facilitates structured
		  explanations, also shows promise with 68% zero-shot
		  accuracy. These results establish baselines for future
		  research into explainable recommendations via multi-aspect
		  preference-based NL reasoning.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2744–2753},
  numpages	= {10},
  keywords	= {benchmark dataset, multi-aspect preference retrieval,
		  natural language reasoning, recipe retrieval},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Proceedings{	  10.1145/3611643,
  title		= {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European
		  Software Engineering Conference and Symposium on the
		  Foundations of Software Engineering},
  year		= {2023},
  isbn		= {9798400703270},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {We are pleased to welcome all delegates to ESEC/FSE 2023,
		  the ACM Joint European Software Engineering Conference and
		  Symposium on the Foundations of Software Engineering.
		  ESEC/FSE is an internationally renowned forum for
		  researchers, practitioners, and educators to present and
		  discuss the most recent innovations, trends, experiences,
		  and challenges in the field of software engineering.
		  ESEC/FSE brings together experts from academia and industry
		  to exchange the latest research results and trends as well
		  as their practical application in all areas of software
		  engineering.},
  location	= {San Francisco, CA, USA}
}

@InProceedings{	  10.1145/3510003.3510159,
  author	= {Wei, Moshi and Harzevili, Nima Shiri and Huang, Yuchao and
		  Wang, Junjie and Wang, Song},
  title		= {CLEAR: &lt;u&gt;c&lt;/u&gt;ontrastive
		  &lt;u&gt;le&lt;/u&gt;arning for &lt;u&gt;A&lt;/u&gt;PI
		  &lt;u&gt;r&lt;/u&gt;ecommendation},
  year		= {2022},
  isbn		= {9781450392211},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3510003.3510159},
  doi		= {10.1145/3510003.3510159},
  abstract	= {Automatic API recommendation has been studied for years.
		  There are two orthogonal lines of approaches for this task,
		  i.e., information-retrieval-based (IR-based) and
		  neural-based methods. Although these approaches were
		  reported having remarkable performance, our observation
		  shows that existing approaches can fail due to the
		  following two reasons: 1) most IR-based approaches treat
		  task queries as bag-of-words and use word embedding to
		  represent queries, which cannot capture the sequential
		  semantic information. 2) both the IR-based and the
		  neural-based approaches are weak at distinguishing the
		  semantic difference among lexically similar queries.In this
		  paper, we propose CLEAR, which leverages BERT sentence
		  embedding and contrastive learning to tackle the above two
		  issues. Specifically, CLEAR embeds the whole sentence of
		  queries and Stack Overflow (SO) posts with a BERT-based
		  model rather than the bag-of-word-based word embedding
		  model, which can preserve the semantic-related sequential
		  information. In addition, CLEAR uses contrastive learning
		  to train the BERT-based embedding model for learning
		  precise semantic representation of programming
		  terminologies regardless of their lexical information.
		  CLEAR also builds a BERT-based re-ranking model to optimize
		  its recommendation results. Given a query, CLEAR first
		  selects a set of candidate SO posts via the BERT sentence
		  embedding-based similarity to reduce search space. CLEAR
		  further leverages a BERT-based re-ranking model to rank
		  candidate SO posts and recommends the APIs from the ranked
		  top SO posts for the query.Our experiment results on three
		  different test datasets confirm the effectiveness of CLEAR
		  for both method-level and class-level API recommendation.
		  Compared to the state-of-the-art API recommendation
		  approaches, CLEAR improves the MAP by 25%-187% at
		  method-level and 10%-100% at class-level.},
  booktitle	= {Proceedings of the 44th International Conference on
		  Software Engineering},
  pages		= {376–387},
  numpages	= {12},
  keywords	= {API recommendation, contrastive learning, semantic
		  difference},
  location	= {Pittsburgh, Pennsylvania},
  series	= {ICSE '22}
}

@Proceedings{	  10.1145/3609437,
  title		= {Internetware '23: Proceedings of the 14th Asia-Pacific
		  Symposium on Internetware},
  year		= {2023},
  isbn		= {9798400708947},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Article{	  10.1145/3591280,
  author	= {Li, Ziyang and Huang, Jiani and Naik, Mayur},
  title		= {Scallop: A Language for Neurosymbolic Programming},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {PLDI},
  url		= {https://doi.org/10.1145/3591280},
  doi		= {10.1145/3591280},
  abstract	= {We present Scallop, a language which combines the benefits
		  of deep learning and logical reasoning. Scallop enables
		  users to write a wide range of neurosymbolic applications
		  and train them in a data- and compute-efficient manner. It
		  achieves these goals through three key features: 1) a
		  flexible symbolic representation that is based on the
		  relational data model; 2) a declarative logic programming
		  language that is based on Datalog and supports recursion,
		  aggregation, and negation; and 3) a framework for automatic
		  and efficient differentiable reasoning that is based on the
		  theory of provenance semirings. We evaluate Scallop on a
		  suite of eight neurosymbolic applications from the
		  literature. Our evaluation demonstrates that Scallop is
		  capable of expressing algorithmic reasoning in diverse and
		  challenging AI tasks, provides a succinct interface for
		  machine learning programmers to integrate logical domain
		  knowledge, and yields solutions that are comparable or
		  superior to state-of-the-art models in terms of accuracy.
		  Furthermore, Scallop's solutions outperform these models in
		  aspects such as runtime and data efficiency,
		  interpretability, and generalizability.},
  journal	= {Proc. ACM Program. Lang.},
  month		= jun,
  articleno	= {166},
  numpages	= {25},
  keywords	= {Differentiable reasoning, Neurosymbolic methods}
}

@InProceedings{	  10.1145/3580305.3599873,
  author	= {Liao, Hao and Peng, Jiahao and Huang, Zhanyi and Zhang,
		  Wei and Li, Guanghua and Shu, Kai and Xie, Xing},
  title		= {MUSER: A MUlti-Step Evidence Retrieval Enhancement
		  Framework for Fake News Detection},
  year		= {2023},
  isbn		= {9798400701030},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3580305.3599873},
  doi		= {10.1145/3580305.3599873},
  abstract	= {The ease of spreading false information online enables
		  individuals with malicious intent to manipulate public
		  opinion and destabilize social stability. Recently, fake
		  news detection based on evidence retrieval has gained
		  popularity in an effort to identify fake news reliably and
		  reduce its impact. Evidence retrieval-based methods can
		  improve the reliability of fake news detection by computing
		  the textual consistency between the evidence and the claim
		  in the news. In this paper, we propose a framework for fake
		  news detection based on MUlti- Step Evidence Retrieval
		  enhancement (MUSER), which simulates the steps of human
		  beings in the process of reading news, summarizing,
		  consulting materials, and inferring whether the news is
		  true or fake. Our model can explicitly model dependencies
		  among multiple pieces of evidence, and perform multi-step
		  associations for the evidence required for news
		  verification through multi-step retrieval. In addition, our
		  model is able to automatically collect existing evidence
		  through paragraph retrieval and key evidence selection,
		  which can save the tedious process of manual evidence
		  collection. We conducted extensive experiments on
		  real-world datasets in different languages, and the results
		  demonstrate that our proposed model outperforms
		  state-of-the-art baseline methods for detecting fake news
		  by at least 3% in F1-Macro and 4% in F1-Micro. Furthermore,
		  it provides interpretable evidence for end users.},
  booktitle	= {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4461–4472},
  numpages	= {12},
  keywords	= {evidence-based fake news detection, explainability,
		  multi-step retrieval},
  location	= {Long Beach, CA, USA},
  series	= {KDD '23}
}

@Article{	  10.1109/taslp.2023.3270771,
  author	= {Li, Shu'ang and Hu, Xuming and Lin, Li and Liu, Aiwei and
		  Wen, Lijie and Yu, Philip S.},
  title		= {A Multi-Level Supervised Contrastive Learning Framework
		  for Low-Resource Natural Language Inference},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3270771},
  doi		= {10.1109/TASLP.2023.3270771},
  abstract	= {Natural Language Inference (NLI) is a growingly essential
		  task in natural language understanding, which requires
		  inferring the relationship between the sentence pairs
		  (&lt;bold&gt;premise&lt;/bold&gt; and
		  &lt;bold&gt;hypothesis&lt;/bold&gt;). Recently,
		  low-resource natural language inference has gained
		  increasing attention, due to significant savings in manual
		  annotation costs and a better fit with real-world
		  scenarios. Existing works fail to characterize
		  discriminative representations between different classes
		  with limited training data, which may cause faults in label
		  prediction. Here we propose a multi-level supervised
		  contrastive learning framework named MultiSCL for
		  low-resource natural language inference. MultiSCL leverages
		  a sentence-level and pair-level contrastive learning
		  objective to discriminate between different classes of
		  sentence pairs by bringing those in one class together and
		  pushing away those in different classes. MultiSCL adopts a
		  data augmentation module that generates different views for
		  input samples to better learn the latent representation.
		  The pair-level representation is obtained from a cross
		  attention module. We conduct extensive experiments on two
		  public NLI datasets in low-resource settings, and the
		  accuracy of MultiSCL exceeds other models by 1.8%, 3.1% and
		  4.1% on SNLI, MNLI and Sick with 5 instances per label
		  respectively. Moreover, our method outperforms the previous
		  state-of-the-art method on cross-domain tasks of text
		  classification.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= apr,
  pages		= {1771–1783},
  numpages	= {13}
}

@Proceedings{	  10.1145/3581641,
  title		= {IUI '23: Proceedings of the 28th International Conference
		  on Intelligent User Interfaces},
  year		= {2023},
  isbn		= {9798400701061},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sydney, NSW, Australia}
}

@Proceedings{	  10.1145/3581807,
  title		= {ICCPR '22: Proceedings of the 2022 11th International
		  Conference on Computing and Pattern Recognition},
  year		= {2022},
  isbn		= {9781450397056},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@InProceedings{	  10.1145/3459930.3469533,
  author	= {Noh, Jiho and Kavuluru, Ramakanth},
  title		= {Joint learning for biomedical NER and entity
		  normalization: encoding schemes, counterfactual examples,
		  and zero-shot evaluation},
  year		= {2021},
  isbn		= {9781450384506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459930.3469533},
  doi		= {10.1145/3459930.3469533},
  abstract	= {Named entity recognition (NER) and normalization (EN) form
		  an indispensable first step to many biomedical natural
		  language processing applications. In biomedical information
		  science, recognizing entities (e.g., genes, diseases, or
		  drugs) and normalizing them to concepts in standard
		  terminologies or thesauri (e.g., Entrez, ICD-10, or RxNorm)
		  is crucial for identifying more informative relations among
		  them that drive disease etiology, progression, and
		  treatment. In this effort we pursue two high level
		  strategies to improve biomedical ER and EN. The first is to
		  decouple standard entity encoding tags (e.g., "B-Drug" for
		  the beginning of a drug) into type tags (e.g., "Drug") and
		  positional tags (e.g., "B"). A second strategy is to use
		  additional counterfactual training examples to handle the
		  issue of models learning spurious correlations between
		  surrounding context and normalized concepts in training
		  data. We conduct elaborate experiments using the
		  MedMentions dataset, the largest dataset of its kind for ER
		  and EN in biomedicine. We find that our first strategy
		  performs better in entity normalization when compared with
		  the standard coding scheme. The second data augmentation
		  strategy uniformly improves performance in span detection,
		  typing, and normalization. The gains from counterfactual
		  examples are more prominent when evaluating in zero-shot
		  settings, for concepts that have never been encountered
		  during training.},
  booktitle	= {Proceedings of the 12th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {55},
  numpages	= {10},
  keywords	= {biomedical natural language processing, deep neural
		  networks, entity normalization, information extraction,
		  named entity recognition},
  location	= {Gainesville, Florida},
  series	= {BCB '21}
}

@InProceedings{	  10.1145/3477495.3532039,
  author	= {Cai, Zefeng and Cai, Zerui},
  title		= {PEVAE: A Hierarchical VAE for Personalized Explainable
		  Recommendation.},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3532039},
  doi		= {10.1145/3477495.3532039},
  abstract	= {Variational autoencoders (VAEs) have been widely applied
		  in recommendations. One reason is that their amortized
		  inferences are beneficial for overcoming the data sparsity.
		  However, in explainable recommendation that generates
		  natural language explanations, they are still rarely
		  explored. Thus, we aim to extend VAE to explainable
		  recommendation. In this task, we find that VAE can generate
		  acceptable explanations for users with few relevant
		  training samples, however, it tends to generate less
		  personalized explanations for users with relatively
		  sufficient samples than autoencoders (AEs). We conjecture
		  that information shared by different users in VAE disturbs
		  the information for a specific user. To deal with this
		  problem, we present PErsonalized VAE (PEVAE) that generates
		  personalized natural language explanations for explainable
		  recommendation. Moreover, we propose two novel mechanisms
		  to aid our model in generating more personalized
		  explanations, including 1) Self-Adaption Fusion (SAF)
		  manipulates the latent space in a self-adaption manner for
		  controlling the influence of shared information. In this
		  way, our model can enjoy the advantage of overcoming the
		  sparsity of data while generating more personalized
		  explanations for a user with relatively sufficient training
		  samples. 2) DEpendence Maximization (DEM) strengthens
		  dependence between recommendations and explanations by
		  maximizing the mutual information. It makes the explanation
		  more specific to the input user-item pair and thus improves
		  the personalization of the generated explanations.
		  Extensive experiments show PEVAE can generate more
		  personalized explanations and further analyses demonstrate
		  the practical effect of our proposed methods.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {692–702},
  numpages	= {11},
  keywords	= {natural language generation, recommender systems,
		  variational inference},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@Proceedings{	  10.1145/3593013,
  title		= {FAccT '23: Proceedings of the 2023 ACM Conference on
		  Fairness, Accountability, and Transparency},
  year		= {2023},
  isbn		= {9798400701924},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Chicago, IL, USA}
}

@InProceedings{	  10.1145/3607827.3616842,
  author	= {Rossetto, Federico and Dalton, Jeffrey and Murray-Smith,
		  Roderick},
  title		= {Generating Multimodal Augmentations with LLMs from Song
		  Metadata for Music Information Retrieval},
  year		= {2023},
  isbn		= {9798400702839},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3607827.3616842},
  doi		= {10.1145/3607827.3616842},
  abstract	= {In this work we propose a set of new automatic text
		  augmentations that leverage Large Language Models from song
		  metadata to improve on music information retrieval tasks.
		  Compared to recent works, our proposed methods leverage
		  large language models and copyright-free corpora from web
		  sources, enabling us to release the knowledge sources
		  collected. We show how combining these representations with
		  the audio signal provides a 21% relative improvement on
		  five of six datasets on genre classification, emotion
		  recognition and music tagging, achieving state-of-the-art
		  in three (GTZAN, FMA-Small and Deezer). We demonstrate the
		  benefit of injecting external knowledge sources by
		  comparing them withintrinsic text representation methods
		  that rely only on the sample's information.},
  booktitle	= {Proceedings of the 1st Workshop on Large Generative Models
		  Meet Multimodal Applications},
  pages		= {51–59},
  numpages	= {9},
  keywords	= {large language models application, multimodal learning,
		  music information retrieval},
  location	= {Ottawa ON, Canada},
  series	= {LGM3A '23}
}

@Proceedings{	  10.1145/3625156,
  title		= {ICISS '23: Proceedings of the 2023 6th International
		  Conference on Information Science and Systems},
  year		= {2023},
  isbn		= {9798400708206},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Edinburgh, United Kingdom}
}

@Article{	  10.1145/3503917,
  author	= {Li, Rui and Yang, Cheng and Li, Tingwei and Su, Sen},
  title		= {MiDTD: A Simple and Effective Distillation Framework for
		  Distantly Supervised Relation Extraction},
  year		= {2022},
  issue_date	= {October 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3503917},
  doi		= {10.1145/3503917},
  abstract	= {Relation extraction (RE), an important information
		  extraction task, faced the great challenge brought by
		  limited annotation data. To this end, distant supervision
		  was proposed to automatically label RE data, and thus
		  largely increased the number of annotated instances.
		  Unfortunately, lots of noise relation annotations brought
		  by automatic labeling become a new obstacle. Some recent
		  studies have shown that the teacher-student framework of
		  knowledge distillation can alleviate the interference of
		  noise relation annotations via label softening.
		  Nevertheless, we find that they still suffer from two
		  problems: propagation of inaccurate dark knowledge and
		  constraint of a unified distillation temperature. In this
		  article, we propose a simple and effective Multi-instance
		  Dynamic Temperature Distillation (MiDTD) framework, which
		  is model-agnostic and mainly involves two modules:
		  multi-instance target fusion (MiTF) and dynamic temperature
		  regulation (DTR). MiTF combines the teacher’s predictions
		  for multiple sentences with the same entity pair to amend
		  the inaccurate dark knowledge in each student’s target.
		  DTR allocates alterable distillation temperatures to
		  different training instances to enable the softness of most
		  student’s targets to be regulated to a moderate range. In
		  experiments, we construct three concrete MiDTD
		  instantiations with BERT, PCNN, and BiLSTM-based RE models,
		  and the distilled students significantly outperform their
		  teachers and the state-of-the-art (SOTA) methods.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {83},
  numpages	= {32},
  keywords	= {Natural language processing, NLP, knowledge distillation,
		  distant supervision, neural network, multi-instance
		  learning, label softening}
}

@InProceedings{	  10.1145/3477495.3531803,
  author	= {Liu, Han and Zhao, Siyang and Zhang, Xiaotong and Zhang,
		  Feng and Sun, Junjie and Yu, Hong and Zhang, Xianchao},
  title		= {A Simple Meta-learning Paradigm for Zero-shot Intent
		  Classification with Mixture Attention Mechanism},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531803},
  doi		= {10.1145/3477495.3531803},
  abstract	= {Zero-shot intent classification is a vital and challenging
		  task in dialogue systems, which aims to deal with numerous
		  fast-emerging unacquainted intents without annotated
		  training data. To obtain more satisfactory performance, the
		  crucial points lie in two aspects: extracting better
		  utterance features and strengthening the model
		  generalization ability. In this paper, we propose a simple
		  yet effective meta-learning paradigm for zero-shot intent
		  classification. To learn better semantic representations
		  for utterances, we introduce a new mixture attention
		  mechanism, which encodes the pertinent word occurrence
		  patterns by leveraging the distributional signature
		  attention and multi-layer perceptron attention
		  simultaneously. To strengthen the transfer ability of the
		  model from seen classes to unseen classes, we reformulate
		  zero-shot intent classification with a meta-learning
		  strategy, which trains the model by simulating multiple
		  zero-shot classification tasks on seen categories, and
		  promotes the model generalization ability with a
		  meta-adapting procedure on mimic unseen categories.
		  Extensive experiments on two real-world dialogue datasets
		  in different languages show that our model outperforms
		  other strong baselines on both standard and generalized
		  zero-shot intent classification tasks.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2047–2052},
  numpages	= {6},
  keywords	= {meta-learning, mixture attention mechanism, zero-shot
		  intent classification},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3511808.3557271,
  author	= {Chen, Jiangui and Zhang, Ruqing and Guo, Jiafeng and Liu,
		  Yiqun and Fan, Yixing and Cheng, Xueqi},
  title		= {CorpusBrain: Pre-train a Generative Retrieval Model for
		  Knowledge-Intensive Language Tasks},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557271},
  doi		= {10.1145/3511808.3557271},
  abstract	= {Knowledge-intensive language tasks (KILT) usually require
		  a large body of information to provide correct answers. A
		  popular paradigm to solve this problem is to combine a
		  search system with a machine reader, where the former
		  retrieves supporting evidences and the latter examines them
		  to produce answers. Recently, the reader component has
		  witnessed significant advances with the help of large-scale
		  pre-trained generative models. Meanwhile most existing
		  solutions in the search component rely on the traditional
		  "index-retrieve-then-rank'' pipeline, which suffers from
		  large memory footprint and difficulty in end-to-end
		  optimization. Inspired by recent efforts in constructing
		  model-based IR models, we propose to replace the
		  traditional multi-step search pipeline with a novel
		  single-step generative model, which can dramatically
		  simplify the search process and be optimized in an
		  end-to-end manner. We show that a strong generative
		  retrieval model can be learned with a set of adequately
		  designed pre-training tasks, and be adopted to improve a
		  variety of downstream KILT tasks with further fine-tuning.
		  We name the pre-trained generative retrieval model as
		  CorpusBrain as all information about the corpus is encoded
		  in its parameters without the need of constructing
		  additional index. Empirical results show that CorpusBrain
		  can significantly outperform strong baselines for the
		  retrieval task on the KILT benchmark and establish new
		  state-of-the-art downstream performances. We also show that
		  CorpusBrain works well under zero- and low-resource
		  settings.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {191–200},
  numpages	= {10},
  keywords	= {generative retrieval, model-based ir, pre-training},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@InProceedings{	  10.1145/3485447.3512168,
  author	= {Wang, Peng and Cai, Renqin and Wang, Hongning},
  title		= {Graph-based Extractive Explainer for Recommendations},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3512168},
  doi		= {10.1145/3485447.3512168},
  abstract	= {Explanations in a recommender system assist users make
		  informed decisions among a set of recommended items.
		  Extensive research attention has been devoted to generate
		  natural language explanations to depict how the
		  recommendations are generated and why the users should pay
		  attention to them. However, due to different limitations of
		  those solutions, e.g., template-based or generation-based,
		  it is hard to make the explanations easily perceivable,
		  reliable, and personalized at the same time. In this work,
		  we develop a graph attentive neural network model that
		  seamlessly integrates user, item, attributes and sentences
		  for extraction-based explanation. The attributes of items
		  are selected as the intermediary to facilitate message
		  passing for user-item specific evaluation of sentence
		  relevance. And to balance individual sentence relevance,
		  overall attribute coverage and content redundancy, we solve
		  an integer linear programming problem to make the final
		  selection of sentences. Extensive empirical evaluations
		  against a set of state-of-the-art baseline methods on two
		  benchmark review datasets demonstrated the generation
		  quality of proposed solution.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {2163–2171},
  numpages	= {9},
  keywords	= {Extraction-based explanation, graph neural networks},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Proceedings{	  10.1145/3629264,
  title		= {ICCDA '23: Proceedings of the 2023 7th International
		  Conference on Computing and Data Analysis},
  year		= {2023},
  isbn		= {9798400700576},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guiyang, China}
}

@Article{	  10.1145/3430937,
  author	= {Reis, Eduardo Souza Dos and Costa, Cristiano Andr\'{e} Da
		  and Silveira, Di\'{o}rgenes Eug\^{e}nio Da and Bavaresco,
		  Rodrigo Simon and Righi, Rodrigo Da Rosa and Barbosa, Jorge
		  Luis Vict\'{o}ria and Antunes, Rodolfo Stoffel and Gomes,
		  M\'{a}rcio Miguel and Federizzi, Gustavo},
  title		= {Transformers aftermath: current research and rising
		  trends},
  year		= {2021},
  issue_date	= {April 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {64},
  number	= {4},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3430937},
  doi		= {10.1145/3430937},
  abstract	= {Attention, particularly self-attention, is a standard in
		  current NLP literature, but to achieve meaningful models,
		  attention is not enough.},
  journal	= {Commun. ACM},
  month		= mar,
  pages		= {154–163},
  numpages	= {10}
}

@InProceedings{	  10.1145/3589132.3625618,
  author	= {Wang, Renzhong and Najafabadi, Maryam and Zhang, Chiqun
		  and Chen, Long-Qi and Olenina, Tanya and Yankov, Dragomir},
  title		= {GPT Applications in Relevance Model Training in Map
		  Search},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589132.3625618},
  doi		= {10.1145/3589132.3625618},
  abstract	= {Understanding map queries and retrieving correct entity
		  results are the two main relevance tasks in Map search.
		  They are usually performed by a set of task specific
		  machine learning models. Collecting large amount of high
		  quality labelled data for training such models is a
		  time-consuming and labor-intensive process. Although
		  various methods have been studied for producing pseudo data
		  labels, they are limited in their effectiveness when
		  applied across different languages or tasks. The recently
		  released Large Language models (LLMs), including ChatGPT
		  and GPT-4 (GPT for short), have demonstrated
		  state-of-the-art performance in text understanding by using
		  simple prompt instructions with only a handful of examples
		  for in-context learning. In this paper, we explore GPT as a
		  cost-effective alternative for both data labeling and
		  synthetic data generation, where we subsequently use data
		  obtained from this approach to train various task specific
		  models such as maps intent detection, address detection,
		  address parsing, geo-entity ranking, and rank scores
		  calibration. GPT demonstrates strong potential in
		  generating otherwise hard-to-synthesize data. We observe
		  significant accuracy and relevance improvement across all
		  task specific models when trained or fine-tuned on data
		  generated by GPT. Lastly, we propose a general framework
		  combining labeled data from GPT with other sources and a
		  prompt fine-tune structure to guide GPT model in completing
		  a given task.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {68},
  numpages	= {4},
  keywords	= {GPT, query processing, maps service, information
		  retrieval},
  location	= {Hamburg, Germany},
  series	= {SIGSPATIAL '23}
}

@Article{	  10.1145/3495162,
  author	= {Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying
		  and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He,
		  Lifang},
  title		= {A Survey on Text Classification: From Traditional to Deep
		  Learning},
  year		= {2022},
  issue_date	= {April 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3495162},
  doi		= {10.1145/3495162},
  abstract	= {Text classification is the most fundamental and essential
		  task in natural language processing. The last decade has
		  seen a surge of research in this area due to the
		  unprecedented success of deep learning. Numerous methods,
		  datasets, and evaluation metrics have been proposed in the
		  literature, raising the need for a comprehensive and
		  updated survey. This paper fills the gap by reviewing the
		  state-of-the-art approaches from 1961 to 2021, focusing on
		  models from traditional models to deep learning. We create
		  a taxonomy for text classification according to the text
		  involved and the models used for feature extraction and
		  classification. We then discuss each of these categories in
		  detail, dealing with both the technical developments and
		  benchmark datasets that support tests of predictions. A
		  comprehensive comparison between different techniques, as
		  well as identifying the pros and cons of various evaluation
		  metrics are also provided in this survey. Finally, we
		  conclude by summarizing key implications, future research
		  directions, and the challenges facing the research area.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= apr,
  articleno	= {31},
  numpages	= {41},
  keywords	= {Deep learning, traditional models, text classification,
		  evaluation metrics, challenges}
}

@InProceedings{	  10.1145/3459637.3482490,
  author	= {Guan, Renchu and Liu, Yonghao and Feng, Xiaoyue and Li,
		  Ximing},
  title		= {VPALG: Paper-publication Prediction with Graph Neural
		  Networks},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482490},
  doi		= {10.1145/3459637.3482490},
  abstract	= {Paper-publication venue prediction aims to predict
		  candidate publication venues that effectively suit given
		  submissions. This technology is developing rapidly with the
		  popularity of machine learning models. However, most
		  previous methods ignore the structure information of
		  papers, while modeling them with graphs can naturally solve
		  this drawback. Meanwhile, they either use hand-crafted or
		  bag-of-word features to represent the papers, ignoring the
		  ones that involve high-level semantics. Moreover, existing
		  methods assume that the venue where a paper is published as
		  a correct venue for the data annotation, which is
		  unrealistic. One paper can be relevant to many venues. In
		  this paper, we attempt to address these problems above and
		  develop a novel prediction model, namelyVenue Prediction
		  with Abstract-Level Graph (Vpalg xspace), which can serve
		  as an effective decision-making tool for venue selections.
		  Specifically, to achieve more discriminative paper abstract
		  representations, we construct each abstract as a semantic
		  graph and perform a dual attention message passing neural
		  network for representation learning. Then, the proposed
		  model can be trained over the learned abstract
		  representations with their labels and generalized via
		  self-training. Empirically, we employ the PubMed dataset
		  and further collect two new datasets from the top journals
		  and conferences in computer science. Experimental results
		  indicate the superior performance of Vpalg xspace,
		  consistently outperforming the existing baseline methods.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {617–626},
  numpages	= {10},
  keywords	= {graph neural networks, paper-publication prediction, text
		  mining},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.1145/3627989,
  author	= {Jin, Weiqiang and Zhao, Biao and Zhang, Yu and Sun, Gege
		  and Yu, Hang},
  title		= {Fintech Key-Phrase: A New Chinese Financial High-Tech
		  Dataset Accelerating Expression-Level Information
		  Retrieval},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {11},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3627989},
  doi		= {10.1145/3627989},
  abstract	= {Expression-level information extraction is a challenging
		  task in natural language processing (NLP), which aims to
		  retrieve crucial semantic information from linguistic
		  documents. However, there is a lack of up-to-date data
		  resources for accelerating expression-level information
		  extraction, particularly in the Chinese financial high
		  technology field. To address this gap, we introduce Fintech
		  Key-Phrase: a human-annotated key-phrase dataset for the
		  Chinese financial high technology domain. This dataset
		  comprises over 12K paragraphs along with annotated
		  domain-specific key-phrases. We extract the publicly
		  released reports, Chinese management’s discussion and
		  analysis (CMD&amp;A), from the renowned Chinese research
		  data services platform (CNRDS) and then filter the reports
		  related to high technology. The high technology key-phrases
		  are annotated following pre-defined philosophy guidelines
		  to ensure annotation quality. In order to better understand
		  the limitations and challenges in the purposed dataset, we
		  conducted comprehensive noise evaluation experiments for
		  the Fintech Key-Phrase, including annotation consistency
		  assessment and absolute annotation quality evaluation. To
		  demonstrate the usefulness of our released Fintech
		  Key-Phrase in retrieving valuable information in the
		  Chinese financial high technology field, we evaluate its
		  significance using several superior information retrieval
		  systems as representative baselines and report
		  corresponding performance statistics. Additionally, we
		  further applied ChatGPT to the text augmentation approach
		  of the Fintech Key-Phrase dataset. Extensive comparative
		  experiments demonstrate that the augmented Fintech
		  Key-Phrase dataset significantly improved the coverage and
		  accuracy of extracting key phrases in the finance and
		  high-tech domains. We believe that this dataset can
		  facilitate scientific research and exploration in the
		  Chinese financial high technology field. We have made the
		  Fintech Key-Phrase dataset and the experimental code of the
		  adopted baselines accessible on Github:
		  https://github.com/albert-jin/Fintech-Key-Phrase. To
		  encourage newcomers to participate in the financial
		  high-tech domain information retrieval research, we have
		  developed a series of tools, including an open website1 and
		  corresponding real-time information retrieval APIs.2},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {253},
  numpages	= {37},
  keywords	= {Information retrieval, expression-level information
		  extraction, financial high technology field, Chinese
		  management’s discussion and analysis, ChatGPT-based data
		  augment}
}

@Proceedings{	  10.1145/3606043,
  title		= {HP3C '23: Proceedings of the 2023 7th International
		  Conference on High Performance Compilation, Computing and
		  Communications},
  year		= {2023},
  isbn		= {9781450399883},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Jinan, China}
}

@InProceedings{	  10.1145/3498366.3505819,
  author	= {Smith, Catherine and Urgo, Kelsey and Arguello, Jaime and
		  Capra, Robert},
  title		= {Learner, Assignment, and Domain: Contextualizing Search
		  for Comprehension},
  year		= {2022},
  isbn		= {9781450391863},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3498366.3505819},
  doi		= {10.1145/3498366.3505819},
  abstract	= {Modern search systems are largely designed and optimized
		  for simple navigational or fact-finding tasks, with little
		  support for complex tasks involving comprehension and
		  learning. In response, the search-as-learning research
		  community has undertaken a wide range of research questions
		  focused on understanding how various types of learning
		  outcomes are affected by searcher characteristics, the
		  search task, and the search system. Typically, these views
		  embed learning within a search system. In this paper we
		  take a different view, embedding search within a framework
		  for an end-to-end learning system designed to support
		  learning in a formal educational context. Our central goal
		  is to motivate research questions aligned to advance
		  progress on techniques for active support of comprehension
		  and formal learning. Thus we intentionally set aside goals
		  for informal and surface learning. We argue that to be
		  effective, such a search-centric learning system must model
		  four key components: individual students (searcher
		  factors), the educational domain (topic factors), academic
		  assignments (task factors), and progress toward learning
		  goals (the objective function of the end-to-end system). In
		  modeling these components, our hypothetical system makes
		  inferences about students’ learning histories, knowledge
		  states, comprehension, and the utilities of different types
		  of information resources. We present examples of possible
		  techniques and data sources for each model. We also
		  introduce the novel concept of leveraging school
		  assignments as rich task context. Our intention is not to
		  propose a functional system, but to frame
		  search-as-learning in the context of comprehension and to
		  inspire research questions arising from an end-to-end view
		  of this important research domain.},
  booktitle	= {Proceedings of the 2022 Conference on Human Information
		  Interaction and Retrieval},
  pages		= {191–201},
  numpages	= {11},
  keywords	= {interfaces for learning, retrieval models,
		  search-as-learning, self-regulated learning, user models},
  location	= {Regensburg, Germany},
  series	= {CHIIR '22}
}

@InProceedings{	  10.1145/3604915.3608775,
  author	= {Wu, Yaxiong and Macdonald, Craig and Ounis, Iadh},
  title		= {Goal-Oriented Multi-Modal Interactive Recommendation with
		  Verbal and Non-Verbal Relevance Feedback},
  year		= {2023},
  isbn		= {9798400702419},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604915.3608775},
  doi		= {10.1145/3604915.3608775},
  abstract	= {Interactive recommendation enables users to provide verbal
		  and non-verbal relevance feedback (such as natural-language
		  critiques and likes/dislikes) when viewing a ranked list of
		  recommendations (such as images of fashion products), in
		  order to guide the recommender system towards their desired
		  items (i.e. goals) across multiple interaction turns. Such
		  a multi-modal interactive recommendation (MMIR) task has
		  been successfully formulated with deep reinforcement
		  learning (DRL) algorithms by simulating the interactions
		  between an environment (i.e. a user) and an agent (i.e. a
		  recommender system). However, it is typically challenging
		  and unstable to optimise the agent to improve the
		  recommendation quality associated with implicit learning of
		  multi-modal representations in an end-to-end fashion in
		  DRL. This is known as the coupling of policy optimisation
		  and representation learning. To address this coupling
		  issue, we propose a novel goal-oriented multi-modal
		  interactive recommendation model (GOMMIR) that uses both
		  verbal and non-verbal relevance feedback to effectively
		  incorporate the users’ preferences over time.
		  Specifically, our GOMMIR model employs a multi-task
		  learning approach to explicitly learn the multi-modal
		  representations using a multi-modal composition network
		  when optimising the recommendation agent. Moreover, we
		  formulate the MMIR task using goal-oriented reinforcement
		  learning and enhance the optimisation objective by
		  leveraging non-verbal relevance feedback for hard negative
		  sampling and providing extra goal-oriented rewards to
		  effectively optimise the recommendation agent. Following
		  previous work, we train and evaluate our GOMMIR model by
		  using user simulators that can generate natural-language
		  feedback about the recommendations as a surrogate for real
		  human users. Experiments conducted on four well-known
		  fashion datasets demonstrate that our proposed GOMMIR model
		  yields significant improvements in comparison to the
		  existing state-of-the-art baseline models.},
  booktitle	= {Proceedings of the 17th ACM Conference on Recommender
		  Systems},
  pages		= {362–373},
  numpages	= {12},
  keywords	= {interactive recommendation, multi-modal, reinforcement
		  learning, relevance feedback},
  location	= {Singapore, Singapore},
  series	= {RecSys '23}
}

@InProceedings{	  10.1145/3608298.3608324,
  author	= {Schl\"{o}r, Daniel and Pfister, Jan and Hotho, Andreas},
  title		= {Optimizing Medical Service Request Processes through
		  Language Modeling and Semantic Search},
  year		= {2023},
  isbn		= {9798400700712},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3608298.3608324},
  doi		= {10.1145/3608298.3608324},
  abstract	= {Medical service requests are a crucial part of the
		  workflow in hospitals and healthcare organizations.
		  However, the process of requesting medical services can be
		  time consuming and can require physicians and medical
		  personnel to navigate complex interfaces and enter detailed
		  information about the requested service. In this paper, we
		  propose a system that uses machine learning techniques such
		  as large language models and semantic search to optimize
		  the process of requesting medical services. Our approach
		  enables physicians to request medical services using
		  natural language rather than navigating complex interfaces,
		  allowing for more efficient and flexible interactions with
		  hospital information systems. We evaluate our approach on
		  real-world data and discuss the implications of our work
		  for the future of digital health care. Our results suggest
		  that our approach has the potential to streamline the
		  process of requesting medical services and reduce the time
		  and manual effort required in the daily hospital routine.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Medical and Health Informatics},
  pages		= {136–141},
  numpages	= {6},
  keywords	= {language modeling, medical service optimization, semantic
		  search},
  location	= {Kyoto, Japan},
  series	= {ICMHI '23}
}

@Article{	  10.1145/3552311,
  author	= {Wang, Yashen and Wang, Zhaoyu and Zhang, Huanhuan and Liu,
		  Zhirun},
  title		= {Microblog Retrieval Based on Concept-Enhanced Pre-Training
		  Model},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {3},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3552311},
  doi		= {10.1145/3552311},
  abstract	= {Despite substantial interest in applications of neural
		  networks to information retrieval, neural ranking models
		  have mostly been applied to conventional ad-hoc retrieval
		  tasks over web pages and newswire articles. This article
		  proposes a concept-enhanced pre-training model for
		  microblog retrieval task, leveraging Semantic Matching
		  Model (SMM) objective and Concept Correlation Model (CCM)
		  objective. The proposed model is a novel neural ranking
		  model specifically designed for ranking short-text
		  microblog, which could merge the advantage of pre-training
		  methodology for generating valid contextualized embedding
		  with the superiority of the prior lexical knowledge (e.g.,
		  concept knowledge) for understanding short-text language
		  semantic. We conduct experiments on widely used real-world
		  datasets, and the experimental results demonstrate the
		  efficiency of the proposed model, even compared with latest
		  state-of-the-art neural-based models and pre-training based
		  models.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= feb,
  articleno	= {41},
  numpages	= {32},
  keywords	= {Microblog retrieval, pre-training mechanism, concept
		  semantic knowledge, representation learning}
}

@InProceedings{	  10.1145/3589132.3625629,
  author	= {Wang, Zhaonan and Jin, Bowen and Hu, Wei and Jiang, Minhao
		  and Kang, Seungyeon and Li, Zhiyuan and Zhou, Sizhe and
		  Han, Jiawei and Wang, Shaowen},
  title		= {Geospatial Knowledge Hypercube},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589132.3625629},
  doi		= {10.1145/3589132.3625629},
  abstract	= {Today a tremendous amount of geospatial knowledge is
		  hidden in massive volumes of text data. To facilitate
		  flexible and powerful geospatial analysis and applications,
		  we introduce a new architecture: geospatial knowledge
		  hypercube, a multi-scale, multidimensional knowledge
		  structure that integrates information from geospatial
		  dimensions, thematic themes and diverse application
		  semantics, extracted and computed from spatial-related text
		  data. To construct such a knowledge hypercube, weakly
		  supervised language models are leveraged for automatic,
		  dynamic and incremental extraction of heterogeneous
		  geospatial data, thematic themes, latent connections and
		  relationships, and application semantics, through combining
		  a variety of information from unstructured text, structured
		  tables, and maps. The hypercube lays a foundation for many
		  knowledge discovery and in-depth spatial analysis, and
		  other advanced applications. We have deployed a prototype
		  web application of proposed geospatial knowledge hypercube
		  for public access at:
		  https://hcwebapp.cigi.illinois.edu/.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {79},
  numpages	= {4},
  keywords	= {knowledge hypercube, geographic information retrieval,
		  weakly-supervised text classification},
  location	= {Hamburg, Germany},
  series	= {SIGSPATIAL '23}
}

@InProceedings{	  10.1145/3534678.3539021,
  author	= {Huang, Jizhou and Wang, Haifeng and Sun, Yibo and Shi,
		  Yunsheng and Huang, Zhengjie and Zhuo, An and Feng,
		  Shikun},
  title		= {ERNIE-GeoL: A Geography-and-Language Pre-trained Model and
		  its Applications in Baidu Maps},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539021},
  doi		= {10.1145/3534678.3539021},
  abstract	= {Pre-trained models (PTMs) have become a fundamental
		  backbone for downstream tasks in natural language
		  processing and computer vision. Despite initial gains that
		  were obtained by applying generic PTMs to geo-related tasks
		  at Baidu Maps, a clear performance plateau over time was
		  observed. One of the main reasons for this plateau is the
		  lack of readily available geographic knowledge in generic
		  PTMs. To address this problem, in this paper, we present
		  ERNIE-GeoL, which is a geography-and-language pre-trained
		  model designed and developed for improving the geo-related
		  tasks at Baidu Maps. ERNIE-GeoL is elaborately designed to
		  learn a universal representation of geography-language by
		  pre-training on large-scale data generated from a
		  heterogeneous graph that contains abundant geographic
		  knowledge. Extensive quantitative and qualitative
		  experiments conducted on large-scale real-world datasets
		  demonstrate the superiority and effectiveness of
		  ERNIE-GeoL. ERNIE-GeoL has already been deployed in
		  production at Baidu Maps since April 2021, which
		  significantly benefits the performance of various
		  downstream tasks. This demonstrates that ERNIE-GeoL can
		  serve as a fundamental backbone for a wide range of
		  geo-related tasks.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3029–3039},
  numpages	= {11},
  keywords	= {graph neural network, heterogeneous graph, pre-training},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3488560.3498378,
  author	= {Gao, Shen and Zhang, Yuchi and Wang, Yongliang and Dong,
		  Yang and Chen, Xiuying and Zhao, Dongyan and Yan, Rui},
  title		= {HeteroQA: Learning towards Question-and-Answering through
		  Multiple Information Sources via Heterogeneous Graph
		  Modeling},
  year		= {2022},
  isbn		= {9781450391320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3488560.3498378},
  doi		= {10.1145/3488560.3498378},
  abstract	= {Community Question Answering (CQA) is a well-defined task
		  that can be used in many scenarios, such as E-Commerce and
		  online user community for special interests. In these
		  communities, users can post articles, give comment, raise a
		  question and answer it. These data form the heterogeneous
		  information sources where each information source have
		  their own special structure and context (comments attached
		  to an article or related question with answers). Most of
		  the CQA methods only incorporate articles or Wikipedia to
		  extract knowledge and answer the user's question. However,
		  various types of information sources in the community are
		  not fully explored by these CQA methods and these multiple
		  information sources (MIS) can provide more related
		  knowledge to user's questions. Thus, we propose a
		  question-aware heterogeneous graph transformer to
		  incorporate the MIS in the user community to automatically
		  generate the answer. To evaluate our proposed method, we
		  conduct the experiments on two datasets: $textMSM ^textplus
		  $ the modified version of benchmark dataset MS-MARCO and
		  the AntQA dataset which is the first large-scale CQA
		  dataset with four types of MIS. Extensive experiments on
		  two datasets show that our model outperforms all the
		  baselines in terms of all the metrics.},
  booktitle	= {Proceedings of the Fifteenth ACM International Conference
		  on Web Search and Data Mining},
  pages		= {307–315},
  numpages	= {9},
  keywords	= {heterogeneous graph, question answering system, text
		  generation},
  location	= {Virtual Event, AZ, USA},
  series	= {WSDM '22}
}

@InProceedings{	  10.1145/3539618.3591997,
  author	= {Lu, Jiaying and Shen, Jiaming and Xiong, Bo and Ma,
		  Wenjing and Staab, Steffen and Yang, Carl},
  title		= {HiPrompt: Few-Shot Biomedical Knowledge Fusion via
		  Hierarchy-Oriented Prompting},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591997},
  doi		= {10.1145/3539618.3591997},
  abstract	= {Medical decision-making processes can be enhanced by
		  comprehensive biomedical knowledge bases, which require
		  fusing knowledge graphs constructed from different sources
		  via a uniform index system. The index system often
		  organizes biomedical terms in a hierarchy to provide the
		  aligned entities with fine-grained granularity. To address
		  the challenge of scarce supervision in the biomedical
		  knowledge fusion (BKF) task, researchers have proposed
		  various unsupervised methods. However, these methods
		  heavily rely on ad-hoc lexical and structural matching
		  algorithms, which fail to capture the rich semantics
		  conveyed by biomedical entities and terms. Recently, neural
		  embedding models have proved effective in semantic-rich
		  tasks, but they rely on sufficient labeled data to be
		  adequately trained. To bridge the gap between the
		  scarce-labeled BKF and neural embedding models, we propose
		  HiPrompt, a supervision-efficient knowledge fusion
		  framework that elicits the few-shot reasoning ability of
		  large language models through hierarchy-oriented prompts.
		  Empirical results on the collected KG-Hi-BKF benchmark
		  datasets demonstrate the effectiveness of HiPrompt.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2052–2056},
  numpages	= {5},
  keywords	= {biomedical knowledge fusion, few-shot prompting, large
		  language models for resource-constrained field, retrieve
		  &amp; re-rank},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Article{	  10.14778/3476311.3476402,
  author	= {Orr, Laurel and Sanyal, Atindriyo and Ling, Xiao and Goel,
		  Karan and Leszczynski, Megan},
  title		= {Managing ML pipelines: feature stores and the coming wave
		  of embedding ecosystems},
  year		= {2021},
  issue_date	= {July 2021},
  publisher	= {VLDB Endowment},
  volume	= {14},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3476311.3476402},
  doi		= {10.14778/3476311.3476402},
  abstract	= {The industrial machine learning pipeline requires
		  iterating on model features, training and deploying models,
		  and monitoring deployed models at scale. Feature stores
		  were developed to manage and standardize the engineer's
		  workflow in this end-to-end pipeline, focusing on
		  traditional tabular feature data. In recent years, however,
		  model development has shifted towards using self-supervised
		  pretrained embeddings as model features. Managing these
		  embeddings and the downstream systems that use them
		  introduces new challenges with respect to managing
		  embedding training data, measuring embedding quality, and
		  monitoring downstream models that use embeddings. These
		  challenges are largely unaddressed in standard feature
		  stores. Our goal in this tutorial is to introduce the
		  feature store system and discuss the challenges and current
		  solutions to managing these new embedding-centric
		  pipelines.},
  journal	= {Proc. VLDB Endow.},
  month		= jul,
  pages		= {3178–3181},
  numpages	= {4}
}

@Article{	  10.1145/3589131,
  author	= {Van Thin, Dang and Hao, Duong Ngoc and Nguyen, Ngan
		  Luu-Thuy},
  title		= {Vietnamese Sentiment Analysis: An Overview and Comparative
		  Study of Fine-tuning Pretrained Language Models},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3589131},
  doi		= {10.1145/3589131},
  abstract	= {Sentiment Analysis (SA) is one of the most active research
		  areas in the Natural Language Processing (NLP) field due to
		  its potential for business and society. With the
		  development of language representation models, numerous
		  methods have shown promising efficiency in fine-tuning
		  pre-trained language models in NLP downstream tasks. For
		  Vietnamese, many available pre-trained language models were
		  also released, including the monolingual and multilingual
		  language models. Unfortunately, all of these models were
		  trained on different architectures, pre-trained data, and
		  pre-processing steps; consequently, fine-tuning these
		  models can be expected to yield different effectiveness. In
		  addition, there is no study focusing on evaluating the
		  performance of these models on the same datasets for the SA
		  task up to now. This article presents a fine-tuning
		  approach to investigate the performance of different
		  pre-trained language models for the Vietnamese SA task. The
		  experimental results show the superior performance of the
		  monolingual PhoBERT model and ViT5 model in comparison with
		  previous studies and provide new state-of-the-art
		  performances on five benchmark Vietnamese SA datasets. To
		  the best of our knowledge, our study is the first attempt
		  to investigate the performance of fine-tuning
		  Transformer-based models on five datasets with different
		  domains and sizes for the Vietnamese SA task.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {166},
  numpages	= {27},
  keywords	= {Vietnamese Sentiment Analysis, fine-tuning language
		  models, monolingual BERT model, multilingual BERT model, T5
		  architecture}
}

@InProceedings{	  10.1145/3539618.3592010,
  author	= {Tavares, Diogo and Semedo, David and Rudnicky, Alexander
		  and Magalhaes, Joao},
  title		= {Learning to Ask Questions for Zero-shot Dialogue State
		  Tracking},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3592010},
  doi		= {10.1145/3539618.3592010},
  abstract	= {We present a method for performing zero-shot Dialogue
		  State Tracking (DST) by casting the task as a
		  learning-to-ask-questions framework. The framework learns
		  to pair the best question generation (QG) strategy with
		  in-domain question answering (QA) methods to extract slot
		  values from a dialogue without any human intervention. A
		  novel self-supervised QA pretraining step using in-domain
		  data is essential to learn the structure without requiring
		  any slot-filling annotations. Moreover, we show that QG
		  methods need to be aligned with the same grammatical person
		  used in the dialogue. Empirical evaluation on the MultiWOZ
		  2.1 dataset demonstrates that our approach, when used
		  alongside robust QA models, outperforms existing zero-shot
		  methods in the challenging task of zero-shot cross domain
		  adaptation-given a comparable amount of domain knowledge
		  during data creation. Finally, we analyze the impact of the
		  types of questions used, and demonstrate that the
		  algorithmic approach outperforms template-based question
		  generation.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2118–2122},
  numpages	= {5},
  keywords	= {dialogue state tracking, question answering, zero-shot},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@InProceedings{	  10.1145/3448016.3457542,
  author	= {Li, Guoliang and Zhou, Xuanhe and Cao, Lei},
  title		= {AI Meets Database: AI4DB and DB4AI},
  year		= {2021},
  isbn		= {9781450383431},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3448016.3457542},
  doi		= {10.1145/3448016.3457542},
  abstract	= {Database and Artificial Intelligence (AI) can benefit from
		  each other. On one hand, AI can make database more
		  intelligent (AI4DB). For example, traditional empirical
		  database optimization techniques (e.g., cost estimation,
		  join order selection, knob tuning, index and view advisor)
		  cannot meet the high-performance requirement for
		  large-scale database instances, various applications and
		  diversified users, especially on the cloud. Fortunately,
		  learning-based techniques can alleviate this problem. On
		  the other hand, database techniques can optimize AI models
		  (DB4AI). For example, AI is hard to deploy, because it
		  requires developers to write complex codes and train
		  complicated models. Database techniques can be used to
		  reduce the complexity of using AI models, accelerate AI
		  algorithms and provide AI capability inside databases.
		  DB4AI and AI4DB have been extensively studied recently. In
		  this tutorial, we review existing studies on AI4DB and
		  DB4AI. For AI4DB, we review the techniques on
		  learning-based database configuration, optimization,
		  design, monitoring, and security. For DB4AI, we review
		  AI-oriented declarative language, data governance, training
		  acceleration, and inference acceleration. Finally, we
		  provide research challenges and future directions in AI4DB
		  and DB4AI.},
  booktitle	= {Proceedings of the 2021 International Conference on
		  Management of Data},
  pages		= {2859–2866},
  numpages	= {8},
  keywords	= {AI, database, machine learning, models},
  location	= {Virtual Event, China},
  series	= {SIGMOD '21}
}

@Article{	  10.1109/taslp.2023.3304481,
  author	= {Zhang, Ying and Meng, Fandong and Chen, Yufeng and Xu,
		  Jinan and Zhou, Jie},
  title		= {Complex Question Enhanced Transfer Learning for Zero-Shot
		  Joint Information Extraction},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3304481},
  doi		= {10.1109/TASLP.2023.3304481},
  abstract	= {Zero-shot information extraction (IE) tasks have attracted
		  great attention recently. However, how to jointly model
		  multiple IE tasks in the zero-shot scenario is still an
		  open question. In this article, we focus on zero-shot joint
		  IE tasks and highlight how to transfer the knowledge of
		  cross-task relations from the source domain to the target
		  domain. To solve this problem, we first unify all IE tasks
		  with a machine reading comprehension (MRC) framework, which
		  can make the most of training data and enhance its ability
		  on span extraction. Then, we generate &lt;italic&gt;complex
		  questions&lt;/italic&gt; to explicitly model cross-task
		  relations with natural language descriptions, thereby
		  providing prior knowledge for pre-defined types and
		  building more general linkages among different entities and
		  triggers as well. Specifically, we define three operations
		  for generating templates for complex questions, i.e.,
		  &lt;italic&gt;intersecting&lt;/italic&gt;,
		  &lt;italic&gt;connecting&lt;/italic&gt;, and
		  &lt;italic&gt;composing&lt;/italic&gt;. Besides, we design
		  an efficient training strategy to exploit the synthetic
		  data with complex questions. We evaluate our approach on
		  four datasets from different domains for various IE tasks.
		  Experimental results show the effectiveness of our approach
		  in improving the performance of zero-shot joint IE tasks in
		  multiple domains.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {261–275},
  numpages	= {15}
}

@InProceedings{	  10.1145/3529372.3530922,
  author	= {Brack, Arthur and Hoppe, Anett and Buscherm\"{o}hle,
		  Pascal and Ewerth, Ralph},
  title		= {Cross-domain multi-task learning for sequential sentence
		  classification in research papers},
  year		= {2022},
  isbn		= {9781450393454},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3529372.3530922},
  doi		= {10.1145/3529372.3530922},
  abstract	= {Sequential sentence classification deals with the
		  categorisation of sentences based on their content and
		  context. Applied to scientific texts, it enables the
		  automatic structuring of research papers and the
		  improvement of academic search engines. However, previous
		  work has not investigated the potential of transfer
		  learning for sentence classification across different
		  scientific domains and the issue of different text
		  structure of full papers and abstracts. In this paper, we
		  derive seven related research questions and present several
		  contributions to address them: First, we suggest a novel
		  uniform deep learning architecture and multi-task learning
		  for cross-domain sequential sentence classification in
		  scientific texts. Second, we tailor two common transfer
		  learning methods, sequential transfer learning and
		  multi-task learning, to deal with the challenges of the
		  given task. Semantic relatedness of tasks is a prerequisite
		  for successful transfer learning of neural models.
		  Consequently, our third contribution is an approach to
		  semi-automatically identify semantically related classes
		  from different annotation schemes and we present an
		  analysis of four annotation schemes. Comprehensive
		  experimental results indicate that models, which are
		  trained on datasets from different scientific domains,
		  benefit from one another when using the proposed multi-task
		  learning architecture. We also report comparisons with
		  several state-of-the-art approaches. Our approach
		  outperforms the state of the art on full paper datasets
		  significantly while being on par for datasets consisting of
		  abstracts.},
  booktitle	= {Proceedings of the 22nd ACM/IEEE Joint Conference on
		  Digital Libraries},
  articleno	= {34},
  numpages	= {13},
  keywords	= {multi-task learning, scholarly communication, sequential
		  sentence classification, transfer learning, zone
		  identification},
  location	= {Cologne, Germany},
  series	= {JCDL '22}
}

@Article{	  10.1145/3539014,
  author	= {Chen, Jiusheng and Xu, Xingkai and Zhang, Xiaoyu},
  title		= {Radial Basis Function Attention for Named Entity
		  Recognition},
  year		= {2022},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3539014},
  doi		= {10.1145/3539014},
  abstract	= {Attention mechanism is an increasingly important approach
		  in the field of natural language processing (NLP). In the
		  attention-based named entity recognition (NER) model, most
		  attention mechanisms can calculate attention coefficient to
		  express the importance of sentence semantic information but
		  cannot adjust the position distribution of contextual
		  feature vectors in the semantic space. To address this
		  issue, a radial basis function attention (RBF-attention)
		  layer is proposed to adaptively regulate the position
		  distribution of sequence contextual feature vectors, which
		  can minimize the relative distance of within-category named
		  entities and maximize the relative distance of
		  between-category named entities in the semantic space. The
		  experimental results on CoNLL2003 English and MSRA Chinese
		  NER datasets indicate that the proposed model performs
		  better than other baseline approaches without relying on
		  any external feature engineering.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {30},
  numpages	= {18},
  keywords	= {RBF-attention, NER, self-attention, BiLSTM}
}

@Article{	  10.1109/taslp.2020.3042006,
  author	= {Duan, Chaoqun and Chen, Kehai and Wang, Rui and Utiyama,
		  Masao and Sumita, Eiichiro and Zhu, Conghui and Zhao,
		  Tiejun},
  title		= {Modeling Future Cost for Neural Machine Translation},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2020.3042006},
  doi		= {10.1109/TASLP.2020.3042006},
  abstract	= {Existing neural machine translation (NMT) systems utilize
		  sequence-to-sequence neural networks to generate target
		  translation word by word, and then make the generated word
		  at each time-step and the counterpart in the references as
		  consistent as possible. However, the trained translation
		  model tends to focus on ensuring the accuracy of the
		  generated target word at the current time-step and does not
		  consider its future cost which means the expected cost of
		  generating the subsequent target translation (i.e., the
		  next target word). To respond to this issue, in this
		  article, we propose a simple and effective method to model
		  the future cost of each target word for NMT systems. In
		  detail, a future cost representation is learned based on
		  the current generated target word and its contextual
		  information to compute an additional loss to guide the
		  training of the NMT model. Furthermore, the learned future
		  cost representation at the current time-step is used to
		  help the generation of the next target word in the
		  decoding. Experimental results on three widely-used
		  translation datasets, including the WMT14
		  English-to-German, WMT14 English-to-French, and WMT17
		  Chinese-to-English, show that the proposed approach
		  achieves significant improvements over strong
		  Transformer-based NMT baseline.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jan,
  pages		= {770–781},
  numpages	= {12}
}

@Article{	  10.14778/3494124.3494129,
  author	= {Yuan, Ye and Ma, Delong and Wen, Zhenyu and Zhang, Zhiwei
		  and Wang, Guoren},
  title		= {Subgraph matching over graph federation},
  year		= {2021},
  issue_date	= {November 2021},
  publisher	= {VLDB Endowment},
  volume	= {15},
  number	= {3},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3494124.3494129},
  doi		= {10.14778/3494124.3494129},
  abstract	= {Many real-life applications require processing graph data
		  across heterogeneous sources. In this paper, we define the
		  graph federation that indicates that the graph data sources
		  are temporarily federated and offer their data for users.
		  Next, we propose a new framework FedGraph to efficiently
		  and effectively perform subgraph matching, which is a
		  crucial application in graph federation. FedGraph consists
		  of three phases, including query decomposition, distributed
		  matching, and distributed joining. We also develop new
		  efficient approximation algorithms and apply them in each
		  phase to attack the NP-hard problem. The evaluations are
		  conducted in a real test bed using both real-life and
		  synthetic graph datasets. FedGraph outperforms the
		  state-of-the-art methods, reducing the execution time and
		  communication cost by 37.3 \texttimes{} and 61.8
		  \texttimes{}, respectively.},
  journal	= {Proc. VLDB Endow.},
  month		= nov,
  pages		= {437–450},
  numpages	= {14}
}

@Proceedings{	  10.1145/3617695,
  title		= {BDIOT '23: Proceedings of the 2023 7th International
		  Conference on Big Data and Internet of Things},
  year		= {2023},
  isbn		= {9798400708015},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@InProceedings{	  10.1145/3557915.3560999,
  author	= {Hong, Zhiqing and Yang, Heng and Wang, Haotian and Lyu,
		  Wenjun and Yang, Yu and Wang, Guang and Liu, Yunhuai and
		  Wang, Yang and Zhang, Desheng},
  title		= {FastAddr: real-time abnormal address detection via
		  contrastive augmentation for location-based services},
  year		= {2022},
  isbn		= {9781450395298},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3557915.3560999},
  doi		= {10.1145/3557915.3560999},
  abstract	= {An address, a textual description of a physical location,
		  plays an important role in location-based services such as
		  on-demand delivery and e-commerce. However, abnormal
		  addresses (i.e., an address without detailed information
		  representing a spatial location) have led to significant
		  costs. In real-world settings like e-commerce, abnormal
		  address detection is not trivial because it needs to be
		  completed in real-time to support massive online queries.
		  In this study, we design FastAddr, a fast abnormal address
		  detection framework, which detects abnormal addresses among
		  millions of addresses in a short time. By investigating and
		  modeling the hierarchical structure of address data, we
		  first design a novel contrastive address augmentation
		  approach to generate training data via learning the entity
		  transition probability matrix. We further design a
		  lightweight multi-head attention model for learning compact
		  address representation by modeling the address
		  characteristics. We conduct a comprehensive three-phase
		  evaluation. (i) We evaluate FastAddr on a real-world
		  dataset and it yields the average F1 of 85.7% in 0.058
		  milliseconds, which outperforms the state-of-the-art models
		  by 47.4% with similar detection time. (ii) An offline A/B
		  test shows that FastAddr outperforms the previous deployed
		  model significantly. (iii) We also conduct an online A/B
		  test to compare FastAddr with the deployed model, which
		  shows an improvement of F1 by more than 20%. Moreover, a
		  real-world case study demonstrates both the efficiency and
		  effectiveness of FastAddr.},
  booktitle	= {Proceedings of the 30th International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {64},
  numpages	= {10},
  keywords	= {contrastive augmentation, geocoding, multi-head attention,
		  real-time abnormal address detection},
  location	= {Seattle, Washington},
  series	= {SIGSPATIAL '22}
}

@InProceedings{	  10.1145/3457682.3457751,
  author	= {Deng, Fei and Zhang, Dongdong and Peng, Jing},
  title		= {Biological Named Entity Recognition and Role Labeling via
		  Deep Multi-task Learning},
  year		= {2021},
  isbn		= {9781450389310},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3457682.3457751},
  doi		= {10.1145/3457682.3457751},
  abstract	= {Bioscience is an experimental science. The qualitative and
		  quantitative findings of the biological experiments are
		  often exclusively available in the form of figures in
		  published papers. In this paper, we introduce the
		  SourceData model, which captures a key aspect of the
		  biological experimental design by categorizing biological
		  entity involved in the experiment into one of the six
		  roles. Our work aims at determining whether a given entity
		  is subjected to a perturbation or is the object of a
		  measurement (entity role labeling) through automatic
		  natural language algorithms. We use state-of-the-art
		  transformer models (e.g., Bert and its variants) as a
		  strong baseline, find that after jointly trained with
		  biological named entity recognition task by deep multi-task
		  learning (MTL), the F1 score gets improved by 2% compared
		  to previous single-task architecture. Also, for named
		  entity recognition task, the MTL method achieves comparable
		  performance in five public datasets. Further analysis
		  reveals the importance of fusing entity information at the
		  input layer of entity role labeling task and incorporating
		  global context.},
  booktitle	= {Proceedings of the 2021 13th International Conference on
		  Machine Learning and Computing},
  pages		= {450–455},
  numpages	= {6},
  keywords	= {entity role labeling, figure caption, multi-task learning,
		  named entity recognition},
  location	= {Shenzhen, China},
  series	= {ICMLC '21}
}

@InProceedings{	  10.1145/3543507.3583250,
  author	= {Li, Yingjie and Zhao, Chenye and Caragea, Cornelia},
  title		= {TTS: A Target-based Teacher-Student Framework for
		  Zero-Shot Stance Detection},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583250},
  doi		= {10.1145/3543507.3583250},
  abstract	= {The goal of zero-shot stance detection (ZSSD) is to
		  identify the stance (in favor of, against, or neutral) of a
		  text towards an unseen target in the inference stage. In
		  this paper, we explore this problem from a novel angle by
		  proposing a Target-based Teacher-Student learning (TTS)
		  framework. Specifically, we first augment the training set
		  by extracting diversified targets that are unseen during
		  training with a keyphrase generation model. Then, we
		  develop a teacher-student framework which effectively
		  utilizes the augmented data. Extensive experiments show
		  that our model significantly outperforms state-of-the-art
		  ZSSD baselines on the available benchmark dataset for this
		  task by 8.9% in macro-averaged F1. In addition, previous
		  ZSSD requires human-annotated targets and labels during
		  training, which may not be available in real-world
		  applications. Therefore, we go one step further by
		  proposing a more challenging open-world ZSSD task:
		  identifying the stance of a text towards an unseen target
		  without human-annotated targets and stance labels. We show
		  that our TTS can be easily adapted to the new task.
		  Remarkably, TTS without human-annotated targets and stance
		  labels even significantly outperforms previous
		  state-of-the-art ZSSD baselines trained with
		  human-annotated data. We publicly release our code 1 to
		  facilitate future research.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1500–1509},
  numpages	= {10},
  keywords	= {data augmentation, stance detection, zero-shot learning},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3581783.3612053,
  author	= {Li, Bobo and Fei, Hao and Liao, Lizi and Zhao, Yu and
		  Teng, Chong and Chua, Tat-Seng and Ji, Donghong and Li,
		  Fei},
  title		= {Revisiting Disentanglement and Fusion on Modality and
		  Context in Conversational Multimodal Emotion Recognition},
  year		= {2023},
  isbn		= {9798400701085},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581783.3612053},
  doi		= {10.1145/3581783.3612053},
  abstract	= {It has been a hot research topic to enable machines to
		  understand human emotions in multimodal contexts under
		  dialogue scenarios, which is tasked with multimodal emotion
		  analysis in conversation (MM-ERC). MM-ERC has received
		  consistent attention in recent years, where a diverse range
		  of methods has been proposed for securing better task
		  performance. Most existing works treat MM-ERC as a standard
		  multimodal classification problem and perform multimodal
		  feature disentanglement and fusion for maximizing feature
		  utility. Yet after revisiting the characteristic of MM-ERC,
		  we argue that both the feature multimodality and
		  conversational contextualization should be properly modeled
		  simultaneously during the feature disentanglement and
		  fusion steps. In this work, we target further pushing the
		  task performance by taking full consideration of the above
		  insights. On the one hand, during feature disentanglement,
		  based on the contrastive learning technique, we devise a
		  Dual-level Disentanglement Mechanism (DDM) to decouple the
		  features into both the modality space and utterance space.
		  On the other hand, during the feature fusion stage, we
		  propose a Contribution-aware Fusion Mechanism (CFM) and a
		  Context Refusion Mechanism (CRM) for multimodal and context
		  integration, respectively. They together schedule the
		  proper integrations of multimodal and context features.
		  Specifically, CFM explicitly manages the multimodal feature
		  contributions dynamically, while CRM flexibly coordinates
		  the introduction of dialogue contexts. On two public MM-ERC
		  datasets, our system achieves new state-of-the-art
		  performance consistently. Further analyses demonstrate that
		  all our proposed mechanisms greatly facilitate the MM-ERC
		  task by making full use of the multimodal and context
		  features adaptively. Note that our proposed methods have
		  the great potential to facilitate a broader range of other
		  conversational multimodal tasks.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Multimedia},
  pages		= {5923–5934},
  numpages	= {12},
  keywords	= {emotion recognition, multimodal learning},
  location	= {Ottawa ON, Canada},
  series	= {MM '23}
}

@InProceedings{	  10.1145/3581641.3584034,
  author	= {Fok, Raymond and Kambhamettu, Hita and Soldaini, Luca and
		  Bragg, Jonathan and Lo, Kyle and Hearst, Marti and Head,
		  Andrew and Weld, Daniel S},
  title		= {Scim: Intelligent Skimming Support for Scientific Papers},
  year		= {2023},
  isbn		= {9798400701061},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581641.3584034},
  doi		= {10.1145/3581641.3584034},
  abstract	= {Scholars need to keep up with an exponentially increasing
		  flood of scientific papers. To aid this challenge, we
		  introduce Scim, a novel intelligent interface that helps
		  experienced researchers skim – or rapidly review – a
		  paper to attain a cursory understanding of its contents.
		  Scim supports the skimming process by highlighting salient
		  paper contents in order to direct a reader’s attention.
		  The system’s highlights are faceted by content type,
		  evenly distributed across a paper, and have a density
		  configurable by readers at both the global and local level.
		  We evaluate Scim with both an in-lab usability study and a
		  longitudinal diary study, revealing how its highlights
		  facilitate the more efficient construction of a
		  conceptualization of a paper. We conclude by discussing
		  design considerations and tensions for the design of future
		  intelligent skimming tools.},
  booktitle	= {Proceedings of the 28th International Conference on
		  Intelligent User Interfaces},
  pages		= {476–490},
  numpages	= {15},
  keywords	= {Intelligent reading interfaces, highlights, scientific
		  papers, skimming},
  location	= {Sydney, NSW, Australia},
  series	= {IUI '23}
}

@InProceedings{	  10.1145/3491102.3517551,
  author	= {Wang, Yunlong and Venkatesh, Priyadarshini and Lim, Brian
		  Y},
  title		= {Interpretable Directed Diversity: Leveraging Model
		  Explanations for Iterative Crowd Ideation},
  year		= {2022},
  isbn		= {9781450391573},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3491102.3517551},
  doi		= {10.1145/3491102.3517551},
  abstract	= {Feedback in creativity support tools can help crowdworkers
		  to improve their ideations. However, current feedback
		  methods require human assessment from facilitators or
		  peers. This is not scalable to large crowds. We propose
		  Interpretable Directed Diversity to automatically predict
		  ideation quality and diversity scores, and provide AI
		  explanations — Attribution, Contrastive Attribution, and
		  Counterfactual Suggestions — to feedback on why ideations
		  were scored (low), and how to get higher scores. These
		  explanations provide multi-faceted feedback as users
		  iteratively improve their ideations. We conducted formative
		  and controlled user studies to understand the usage and
		  usefulness of explanations to improve ideation diversity
		  and quality. Users appreciated that explanation feedback
		  helped focus their efforts and provided directions for
		  improvement. This resulted in explanations improving
		  diversity compared to no feedback or feedback with scores
		  only. Hence, our approach opens opportunities for
		  explainable AI towards scalable and rich feedback for
		  iterative crowd ideation and creativity support tools.},
  booktitle	= {Proceedings of the 2022 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {183},
  numpages	= {28},
  keywords	= {Collective Creativity, Crowdsourcing, Diversity,
		  Explainable AI},
  location	= {New Orleans, LA, USA},
  series	= {CHI '22}
}

@InProceedings{	  10.1145/3583780.3614923,
  author	= {Dong, Qian and Liu, Yiding and Ai, Qingyao and Li, Haitao
		  and Wang, Shuaiqiang and Liu, Yiqun and Yin, Dawei and Ma,
		  Shaoping},
  title		= {I3 Retriever: Incorporating Implicit Interaction in
		  Pre-trained Language Models for Passage Retrieval},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614923},
  doi		= {10.1145/3583780.3614923},
  abstract	= {Passage retrieval is a fundamental task in many
		  information systems, such as web search and question
		  answering, where both efficiency and effectiveness are
		  critical concerns. In recent years, neural retrievers based
		  on pre-trained language models (PLM), such as
		  dual-encoders, have achieved huge success. Yet, studies
		  have found that the performance of dual-encoders are often
		  limited due to the neglecting of the interaction
		  information between queries and candidate passages.
		  Therefore, various interaction paradigms have been proposed
		  to improve the performance of vanilla dual-encoders.
		  Particularly, recent state-of-the-art methods often
		  introduce late-interaction during the model inference
		  process. However, such late-interaction based methods
		  usually bring extensive computation and storage cost on
		  large corpus. Despite their effectiveness, the concern of
		  efficiency and space footprint is still an important factor
		  that limits the application of interaction-based neural
		  retrieval models. To tackle this issue, we Incorporate
		  Implicit Interaction into dual-encoders, and propose I3
		  retriever. In particular, our implicit interaction paradigm
		  leverages generated pseudo-queries to simulate
		  query-passage interaction, which jointly optimizes with
		  query and passage encoders in an end-to-end manner. It can
		  be fully pre-computed and cached, and its inference process
		  only involves simple dot product operation of the query
		  vector and passage vector, which makes it as efficient as
		  the vanilla dual encoders. We conduct comprehensive
		  experiments on MSMARCO and TREC2019 Deep Learning Datasets,
		  demonstrating the I3 retriever's superiority in terms of
		  both effectiveness and efficiency. Moreover, the proposed
		  implicit interaction is compatible with special
		  pre-training and knowledge distillation for passage
		  retrieval, which brings a new state-of-the-art performance.
		  The codes are available at
		  https://github.com/Deriq-Qian-Dong/III-Retriever.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {441–451},
  numpages	= {11},
  keywords	= {language models, learning to rank, semantic matching},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3469213.3471370,
  author	= {Qiu, Jing},
  title		= {Design and Implementation of English Online Translation
		  Software Based on Intelligent Proofreading System},
  year		= {2021},
  isbn		= {9781450390200},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3469213.3471370},
  doi		= {10.1145/3469213.3471370},
  abstract	= {With the continuous development of modern information
		  technology, the ways and means of learning English have
		  become more diversified. Especially, the emergence of
		  English electronic dictionary makes English learning more
		  and more simple. This paper puts forward the design idea of
		  English online translation software based on intelligent
		  proofreading system. Firstly, the semantic ontology model
		  is created by creating semantic ontology model and phrase
		  translation combination translation algorithm. In addition,
		  the overall system is designed. The hardware and software
		  of the system are designed through the overall architecture
		  of the system, and the hardware design is mainly the search
		  module, so as to improve the search accuracy. In the
		  process of software module, it mainly includes information
		  transmission model, system network topology design and
		  system function design, and can support translation and
		  proofreading of multiple languages. Finally, the designed
		  system is tested. The test results show that this method is
		  accurate and intelligent in English automatic
		  translation.},
  booktitle	= {2021 2nd International Conference on Artificial
		  Intelligence and Information Systems},
  articleno	= {339},
  numpages	= {5},
  location	= {Chongqing, China},
  series	= {ICAIIS 2021}
}

@InProceedings{	  10.1145/3485447.3512031,
  author	= {Yang, Aobo and Wang, Nan and Cai, Renqin and Deng, Hongbo
		  and Wang, Hongning},
  title		= {Comparative Explanations of Recommendations},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3512031},
  doi		= {10.1145/3485447.3512031},
  abstract	= {As recommendation is essentially a comparative (or
		  ranking) process, a good explanation should illustrate to
		  users why an item is believed to be better than another,
		  i.e., comparative explanations about the recommended items.
		  Ideally, after reading the explanations, a user should
		  reach the same ranking of items as the system’s.
		  Unfortunately, little research attention has yet been paid
		  on such comparative explanations. In this work, we develop
		  an extract-and-refine architecture to explain the relative
		  comparisons among a set of ranked items from a recommender
		  system. For each recommended item, we first extract one
		  sentence from its associated reviews that best suits the
		  desired comparison against a set of reference items. Then
		  this extracted sentence is further articulated with respect
		  to the target user through a generative model to better
		  explain why the item is recommended. We design a new
		  explanation quality metric based on BLEU to guide the
		  end-to-end training of the extraction and refinement
		  components, which avoids generation of generic content.
		  Extensive offline evaluations on two large recommendation
		  benchmark datasets and serious user studies against an
		  array of state-of-the-art explainable recommendation
		  algorithms demonstrate the necessity of comparative
		  explanations and the effectiveness of our solution.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {3113–3123},
  numpages	= {11},
  keywords	= {comparative explanation, explainable recommendation,
		  extract-and-refine, text generation},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Proceedings{	  10.1145/3575882,
  title		= {IC3INA '22: Proceedings of the 2022 International
		  Conference on Computer, Control, Informatics and Its
		  Applications},
  year		= {2022},
  isbn		= {9781450397902},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, Indonesia}
}

@Article{	  10.1145/3582495,
  author	= {Zhou, Kun and Wang, Hui and Wen, Ji-rong and Zhao, Wayne
		  Xin},
  title		= {Enhancing Multi-View Smoothness for Sequential
		  Recommendation Models},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3582495},
  doi		= {10.1145/3582495},
  abstract	= {Sequential recommendation models aim to predict the
		  interested items to a user based on his historical
		  behaviors. To train sequential recommenders, implicit
		  feedback data is widely adopted since it is easier to
		  obtain than explicit feedback data. In the setting of
		  implicit feedback, a user’s historical behaviors can be
		  characterized as a chronologically ordered sequence of
		  interacted items. From a perspective of machine learning,
		  the historical interaction sequence and the recommended
		  items can be considered as context and label, respectively,
		  which are usually in one-hot representations in the
		  recommendation models.However, due to the discrete nature,
		  one-hot representations are hard to sufficiently reflect
		  the underlying user preference, and might also contain
		  noise from implicit feedback that will mislead the model
		  training. To solve these issues, we propose a general
		  optimization framework, Multi-View Smoothness (MVS), to
		  enhance the smoothness of sequential recommendation models
		  in both data representations and model learning.
		  Specifically, with the help of a complementary model, we
		  smooth and enrich the one-hot representations of contexts
		  and labels to better depict the underlying user preference
		  (i.e., context smoothness and label smoothness), and devise
		  a model regularization strategy to enforce the neighborhood
		  smoothness of the model itself (i.e., model smoothness).
		  Based on these strategies, we design three regularizers to
		  constrain and improve the training of sequential
		  recommendation models. Extensive experiments on five
		  datasets show that our approach is able to improve the
		  performance of various base models consistently and
		  outperform other regularization training methods.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= apr,
  articleno	= {107},
  numpages	= {27},
  keywords	= {Sequential recommendation, data smoothness, model
		  smoothness}
}

@InProceedings{	  10.1145/3510003.3510068,
  author	= {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu,
		  Yifan and Hoffmann, Henry and Maire, Michael and Lu, Shan},
  title		= {Automated testing of software that uses machine learning
		  APIs},
  year		= {2022},
  isbn		= {9781450392211},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3510003.3510068},
  doi		= {10.1145/3510003.3510068},
  abstract	= {An increasing number of software applications incorporate
		  machine learning (ML) solutions for cognitive tasks that
		  statistically mimic human behaviors. To test such software,
		  tremendous human effort is needed to design
		  image/text/audio inputs that are relevant to the software,
		  and to judge whether the software is processing these
		  inputs as most human beings do. Even when misbehavior is
		  exposed, it is often unclear whether the culprit is inside
		  the cognitive ML API or the code using the API.This paper
		  presents Keeper, a new testing tool for software that uses
		  cognitive ML APIs. Keeper designs a pseudo-inverse function
		  for each ML API that reverses the corresponding cognitive
		  task in an empirical way (e.g., an image search engine
		  pseudo-reverses the image-classification API), and
		  incorporates these pseudo-inverse functions into a symbolic
		  execution engine to automatically generate relevant
		  image/text/audio inputs and judge output correctness. Once
		  misbehavior is exposed, Keeper attempts to change how ML
		  APIs are used in software to alleviate the misbehavior. Our
		  evaluation on a variety of open-source applications shows
		  that Keeper greatly improves the branch coverage, while
		  identifying many previously unknown bugs.},
  booktitle	= {Proceedings of the 44th International Conference on
		  Software Engineering},
  pages		= {212–224},
  numpages	= {13},
  keywords	= {machine learning, machine learning API, software testing},
  location	= {Pittsburgh, Pennsylvania},
  series	= {ICSE '22}
}

@Proceedings{	  10.1145/3603781,
  title		= {CNIOT '23: Proceedings of the 2023 4th International
		  Conference on Computing, Networks and Internet of Things},
  year		= {2023},
  isbn		= {9798400700705},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@InProceedings{	  10.1145/3404835.3463234,
  author	= {Jia, Qinglin and Li, Jingjie and Zhang, Qi and He,
		  Xiuqiang and Zhu, Jieming},
  title		= {RMBERT: News Recommendation via Recurrent Reasoning Memory
		  Network over BERT},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3463234},
  doi		= {10.1145/3404835.3463234},
  abstract	= {Personalized news recommendation aims to alleviate
		  information overload and help users find news of their
		  interests. Accurately matching candidate news and users'
		  interests is the key to news recommendation. Most existing
		  methods separately encode each user and news into vectors
		  by news contents and then match the two vectors. However, a
		  user's interest may differ in each news or each topic of
		  one news. It's necessary to dynamically learn user and news
		  vector and model their interaction. In this work, we
		  present Recurrent Reasoning Memory Network over BERT
		  (RMBERT) for news recommendation. Compared with other
		  methods, our approach can leverage the ability of content
		  modeling from BERT. Moreover, the recurrent reasoning
		  memory network which performs a series of attention based
		  reasoning steps can dynamically learn user and news vector
		  and model their interaction in each step. As a result, our
		  approach can better model user's interests. We conduct
		  extensive experiments on a real-world news recommendation
		  dataset and the results show that our approach
		  significantly outperforms existing state-of-the-art
		  methods.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1773–1777},
  numpages	= {5},
  keywords	= {attention model, deep neural network, news
		  recommendation},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@Article{	  10.1145/3622863,
  author	= {Chen, Qiaochu and Banerjee, Arko and Demiralp,
		  \c{C}a\u{g}atay and Durrett, Greg and Dillig, I\c{s}\i{}l},
  title		= {Data Extraction via Semantic Regular Expression
		  Synthesis},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {OOPSLA2},
  url		= {https://doi.org/10.1145/3622863},
  doi		= {10.1145/3622863},
  abstract	= {Many data extraction tasks of practical relevance require
		  not only syntactic pattern matching but also semantic
		  reasoning about the content of the underlying text. While
		  regular expressions are very well suited for tasks that
		  require only syntactic pattern matching, they fall short
		  for data extraction tasks that involve both a syntactic and
		  semantic component. To address this issue, we introduce
		  semantic regexes, a generalization of regular expressions
		  that facilitates combined syntactic and semantic reasoning
		  about textual data. We also propose a novel learning
		  algorithm that can synthesize semantic regexes from a small
		  number of positive and negative examples. Our proposed
		  learning algorithm uses a combination of neural sketch
		  generation and compositional type-directed synthesis for
		  fast and effective generalization from a small number of
		  examples. We have implemented these ideas in a new tool
		  called Smore and evaluated it on representative data
		  extraction tasks involving several textual datasets. Our
		  evaluation shows that semantic regexes can better support
		  complex data extraction tasks than standard regular
		  expressions and that our learning algorithm significantly
		  outperforms existing tools, including state-of-the-art
		  neural networks and program synthesis tools.},
  journal	= {Proc. ACM Program. Lang.},
  month		= oct,
  articleno	= {287},
  numpages	= {30},
  keywords	= {Program Synthesis, Regular Expression}
}

@Proceedings{	  10.1145/3625469,
  title		= {IMMS '23: Proceedings of the 2023 6th International
		  Conference on Information Management and Management
		  Science},
  year		= {2023},
  isbn		= {9798400707681},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Chengdu, China}
}

@InProceedings{	  10.1145/3447548.3467227,
  author	= {Xiong, Hao and Yan, Junchi and Pan, Li},
  title		= {Contrastive Multi-View Multiplex Network Embedding with
		  Applications to Robust Network Alignment},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467227},
  doi		= {10.1145/3447548.3467227},
  abstract	= {Despite its success in learning network node
		  representations, network embedding is still relatively new
		  for multiplex networks (MNs) with multiple types of edges.
		  In such networks, the inter-layer anchor links are usually
		  missing, which represent the alignment relations between
		  nodes on different layers and are a crucial prerequisite
		  for many cross-network applications like network alignment.
		  For mining such anchor links between layers for MNs,
		  multiplex network embedding (MNE) has become one of the
		  most promising techniques. In this paper, we consider two
		  problems for MNs: 1) edges can be missing to different
		  extent, and data augmentation may mitigate this issue; 2)
		  the known alignment anchor links between layers can be
		  misleading since the behaviors of nodes on different layers
		  are not always consistent, so the most informative ones
		  should be emphasized compared with those misleading ones.
		  However, most existing works neglect the two problems and
		  simply 1) adopt one structural view for all the layers
		  (e.g. random walk with the same window size) and 2) equally
		  extract information from all the anchor links. We propose
		  an end-to-end contrastive framework called cM2NE for MNE,
		  utilizing multiple structural views for each layer and
		  learning with several plug-in components for different
		  scenarios. Through end-to-end optimization on three levels,
		  the intra-view, inter-view, and inter-layer level, our
		  framework achieves to select the fitted views for different
		  layers and maximize the inter-layer mutual information by
		  emphasizing those most informative anchor links. Extensive
		  experimental results on real-world datasets for node
		  classification and multi-network alignment show that our
		  approach consistently outperforms peer methods.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {1913–1923},
  numpages	= {11},
  keywords	= {contrastive learning, multiple structural views, multiplex
		  network embedding, network alignment, node classification},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@Article{	  10.1145/3581786,
  author	= {Dusart, Alexis and Pinel-Sauvagnat, Karen and Hubert,
		  Gilles},
  title		= {TSSuBERT: How to Sum Up Multiple Years of Reading in a Few
		  Tweets},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3581786},
  doi		= {10.1145/3581786},
  abstract	= {The development of deep neural networks and the emergence
		  of pre-trained language models such as BERT allow to
		  increase performance on many NLP tasks. However, these
		  models do not meet the same popularity for tweet stream
		  summarization, which is probably because their computation
		  limitation requires to drastically truncate the textual
		  input.Our contribution in this article is threefold. First,
		  we propose a neural model to automatically and
		  incrementally summarize huge tweet streams. This extractive
		  model combines in an original way pre-trained language
		  models and vocabulary frequency based representations to
		  predict tweet salience. An additional advantage of the
		  model is that it automatically adapts the size of the
		  output summary according to the input tweet stream. Second,
		  we detail an original methodology to construct tweet stream
		  summarization datasets requiring little human effort.
		  Third, we release the TES 2012-2016 dataset constructed
		  using the aforementioned methodology. Baselines, oracle
		  summaries, gold standard, and qualitative assessments are
		  made publicly available.To evaluate our approach, we
		  conducted extensive quantitative experiments using three
		  different tweet collections as well as an additional
		  qualitative evaluation. Results show that our method
		  outperforms state-of-the-art ones. We believe that this
		  work opens avenues of research for incremental
		  summarization, which has not received much attention yet.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= apr,
  articleno	= {109},
  numpages	= {33},
  keywords	= {Text summarization, tweet stream, Twitter, BERT, dataset}
}

@Article{	  10.1145/3592601,
  author	= {Gharagozlou, Hamid and Mohammadzadeh, Javad and
		  Bastanfard, Azam and Ghidary, Saeed Shiry},
  title		= {Semantic Relation Extraction: A Review of Approaches,
		  Datasets, and Evaluation Methods With Looking at the
		  Methods and Datasets in the Persian Language},
  year		= {2023},
  issue_date	= {July 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {7},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3592601},
  doi		= {10.1145/3592601},
  abstract	= {A large volume of unstructured data, especially text data,
		  is generated and exchanged daily. Consequently, the
		  importance of extracting patterns and discovering knowledge
		  from textual data is significantly increasing. As the task
		  of automatically recognizing the relations between two or
		  more entities, semantic relation extraction has a prominent
		  role in the exploitation of raw text. This article surveys
		  different approaches and types of relation extraction in
		  English and the most prominent proposed methods in Persian.
		  We also introduce, analyze, and compare the most important
		  datasets available for relation extraction in Persian and
		  English. Furthermore, traditional and emerging evaluation
		  metrics for supervised, semi-supervised, and unsupervised
		  methods are described, along with pointers to commonly used
		  performance evaluation datasets. Finally, we briefly
		  describe challenges in extracting relationships in Persian
		  and English and dataset creation challenges.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  articleno	= {189},
  numpages	= {29},
  keywords	= {Semantic relations, relation extraction, Natural Language
		  Processing (NLP), automatic extraction, Persian text
		  processing, linguistics, dataset, evaluation methods,
		  information extraction}
}

@InProceedings{	  10.1145/3442381.3450111,
  author	= {Pelrine, Kellin and Danovitch, Jacob and Rabbany,
		  Reihaneh},
  title		= {The Surprising Performance of Simple Baselines for
		  Misinformation Detection},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450111},
  doi		= {10.1145/3442381.3450111},
  abstract	= {As social media becomes increasingly prominent in our day
		  to day lives, it is increasingly important to detect
		  informative content and prevent the spread of
		  disinformation and unverified rumours. While many
		  sophisticated and successful models have been proposed in
		  the literature, they are often compared with older NLP
		  baselines such as SVMs, CNNs, and LSTMs. In this paper, we
		  examine the performance of a broad set of modern
		  transformer-based language models and show that with basic
		  fine-tuning, these models are competitive with and can even
		  significantly outperform recently proposed state-of-the-art
		  methods. We present our framework as a baseline for
		  creating and evaluating new methods for misinformation
		  detection. We further study a comprehensive set of
		  benchmark datasets, and discuss potential data leakage and
		  the need for careful design of the experiments and
		  understanding of datasets to account for confounding
		  variables. As an extreme case example, we show that
		  classifying only based on the first three digits of tweet
		  ids, which contain information on the date, gives
		  state-of-the-art performance on a commonly used benchmark
		  dataset for fake news detection –Twitter16. We provide a
		  simple tool to detect this problem and suggest steps to
		  mitigate it in future datasets.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {3432–3441},
  numpages	= {10},
  keywords	= {COVID-19, datasets, misinformation, natural language
		  processing, social media},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Proceedings{	  10.1145/3584684,
  title		= {ApPLIED 2023: Proceedings of the 5th workshop on Advanced
		  tools, programming languages, and PLatforms for
		  Implementing and Evaluating algorithms for Distributed
		  systems},
  year		= {2023},
  isbn		= {9798400701283},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Orlando, FL, USA}
}

@InProceedings{	  10.1145/3583780.3614896,
  author	= {Liu, Meizhen and He, Jiakai and Guo, Xu and Chen, Jianye
		  and Hui, Siu Cheung and Zhou, Fengyu},
  title		= {GranCATs: Cross-Lingual Enhancement through
		  Granularity-Specific Contrastive Adapters},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614896},
  doi		= {10.1145/3583780.3614896},
  abstract	= {Multilingual language models (MLLMs) have demonstrated
		  remarkable success in various cross-lingual downstream
		  tasks, facilitating the transfer of knowledge across
		  numerous languages, whereas this transfer is not
		  universally effective. Our study reveals that while
		  existing MLLMs like mBERT can capturephrase-level
		  alignments across the language families, they struggle to
		  effectively capturesentence-level andparagraph-level
		  alignments. To address this limitation, we propose
		  GranCATs, Granularity-specific Contrastive AdapTers. We
		  collect a new dataset that observes each sample at three
		  distinct levels of granularity and employ contrastive
		  learning as a pre-training task to train GranCATs on this
		  dataset. Our objective is to enhance MLLMs' adaptation to a
		  broader range of cross-lingual tasks by equipping them with
		  improved capabilities to capture global information at
		  different levels of granularity. Extensive experiments show
		  that MLLMs with GranCATs yield significant performance
		  advancements across various language tasks with different
		  text granularities, including entity alignment, relation
		  extraction, sentence classification and retrieval, and
		  question-answering. These results validate the
		  effectiveness of our proposed GranCATs in enhancing
		  cross-lingual alignments across various text granularities
		  and effectively transferring this knowledge to downstream
		  tasks.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1461–1471},
  numpages	= {11},
  keywords	= {adapters, cross-lingual alignments, multilingual language
		  models, universal patterns},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3543507.3583513,
  author	= {Zhang, Chi and Chen, Rui and Zhao, Xiangyu and Han, Qilong
		  and Li, Li},
  title		= {Denoising and Prompt-Tuning for Multi-Behavior
		  Recommendation},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583513},
  doi		= {10.1145/3543507.3583513},
  abstract	= {In practical recommendation scenarios, users often
		  interact with items under multi-typed behaviors (e.g.,
		  click, add-to-cart, and purchase). Traditional
		  collaborative filtering techniques typically assume that
		  users only have a single type of behavior with items,
		  making it insufficient to utilize complex collaborative
		  signals to learn informative representations and infer
		  actual user preferences. Consequently, some pioneer studies
		  explore modeling multi-behavior heterogeneity to learn
		  better representations and boost the performance of
		  recommendations for a target behavior. However, a large
		  number of auxiliary behaviors (i.e., click and add-to-cart)
		  could introduce irrelevant information to recommenders,
		  which could mislead the target behavior (i.e., purchase)
		  recommendation, rendering two critical challenges: (i)
		  denoising auxiliary behaviors and (ii) bridging the
		  semantic gap between auxiliary and target behaviors.
		  Motivated by the above observation, we propose a novel
		  framework–Denoising and Prompt-Tuning (DPT) with a
		  three-stage learning paradigm to solve the aforementioned
		  challenges. In particular, DPT is equipped with a
		  pattern-enhanced graph encoder in the first stage to learn
		  complex patterns as prior knowledge in a data-driven manner
		  to guide learning informative representation and
		  pinpointing reliable noise for subsequent stages.
		  Accordingly, we adopt different lightweight tuning
		  approaches with effectiveness and efficiency in the
		  following stages to further attenuate the influence of
		  noise and alleviate the semantic gap among multi-typed
		  behaviors. Extensive experiments on two real-world datasets
		  demonstrate the superiority of DPT over a wide range of
		  state-of-the-art methods. The implementation code is
		  available online at https://github.com/zc-97/DPT.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1355–1363},
  numpages	= {9},
  keywords	= {Multi-behavior recommendation, auxiliary behavior
		  denoising, graph neural networks, prompt tuning},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@Proceedings{	  10.1145/3570991,
  title		= {CODS-COMAD '23: Proceedings of the 6th Joint International
		  Conference on Data Science &amp; Management of Data (10th
		  ACM IKDD CODS and 28th COMAD)},
  year		= {2023},
  isbn		= {9781450397971},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Mumbai, India}
}

@Proceedings{	  10.1145/3450569,
  title		= {SACMAT '21: Proceedings of the 26th ACM Symposium on
		  Access Control Models and Technologies},
  year		= {2021},
  isbn		= {9781450383653},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the ACM
		  Symposium on Access Control Models and Technologies (SACMAT
		  2021). This year's symposium continues its tradition of
		  being the premier forum for the presentation of research
		  results and experience reports on leading-edge issues of
		  access control, including models, systems, applications,
		  and theory, while also embracing a renovated focus on the
		  general area of security.The aim of the symposium is to
		  share novel access control and security solutions that
		  fulfill the needs of heterogeneous applications and
		  environments, and to identify new directions for future
		  research and development.SACMAT provides researchers and
		  practitioners with a unique opportunity to share their
		  perspectives with others interested in the various aspects
		  of access control and security.},
  location	= {Virtual Event, Spain}
}

@Article{	  10.1145/3491206,
  author	= {Zhou, Jingya and Liu, Ling and Wei, Wenqi and Fan,
		  Jianxi},
  title		= {Network Representation Learning: From Preprocessing,
		  Feature Extraction to Node Embedding},
  year		= {2022},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3491206},
  doi		= {10.1145/3491206},
  abstract	= {Network representation learning (NRL) advances the
		  conventional graph mining of social networks, knowledge
		  graphs, and complex biomedical and physics information
		  networks. Dozens of NRL algorithms have been reported in
		  the literature. Most of them focus on learning node
		  embeddings for homogeneous networks, but they differ in the
		  specific encoding schemes and specific types of node
		  semantics captured and used for learning node embedding.
		  This article reviews the design principles and the
		  different node embedding techniques for NRL over
		  homogeneous networks. To facilitate the comparison of
		  different node embedding algorithms, we introduce a unified
		  reference framework to divide and generalize the node
		  embedding learning process on a given network into
		  preprocessing steps, node feature extraction steps, and
		  node embedding model training for an NRL task such as link
		  prediction and node clustering. With this unifying
		  reference framework, we highlight the representative
		  methods, models, and techniques used at different stages of
		  the node embedding model learning process. This survey not
		  only helps researchers and practitioners gain an in-depth
		  understanding of different NRL techniques but also provides
		  practical guidelines for designing and developing the next
		  generation of NRL algorithms and systems.},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  articleno	= {38},
  numpages	= {35},
  keywords	= {Network representation learning, data preprocessing,
		  feature extraction, node embedding}
}

@Article{	  10.1145/3604552,
  author	= {Qin, Chuan and Zhu, Hengshu and Shen, Dazhong and Sun,
		  Ying and Yao, Kaichun and Wang, Peng and Xiong, Hui},
  title		= {Automatic Skill-Oriented Question Generation and
		  Recommendation for Intelligent Job Interviews},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3604552},
  doi		= {10.1145/3604552},
  abstract	= {Job interviews are the most widely accepted method for
		  companies to select suitable candidates, and a critical
		  challenge is finding the right questions to ask job
		  candidates. Moreover, there is a lack of integrated tools
		  for automatically generating interview questions and
		  recommending the right questions to interviewers. To this
		  end, in this paper, we propose an intelligent system for
		  assisting job interviews, namely, DuerQues. To build this
		  system, we first investigate how to automatically generate
		  skill-oriented interview questions in a scalable way by
		  learning external knowledge from online knowledge-sharing
		  communities. Along this line, we develop a novel distantly
		  supervised skill entity recognition method to identify
		  skill entities from large-scale search queries and web page
		  titles with less need for human annotation. Additionally,
		  we propose a neural generative model for generating
		  skill-oriented interview questions. In particular, we
		  introduce a data-driven solution to create high-quality
		  training instances and design a learning algorithm to
		  improve the performance of question generation.
		  Furthermore, we exploit click-through data from query logs
		  and design a recommender system for recommending suitable
		  questions to interviewers. Specifically, we introduce a
		  graph-enhanced algorithm to efficiently recommend suitable
		  questions given a set of queried skills. Finally, extensive
		  experiments on real-world datasets demonstrate the
		  effectiveness of our DuerQues system in terms of the
		  quality of generated skill-oriented questions and the
		  performance of question recommendation.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= aug,
  articleno	= {27},
  numpages	= {32},
  keywords	= {Job interview assessment, question generation, question
		  recommendation}
}

@InProceedings{	  10.1145/3577190.3614158,
  author	= {Hensel, Laura Birka and Yongsatianchot, Nutchanon and
		  Torshizi, Parisa and Minucci, Elena and Marsella, Stacy},
  title		= {Large language models in textual analysis for gesture
		  selection},
  year		= {2023},
  isbn		= {9798400700552},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3577190.3614158},
  doi		= {10.1145/3577190.3614158},
  abstract	= {Gestures perform a variety of communicative functions that
		  powerfully influence human face-to-face interaction. How
		  this communicative function is achieved varies greatly
		  between individuals and depends on the role of the speaker
		  and the context of the interaction. Approaches to automatic
		  gesture generation vary not only in the degree to which
		  they rely on data-driven techniques but also the degree to
		  which they can produce context and speaker specific
		  gestures. However, these approaches face two major
		  challenges: The first is obtaining sufficient training data
		  that is appropriate for the context and the goal of the
		  application. The second is related to designer control to
		  realize their specific intent for the application. Here, we
		  approach these challenges by using large language models
		  (LLMs) to show that these powerful models of large amounts
		  of data can be adapted for gesture analysis and generation.
		  Specifically, we used ChatGPT as a tool for suggesting
		  context-specific gestures that can realize designer intent
		  based on minimal prompts. We also find that ChatGPT can
		  suggests novel yet appropriate gestures not present in the
		  minimal training data. The use of LLMs is a promising
		  avenue for gesture generation that reduce the need for
		  laborious annotations and has the potential to flexibly and
		  quickly adapt to different designer intents.},
  booktitle	= {Proceedings of the 25th International Conference on
		  Multimodal Interaction},
  pages		= {378–387},
  numpages	= {10},
  keywords	= {gesture analysis, gesture selection, large language
		  models},
  location	= {Paris, France},
  series	= {ICMI '23}
}

@Article{	  10.1145/3609796,
  author	= {Ma, Yixiao and Wu, Yueyue and Ai, Qingyao and Liu, Yiqun
		  and Shao, Yunqiu and Zhang, Min and Ma, Shaoping},
  title		= {Incorporating Structural Information into Legal Case
		  Retrieval},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3609796},
  doi		= {10.1145/3609796},
  abstract	= {Legal case retrieval has received increasing attention in
		  recent years. However, compared to ad hoc retrieval tasks,
		  legal case retrieval has its unique challenges. First, case
		  documents are rather lengthy and contain complex legal
		  structures. Therefore, it is difficult for most existing
		  dense retrieval models to encode an entire document and
		  capture its inherent complex structure information. Most
		  existing methods simply truncate part of the document
		  content to meet the input length limit of PLMs, which will
		  lead to information loss. Additionally, the definition of
		  relevance in the legal domain differs from that in the
		  general domain. Previous semantic-based or lexical-based
		  methods fail to provide a comprehensive understanding of
		  the relevance of legal cases. In this article, we propose a
		  Structured Legal case Retrieval (SLR) framework, which
		  incorporates internal and external structural information
		  to address the above two challenges. Specifically, to avoid
		  the truncation of long legal documents, the internal
		  structural information, which is the organization pattern
		  of legal documents, can be utilized to split a case
		  document into segments. By dividing the document-level
		  semantic matching task into segment-level subtasks, SLR can
		  separately process segments using different methods based
		  on the characteristic of each segment. In this way, the key
		  elements of a case document can be highlighted without
		  losing other content information. Second, toward a better
		  understanding of relevance in the legal domain, we
		  investigate the connections between criminal charges
		  appearing in large-scale case corpus to generate a
		  chargewise relation graph. Then, the similarity between
		  criminal charges can be pre-computed as the external
		  structural information to enhance the recognition of
		  relevant cases. Finally, a learning-to-rank algorithm
		  integrates the features collected from internal and
		  external structures to output the final retrieval results.
		  Experimental results on public legal case retrieval
		  benchmarks demonstrate the superior effectiveness of SLR
		  over existing state-of-the-art baselines, including
		  traditional bag-of-words and neural-based methods.
		  Furthermore, we conduct a case study to visualize how the
		  proposed model focuses on key elements and improves
		  retrieval performance.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= nov,
  articleno	= {40},
  numpages	= {28},
  keywords	= {Legal case retrieval, structural information, relevance}
}

@Article{	  10.1109/tcbb.2021.3079339,
  author	= {Peng, Keqin and Yin, Chuantao and Rong, Wenge and Lin,
		  Chenghua and Zhou, Deyu and Xiong, Zhang},
  title		= {Named Entity Aware Transfer Learning for Biomedical
		  Factoid Question Answering},
  year		= {2021},
  issue_date	= {July-Aug. 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {4},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2021.3079339},
  doi		= {10.1109/TCBB.2021.3079339},
  abstract	= {Biomedical factoid question answering is an important task
		  in biomedical question answering applications. It has
		  attracted much attention because of its reliability. In
		  question answering systems, better representation of words
		  is of great importance, and proper word embedding can
		  significantly improve the performance of the system. With
		  the success of pretrained models in general natural
		  language processing tasks, pretrained models have been
		  widely used in biomedical areas, and many pretrained
		  model-based approaches have been proven effective in
		  biomedical question-answering tasks. In addition to proper
		  word embedding, name entities also provide important
		  information for biomedical question answering. Inspired by
		  the concept of transfer learning, in this study, we
		  developed a mechanism to fine-tune BioBERT with a named
		  entity dataset to improve the question answering
		  performance. Furthermore, we applied BiLSTM to encode the
		  question text to obtain sentence-level information. To
		  better combine the question level and token level
		  information, we use bagging to further improve the overall
		  performance. The proposed framework was evaluated on BioASQ
		  6b and 7b datasets, and the results have shown that our
		  proposed framework can outperform all baselines.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= may,
  pages		= {2365–2376},
  numpages	= {12}
}

@InProceedings{	  10.1145/3437963.3441833,
  author	= {Rosin, Guy D. and Guy, Ido and Radinsky, Kira},
  title		= {Event-Driven Query Expansion},
  year		= {2021},
  isbn		= {9781450382977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3437963.3441833},
  doi		= {10.1145/3437963.3441833},
  abstract	= {A significant number of event-related queries are issued
		  in Web search. In this paper, we seek to improve retrieval
		  performance by leveraging events and specifically target
		  the classic task of query expansion. We propose a method to
		  expand an event-related query by first detecting the events
		  related to it. Then, we derive the candidates for expansion
		  as terms semantically related to both the query and the
		  events. To identify the candidates, we utilize a novel
		  mechanism to simultaneously embed words and events in the
		  same vector space. We show that our proposed method of
		  leveraging events improves query expansion performance
		  significantly compared with state-of-the-art methods on
		  various newswire TREC datasets.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {391–399},
  numpages	= {9},
  keywords	= {query expansion, temporal semantics, word embeddings},
  location	= {Virtual Event, Israel},
  series	= {WSDM '21}
}

@InProceedings{	  10.1145/3543507.3583191,
  author	= {Zhang, Xuefeng and Zhang, Richong and Li, Xiaoyang and
		  Kong, Fanshuang and Chen, Junfan and Mensah, Samuel and
		  Mao, Yongyi},
  title		= {Word Sense Disambiguation by Refining Target Word
		  Embedding},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583191},
  doi		= {10.1145/3543507.3583191},
  abstract	= {Word Sense Disambiguation (WSD) which aims to identify the
		  correct sense of a target word appearing in a specific
		  context is essential for web text analysis. The use of
		  glosses has been explored as a means for WSD. However, only
		  a few works model the correlation between the target
		  context and gloss. We add to the body of literature by
		  presenting a model that employs a multi-head attention
		  mechanism on deep contextual features of the target word
		  and candidate glosses to refine the target word embedding.
		  Furthermore, to encourage the model to learn the relevant
		  part of target features that align with the correct gloss,
		  we recursively alternate attention on target word features
		  and that of candidate glosses to gradually extract the
		  relevant contextual features of the target word, refining
		  its representation and strengthening the final
		  disambiguation results. Empirical studies on the five most
		  commonly used benchmark datasets show that our proposed
		  model is effective and achieves state-of-the-art results.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1405–1414},
  numpages	= {10},
  keywords	= {Embedding Refinement, Recursive Attention, Word Sense
		  Disambiguation},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3448016.3452763,
  author	= {Ho, Vinh Thinh and Pal, Koninika and Weikum, Gerhard},
  title		= {QuTE: Answering Quantity Queries from Web Tables},
  year		= {2021},
  isbn		= {9781450383431},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3448016.3452763},
  doi		= {10.1145/3448016.3452763},
  abstract	= {Quantities are financial, technological, physical and
		  other measures that denote relevant properties of entities,
		  such as revenue of companies, energy efficiency of cars or
		  distance and brightness of stars and galaxies. Queries with
		  filter conditions on quantities are an important building
		  block for downstream analytics, and pose challenges when
		  the content of interest is spread across a huge number of
		  web tables and other ad-hoc datasets. Search engines
		  support quantity lookups, but largely fail on quantity
		  filters. The QuTE system presented in this paper aims to
		  overcome these problems. It comprises methods for
		  automatically extracting entity-quantity facts from web
		  tables, as well as methods for online query processing,
		  with new techniques for query matching and answer
		  ranking.},
  booktitle	= {Proceedings of the 2021 International Conference on
		  Management of Data},
  pages		= {2740–2744},
  numpages	= {5},
  keywords	= {information extraction, quantity search, web tables},
  location	= {Virtual Event, China},
  series	= {SIGMOD '21}
}

@InProceedings{	  10.1145/3474085.3476968,
  author	= {Zheng, Changmeng and Feng, Junhao and Fu, Ze and Cai, Yi
		  and Li, Qing and Wang, Tao},
  title		= {Multimodal Relation Extraction with Efficient Graph
		  Alignment},
  year		= {2021},
  isbn		= {9781450386517},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3474085.3476968},
  doi		= {10.1145/3474085.3476968},
  abstract	= {Relation extraction (RE) is a fundamental process in
		  constructing knowledge graphs. However, previous methods on
		  relation extraction suffer sharp performance decline in
		  short and noisy social media texts due to a lack of
		  contexts. Fortunately, the related visual contents (objects
		  and their relations) in social media posts can supplement
		  the missing semantics and help to extract relations
		  precisely. We introduce the multimodal relation extraction
		  (MRE), a task that identifies textual relations with visual
		  clues. To tackle this problem, we present a large-scale
		  dataset which contains 15000+ sentences with 23 pre-defined
		  relation categories. Considering that the visual relations
		  among objects are corresponding to textual relations, we
		  develop a dual graph alignment method to capture this
		  correlation for better performance. Experimental results
		  demonstrate that visual contents help to identify relations
		  more precisely against the text-only baselines. Besides,
		  our alignment method can find the correlations between
		  vision and language, resulting in better performance. Our
		  dataset and code are available at
		  https://github.com/thecharm/Mega.},
  booktitle	= {Proceedings of the 29th ACM International Conference on
		  Multimedia},
  pages		= {5298–5306},
  numpages	= {9},
  keywords	= {graph alignment, multimodal dataset, multimodal relation
		  extraction},
  location	= {Virtual Event, China},
  series	= {MM '21}
}

@InProceedings{	  10.1145/3579142.3594293,
  author	= {Sun, Ricky and Chen, Jamie},
  title		= {Design of Highly Scalable Graph Database Systems without
		  Exponential Performance Degradation},
  year		= {2023},
  isbn		= {9798400700934},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579142.3594293},
  doi		= {10.1145/3579142.3594293},
  abstract	= {The main challenge faced by today's graph database systems
		  is sacrificing performance (computation) for scalability
		  (storage). Such systems probably can store a large amount
		  of data across many instances but can't offer adequate
		  graph-computing power to deeply penetrate dynamic graph
		  dataset in real time. A seemingly simple and intuitive
		  graph query like K-hop traversal or finding all shortest
		  paths may lead to deep traversal of large amount of graph
		  data, which tends to cause a typical BSP (Bulky Synchronous
		  Processing) system to exchange heavily amongst its
		  distributed instances, therefore causing significant
		  latencies. This paper proposes three schools of
		  architectural designs for distributed and horizontally
		  scalable graph database while achieving highly performant
		  graph data processing capabilities. The first school,
		  coined HTAP, augments distributed consensus algorithm RAFT
		  paired with vector-based computing acceleration to achieve
		  fast online data ingestion and real-time deep-data
		  traversal in a TP and AP hybrid mode. The second school,
		  named as GRID, leverages human-intelligence for data
		  partitioning, and preserving the HTAP data processing
		  capabilities across all partitioned clusters. The last
		  school incorporates SHARD and advanced GQL optimization
		  techniques to allow data partitioning to be done fully
		  automated yet strive to achieve lower latency via minimum
		  I/O cost data migration model when queries spread across
		  multiple clusters.},
  booktitle	= {Proceedings of the International Workshop on Big Data in
		  Emergent Distributed Environments},
  articleno	= {4},
  numpages	= {6},
  keywords	= {distributed graph database, GQL, graph query optimization,
		  graph data modeling, graph analytics, XAI, HTAP, grid,
		  shard, linear scalability, deep traversal, real-time data
		  processing},
  location	= {Seattle, WA, USA},
  series	= {BiDEDE '23}
}

@Proceedings{	  10.5555/3606013,
  title		= {ICSE '23: Proceedings of the 45th International Conference
		  on Software Engineering: Companion Proceedings},
  year		= {2023},
  isbn		= {9798350322637},
  publisher	= {IEEE Press},
  abstract	= {ICSE is the leading and by far the largest conference in
		  Software Engineering, attracting researchers, practitioners
		  and students from around the world. ICSE2023 is co-located
		  with 10 conferences and symposia this year, many
		  long-established and prestigious venues in their own
		  right.},
  location	= {Melbourne, Victoria, Australia}
}

@Article{	  10.1145/3465055,
  author	= {Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor
		  and Ramanath, Rohan},
  title		= {An Attentive Survey of Attention Models},
  year		= {2021},
  issue_date	= {October 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {12},
  number	= {5},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3465055},
  doi		= {10.1145/3465055},
  abstract	= {Attention Model has now become an important concept in
		  neural networks that has been researched within diverse
		  application domains. This survey provides a structured and
		  comprehensive overview of the developments in modeling
		  attention. In particular, we propose a taxonomy that groups
		  existing techniques into coherent categories. We review
		  salient neural architectures in which attention has been
		  incorporated and discuss applications in which modeling
		  attention has shown a significant impact. We also describe
		  how attention has been used to improve the interpretability
		  of neural networks. Finally, we discuss some future
		  research directions in attention. We hope this survey will
		  provide a succinct introduction to attention models and
		  guide practitioners while developing approaches for their
		  applications.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  articleno	= {53},
  numpages	= {32},
  keywords	= {Attention, attention models, neural networks}
}

@Article{	  10.1145/3554734,
  author	= {Nag, Arijit and Samanta, Bidisha and Mukherjee, Animesh
		  and Ganguly, Niloy and Chakrabarti, Soumen},
  title		= {Transfer Learning for Low-Resource Multilingual Relation
		  Classification},
  year		= {2023},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3554734},
  doi		= {10.1145/3554734},
  abstract	= {Relation classification (sometimes called relation
		  extraction) requires trustworthy datasets for fine-tuning
		  large language models, as well as for evaluation. Data
		  collection is challenging for Indian languages, because
		  they are syntactically and morphologically diverse, as well
		  as different from resource-rich languages like English.
		  Despite recent interest in deep generative models for
		  Indian languages, relation classification is still not well
		  served by public datasets. In response, we present IndoRE,
		  a dataset with 21K entity- and relation-tagged gold
		  sentences in three Indian languages (Bengali, Hindi, and
		  Telugu), plus English. We start with a multilingual BERT
		  (mBERT)-based system that captures entity span positions
		  and type information, and provides competitive performance
		  on monolingual relation classification. Using this baseline
		  system, we explore transfer mechanisms between languages
		  and the scope to reduce expensive data annotation while
		  achieving reasonable relation extraction performance.
		  Specifically, we (a)study the accuracy-efficiency trade-off
		  between expensive, manually labeled gold instances vs.
		  automatically translated and aligned silver instances to
		  train a relation extractor,(b)device a simple mechanism for
		  budgeted gold data annotation by intelligently converting
		  distant-supervised silver training instances to gold
		  training instances with human annotators using active
		  learning, and finally(c)propose an ensemble model to
		  provide a performance boost over that achieved via limited
		  gold training instances. We release the dataset for future
		  research.1},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= mar,
  articleno	= {50},
  numpages	= {24},
  keywords	= {Relation extraction}
}

@InProceedings{	  10.1145/3543507.3583238,
  author	= {Yang, Yuting and Lei, Wenqiang and Huang, Pei and Cao,
		  Juan and Li, Jintao and Chua, Tat-Seng},
  title		= {A Dual Prompt Learning Framework for Few-Shot Dialogue
		  State Tracking},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583238},
  doi		= {10.1145/3543507.3583238},
  abstract	= {Dialogue State Tracking (DST) module is an essential
		  component of task-oriented dialog systems to understand
		  users’ goals and needs. Collecting dialogue state labels
		  including slots and values can be costly, requiring experts
		  to annotate all (slot, value) information for each turn in
		  dialogues. It is also difficult to define all possible
		  slots and values in advance, especially with the wide
		  application of dialogue systems in more and more new-rising
		  applications. In this paper, we focus on improving DST
		  module to generate dialogue states in circumstances with
		  limited annotations and knowledge about slot ontology. To
		  this end, we design a dual prompt learning framework for
		  few-shot DST. The dual framework aims to explore how to
		  utilize the language understanding and generation
		  capabilities of pre-trained language models for DST
		  efficiently. Specifically, we consider the learning of slot
		  generation and value generation as dual tasks, and two
		  kinds of prompts are designed based on this dual structure
		  to incorporate task-related knowledge of these two tasks
		  respectively. In this way, the DST task can be formulated
		  as a language modeling task efficiently under few-shot
		  settings. To evaluate the proposed framework, we conduct
		  experiments on two task-oriented dialogue datasets. The
		  results demonstrate that the proposed method not only
		  outperforms existing state-of-the-art few-shot methods, but
		  also can generate unseen slots. It indicates that
		  DST-related knowledge can be probed from pre-trained
		  language models and utilized to address low-resource DST
		  efficiently with the help of prompt learning.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1468–1477},
  numpages	= {10},
  keywords	= {dialogue state tracking, few-shot learning, prompt
		  learning},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@Article{	  10.1145/3579602,
  author	= {Liu, Yiren and Mayfield, Ryan and Huang, Yun},
  title		= {Discovering the Hidden Facts of User-Dispatcher
		  Interactions via Text-based Reporting Systems for Community
		  Safety},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {CSCW1},
  url		= {https://doi.org/10.1145/3579602},
  doi		= {10.1145/3579602},
  abstract	= {Recently, an increasing number of safety organizations in
		  the U.S. have incorporated text-based risk reporting
		  systems to respond to safety incident reports from their
		  community members. To gain a better understanding of the
		  interaction between community members and dispatchers using
		  text-based risk reporting systems, this study conducts a
		  system log analysis ofLiveSafe, a community safety
		  reporting system, to provide empirical evidence of the
		  conversational patterns between users and dispatchers using
		  both quantitative and qualitative methods. We created an
		  ontology to capture information (e.g., location, attacker,
		  target, weapon, start-time, and end-time, etc.) that
		  dispatchers often collected from users regarding their
		  incident tips. Applying the proposed ontology, we found
		  that dispatchers often asked users for different
		  information across varied event types (e.g.,Attacker
		  forAbuse andAttack events,Target forHarassment events).
		  Additionally, using emotion detection and regression
		  analysis, we found an inconsistency in dispatchers'
		  emotional support and responsiveness to users' messages
		  between different organizations and between incident
		  categories. The results also showed that users had a higher
		  response rate and responded quicker when dispatchers
		  provided emotional support. These novel findings brought
		  significant insights to both practitioners and system
		  designers, e.g., AI-based solutions to augment human
		  agents' skills for improved service quality.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= apr,
  articleno	= {126},
  numpages	= {31},
  keywords	= {conversation analysis, emergency dispatcher, live chat,
		  safety reporting, text-based reporting system}
}

@Article{	  10.14778/3554821.3554899,
  author	= {Fan, Wenfei},
  title		= {Big graphs: challenges and opportunities},
  year		= {2022},
  issue_date	= {August 2022},
  publisher	= {VLDB Endowment},
  volume	= {15},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3554821.3554899},
  doi		= {10.14778/3554821.3554899},
  abstract	= {Big data is typically characterized with 4V's: Volume,
		  Velocity, Variety and Veracity. When it comes to big
		  graphs, these challenges become even more staggering. Each
		  and every of the 4V's raises new questions, from theory to
		  systems and practice. Is it possible to parallelize
		  sequential graph algorithms and guarantee the correctness
		  of the parallelized computations? Given a computational
		  problem, does there exist a parallel algorithm for it that
		  guarantees to reduce parallel runtime when more machines
		  are used? Is there a systematic method for developing
		  incremental algorithms with effectiveness guarantees in
		  response to frequent updates? Is it possible to write
		  queries across relational databases and semistructured
		  graphs in SQL? Can we unify logic rules and machine
		  learning, to improve the quality of graph-structured data,
		  and deduce associations between entities? This paper aims
		  to incite interest and curiosity in these topics. It raises
		  as many questions as it answers.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {3782–3797},
  numpages	= {16}
}

@InProceedings{	  10.1145/3543507.3583379,
  author	= {Hou, Zhenyu and He, Yufei and Cen, Yukuo and Liu, Xiao and
		  Dong, Yuxiao and Kharlamov, Evgeny and Tang, Jie},
  title		= {GraphMAE2: A Decoding-Enhanced Masked Self-Supervised
		  Graph Learner},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583379},
  doi		= {10.1145/3543507.3583379},
  abstract	= {Graph self-supervised learning (SSL), including
		  contrastive and generative approaches, offers great
		  potential to address the fundamental challenge of label
		  scarcity in real-world graph data. Among both sets of graph
		  SSL techniques, the masked graph autoencoders (e.g.,
		  GraphMAE)—one type of generative methods—have recently
		  produced promising results. The idea behind this is to
		  reconstruct the node features (or structures)—that are
		  randomly masked from the input—with the autoencoder
		  architecture. However, the performance of masked feature
		  reconstruction naturally relies on the discriminability of
		  the input features and is usually vulnerable to disturbance
		  in the features. In this paper, we present a masked
		  self-supervised learning framework1 GraphMAE2 with the goal
		  of overcoming this issue. The idea is to impose
		  regularization on feature reconstruction for graph SSL.
		  Specifically, we design the strategies of multi-view random
		  re-mask decoding and latent representation prediction to
		  regularize the feature reconstruction. The multi-view
		  random re-mask decoding is to introduce randomness into
		  reconstruction in the feature space, while the latent
		  representation prediction is to enforce the reconstruction
		  in the embedding space. Extensive experiments show that
		  GraphMAE2 can consistently generate top results on various
		  public datasets, including at least 2.45% improvements over
		  state-of-the-art baselines on ogbn-Papers100M with 111M
		  nodes and 1.6B edges.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {737–746},
  numpages	= {10},
  keywords	= {Graph Neural Networks, Graph Representation Learning,
		  Pre-Training, Self-Supervised Learning},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@Article{	  10.1145/3572403,
  author	= {Yang, Yingguang and Yang, Renyu and Li, Yangyang and Cui,
		  Kai and Yang, Zhiqin and Wang, Yue and Xu, Jie and Xie,
		  Haiyong},
  title		= {RoSGAS: Adaptive Social Bot Detection with Reinforced
		  Self-supervised GNN Architecture Search},
  year		= {2023},
  issue_date	= {August 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {3},
  issn		= {1559-1131},
  url		= {https://doi.org/10.1145/3572403},
  doi		= {10.1145/3572403},
  abstract	= {Social bots are referred to as the automated accounts on
		  social networks that make attempts to behave like humans.
		  While Graph Neural Networks (GNNs) have been massively
		  applied to the field of social bot detection, a huge amount
		  of domain expertise and prior knowledge is heavily engaged
		  in the state-of-the-art approaches to design a dedicated
		  neural network architecture for a specific classification
		  task. Involving oversized nodes and network layers in the
		  model design, however, usually causes the over-smoothing
		  problem and the lack of embedding discrimination. In this
		  article, we propose RoSGAS, a novel Reinforced and
		  Self-supervised GNN Architecture Search framework to
		  adaptively pinpoint the most suitable multi-hop
		  neighborhood and the number of layers in the GNN
		  architecture. More specifically, we consider the social bot
		  detection problem as a user-centric subgraph embedding and
		  classification task. We exploit the heterogeneous
		  information network to present the user connectivity by
		  leveraging account metadata, relationships, behavioral
		  features, and content features. RoSGAS uses a multi-agent
		  deep reinforcement learning (RL), 31 pages. mechanism for
		  navigating the search of optimal neighborhood and network
		  layers to learn individually the subgraph embedding for
		  each target user. A nearest neighbor mechanism is developed
		  for accelerating the RL training process, and RoSGAS can
		  learn more discriminative subgraph embedding with the aid
		  of self-supervised learning. Experiments on five Twitter
		  datasets show that RoSGAS outperforms the state-of-the-art
		  approaches in terms of accuracy, training efficiency, and
		  stability and has better generalization when handling
		  unseen samples.},
  journal	= {ACM Trans. Web},
  month		= may,
  articleno	= {15},
  numpages	= {31},
  keywords	= {Graph neural network, architecture search, reinforcement
		  learning}
}

@InProceedings{	  10.1145/3573428.3573598,
  author	= {Yao, Shunyu and Hu, Jie and Sun, Chuxiong and Gao, Zhiqiao
		  and Liu, Ning},
  title		= {Key Phrase Extraction based on Pre-trained Language
		  Models},
  year		= {2023},
  isbn		= {9781450397148},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3573428.3573598},
  doi		= {10.1145/3573428.3573598},
  abstract	= {With the explosion of information and a large amount of
		  data appearing every moment, it is a meaningful task to
		  quickly find the information people want to know in a large
		  amount of text and to present long texts in a streamlined
		  form. Key phrase extraction, which aims to extract from
		  documents a collection of key phrases that express the
		  topic and content of the document, is important for text
		  processing tasks such as information retrieval and document
		  classification and can provide readers with a more
		  comprehensive overview of the topic. We use two types of
		  pre-trained language models for key phrase extraction,
		  namely DeBERTa and RoBERTa, which are first pre-trained on
		  the dataset and then fine-tuned, and the experimental
		  results of these models proved that DeBERTa-V3-Large has
		  reached an F1 score of 0.8925, which is the best result
		  among these models.},
  booktitle	= {Proceedings of the 2022 6th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {941–945},
  numpages	= {5},
  keywords	= {Artificial Intelligence, Key Phrase Extraction, Natural
		  Language Processing, Neural Network},
  location	= {Xiamen, China},
  series	= {EITCE '22}
}

@Article{	  10.1145/3586075,
  author	= {Das, Ringki and Singh, Thoudam Doren},
  title		= {Multimodal Sentiment Analysis: A Survey of Methods,
		  Trends, and Challenges},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {13s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3586075},
  doi		= {10.1145/3586075},
  abstract	= {Sentiment analysis has come long way since it was
		  introduced as a natural language processing task nearly 20
		  years ago. Sentiment analysis aims to extract the
		  underlying attitudes and opinions toward an entity. It has
		  become a powerful tool used by governments, businesses,
		  medicine, marketing, and others. The traditional sentiment
		  analysis model focuses mainly on text content. However,
		  technological advances have allowed people to express their
		  opinions and feelings through audio, image and video
		  channels. As a result, sentiment analysis is shifting from
		  unimodality to multimodality. Multimodal sentiment analysis
		  brings new opportunities with the rapid increase of
		  sentiment analysis as complementary data streams enable
		  improved and deeper sentiment detection which goes beyond
		  text-based analysis. Audio and video channels are included
		  in multimodal sentiment analysis in terms of broadness.
		  People have been working on different approaches to improve
		  sentiment analysis system performance by employing complex
		  deep neural architectures. Recently, sentiment analysis has
		  achieved significant success using the transformer-based
		  model. This paper presents a comprehensive study of
		  different sentiment analysis approaches, applications,
		  challenges, and resources then concludes that it holds
		  tremendous potential. The primary motivation of this survey
		  is to highlight changing trends in the unimodality to
		  multimodality for solving sentiment analysis tasks.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {270},
  numpages	= {38},
  keywords	= {Multimodal sentiment analysis, text sentiment analysis,
		  image sentiment analysis, audio sentiment analysis,
		  transfer learning}
}

@Article{	  10.1109/taslp.2023.3316459,
  author	= {Atri, Yash Kumar and Goyal, Vikram and Chakraborty,
		  Tanmoy},
  title		= {Multi-Document Summarization Using Selective Attention
		  Span and Reinforcement Learning},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3316459},
  doi		= {10.1109/TASLP.2023.3316459},
  abstract	= {Abstractive text summarization systems using recently
		  improved RNN-based sequence-to-sequence architecture have
		  shown great promise for single-document summarization.
		  However, such neural models fail to perpetuate the
		  performance in the multi-document summarization setting
		  owing to the long-range dependencies within the documents,
		  overlapping/contradicting facts and extrinsic model
		  hallucinations. These shortcomings augment the model to
		  generate inconsistent, repetitive and non-factual
		  summaries. In this work, we introduce
		  &lt;monospace&gt;REISA&lt;/monospace&gt;, a
		  sequence-to-sequence model with a novel
		  &lt;italic&gt;reinforced selective attention
		  span&lt;/italic&gt; that attends over the input and
		  recalibrates the local attention weights to focus on
		  important segments while generating output at each time
		  step. &lt;monospace&gt;REISA&lt;/monospace&gt; utilizes a
		  reinforcement learning-based policy gradient algorithm to
		  reward the model and formulate attention distributions over
		  the encoder input. We further benchmark
		  &lt;monospace&gt;REISA&lt;/monospace&gt; on two widely-used
		  multi-document summarization corpora – Multinews and
		  CQASumm, and observe an improvement of
		  &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$+2.91$&lt;/tex-math&gt;&lt;/inline-formula&gt;
		  and &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$+6.64$&lt;/tex-math&gt;&lt;/inline-formula&gt;
		  ROUGE-L scores, respectively. The qualitative analyses on
		  semantic similarity by BERTScore, faithfulness by
		  question-answer evaluation and human evaluation show
		  significant improvement over the baseline-generated
		  summaries.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= sep,
  pages		= {3457–3467},
  numpages	= {11}
}

@InProceedings{	  10.1145/3555776.3578577,
  author	= {Sousa, Hugo and Mario Jorge, Alipio and Pasquali, Arian
		  and Santos, Catarina and Lopes, Mario},
  title		= {A Biomedical Entity Extraction Pipeline for Oncology
		  Health Records in Portuguese},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555776.3578577},
  doi		= {10.1145/3555776.3578577},
  abstract	= {Textual health records of cancer patients are usually
		  protracted and highly unstructured, making it very
		  time-consuming for health professionals to get a complete
		  overview of the patient's therapeutic course. As such
		  limitations can lead to suboptimal and/or inefficient
		  treatment procedures, healthcare providers would greatly
		  benefit from a system that effectively summarizes the
		  information of those records. With the advent of deep
		  neural models, this objective has been partially attained
		  for English clinical texts, however, the research community
		  still lacks an effective solution for languages with
		  limited resources. In this paper, we present the approach
		  we developed to extract procedures, drugs, and diseases
		  from oncology health records written in European
		  Portuguese. This project was conducted in collaboration
		  with the Portuguese Institute for Oncology which, besides
		  holding over 10 years of duly protected medical records,
		  also provided oncologist expertise throughout the
		  development of the project. Since there is no annotated
		  corpus for biomedical entity extraction in Portuguese, we
		  also present the strategy we followed in annotating the
		  corpus for the development of the models. The final models,
		  which combined a neural architecture with entity linking,
		  achieved F1 scores of 88.6, 95.0, and 55.8 per cent in the
		  mention extraction of procedures, drugs, and diseases,
		  respectively.},
  booktitle	= {Proceedings of the 38th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {950–956},
  numpages	= {7},
  keywords	= {biomedical entity recognition, data mining, oncology
		  electronic health records},
  location	= {Tallinn, Estonia},
  series	= {SAC '23}
}

@Article{	  10.1145/3617330,
  author	= {Fan, Lihang and Fan, Wenfei and Lu, Ping and Tian, Chao
		  and Yin, Qiang},
  title		= {Enriching Recommendation Models with Logic Conditions},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {3},
  url		= {https://doi.org/10.1145/3617330},
  doi		= {10.1145/3617330},
  abstract	= {This paper proposes RecLogic, a framework for improving
		  the accuracy of machine learning (ML) models for
		  recommendation. It aims to enhance existing ML models with
		  logic conditions to reduce false positives and false
		  negatives, without training a new model. Underlying
		  RecLogic are (a) a class of prediction rules on graphs,
		  denoted by TIEs, (b) a new approach to learning TIEs, and
		  (c) a new paradigm for recommendation with TIEs. TIEs may
		  embed ML recommendation models as predicates; as opposed to
		  prior graph rules, it is tractable to decide whether a
		  graph satisfies a set of TIEs. To enrich ML models,
		  RecLogic iteratively trains a generator with feedback from
		  each round, to learn TIEs with a probabilistic bound.
		  RecLogic also provides a PTIME parallel algorithm for
		  making recommendations with the learned TIEs. Using
		  real-life data, we empirically verify that RecLogic
		  improves the accuracy of ML predictions by 22.89% on
		  average in an area where the prediction strength is neither
		  sufficiently large nor sufficiently small, up to 33.10%.},
  journal	= {Proc. ACM Manag. Data},
  month		= nov,
  articleno	= {210},
  numpages	= {28},
  keywords	= {prediction rules, recommender system, rule discovery}
}

@Proceedings{	  10.5555/3590145,
  title		= {ASONAM '22: Proceedings of the 2022 IEEE/ACM International
		  Conference on Advances in Social Networks Analysis and
		  Mining},
  year		= {2022},
  isbn		= {9781665456616},
  publisher	= {IEEE Press},
  abstract	= {We were delighted to welcome each participant at ASONAM
		  2022 and thank you for having contributed virtually or in
		  person in Istanbul. ASONAM 2022 was the fourteenth annual
		  conference in the successful ASONAM conferences series and
		  also the first hybrid version of the conference. Previous
		  ASONAM conferences were held in Athens (2009), Odense
		  (2010), Kaohsiung (2011), Istanbul (2012), Niagara Falls
		  (2013), Beijing (2014), Paris (2015), San Francisco (2016),
		  Sydney (2017), Barcelona (2018), Vancouver (2019), Virtual
		  (2020), Virtual (2021). The pre-pandemic locations of the
		  conferences have enabled the participants to enjoy local
		  sights and to engage in person-to-person interactions,
		  making new contacts and form new scientific collaborations.
		  These possibilities were only available in a limited form
		  during the virtual conferences. As the covid pandemic seems
		  to be moving towards an endemic form it was decided to have
		  the conference in the hybrid form, as a move towards normal
		  endemic in-person conferences.For more than a century,
		  social networks have been studied in a variety of
		  disciplines including sociology, anthropology, psychology,
		  and economics. The Internet, the social Web, and other
		  large-scale, sociotechnological infrastructures have
		  triggered a growing interest and resulted in significant
		  methodological advancements in social network analysis and
		  mining. Method development in graph theory, statistics,
		  data mining, machine learning, and AI have inspired new
		  research problems and, in turn, opens up further
		  possibilities for application. These spiraling trends have
		  led to a rising prominence of social network analysis and
		  mining methods and tools in academia, politics, security,
		  and business.},
  location	= {Istanbul, Turkey}
}

@Article{	  10.1109/taslp.2023.3277245,
  author	= {Liu, Zhidong and Li, Junhui and Zhu, Muhua},
  title		= {Alleviating Exposure Bias for Neural Machine Translation
		  via Contextual Augmentation and Self Distillation},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3277245},
  doi		= {10.1109/TASLP.2023.3277245},
  abstract	= {In neural machine translation (NMT), most
		  sequence-to-sequence (seq2seq) models are trained only with
		  the teacher-forcing paradigm, where the ground truth
		  history is used to predict the next ground truth word. At
		  the inference stage, however, the decoder predicts the next
		  token solely based on history generated from scratch. Both
		  using ground truth history and predicting ground truth
		  words potentially lead to exposure bias. On the one hand,
		  to alleviate the issue of exposure bias caused by using
		  ground truth history, we propose contextual augmentation by
		  allowing substitution, insertion, and deletion of words.
		  The contextual augmentation applies to target sequence to
		  generate non-ground truth and natural history when
		  predicting next words. On the other hand, to alleviate the
		  exposure bias caused by predicting ground truth words, we
		  further apply self distillation to guide the model to carry
		  out optimization according to smoothed prediction
		  distribution, i.e, enable the model to predict not only
		  ground truth words, but also other potentially correct and
		  reasonable words. Experimental results on WMT14 English
		  &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$leftrightarrow$&lt;/tex-math&gt;&lt;/inline-formula&gt;
		  German and IWSLT14 German
		  &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$rightarrow$&lt;/tex-math&gt;&lt;/inline-formula&gt;
		  English translation tasks demonstrate that our approach
		  achieves significant improvements over Transformer on
		  standard benchmarks. Detailed experimental analyses further
		  reveal the effectiveness of our proposed approach in
		  improving translation quality.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= may,
  pages		= {2079–2089},
  numpages	= {11}
}

@Article{	  10.1145/3578362,
  author	= {Meng, Qing and Yan, Hui and Liu, Bo and Sun, Xiangguo and
		  Hu, Mingrui and Cao, Jiuxin},
  title		= {Recognize News Transition from Collective Behavior for
		  News Recommendation},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3578362},
  doi		= {10.1145/3578362},
  abstract	= {In the news recommendation, users are overwhelmed by
		  thousands of news daily, which makes the users’ behavior
		  data have high sparsity. Therefore, only considering a
		  single user’s personalized preferences cannot support the
		  news recommendation. How to improve the relatedness of news
		  and users and reduce data sparsity has become a hot issue.
		  Recent studies have attempted to use graph models to enrich
		  the relationship between users and news, but they are still
		  limited to modeling the historical behaviors of a single
		  user. To fill the gap, we integrate user-news relationships
		  and the overall user historical clicked news sequences to
		  construct a global heterogeneous transition graph. And a
		  refinement approach is proposed to recognize the news
		  transition patterns in the graph. Based on the global
		  heterogeneous transition graph, we propose a heterogeneous
		  transition graph attention network to capture the common
		  behavior patterns of most users to enhance the
		  representation of user interest. Fusing the users’
		  personalized and common interest, we propose the GAINRec
		  model to recommend news effectively. Extensive experiments
		  are conducted on two public news recommendation datasets,
		  and the results show the superiority of the proposed
		  GAINRec model compared with the state-of-the-art news
		  recommendation models. The implementation of our model is
		  available at .},
  journal	= {ACM Trans. Inf. Syst.},
  month		= apr,
  articleno	= {93},
  numpages	= {30},
  keywords	= {News recommendation, transition graph, graph attention
		  network, collective behavior}
}

@InProceedings{	  10.1145/3442442.3452303,
  author	= {Berendt, Bettina and Karadeniz, \"{O}zg\"{u}r and Mertens,
		  Stefan and d'Haenens, Leen},
  title		= {Fairness beyond “equal”: The Diversity Searcher as a
		  Tool to Detect and Enhance the Representation of
		  Socio-political Actors in News Media},
  year		= {2021},
  isbn		= {9781450383134},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442442.3452303},
  doi		= {10.1145/3442442.3452303},
  abstract	= {“Fairness” is a multi-faceted concept that is
		  contested within and across disciplines. In machine
		  learning, it usually denotes some form of equality of
		  measurable outcomes of algorithmic decision making. In this
		  paper, we start from a viewpoint of sociology and media
		  studies, which highlights that to even claim fair
		  treatment, individuals and groups first have to be visible.
		  We draw on a notion and a quantitative measure of diversity
		  that expresses this wider requirement. We used the measure
		  to design and build the Diversity Searcher, a Web-based
		  tool to detect and enhance the representation of
		  socio-political actors in news media. We show how the
		  tool's combination of natural language processing and a
		  rich user interface can help news producers and consumers
		  detect and understand diversity-relevant aspects of
		  representation, which can ultimately contribute to
		  enhancing diversity and fairness in media. We comment on
		  our observation that, through interactions with target
		  users during the construction of the tool, NLP results and
		  interface questions became increasingly important, such
		  that the formal measure of diversity has become a catalyst
		  for functionality, but in itself less important.},
  booktitle	= {Companion Proceedings of the Web Conference 2021},
  pages		= {202–212},
  numpages	= {11},
  keywords	= {Fairness-aware recommender systems and diversity in
		  recommendation,&nbsp;Innovative methods for
		  studying/analyzing the fairness, accountability,
		  transparency and ethics of web platforms,&nbsp;Usability
		  challenges of machine learning},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Article{	  10.14778/3489496.3489504,
  author	= {Ge, Congcong and Liu, Xiaoze and Chen, Lu and Gao, Yunjun
		  and Zheng, Baihua},
  title		= {LargeEA: aligning entities for large-scale knowledge
		  graphs},
  year		= {2021},
  issue_date	= {October 2021},
  publisher	= {VLDB Endowment},
  volume	= {15},
  number	= {2},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3489496.3489504},
  doi		= {10.14778/3489496.3489504},
  abstract	= {Entity alignment (EA) aims to find equivalent entities in
		  different knowledge graphs (KGs). Current EA approaches
		  suffer from scalability issues, limiting their usage in
		  real-world EA scenarios. To tackle this challenge, we
		  propose LargeEA to align entities between large-scale KGs.
		  LargeEA consists of two channels, i.e., structure channel
		  and name channel. For the structure channel, we present
		  METIS-CPS, a memory-saving mini-batch generation strategy,
		  to partition large KGs into smaller mini-batches. LargeEA,
		  designed as a general tool, can adopt any existing EA
		  approach to learn entities' structural features within each
		  mini-batch independently. For the name channel, we first
		  introduce NFF, a name feature fusion method, to capture
		  rich name features of entities without involving any
		  complex training process; we then exploit a name-based data
		  augmentation to generate seed alignment without any human
		  intervention. Such design fits common real-world scenarios
		  much better, as seed alignment is not always available.
		  Finally, LargeEA derives the EA results by fusing the
		  structural features and name features of entities. Since no
		  widely-acknowledged benchmark is available for large-scale
		  EA evaluation, we also develop a large-scale EA benchmark
		  called DBP1M extracted from real-world KGs. Extensive
		  experiments confirm the superiority of LargeEA against
		  state-of-the-art competitors.},
  journal	= {Proc. VLDB Endow.},
  month		= oct,
  pages		= {237–245},
  numpages	= {9}
}

@Article{	  10.1145/3479604,
  author	= {Zhao, Rui and Atkinson, Malcolm and Papapanagiotou, Petros
		  and Magnoni, Federica and Fleuriot, Jacques},
  title		= {Dr.Aid: Supporting Data-governance Rule Compliance for
		  Decentralized Collaboration in an Automated Way},
  year		= {2021},
  issue_date	= {October 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {CSCW2},
  url		= {https://doi.org/10.1145/3479604},
  doi		= {10.1145/3479604},
  abstract	= {Collaboration across institutional boundaries is
		  widespread and increasing today. It depends on federations
		  sharing data that often have governance rules or external
		  regulations restricting their use. However, the handling of
		  data governance rules (aka. data-use policies) remains
		  manual, time-consuming and error-prone, limiting the rate
		  at which collaborations can form and respond to challenges
		  and opportunities, inhibiting citizen science and reducing
		  data providers' trust in compliance. Using an automated
		  system to facilitate compliance handling reduces
		  substantially the time needed for such non-mission work,
		  thereby accelerating collaboration and improving
		  productivity. We present a framework, Dr.Aid, that helps
		  individuals, organisations and federations comply with data
		  rules, using automation to track which rules are applicable
		  as data is passed between processes and as derived data is
		  generated. It encodes data-governance rules using a formal
		  language and performs reasoning on multi-input-multi-output
		  data-flow graphs in decentralised contexts. We test its
		  power and utility by working with users performing cyclone
		  tracking and earthquake modelling to support mitigation and
		  emergency response. We query standard provenance traces to
		  detach Dr.Aid from details of the tools and systems they
		  are using, as these inevitably vary across members of a
		  federation and through time. We evaluate the model in three
		  aspects by encoding real-life data-use policies from
		  diverse fields, showing its capability for real-world usage
		  and its advantages compared with traditional frameworks. We
		  argue that this approach will lead to more agile, more
		  productive and more trustworthy collaborations and show
		  that the approach can be adopted incrementally. This,
		  in-turn, will allow more appropriate data policies to
		  emerge opening up new forms of collaboration.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= oct,
  articleno	= {460},
  numpages	= {43},
  keywords	= {automated reasoning, data governance, data policy, formal
		  model, obligation policy}
}

@Article{	  10.1145/3589784,
  author	= {Dhelim, Sahraoui and Chen, Liming and Das, Sajal K. and
		  Ning, Huansheng and Nugent, Chris and Leavey, Gerard and
		  Pesch, Dirk and Bantry-White, Eleanor and Burns, Devin},
  title		= {Detecting Mental Distresses Using Social Behavior Analysis
		  in the Context of COVID-19: A Survey},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {14s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3589784},
  doi		= {10.1145/3589784},
  abstract	= {Online social media provides a channel for monitoring
		  people’s social behaviors from which to infer and detect
		  their mental distresses. During the COVID-19 pandemic,
		  online social networks were increasingly used to express
		  opinions, views, and moods due to the restrictions on
		  physical activities and in-person meetings, leading to a
		  significant amount of diverse user-generated social media
		  content. This offers a unique opportunity to examine how
		  COVID-19 changed global behaviors regarding its
		  ramifications on mental well-being. In this article, we
		  surveyed the literature on social media analysis for the
		  detection of mental distress, with a special emphasis on
		  the studies published since the COVID-19 outbreak. We
		  analyze relevant research and its characteristics and
		  propose new approaches to organizing the large amount of
		  studies arising from this emerging research area, thus
		  drawing new views, insights, and knowledge for interested
		  communities. Specifically, we first classify the studies in
		  terms of feature extraction types, language usage patterns,
		  aesthetic preferences, and online behaviors. We then
		  explored various methods (including machine learning and
		  deep learning techniques) for detecting mental health
		  problems. Building upon the in-depth review, we present our
		  findings and discuss future research directions and niche
		  areas in detecting mental health problems using social
		  media data. We also elaborate on the challenges of this
		  fast-growing research area, such as technical issues in
		  deploying such systems at scale as well as privacy and
		  ethical concerns.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {318},
  numpages	= {30},
  keywords	= {Social media analysis, mental disorder detection,
		  COVID-19, mental health}
}

@Article{	  10.1145/3626766,
  author	= {Miao, Zhengjie and Wang, Jin},
  title		= {Watchog: A Light-weight Contrastive Learning based
		  Framework for Column Annotation},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {4},
  url		= {https://doi.org/10.1145/3626766},
  doi		= {10.1145/3626766},
  abstract	= {Relational Web tables provide valuable resources for
		  numerous downstream applications, making table
		  understanding, especially column annotation that identifies
		  semantic types and relations of columns, a hot topic in the
		  field of data management. Despite recent efforts to improve
		  different tasks in table understanding by using the power
		  of large pre-trained language models, existing methods
		  heavily rely on large-scale and high-quality labeled
		  instances, while they still suffer from the data sparsity
		  problem due to the imbalanced data distribution among
		  different classes. In this paper, we propose the Watchog
		  framework, which employs contrastive learning techniques to
		  learn robust representations for tables by leveraging a
		  large-scale unlabeled table corpus with minimal overhead.
		  Our approach enables the learned table representations to
		  enhance fine tuning with much fewer additional labeled
		  instances than in prior studies for downstream column
		  annotation tasks. Besides, we further proposed optimization
		  techniques for semi-supervised settings. Experimental
		  results on popular benchmarking datasets illustrate the
		  superiority of our proposed techniques in two column
		  annotation tasks under different settings. In particular,
		  our Watchog framework effectively alleviates the class
		  imbalance issue caused by a long-tailed label distribution.
		  In the semi-supervised setting, Watchog outperforms the
		  best-known method by up to 26% and 41% in Micro and Macro
		  F1 scores, respectively, on the task of semantic type
		  detection.},
  journal	= {Proc. ACM Manag. Data},
  month		= dec,
  articleno	= {272},
  numpages	= {24},
  keywords	= {contrastive learning, semi-supervised learning, table
		  understanding, web tables}
}

@Article{	  10.1145/3563041,
  author	= {Sun, Wei and Ji, Shaoxiong and Cambria, Erik and
		  Marttinen, Pekka},
  title		= {Multitask Balanced and Recalibrated Network for Medical
		  Code Prediction},
  year		= {2022},
  issue_date	= {February 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {1},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3563041},
  doi		= {10.1145/3563041},
  abstract	= {Human coders assign standardized medical codes to clinical
		  documents generated during patients’ hospitalization,
		  which is error prone and labor intensive. Automated medical
		  coding approaches have been developed using machine
		  learning methods, such as deep neural networks.
		  Nevertheless, automated medical coding is still challenging
		  because of complex code association, noise in lengthy
		  documents, and the imbalanced class problem. We propose a
		  novel neural network, called the Multitask Balanced and
		  Recalibrated Neural Network, to solve these issues.
		  Significantly, the multitask learning scheme shares the
		  relationship knowledge between different coding branches to
		  capture code association. A recalibrated aggregation module
		  is developed by cascading convolutional blocks to extract
		  high-level semantic features that mitigate the impact of
		  noise in documents. Also, the cascaded structure of the
		  recalibrated module can benefit learning from lengthy
		  notes. To solve the imbalanced class problem, we deploy
		  focal loss to redistribute the attention on low- and
		  high-frequency medical codes. Experimental results show
		  that our proposed model outperforms competitive baselines
		  on a real-world clinical dataset called the Medical
		  Information Mart for Intensive Care (MIMIC-III).},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= nov,
  articleno	= {17},
  numpages	= {20},
  keywords	= {Medical code prediction, multitask learning, imbalanced
		  class problem, balanced and recalibrated network}
}

@Proceedings{	  10.1145/3592573,
  title		= {LSC '23: Proceedings of the 6th Annual ACM Lifelog Search
		  Challenge},
  year		= {2023},
  isbn		= {9798400701887},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Thessaloniki, Greece}
}

@InProceedings{	  10.1145/3589132.3625600,
  author	= {Liu, Mengyi and Wang, Xieyang and Xu, Jianqiu and Lu,
		  Hua},
  title		= {NALSpatial: An Effective Natural Language Transformation
		  Framework for Queries over Spatial Data},
  year		= {2023},
  isbn		= {9798400701689},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589132.3625600},
  doi		= {10.1145/3589132.3625600},
  abstract	= {Spatial databases play a vital role in many applications
		  that access spatial data via appropriate queries. However,
		  most application users lack the expertise necessary for
		  formulating spatial queries. To fill in this gap, we
		  propose an effective framework called NALSpatial that
		  translates natural language queries over spatial data into
		  executable database queries. NALSpatial consists of two
		  core phases. The natural language understanding phase
		  extracts key entity information, comprehends the query
		  intent and determines the query type. The key entities and
		  query type are passed to the subsequent natural language
		  translation phase, which employs entity mapping rules and
		  structured language models to construct executable database
		  queries accordingly. We implement NALSpatial on the
		  open-source extensible database system SECONDO to support
		  range queries, nearest neighbor queries, spatial joins and
		  aggregation queries. Extensive experiments show that
		  NALSpatial on average achieves response time of about 2.5
		  seconds, translatability of 95% and translation precision
		  of 92%, outperforming state-of-the-art natural language
		  transformation methods.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Advances in Geographic Information Systems},
  articleno	= {57},
  numpages	= {4},
  keywords	= {spatial database, natural language interface, semantic
		  parsing, query processing},
  location	= {Hamburg, Germany},
  series	= {SIGSPATIAL '23}
}

@Article{	  10.1109/taslp.2021.3054309,
  author	= {Balaraman, Vevake and Magnini, Bernardo},
  title		= {Domain-Aware Dialogue State Tracker for Multi-Domain
		  Dialogue Systems},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3054309},
  doi		= {10.1109/TASLP.2021.3054309},
  abstract	= {In task-oriented dialogue systems the dialogue state
		  tracker component (DST) is responsible for predicting the
		  current state of the dialogue based on the dialogue history
		  and the user utterance. Current DST approaches rely on a
		  predefined domain ontology, a fact that limits their
		  effective usage for large scale conversational agents,
		  where the DST constantly needs to be interfaced with
		  ever-increasing services and APIs. Focused towards
		  overcoming this drawback, we propose a domain-aware
		  dialogue state tracker, that is completely data-driven and
		  it is modeled to predict for dynamic service schemas,
		  including zero-shot domains. Unlike approaches that propose
		  separate models for prediction of intents, requested slots,
		  slot status, categorical slots and non-categorical slots,
		  we propose a single model in an end-to-end architecture.
		  The proposed model utilizes domain and slot information to
		  extract both domain and slot specific representations from
		  a given dialogue, and then uses such representations to
		  predict the values of the corresponding slot in a given
		  domain. Integrating this mechanism with pretrained language
		  models, our approach can effectively learn semantic
		  relations and effectively perform transfer learning between
		  domains or zero-shot tracking for domains not present in
		  training.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jan,
  pages		= {866–873},
  numpages	= {8}
}

@InProceedings{	  10.1145/3539618.3591877,
  author	= {Zhan, Haolan and Li, Zhuang and Wang, Yufei and Luo,
		  Linhao and Feng, Tao and Kang, Xiaoxi and Hua, Yuncheng and
		  Qu, Lizhen and Soon, Lay-Ki and Sharma, Suraj and Zukerman,
		  Ingrid and Semnani-Azad, Zhaleh and Haffari, Gholamreza},
  title		= {SocialDial: A Benchmark for Socially-Aware Dialogue
		  Systems},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591877},
  doi		= {10.1145/3539618.3591877},
  abstract	= {Content Warning: this paper may contain content that is
		  offensive or upsetting.Dialogue systems have been widely
		  applied in many scenarios and are now more powerful and
		  ubiquitous than ever before. With large neural models and
		  massive available data, current dialogue systems have
		  access to more knowledge than any people in their life.
		  However, current dialogue systems still do not perform at a
		  human level. One major gap between conversational agents
		  and humans lies in their abilities to be aware of social
		  norms. The development of socially-aware dialogue systems
		  is impeded due to the lack of resources. In this paper, we
		  present the first socially-aware dialogue corpus --
		  SocialDial based on Chinese social culture. SocialDial
		  consists of two parts: 1,563 multi-turn dialogues between
		  two human speakers with fine-grained labels, and 4,870
		  synthetic conversations generated by ChatGPT. The human
		  corpus covers five categories of social norms, which have
		  14 sub-categories in total. Specifically, it contains
		  social factor annotations including social relation,
		  context, social distance, and social norms. However,
		  collecting sufficient socially-aware dialogues is costly.
		  Thus, we harness the power of ChatGPT and devise an
		  ontology-based synthetic data generation framework. This
		  framework is able to generate synthetic data at scale. To
		  ensure the quality of synthetic dialogues, we design
		  several mechanisms for quality control during data
		  collection. Finally, we evaluate our dataset using several
		  pre-trained models, such as BERT and RoBERTa. Comprehensive
		  empirical results based on state-of-the-art neural models
		  demonstrate that modeling of social norms for dialogue
		  systems is a promising research direction. To the best of
		  our knowledge, SocialDial is the first socially-aware
		  dialogue dataset that covers multiple social factors and
		  has fine-grained labels.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2712–2722},
  numpages	= {11},
  keywords	= {datasets, social norms, socially-aware dialogue},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Article{	  10.1145/3615355,
  author	= {Davis, Ernest},
  title		= {Benchmarks for Automated Commonsense Reasoning: A Survey},
  year		= {2023},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3615355},
  doi		= {10.1145/3615355},
  abstract	= {More than one hundred benchmarks have been developed to
		  test the commonsense knowledge and commonsense reasoning
		  abilities of artificial intelligence (AI) systems. However,
		  these benchmarks are often flawed, and many aspects of
		  common sense remain untested. Consequently, there is
		  currently no reliable way of measuring to what extent
		  existing AI systems have achieved these abilities.This
		  article surveys the development and uses of AI commonsense
		  benchmarks. It enumerates 139 commonsense benchmarks that
		  have been developed: 102 text-based, 18 image-based, 12
		  video-based, and 7 based in simulated physical
		  environments. It gives more detailed descriptions of twelve
		  of these, three from each category. It surveys the various
		  methods used to construct commonsense benchmarks. It
		  discusses the nature of common sense, the role of common
		  sense in AI, the goals served by constructing commonsense
		  benchmarks, desirable features of commonsense benchmarks,
		  and flaws and gap in existing benchmarks. It concludes with
		  a number of recommendations for future development of
		  commonsense AI benchmarks; most importantly, that the
		  creators of benchmarks invest the work needed to ensure
		  that benchmark examples are consistently high quality.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {81},
  numpages	= {41},
  keywords	= {Common sense, commonsense knowledge, commonsense
		  reasoning, benchmarks, evaluation}
}

@Proceedings{	  10.1145/3584371,
  title		= {BCB '23: Proceedings of the 14th ACM International
		  Conference on Bioinformatics, Computational Biology, and
		  Health Informatics},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {ACM-BCB is the flagship conference of the ACM SIGBio, the
		  ACM Special Interest Group in Bioinformatics, Computational
		  Biology, and Biomedical Informatics. Continuing the annual
		  tradition, the conference focuses on interdisciplinary
		  research linking computer science, mathematics, statistics,
		  biology, bioinformatics, biomedical informatics, and health
		  informatics.},
  location	= {Houston, TX, USA}
}

@InProceedings{	  10.1145/3447535.3462640,
  author	= {Woloszyn, Vinicius and Cortes, Eduardo G. and Amantea,
		  Rafael and Schmitt, Vera and Barone, Dante A. C. and
		  M\"{o}ller, Sebastian},
  title		= {Towards a Novel Benchmark for Automatic Generation of
		  ClaimReview Markup},
  year		= {2021},
  isbn		= {9781450383301},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447535.3462640},
  doi		= {10.1145/3447535.3462640},
  abstract	= {The spreading of disinformation throughout the web has
		  become a critical problem for a democratic society. The
		  dissemination of fake news has become a profitable business
		  and a common practice among politicians and content
		  producers. On the other hand, journalists and fact-checkers
		  work unceasingly to debunk misinformation and prevent it
		  from further spreading. In 2015, a new web markup called
		  ClaimReview has been introduced to grant access to the
		  fact-checking article’s meaning by search engines. It is
		  an important initiative to fight fake news by promoting and
		  highlighting fact-check articles among users. However,
		  barely half of fact-checkers have adopted the ClaimReview
		  markup so far, resulting in low findability of fact-check
		  articles, especially in under-represented countries and
		  languages. In this work, we investigate the viability of
		  using Artificial Intelligence for generating ClaimReview
		  automatically. Besides promoting fact-check articles, the
		  automatic generating of ClaimReview is an important step
		  towards the creation of updated multilingual knowledge base
		  for fighting disinformation. Our experiments show
		  noticeable results, which indicate a viable solution in a
		  production environment. Furthermore, this work has created
		  a benchmark that can be used in upcoming investigations in
		  this domain.},
  booktitle	= {Proceedings of the 13th ACM Web Science Conference 2021},
  pages		= {29–35},
  numpages	= {7},
  keywords	= {ClaimReview, data sets, fact-checking, machine learning,
		  misinformation},
  location	= {Virtual Event, United Kingdom},
  series	= {WebSci '21}
}

@InProceedings{	  10.1145/3534678.3539304,
  author	= {Wei, Ying and Li, Qi},
  title		= {SagDRE: Sequence-Aware Graph-Based Document-Level Relation
		  Extraction with Adaptive Margin Loss},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539304},
  doi		= {10.1145/3534678.3539304},
  abstract	= {Relation extraction (RE) is an important task for many
		  natural language processing applications. Document-level
		  relation extraction task aims to extract the relations
		  within a document and poses many challenges to the RE tasks
		  as it requires reasoning across sentences and handling
		  multiple relations expressed in the same document. Existing
		  state-of-the-art document-level RE models use the graph
		  structure to better connect long-distance correlations. In
		  this work, we propose SagDRE model, which further considers
		  and captures the original sequential information from the
		  text. The proposed model learns sentence-level directional
		  edges to capture the information flow in the document and
		  uses the token-level sequential information to encode the
		  shortest paths from one entity to the other. In addition,
		  we propose an adaptive margin loss to address the
		  long-tailed multi-label problem of document-level RE tasks,
		  where multiple relations can be expressed in a document for
		  an entity pair and there are a few popular relations. The
		  loss function aims to encourage separations between
		  positive and negative classes. The experimental results on
		  datasets from various domains demonstrate the effectiveness
		  of the proposed methods.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2000–2008},
  numpages	= {9},
  keywords	= {document-level re, graph, relation extraction, sequence
		  information},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@Article{	  10.1109/taslp.2021.3076876,
  author	= {Jeong, Myeongho and Choi, Seungtaek and Yeo, Jinyoung and
		  Hwang, Seung-won},
  title		= {Label and Context Augmentation for Response Selection at
		  DSTC8},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3076876},
  doi		= {10.1109/TASLP.2021.3076876},
  abstract	= {This paper studies the dialogue response selection task.
		  As state-of-the-arts are neural models requiring a large
		  training set, data augmentation has been considered as a
		  means to overcome the sparsity of observational annotation,
		  where only one observed response is annotated as gold. In
		  this paper, we first consider label augmentation, of
		  selecting, among unobserved utterances, that would
		  “counterfactually” replace the labeled response, for
		  the given context, and augmenting labels only if that is
		  the case. The key advantage of this model is not incurring
		  human annotation overhead, thus not increasing the training
		  cost, i.e., for low-resource scenarios. In addition, we
		  consider context augmentation scenarios where the given
		  dialogue context is not sufficient for label augmentation.
		  In this case, inspired by open-domain question answering,
		  we “decontextualize” by retrieving missing contexts,
		  such as related persona. We empirically show that our
		  pipeline improves BERT-based models in two different
		  response selection tasks without incurring annotation
		  overheads.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= apr,
  pages		= {2541–2550},
  numpages	= {10}
}

@InProceedings{	  10.1145/3442381.3449988,
  author	= {Xia, Tingyu and Wang, Yue and Tian, Yuan and Chang, Yi},
  title		= {Using Prior Knowledge to Guide BERT’s Attention in
		  Semantic Textual Matching Tasks},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3449988},
  doi		= {10.1145/3442381.3449988},
  abstract	= {We study the problem of incorporating prior knowledge into
		  a deep Transformer-based model, i.e., Bidirectional Encoder
		  Representations from Transformers (BERT), to enhance its
		  performance on semantic textual matching tasks. By probing
		  and analyzing what BERT has already known when solving this
		  task, we obtain better understanding of what task-specific
		  knowledge BERT needs the most and where it is most needed.
		  The analysis further motivates us to take a different
		  approach than most existing works. Instead of using prior
		  knowledge to create a new training task for fine-tuning
		  BERT, we directly inject knowledge into BERT’s multi-head
		  attention mechanism. This leads us to a simple yet
		  effective approach that enjoys fast training stage as it
		  saves the model from training on additional data or tasks
		  other than the main task. Extensive experiments demonstrate
		  that the proposed knowledge-enhanced BERT is able to
		  consistently improve semantic textual matching performance
		  over the original BERT model, and the performance benefit
		  is most salient when training data is scarce.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {2466–2475},
  numpages	= {10},
  keywords	= {BERT, Deep Neural Networks, Prior Knowledge, Semantic
		  Textual Similarity},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Article{	  10.1145/3597610,
  author	= {Huang, Heyan and Yuan, Changsen and Liu, Qian and Cao,
		  Yixin},
  title		= {Document-level Relation Extraction via Separate Relation
		  Representation and Logical Reasoning},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3597610},
  doi		= {10.1145/3597610},
  abstract	= {Document-level relation extraction (RE) extends the
		  identification of entity/mentions’ relation from the
		  single sentence to the long document. It is more realistic
		  and poses new challenges to relation representation and
		  reasoning skills. In this article, we propose a novel
		  model, SRLR, using Separate Relation Representation and
		  Logical Reasoning considering the indirect relation
		  representation and complex reasoning of evidence sentence
		  problems. Specifically, we first expand the judgment of
		  relational facts from the entity-level to the
		  mention-level, highlighting fine-grained information to
		  capture the relation representation for the entity pair.
		  Second, we propose a logical reasoning module to identify
		  evidence sentences and conduct relational reasoning.
		  Extensive experiments on two publicly available benchmark
		  datasets demonstrate the effectiveness of our proposed SRLR
		  as compared to 19 baseline models. Further ablation study
		  also verifies the effects of the key components.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= aug,
  articleno	= {22},
  numpages	= {24},
  keywords	= {Document-level Relation Extraction, Separate Relation
		  Representation, Mention-level, Logical Reasoning}
}

@InProceedings{	  10.1145/3579051.3579063,
  author	= {Pozzi, Riccardo and Moiraghi, Federico and Lodi, Fausto
		  and Palmonari, Matteo},
  title		= {Evaluation of Incremental Entity Extraction with
		  Background Knowledge and Entity Linking},
  year		= {2023},
  isbn		= {9781450399876},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579051.3579063},
  doi		= {10.1145/3579051.3579063},
  abstract	= {Named entity extraction is a crucial task to support the
		  population of Knowledge Bases (KBs) from documents written
		  in natural language. However, in many application domains,
		  these documents must be collected and processed
		  incrementally to update the KB as more data are ingested.
		  In some cases, quality concerns may even require human
		  validation mechanisms along the process. While very recent
		  work in the NLP community has discussed the importance of
		  evaluating and benchmarking continuous entity extraction,
		  it has proposed methods and datasets that avoid Named
		  Entity Linking (NEL) as a component of the extraction
		  process. In this paper, we advocate for batch-based
		  incremental entity extraction methods that can exploit NEL
		  with a background KB, detect mentions of entities that are
		  not present in the KB yet (NIL mentions), and update the KB
		  with the novel entities. Based on this assumption, we
		  present a methodology to evaluate NEL-based incremental
		  entity extraction, which can be applied to a “static”
		  dataset for evaluating NEL into a dataset for evaluating
		  incremental entity extraction. We apply this methodology to
		  an existing benchmark for evaluating NEL algorithms, and
		  evaluate an incremental extraction pipeline that
		  orchestrates different strong state-of-the-art and baseline
		  algorithms for the tasks involved in the extraction
		  process, namely, NEL, NIL prediction, and NIL clustering.
		  In presenting our experiments, we demonstrate the increased
		  difficulty of the information extraction task in
		  incremental settings and discuss the strengths of the
		  available solutions as well as open challenges.},
  booktitle	= {Proceedings of the 11th International Joint Conference on
		  Knowledge Graphs},
  pages		= {30–38},
  numpages	= {9},
  keywords	= {Entity Extraction, Incremental Entity Extraction,
		  Knowledge Base Population, Named Entity Linking},
  location	= {Hangzhou, China},
  series	= {IJCKG '22}
}

@Article{	  10.1145/3631521,
  author	= {Zhang, Yuting and Sun, Ying and Zhuang, Fuzhen and Zhu,
		  Yongchun and An, Zhulin and Xu, Yongjun},
  title		= {Triple Dual Learning for Opinion-based Explainable
		  Recommendation},
  year		= {2023},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {3},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3631521},
  doi		= {10.1145/3631521},
  abstract	= {Recently, with the aim of enhancing the trustworthiness of
		  recommender systems, explainable recommendation has
		  attracted much attention from the research community.
		  Intuitively, users’ opinions toward different aspects of
		  an item determine their ratings (i.e., users’
		  preferences) for the item. Therefore, rating prediction
		  from the perspective of opinions can realize personalized
		  explanations at the level of item aspects and user
		  preferences. However, there are several challenges in
		  developing an opinion-based explainable recommendation: (1)
		  The complicated relationship between users’ opinions and
		  ratings. (2) The difficulty of predicting the potential
		  (i.e., unseen) user-item opinions because of the sparsity
		  of opinion information. To tackle these challenges, we
		  propose an overall preference-aware opinion-based
		  explainable rating prediction model by jointly modeling the
		  multiple observations of user-item interaction (i.e.,
		  review, opinion, rating). To alleviate the sparsity problem
		  and raise the effectiveness of opinion prediction, we
		  further propose a triple dual learning-based framework with
		  a novelly designed triple dual constraint. Finally,
		  experiments on three popular datasets show the
		  effectiveness and great explanation performance of our
		  framework.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= dec,
  articleno	= {70},
  numpages	= {27},
  keywords	= {Explainable recommendation; triple dual learning;
		  opinion-based explanation}
}

@InProceedings{	  10.1145/3446132.3446408,
  author	= {Kong, Siyin and Zhu, Ping and Yang, Qian and Wei, Zhihua},
  title		= {HSCKE: A Hybrid Supervised Method for Chinese Keywords
		  Extraction},
  year		= {2021},
  isbn		= {9781450388115},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3446132.3446408},
  doi		= {10.1145/3446132.3446408},
  abstract	= {Automatic keywords extraction refers to extracting words
		  or phrases from a single text or text collection.
		  Supervised methods outperform unsupervised methods, but it
		  requires a large volume of labeled corpus for training. To
		  address the problem, extra knowledge is obtained through
		  labels generated by other tools. Moreover, the
		  preprocessing of Chinese text is more challenging than that
		  in English because of the fragments caused by word segment.
		  Hence the named entity recognition in the preprocessing is
		  introduced to enhance the accuracy. On the other hand, text
		  contains different separate parts, and each part conveys
		  information to readers on different levels. Thus, we
		  present a text weighting method based on priority that
		  takes into consideration the importance of different
		  texture parts. In this paper, we integrate the three ideas
		  above and propose a novel hybrid method for Chinese
		  keywords extraction (HSCKE). To evaluate the performance of
		  our proposed approach, we compare HSCKE with four most
		  commonly used methods on two typical Chinese keywords
		  extraction datasets. The experimental results show that the
		  proposed approach achieves the optimal performance in terms
		  of precision, recall and F1 score.},
  booktitle	= {Proceedings of the 2020 3rd International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  articleno	= {81},
  numpages	= {7},
  keywords	= {Chinese keywords extraction, Pre-trained model, Supervised
		  method},
  location	= {Sanya, China},
  series	= {ACAI '20}
}

@Proceedings{	  10.1145/3571560,
  title		= {ICAAI '22: Proceedings of the 6th International Conference
		  on Advances in Artificial Intelligence},
  year		= {2022},
  isbn		= {9781450396943},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Birmingham, United Kingdom}
}

@InProceedings{	  10.1145/3543507.3583504,
  author	= {Zhang, Mi and Qian, Tieyun and Zhang, Ting and Miao, Xin},
  title		= {Towards Model Robustness: Generating Contextual
		  Counterfactuals for Entities in Relation Extraction},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583504},
  doi		= {10.1145/3543507.3583504},
  abstract	= {The goal of relation extraction (RE) is to extract the
		  semantic relations between/among entities in the text. As a
		  fundamental task in information systems, it is crucial to
		  ensure the robustness of RE models. Despite the high
		  accuracy current deep neural models have achieved in RE
		  tasks, they are easily affected by spurious correlations.
		  One solution to this problem is to train the model with
		  counterfactually augmented data (CAD) such that it can
		  learn the causation rather than the confounding. However,
		  no attempt has been made on generating counterfactuals for
		  RE tasks. In this paper, we formulate the problem of
		  automatically generating CAD for RE tasks from an
		  entity-centric viewpoint, and develop a novel approach to
		  derive contextual counterfactuals for entities.
		  Specifically, we exploit two elementary topological
		  properties, i.e., the centrality and the shortest path, in
		  syntactic and semantic dependency graphs, to first identify
		  and then intervene on the contextual causal features for
		  entities. We conduct a comprehensive evaluation on four RE
		  datasets by combining our proposed approach with a variety
		  of RE backbones. Results prove that our approach not only
		  improves the performance of the backbones but also makes
		  them more robust in the out-of-domain test &nbsp;1.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {1832–1842},
  numpages	= {11},
  keywords	= {counterfactual reasoning, relation extraction, semantic
		  and syntactic graph topology},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@Article{	  10.1145/3545572,
  author	= {Jabeen, Summaira and Li, Xi and Amin, Muhammad Shoib and
		  Bourahla, Omar and Li, Songyuan and Jabbar, Abdul},
  title		= {A Review on Methods and Applications in Multimodal Deep
		  Learning},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {2s},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3545572},
  doi		= {10.1145/3545572},
  abstract	= {Deep Learning has implemented a wide range of applications
		  and has become increasingly popular in recent years. The
		  goal of multimodal deep learning (MMDL) is to create models
		  that can process and link information using various
		  modalities. Despite the extensive development made for
		  unimodal learning, it still cannot cover all the aspects of
		  human learning. Multimodal learning helps to understand and
		  analyze better when various senses are engaged in the
		  processing of information. This article focuses on multiple
		  types of modalities, i.e., image, video, text, audio, body
		  gestures, facial expressions, physiological signals, flow,
		  RGB, pose, depth, mesh, and point cloud. Detailed analysis
		  of the baseline approaches and an in-depth study of recent
		  advancements during the past five years (2017 to 2021) in
		  multimodal deep learning applications has been provided. A
		  fine-grained taxonomy of various multimodal deep learning
		  methods is proposed, elaborating on different applications
		  in more depth. Last, main issues are highlighted separately
		  for each domain, along with their possible future research
		  directions.},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= feb,
  articleno	= {76},
  numpages	= {41},
  keywords	= {Deep learning, multimedia, multimodal learning, datasets,
		  neural networks, survey}
}

@Proceedings{	  10.1145/3545822,
  title		= {ICMSSP '22: Proceedings of the 2022 7th International
		  Conference on Multimedia Systems and Signal Processing},
  year		= {2022},
  isbn		= {9781450396424},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shenzhen, China}
}

@InProceedings{	  10.1145/3579375.3579391,
  author	= {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla,
		  Sandeep Kumar},
  title		= {TTPHunter: Automated Extraction of Actionable Intelligence
		  as TTPs from Narrative Threat Reports},
  year		= {2023},
  isbn		= {9798400700057},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3579375.3579391},
  doi		= {10.1145/3579375.3579391},
  abstract	= {With the proliferation of attacks from various Advanced
		  Persistent Threats (APT) groups, it is essential to
		  comprehend the threat actor’s attack patterns to
		  accelerate threat detection and response. The MITRE
		  ATT&amp;CK framework’s Tactics, Techniques, and
		  Procedures (TTPs) help to decipher attack patterns. The APT
		  reports, published by security firms, contain rich
		  information on tools and techniques used by threat actors.
		  These reports are available in unstructured and natural
		  language texts. There is a need for an automated tool to
		  extract TTPs present in natural language text. However,
		  there are few tools available in the literature, but their
		  performance is not very satisfactory. In this work, we
		  propose TTPHunter, to extract TTPs from APT reports by
		  mapping sentence context to relevant TTPs. We fine-tune
		  linear classifiers, which take input as BERT (Bidirectional
		  Encoder Representations from Transformers) embeddings of
		  sentences. We create two datasets: sentence-based (8,387
		  sentence samples) and document-based (50 threat reports) to
		  validate TTPHunter. TTPHunter achieves the F1-score of 88%
		  and 75% for both datasets, respectively. We compare the
		  TTPHunter with rcATT and AttacKG baseline models, and it
		  outperforms both baselines.},
  booktitle	= {Proceedings of the 2023 Australasian Computer Science
		  Week},
  pages		= {126–134},
  numpages	= {9},
  keywords	= {Cybersecurity, MITRE ATT&amp;CK, Natural Language
		  Processing, TTP Extraction, Threat Intelligence},
  location	= {Melbourne, VIC, Australia},
  series	= {ACSW '23}
}

@Proceedings{	  10.1145/3599957,
  title		= {RACS '23: Proceedings of the 2023 International Conference
		  on Research in Adaptive and Convergent Systems},
  year		= {2023},
  isbn		= {9798400702280},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {With the expansion of both the Internet and the advanced
		  information technology development profession, reliable and
		  convergent computing has attracted increasing interest in
		  both academia and industry. To cope with this important
		  problem, the Research in Adaptive and Convergent Systems
		  (RACS) provides a forum for exchanging highly original
		  ideas about an important class of computing systems. The
		  RACS aims primarily at researchers who have experience in
		  reliable and convergent computing systems and are engaged
		  in the design and implementation of new computing
		  applications. Each year RACS brings together engineers and
		  scientists from diverse communities with interests in
		  practical computing technologies and creates an environment
		  for them to discuss and report experimental results, novel
		  designs, work-in-progress, experiences, case studies, and
		  trend-setting ideas.},
  location	= {Gdansk, Poland}
}

@Proceedings{	  10.1145/3555776,
  title		= {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on
		  Applied Computing},
  year		= {2023},
  isbn		= {9781450395175},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tallinn, Estonia}
}

@InProceedings{	  10.1145/3511808.3557484,
  author	= {Arnaout, Hiba and Razniewski, Simon and Weikum, Gerhard
		  and Pan, Jeff Z.},
  title		= {UnCommonSense: Informative Negative Knowledge about
		  Everyday Concepts},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557484},
  doi		= {10.1145/3511808.3557484},
  abstract	= {Commonsense knowledge about everyday concepts is an
		  important asset for AI applications, such as question
		  answering and chatbots. Recently, we have seen an
		  increasing interest in the construction of structured
		  commonsense knowledge bases (CSKBs). An important part of
		  human commonsense is about properties that do not apply to
		  concepts, yet existing CSKBs only store positive
		  statements. Moreover, since CSKBs operate under the
		  open-world assumption, absent statements are considered to
		  have unknown truth rather than being invalid. This paper
		  presents the UNCOMMONSENSE framework for materializing
		  informative negative commonsense statements. Given a target
		  concept, comparable concepts are identified in the CSKB,
		  for which a local closed-world assumption is postulated.
		  This way, positive statements about comparable concepts
		  that are absent for the target concept become seeds for
		  negative statement candidates. The large set of candidates
		  is then scrutinized, pruned and ranked by informativeness.
		  Intrinsic and extrinsic evaluations show that our method
		  significantly outperforms the state-of-the-art. A large
		  dataset of informative negations is released as a resource
		  for future research.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {37–46},
  numpages	= {10},
  keywords	= {commonsense, knowledge base, negation},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@Proceedings{	  10.1145/3548608,
  title		= {ICCIR '22: Proceedings of the 2022 2nd International
		  Conference on Control and Intelligent Robotics},
  year		= {2022},
  isbn		= {9781450397179},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Nanjing, China}
}

@Proceedings{	  10.1145/3600100,
  title		= {BuildSys '23: Proceedings of the 10th ACM International
		  Conference on Systems for Energy-Efficient Buildings,
		  Cities, and Transportation},
  year		= {2023},
  isbn		= {9798400702303},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Istanbul, Turkey}
}

@InProceedings{	  10.1145/3511808.3557067,
  author	= {Shin, Wonyoung and Park, Jonghun and Woo, Taekang and Cho,
		  Yongwoo and Oh, Kwangjin and Song, Hwanjun},
  title		= {e-CLIP: Large-Scale Vision-Language Representation
		  Learning in E-commerce},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557067},
  doi		= {10.1145/3511808.3557067},
  abstract	= {Understanding vision and language representations of
		  product content is vital for search and recommendation
		  applications in e-commerce. As a backbone for online
		  shopping platforms and inspired by the recent success in
		  representation learning research, we propose a contrastive
		  learning framework that aligns language and visual models
		  using unlabeled raw product text and images. We present
		  techniques we used to train large-scale representation
		  learning models and share solutions that address
		  domain-specific challenges. We study the performance using
		  our pre-trained model as backbones for diverse downstream
		  tasks, including category classification, attribute
		  extraction, product matching, product clustering, and adult
		  product recognition. Experimental results show that our
		  proposed method outperforms the baseline in each downstream
		  task regarding both single modality and multiple
		  modalities.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3484–3494},
  numpages	= {11},
  keywords	= {large-scale pre-training, multimodal pre-training},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@Proceedings{	  10.1145/3563359,
  title		= {UMAP '23 Adjunct: Adjunct Proceedings of the 31st ACM
		  Conference on User Modeling, Adaptation and
		  Personalization},
  year		= {2023},
  isbn		= {9781450398916},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Limassol, Cyprus}
}

@Article{	  10.1613/jair.1.12918,
  author	= {Erdem, Erkut and Kuyu, Menekse and Yagcioglu, Semih and
		  Frank, Anette and Parcalabescu, Letitia and Plank, Barbara
		  and Babii, Andrii and Turuta, Oleksii and Erdem, Aykut and
		  Calixto, Iacer and Lloret, Elena and Apostol, Elena-Simona
		  and Truic\u{a}, Ciprian-Octavian and \v{S}andrih,
		  Branislava and Martin\v{c}i\'{c}-Ip\v{s}i\'{c}, Sanda and
		  Berend, G\'{a}bor and Gatt, Albert and Korvel, Gr\u{a}zina},
  title		= {Neural Natural Language Generation: A Survey on
		  Multilinguality, Multimodality, Controllability and
		  Learning},
  year		= {2022},
  issue_date	= {May 2022},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {73},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.12918},
  doi		= {10.1613/jair.1.12918},
  abstract	= {Developing artificial learning systems that can understand
		  and generate natural language has been one of the
		  long-standing goals of artificial intelligence. Recent
		  decades have witnessed an impressive progress on both of
		  these problems, giving rise to a new family of approaches.
		  Especially, the advances in deep learning over the past
		  couple of years have led to neural approaches to natural
		  language generation (NLG). These methods combine generative
		  language learning techniques with neural-networks based
		  frameworks. With a wide range of applications in natural
		  language processing, neural NLG (NNLG) is a new and fast
		  growing field of research. In this state-of-the-art report,
		  we investigate the recent developments and applications of
		  NNLG in its full extent from a multidimensional view,
		  covering critical perspectives such as multimodality,
		  multilinguality, controllability and learning strategies.
		  We summarize the fundamental building blocks of NNLG
		  approaches from these aspects and provide detailed reviews
		  of commonly used preprocessing steps and basic neural
		  architectures. This report also focuses on the seminal
		  applications of these NNLG models such as machine
		  translation, description generation, automatic speech
		  recognition, abstractive summarization, text
		  simplification, question answering and generation, and
		  dialogue generation. Finally, we conclude with a thorough
		  discussion of the described frameworks by pointing out some
		  open research directions.},
  journal	= {J. Artif. Int. Res.},
  month		= may,
  numpages	= {77},
  keywords	= {natural language, neural networks}
}

@Proceedings{	  10.1145/3546607,
  title		= {ICVARS '22: Proceedings of the 2022 6th International
		  Conference on Virtual and Augmented Reality Simulations},
  year		= {2022},
  isbn		= {9781450387330},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Brisbane, QLD, Australia}
}

@InProceedings{	  10.1145/3442442.3451385,
  author	= {Pei, Yulong and Zhang, Qian},
  title		= {GOAT at the FinSim-2 task: Learning Word Representations
		  of Financial Data with Customized Corpus},
  year		= {2021},
  isbn		= {9781450383134},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442442.3451385},
  doi		= {10.1145/3442442.3451385},
  abstract	= {In this paper, we present our approaches for the FinSim
		  2021 Shared Task on Learning Semantic Similarities for the
		  Financial Domain. The aim of the FinSim shared task is to
		  automatically classify a given list of terms from the
		  financial domain into the most relevant hypernym (or
		  top-level) concept in an external ontology. Two different
		  word representations have been compared in our study, i.e.,
		  customized word2vec provided by the shared task and
		  FinBERT. We first create a customized corpus from the given
		  prospectuses and relevant articles from Investopedia. Then
		  we train the domain-specific word2vec embeddings using the
		  customized data with customized word2vec and FinBERT as the
		  initialized embeddings respectively. Our experimental
		  results demonstrate that these customized word embeddings
		  can effectively improve the classification performance and
		  achieve better results than the direct utilization of the
		  provided word embeddings. The class imbalance issue of the
		  given data is also explored. We empirically study the
		  classification performance by employing several different
		  strategies for imbalanced classification problems. Our
		  system ranks 2nd on both Average Accuracy and Mean Rank
		  metrics.},
  booktitle	= {Companion Proceedings of the Web Conference 2021},
  pages		= {307–310},
  numpages	= {4},
  keywords	= {BERT, Word representations, imbalance classification,
		  word2vec},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Article{	  10.1145/3464380,
  author	= {Tian, Xiuxia and Li, Can and Zhao, Bo},
  title		= {A Novel Classification Model SA-MPCNN for Power Equipment
		  Defect Text},
  year		= {2021},
  issue_date	= {November 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3464380},
  doi		= {10.1145/3464380},
  abstract	= {The text classification of power equipment defect is of
		  great significance to equipment health condition evaluation
		  and power equipment maintenance decisions. Most of the
		  existing classification methods do not sufficiently
		  consider the semantic relation between words in the same
		  sentence and cannot extract deep semantic features. To
		  tackle those problems, this article proposes a novel
		  classification method by combining the self-attention
		  mechanism and multi-channel pyramid convolution neural
		  networks. We utilize the bidirectional gated recurrent unit
		  to model the text sequence and, on this basis, improve
		  self-attention layer to dot multiplication on the forward
		  and backward features to obtain the global attention score.
		  Thereby, effective features are enhanced, invalid features
		  are weakened, and important text representation vectors are
		  obtained. To solve the problem that the shallow network
		  structure cannot extract deep semantic features, we design
		  a multi-channel pyramid convolution network, which first
		  extracts deep text features from the channels of different
		  windows and then fuses the text features of each channel.
		  By comparing with the state-of-the-art methods, the model
		  in this article has better performance in text
		  classification of power equipment defects.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  articleno	= {105},
  numpages	= {21},
  keywords	= {Text classification, defect texts, self-attention
		  mechanism, bidirectional gated recurrent unit, pyramid
		  convolution}
}

@Article{	  10.1109/tcbb.2022.3215257,
  author	= {Wu, Kaitao and Wang, Lexiang and Liu, Bo and Liu, Yang and
		  Wang, Yadong and Li, Junyi},
  title		= {PSPGO: Cross-Species Heterogeneous Network Propagation for
		  Protein Function Prediction},
  year		= {2022},
  issue_date	= {May-June 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {3},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3215257},
  doi		= {10.1109/TCBB.2022.3215257},
  abstract	= {How to use computational methods to effectively predict
		  the function of proteins remains a challenge. Most
		  prediction methods based on single species or single data
		  source have some limitations: the former need to train
		  different models for different species, the latter only to
		  infer protein function from a single perspective, such as
		  the method only using Protein-Protein Interaction (PPI)
		  network just considers the protein environment but ignore
		  the intrinsic characteristics of protein sequences. We
		  found that in some network-based multi-species methods the
		  networks of each species are isolated, which means there is
		  no communication between networks of different species. To
		  solve these problems, we propose a cross-species
		  heterogeneous network propagation method based on graph
		  attention mechanism, PSPGO, which can propagate feature and
		  label information on sequence similarity (SS) network and
		  PPI network for predicting gene ontology terms. Our model
		  is evaluated on a large multi-species dataset split based
		  on time and is compared with several state-of-the-art
		  methods. The results show that our method has good
		  performance. We also explore the predictive performance of
		  PSPGO for a single species. The results illustrate that
		  PSPGO also performs well in prediction for single
		  species.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= oct,
  pages		= {1713–1724},
  numpages	= {12}
}

@InProceedings{	  10.1145/3543507.3583303,
  author	= {Du, Yuntao and Lian, Jianxun and Yao, Jing and Wang,
		  Xiting and Wu, Mingqi and Chen, Lu and Gao, Yunjun and Xie,
		  Xing},
  title		= {Towards Explainable Collaborative Filtering with Taste
		  Clusters Learning},
  year		= {2023},
  isbn		= {9781450394161},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3543507.3583303},
  doi		= {10.1145/3543507.3583303},
  abstract	= {Collaborative Filtering (CF) is a widely used and
		  effective technique for recommender systems. In recent
		  decades, there have been significant advancements in latent
		  embedding-based CF methods for improved accuracy, such as
		  matrix factorization, neural collaborative filtering, and
		  LightGCN. However, the explainability of these models has
		  not been fully explored. Adding explainability to
		  recommendation models can not only increase trust in the
		  decision-making process, but also have multiple benefits
		  such as providing persuasive explanations for item
		  recommendations, creating explicit profiles for users and
		  items, and assisting item producers in design improvements.
		  In this paper, we propose a neat and effective Explainable
		  Collaborative Filtering (ECF) model that leverages
		  interpretable cluster learning to achieve the two most
		  demanding objectives: (1) Precise - the model should not
		  compromise accuracy in the pursuit of explainability; and
		  (2) Self-explainable - the model’s explanations should
		  truly reflect its decision-making process, not generated
		  from post-hoc methods. The core of ECF is mining taste
		  clusters from user-item interactions and item profiles. We
		  map each user and item to a sparse set of taste clusters,
		  and taste clusters are distinguished by a few
		  representative tags. The user-item preference,
		  users/items’ cluster affiliations, and the generation of
		  taste clusters are jointly optimized in an end-to-end
		  manner. Additionally, we introduce a forest mechanism to
		  ensure the model’s accuracy, explainability, and
		  diversity. To comprehensively evaluate the explainability
		  quality of taste clusters, we design several quantitative
		  metrics, including in-cluster item coverage, tag
		  utilization, silhouette, and informativeness. Our model’s
		  effectiveness is demonstrated through extensive experiments
		  on three real-world datasets.},
  booktitle	= {Proceedings of the ACM Web Conference 2023},
  pages		= {3712–3722},
  numpages	= {11},
  keywords	= {Clustering, Collaborative Filtering, Explanability,
		  Recommendation},
  location	= {Austin, TX, USA},
  series	= {WWW '23}
}

@InProceedings{	  10.1145/3477495.3531731,
  author	= {Dietz, Laura and Chatterjee, Shubham and Lennox, Connor
		  and Kashyapi, Sumanta and Oza, Pooja and Gamari, Ben},
  title		= {Wikimarks: Harvesting Relevance Benchmarks from
		  Wikipedia},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531731},
  doi		= {10.1145/3477495.3531731},
  abstract	= {We provide a resource for automatically harvesting
		  relevance benchmarks from Wikipedia -- which we refer to as
		  "Wikimarks" to differentiate them from manually created
		  benchmarks. Unlike simulated benchmarks, they are based on
		  manual annotations of Wikipedia authors. Studies on the
		  TREC Complex Answer Retrieval track demonstrated that
		  leaderboards under Wikimarks and manually annotated
		  benchmarks are very similar. Because of their availability,
		  Wikimarks can fill an important need for Information
		  Retrieval research. We provide a meta-resource to harvest
		  Wikimarks for several information retrieval tasks across
		  different languages: paragraph retrieval, entity ranking,
		  query-specific clustering, outline prediction, and relevant
		  entity linking and many more. In addition, we provide
		  example Wikimarks for English, Simple English, and Japanese
		  derived from the 01/01/2022 Wikipedia dump. Resource
		  available: https://trema-unh.github.io/wikimarks/},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3003–3012},
  numpages	= {10},
  keywords	= {query-specific clustering, relevant entity linking, test
		  collections},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@Proceedings{	  10.1145/3603607,
  title		= {HUMAN '23: Proceedings of the 6th Workshop on Human
		  Factors in Hypertext},
  year		= {2023},
  isbn		= {9798400702396},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rome, Italy}
}

@Article{	  10.1109/taslp.2022.3199648,
  author	= {Wang, Huadong and Shen, Xin and Tu, Mei and Zhuang, Yimeng
		  and Liu, Zhiyuan},
  title		= {Improved Transformer With Multi-Head Dense Collaboration},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3199648},
  doi		= {10.1109/TASLP.2022.3199648},
  abstract	= {Recently, the attention mechanism boosts the performance
		  of many neural network models in Natural Language
		  Processing (NLP). Among the various attention mechanisms,
		  Multi-Head Attention (MHA) is a powerful and popular
		  variant. MHA helps the model to attend to different feature
		  subspaces independently which is an essential component of
		  Transformer. Despite its success, we conjecture that the
		  different heads of the existing MHA may not collaborate
		  properly. To validate this assumption and further improve
		  the performance of Transformer, we study the collaboration
		  problem for MHA in this paper. First, we propose the
		  Single-Layer Collaboration (SLC) mechanism to help each
		  attention head improve its attention distribution based on
		  the feedback of other heads. Furthermore, we extend SLC to
		  the cross-layer Multi-Head Dense Collaboration (MHDC)
		  mechanism. MHDC helps each MHA layer learn the attention
		  distributions considering the knowledge from the other MHA
		  layers. Both SLC and MHDC are implemented as lightweight
		  modules with very few additional parameters. When equipped
		  with these modules, our new framework, i.e., Collaborative
		  TransFormer (&lt;italic&gt;CollFormer&lt;/italic&gt;),
		  significantly outperforms the vanilla Transformer on a
		  range of NLP tasks, including machine translation, sentence
		  semantic relatedness, natural language inference, sentence
		  classification, and reading comprehension. Besides, we also
		  carry out extensive quantitative experiments to analyze the
		  properties of the MHDC in different settings. The
		  experimental results validate the effectiveness and
		  universality of MHDC as well as
		  &lt;italic&gt;CollFormer&lt;/italic&gt;.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {2754–2767},
  numpages	= {14}
}

@Proceedings{	  10.1145/3573942,
  title		= {AIPR '22: Proceedings of the 2022 5th International
		  Conference on Artificial Intelligence and Pattern
		  Recognition},
  year		= {2022},
  isbn		= {9781450396899},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@Proceedings{	  10.1145/3599696,
  title		= {OASIS '23: Proceedings of the 3rd International Workshop
		  on Open Challenges in Online Social Networks},
  year		= {2023},
  isbn		= {9798400702259},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rome, Italy}
}

@Proceedings{	  10.1145/3594536,
  title		= {ICAIL '23: Proceedings of the Nineteenth International
		  Conference on Artificial Intelligence and Law},
  year		= {2023},
  isbn		= {9798400701979},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is my great pleasure to present to you the proceedings
		  of the Nineteenth International Conference on Artificial
		  Intelligence and Law (ICAIL 2023). The conference will be
		  held June 19-23 at the Universidade do Minho in Braga,
		  Portugal. It has been organized by the International
		  Association for Artificial Intelligence and Law (IAAIL) and
		  is held in cooperation with AAAI and ACM SIGAI. IAAIL's
		  mission is to facilitate research, collaboration, and
		  interdisciplinary communication at the intersection of law
		  and the technical disciplines belonging to the field of
		  artificial intelligence. The first ICAIL conference was
		  held in 1987 and its 2023 iteration is the first to be held
		  in person again after the Covid-19 pandemic.},
  location	= {Braga, Portugal}
}

@Article{	  10.1145/3557894,
  author	= {Liao, Junwei and Eskimez, Sefik and Lu, Liyang and Shi, Yu
		  and Gong, Ming and Shou, Linjun and Qu, Hong and Zeng,
		  Michael},
  title		= {Improving Readability for Automatic Speech Recognition
		  Transcription},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3557894},
  doi		= {10.1145/3557894},
  abstract	= {Modern Automatic Speech Recognition (ASR) systems can
		  achieve high performance in terms of recognition accuracy.
		  However, a perfectly accurate transcript still can be
		  challenging to read due to grammatical errors, disfluency,
		  and other noises common in spoken communication. These
		  readable issues introduced by speakers and ASR systems will
		  impair the performance of downstream tasks and the
		  understanding of human readers. In this work, we present a
		  task called ASR post-processing for readability (APR) and
		  formulate it as a sequence-to-sequence text generation
		  problem. The APR task aims to transform the noisy ASR
		  output into a readable text for humans and downstream tasks
		  while maintaining the semantic meaning of speakers. We
		  further study the APR task from the benchmark dataset,
		  evaluation metrics, and baseline models: First, to address
		  the lack of task-specific data, we propose a method to
		  construct a dataset for the APR task by using the data
		  collected for grammatical error correction. Second, we
		  utilize metrics adapted or borrowed from similar tasks to
		  evaluate model performance on the APR task. Lastly, we use
		  several typical or adapted pre-trained models as the
		  baseline models for the APR task. Furthermore, we fine-tune
		  the baseline models on the constructed dataset and compare
		  their performance with a traditional pipeline method in
		  terms of proposed evaluation metrics. Experimental results
		  show that all the fine-tuned baseline models perform better
		  than the traditional pipeline method, and our adapted
		  RoBERTa model outperforms the pipeline method by 4.95 and
		  6.63 BLEU points on two test sets, respectively. The human
		  evaluation and case study further reveal the ability of the
		  proposed model to improve the readability of ASR
		  transcripts.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {142},
  numpages	= {23},
  keywords	= {Automatic speech recognition, post-processing for
		  readability, data synthesis, pre-trained model}
}

@InProceedings{	  10.1145/3544548.3581393,
  author	= {Yang, Qian and Hao, Yuexing and Quan, Kexin and Yang,
		  Stephen and Zhao, Yiran and Kuleshov, Volodymyr and Wang,
		  Fei},
  title		= {Harnessing Biomedical Literature to Calibrate
		  Clinicians’ Trust in AI Decision Support Systems},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3581393},
  doi		= {10.1145/3544548.3581393},
  abstract	= {Clinical decision support tools (DSTs), powered by
		  Artificial Intelligence (AI), promise to improve
		  clinicians’ diagnostic and treatment decision-making.
		  However, no AI model is always correct. DSTs must enable
		  clinicians to validate each AI suggestion, convincing them
		  to take the correct suggestions while rejecting its errors.
		  While prior work often tried to do so by explaining AI’s
		  inner workings or performance, we chose a different
		  approach: We investigated how clinicians validated each
		  other’s suggestions in practice (often by referencing
		  scientific literature) and designed a new DST that embraces
		  these naturalistic interactions. This design uses GPT-3 to
		  draw literature evidence that shows the AI suggestions’
		  robustness and applicability (or the lack thereof). A
		  prototyping study with clinicians from three disease areas
		  proved this approach promising. Clinicians’ interactions
		  with the prototype also revealed new design and research
		  opportunities around (1) harnessing the complementary
		  strengths of literature-based and predictive decision
		  supports; (2) mitigating risks of de-skilling clinicians;
		  and (3) offering low-data decision support with
		  literature.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {14},
  numpages	= {14},
  keywords	= {Biomedical Literature, Clinical AI, Qualitative Method,
		  XAI},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@Proceedings{	  10.1145/3617184,
  title		= {ICCSIE '23: Proceedings of the 8th International
		  Conference on Cyber Security and Information Engineering},
  year		= {2023},
  isbn		= {9798400708800},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Putrajaya, Malaysia}
}

@InProceedings{	  10.1145/3604951.3605514,
  author	= {Anuradha Nanomi Arachchige, Isuri and Ha, Le and Mitkov,
		  Ruslan and Steinert, Johannes-Dieter},
  title		= {Enhancing Named Entity Recognition for Holocaust
		  Testimonies through Pseudo Labelling and Transformer-based
		  Models},
  year		= {2023},
  isbn		= {9798400708411},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3604951.3605514},
  doi		= {10.1145/3604951.3605514},
  abstract	= {The Holocaust was a tragic and catastrophic event in World
		  War II (WWII) history that resulted in the loss of millions
		  of lives. In recent years, the emergence of the field of
		  digital humanities has made the study of Holocaust
		  testimonies an important area of research for historians,
		  Holocaust educators, social scientists, and linguists. One
		  of the challenges in analysing Holocaust testimonies is the
		  recognition and categorisation of named entities such as
		  concentration camps, military officers, ships, and ghettos,
		  due to the scarcity of annotated data. This paper presents
		  a research study on a domain-specific hybrid named-entity
		  recognition model, which focuses on developing NER models
		  specifically tailored for the Holocaust domain. To overcome
		  the problem of data scarcity, we employed hybrid annotation
		  approach to training different transformer model
		  architectures in order to recognise the named entities.
		  Results show transformer models to have good performance
		  compared to other approaches.},
  booktitle	= {Proceedings of the 7th International Workshop on
		  Historical Document Imaging and Processing},
  pages		= {85–90},
  numpages	= {6},
  keywords	= {Holocaust Testimonies, NER, Pseudo Labelling,
		  Transformers},
  location	= {San Jose, CA, USA},
  series	= {HIP '23}
}

@Article{	  10.14778/3538598.3538608,
  author	= {Fan, Wenfei and Jin, Ruochun and Lu, Ping and Tian, Chao
		  and Xu, Ruiqi},
  title		= {Towards event prediction in temporal graphs},
  year		= {2022},
  issue_date	= {May 2022},
  publisher	= {VLDB Endowment},
  volume	= {15},
  number	= {9},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3538598.3538608},
  doi		= {10.14778/3538598.3538608},
  abstract	= {This paper proposes a class of temporal association rules,
		  denoted by TACOs, for event prediction. As opposed to
		  previous graph rules, TACOs monitor updates to graphs, and
		  can be used to capture temporal interests in recommendation
		  and catch frauds in response to behavior changes, among
		  other things. TACOs are defined on temporal graphs in terms
		  of change patterns and (temporal) conditions, and may carry
		  machine learning (ML) predicates for temporal event
		  prediction. We settle the complexity of reasoning about
		  TACOs, including their satisfiability, implication and
		  prediction problems. We develop a system, referred to as
		  TASTE. TASTE discovers TACOs by iteratively training a rule
		  creator based on generative ML models in a creator-critic
		  framework. Moreover, it predicts events by applying the
		  discovered TACOs. Using real-life and synthetic datasets,
		  we experimentally verify that TASTE is on average 31.4
		  times faster than conventional data mining methods in TACO
		  discovery, and it improves the accuracy of state-of-the-art
		  event prediction models by 23.4%.},
  journal	= {Proc. VLDB Endow.},
  month		= may,
  pages		= {1861–1874},
  numpages	= {14}
}

@Article{	  10.1145/3462476,
  author	= {Tamine, Lynda and Goeuriot, Lorraine},
  title		= {Semantic Information Retrieval on Medical Texts: Research
		  Challenges, Survey, and Open Issues},
  year		= {2021},
  issue_date	= {September 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {7},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3462476},
  doi		= {10.1145/3462476},
  abstract	= {The explosive growth and widespread accessibility of
		  medical information on the Internet have led to a surge of
		  research activity in a wide range of scientific communities
		  including health informatics and information retrieval
		  (IR). One of the common concerns of this research, across
		  these disciplines, is how to design either clinical
		  decision support systems or medical search engines capable
		  of providing adequate support for both novices (e.g.,
		  patients and their next-of-kin) and experts (e.g.,
		  physicians, clinicians) tackling complex tasks (e.g.,
		  search for diagnosis, search for a treatment). However,
		  despite the significant multi-disciplinary research
		  advances, current medical search systems exhibit low levels
		  of performance. This survey provides an overview of the
		  state of the art in the disciplines of IR and health
		  informatics, and bridging these disciplines shows how
		  semantic search techniques can facilitate medical IR.
		  First,we will give a broad picture of semantic search and
		  medical IR and then highlight the major scientific
		  challenges. Second, focusing on the semantic gap challenge,
		  we will discuss representative state-of-the-art work
		  related to feature-based as well as semantic-based
		  representation and matching models that support medical
		  search systems. In addition to seminal works, we will
		  present recent works that rely on research advancements in
		  deep learning. Third, we make a thorough cross-model
		  analysis and provide some findings and lessons learned.
		  Finally, we discuss some open issues and possible promising
		  directions for future research trends.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {146},
  numpages	= {38},
  keywords	= {Information retrieval, evaluation, knowledge resources,
		  medical texts, relevance}
}

@Proceedings{	  10.1145/3585088,
  title		= {IDC '23: Proceedings of the 22nd Annual ACM Interaction
		  Design and Children Conference},
  year		= {2023},
  isbn		= {9798400701313},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Chicago, IL, USA}
}

@Article{	  10.1145/3569423,
  author	= {Li, Lei and Zhang, Yongfeng and Chen, Li},
  title		= {On the Relationship between Explanation and
		  Recommendation: Learning to Rank Explanations for Improved
		  Performance},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3569423},
  doi		= {10.1145/3569423},
  abstract	= {Explaining to users why some items are recommended is
		  critical, as it can help users to make better decisions,
		  increase their satisfaction, and gain their trust in
		  recommender systems (RS). However, existing explainable RS
		  usually consider explanation as a side output of the
		  recommendation model, which has two problems: (1) It is
		  difficult to evaluate the produced explanations, because
		  they are usually model-dependent, and (2) as a result, how
		  the explanations impact the recommendation performance is
		  less investigated.In this article, explaining
		  recommendations is formulated as a ranking task and learned
		  from data, similarly to item ranking for recommendation.
		  This makes it possible for standard evaluation of
		  explanations via ranking metrics (e.g., Normalized
		  Discounted Cumulative Gain). Furthermore, this article
		  extends traditional item ranking to an item–explanation
		  joint-ranking formalization to study if purposely selecting
		  explanations could reach certain learning goals, e.g.,
		  improving recommendation performance. A great challenge,
		  however, is that the sparsity issue in the
		  user-item-explanation data would be inevitably severer than
		  that in traditional user–item interaction data, since not
		  every user–item pair can be associated with all
		  explanations. To mitigate this issue, this article proposes
		  to perform two sets of matrix factorization by considering
		  the ternary relationship as two groups of binary
		  relationships. Experiments on three large datasets verify
		  the solution’s effectiveness on both explanation ranking
		  and item recommendation.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= feb,
  articleno	= {21},
  numpages	= {24},
  keywords	= {Explainable recommendation, explanation ranking, learning
		  to explain}
}

@InProceedings{	  10.1145/3459637.3482261,
  author	= {Liu, Zhihong and Li, Huiyu and Li, Ruixin and Zeng, Yong
		  and Ma, Jianfeng},
  title		= {Graph Embedding Based on Euclidean Distance Matrix and its
		  Applications},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482261},
  doi		= {10.1145/3459637.3482261},
  abstract	= {Graph embedding converts a graph into a multi-dimensional
		  space in which the graph structural information or graph
		  properties are maximumly preserved. It is an effective and
		  efficient way to provide users a deeper understanding of
		  what is behind the data and thus can benefit a lot of
		  useful applications. However, most graph embedding methods
		  suffer from high computation and space costs. In this
		  paper, we present a simple graph embedding method that
		  directly embeds the graph into its Euclidean distance
		  space. This method does not require the learned
		  representations to be low dimensional, but it has several
		  good characteristics. We find that the centrality of
		  nodes/edges can be represented by the position of nodes or
		  the length of edges when a graph is embedded. Besides, the
		  edge length is closely related to the density of regions in
		  a graph. We then apply this graph embedding method into
		  graph analytics, such as community detection, graph
		  compression, and wormhole detection, etc. Our evaluation
		  shows the effectiveness and efficiency of this embedding
		  method and contends that it yields a promising approach to
		  graph analytics.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1140–1149},
  numpages	= {10},
  keywords	= {community detection, graph compression, graph embedding,
		  social networks, wormhole detection},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.14778/3529337.3529355,
  author	= {Leone, Manuel and Huber, Stefano and Arora, Akhil and
		  Garc\'{\i}a-Dur\'{a}n, Alberto and West, Robert},
  title		= {A critical re-evaluation of neural methods for entity
		  alignment},
  year		= {2022},
  issue_date	= {April 2022},
  publisher	= {VLDB Endowment},
  volume	= {15},
  number	= {8},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3529337.3529355},
  doi		= {10.14778/3529337.3529355},
  abstract	= {Neural methods have become the de-facto choice for the
		  vast majority of data analysis tasks, and entity alignment
		  (EA) is no exception. Not surprisingly, more than 50
		  different neural EA methods have been published since 2017.
		  However, surprisingly, an analysis of the differences
		  between neural and non-neural EA methods has been lacking.
		  We bridge this gap by performing an in-depth comparison
		  among five carefully chosen representative state-of-the-art
		  methods from the pre-neural and neural era. We unravel, and
		  consequently mitigate, the inherent deficiencies in the
		  experimental setup utilized for evaluating neural EA
		  methods. To ensure fairness in evaluation, we homogenize
		  the entity matching modules of neural and non-neural
		  methods. Additionally, for the first time, we draw a
		  parallel between EA and record linkage (RL) by empirically
		  showcasing the ability of RL methods to perform EA. Our
		  results indicate that Paris, the state-of-the-art
		  non-neural method, statistically significantly outperforms
		  all the representative state-of-the-art neural methods in
		  terms of both efficacy and efficiency across a wide variety
		  of dataset types and scenarios, and is second only to
		  BERT-INT for a specific scenario of cross-lingual EA. Our
		  findings shed light on the potential problems resulting
		  from an impulsive application of neural methods as a
		  panacea for all data analytics tasks. Overall, our work
		  results in two overarching conclusions: (1) Paris should be
		  used as a baseline in every follow-up work on EA, and (2)
		  neural methods need to be positioned better to showcase
		  their true potential, for which we provide multiple
		  recommendations.},
  journal	= {Proc. VLDB Endow.},
  month		= apr,
  pages		= {1712–1725},
  numpages	= {14}
}

@Proceedings{	  10.1145/3578503,
  title		= {WebSci '23: Proceedings of the 15th ACM Web Science
		  Conference 2023},
  year		= {2023},
  isbn		= {9798400700897},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Austin, TX, USA}
}

@Article{	  10.1145/3578553,
  author	= {Wanjawa, Barack W. and Wanzare, Lilian D. A. and Indede,
		  Florence and Mconyango, Owen and Muchemi, Lawrence and
		  Ombui, Edward},
  title		= {KenSwQuAD—A Question Answering Dataset for Swahili
		  Low-resource Language},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3578553},
  doi		= {10.1145/3578553},
  abstract	= {The need for question-answering (QA) datasets in
		  low-resource languages is the motivation of this research,
		  leading to the development of the Kencorpus Swahili
		  Question Answering Dataset (KenSwQuAD). This dataset is
		  annotated from raw story texts of Swahili, a low-resource
		  language that is predominantly spoken in eastern Africa and
		  in other parts of the world. Question-answering datasets
		  are important for machine comprehension of natural language
		  for tasks such as internet search and dialog systems.
		  Machine learning systems need training data such as the
		  gold-standard question-answering set developed in this
		  research. The research engaged annotators to formulate QA
		  pairs from Swahili texts collected by the Kencorpus
		  project, a Kenyan languages corpus. The project annotated
		  1,445 texts from the total 2,585 texts with at least 5 QA
		  pairs each, resulting in a final dataset of 7,526 QA pairs.
		  A quality assurance set of 12.5% of the annotated texts
		  confirmed that the QA pairs were all correctly annotated. A
		  proof of concept on applying the set to the QA task
		  confirmed that the dataset can be usable for such tasks.
		  KenSwQuAD has also contributed to resourcing of the Swahili
		  language.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {113},
  numpages	= {20},
  keywords	= {Swahili, question answer, low-resource languages}
}

@InProceedings{	  10.1145/3485447.3512026,
  author	= {Balsebre, Pasquale and Yao, Dezhong and Cong, Gao and Hai,
		  Zhen},
  title		= {Geospatial Entity Resolution},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3512026},
  doi		= {10.1145/3485447.3512026},
  abstract	= {A geospatial database is today at the core of an ever
		  increasing number of services. Building and maintaining it
		  remains challenging due to the need to merge information
		  from multiple providers. Entity Resolution (ER) consists of
		  finding entity mentions from different sources that refer
		  to the same real world entity. In geospatial ER, entities
		  are often represented using different schemes and are
		  subject to incomplete information and inaccurate location,
		  making ER and deduplication daunting tasks. While
		  tremendous advances have been made in traditional entity
		  resolution and natural language processing, geospatial data
		  integration approaches still heavily rely on static
		  similarity measures and human-designed rules. In order to
		  achieve automatic linking of geospatial data, a unified
		  representation of entities with heterogeneous attributes
		  and their geographical context, is needed. To this end, we
		  propose Geo-ER1, a joint framework that combines
		  Transformer-based language models, that have been
		  successfully applied in ER, with a novel learning-based
		  architecture to represent the geospatial character of the
		  entity. Different from existing solutions, Geo-ER does not
		  rely on pre-defined rules and is able to capture
		  information from surrounding entities in order to make
		  context-based, accurate predictions. Extensive experiments
		  on eight real world datasets demonstrate the effectiveness
		  of our solution over state-of-the-art methods. Moreover,
		  Geo-ER proves to be robust in settings where there is no
		  available training data for a specific city.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {3061–3070},
  numpages	= {10},
  keywords	= {Entity resolution, geospatial data, graph attention,
		  neighbourhood embedding, neural networks},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@InProceedings{	  10.1145/3459637.3481937,
  author	= {Yu, Tan and Yang, Yi and Li, Yi and Liu, Lin and Sun,
		  Mingming and Li, Ping},
  title		= {Multi-modal Dictionary BERT for Cross-modal Video Search
		  in Baidu Advertising},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3481937},
  doi		= {10.1145/3459637.3481937},
  abstract	= {Due to their attractiveness, video advertisements are
		  adored by advertisers. Baidu, as one of the leading search
		  advertisement platforms in China, is putting more and more
		  effort into video advertisements for its advertisement
		  customers. Search-based video advertisement display is, in
		  essence, a cross-modal retrieval problem, which is normally
		  tackled through joint embedding methods. Nevertheless, due
		  to the lack of interactions between text features and image
		  features, joint embedding methods cannot achieve as high
		  accuracy as its counterpart based on attention. Inspired by
		  the great success achieved by BERT in NLP tasks, many
		  cross-modal BERT models emerge and achieve excellent
		  performance in cross-modal retrieval. Last year, Baidu also
		  launched a cross-modal BERT, CAN, in video advertisement
		  platform, and achieved considerably better performance than
		  the previous joint-embedding model. In this paper, we
		  present our recent work for video advertisement retrieval,
		  Multi-modal Dictionary BERT (MDBERT) model. Compared with
		  CAN and other cross-modal BERT models, MDBERT integrates a
		  joint dictionary, which is shared among video features and
		  word features. It maps the relevant word features and video
		  features into the same codeword and thus fosters effective
		  cross-modal attention. To support end-to-end training, we
		  propose to soften the codeword assignment. Meanwhile, to
		  enhance the inference efficiency, we adopt the product
		  quantization to achieve fine-level feature space partition
		  at a low cost. After launching MDBERT in Baidu video
		  advertising platform, the conversion ratio (CVR) increases
		  by 3.34%, bringing a considerable revenue boost for
		  advertisers in Baidu.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {4341–4351},
  numpages	= {11},
  keywords	= {advertisement, computer vision, cross-modal retrieval,
		  deep learning, natural language processing, search},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3571884.3597138,
  author	= {Braunschweiler, Norbert and Doddipatla, Rama Sanand and
		  Keizer, Simon and Stoyanchev, Svetlana},
  title		= {Enabling Semi-Structured Knowledge Access via a
		  Question-Answering Module in Task-oriented Dialogue
		  Systems},
  year		= {2023},
  isbn		= {9798400700149},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3571884.3597138},
  doi		= {10.1145/3571884.3597138},
  abstract	= {Users of task-oriented dialogue systems are often limited
		  to ‘in-schema queries’, i.e., questions constrained by
		  a predefined database structure. Providing access to
		  additional semi- or unstructured knowledge could enable
		  users to enter a wider range of queries answerable by the
		  system. To this end, we have integrated a
		  Question-Answering (QA)-module in an interactive restaurant
		  search system and evaluated its impact using a
		  crowd-sourced user evaluation. The QA-module includes
		  knowledge selection and response generation components,
		  both driven by fine-tuned GPT-2 language models, and a
		  method to prevent responses unrelated to a user question
		  (‘off-topic responses’). The results show that systems
		  with QA-module are significantly preferred over the
		  baseline without QA-module. Moreover, while the off-topic
		  response prevention method was correctly triggered in 98.1%
		  of questions not covered in the knowledge base, users
		  showed more preference to the system that can retrieve
		  information irrespective of whether it is relevant or
		  not.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Conversational User Interfaces},
  articleno	= {36},
  numpages	= {11},
  keywords	= {dialogue, dialogue systems, human-computer interaction,
		  information retrieval, question-answering},
  location	= {Eindhoven, Netherlands},
  series	= {CUI '23}
}

@Article{	  10.1145/3626095,
  author	= {Zamudio Padilla, Juan Diego and Wang, Liuqin},
  title		= {Binary Semantic Pattern Rules for Chinese-English Machine
		  Translation Based on Machine Learning Algorithms},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3626095},
  doi		= {10.1145/3626095},
  abstract	= {With the increase of internationalization and the
		  exponential growth of intercultural communication, the
		  importance of interlingual translation has become
		  increasingly prominent. Machine translation has been a
		  booming area of research as technology has advanced. Due to
		  the complexity of language ability and limited
		  understanding of language laws, there are challenges for
		  machine translation. This paper focused on how to construct
		  and apply binary semantic pattern rules through machine
		  learning to improve the translation effect in
		  Chinese-English machine translation. The research results
		  of this paper would contribute to the further development
		  and improvement of Chinese-English machine translation
		  technology. In order to produce high-quality translation
		  results, research in machine translation has recognized the
		  need to analyze and understand the semantics of natural
		  language. To address the important issue of lexical and
		  syntactic ambiguity, representations of binary semantic
		  pattern rules have been developed to formally describe
		  these rules. Based on this, this paper designed and
		  implemented a corpus-based binary semantic rule extraction
		  and optimization algorithm, which used machine learning
		  algorithms to automatically detect the semantic rules of
		  two or more than two phrases in the Chinese corpus, and
		  then automatically optimized and converted them according
		  to the statistical results, and realized the design of
		  Chinese-English machine translation system. The article
		  evaluated the quality of machine translation to test the
		  effectiveness of machine translation binary semantic
		  pattern rules based on machine learning algorithms. The
		  study found that compared with the rule set A, the rule
		  sets B and C obtained automatically by the rule mining
		  algorithm had significantly improved accuracy, both
		  reaching more than 90%. This showed that the binary
		  semantic pattern rule mining algorithm and optimization
		  algorithm proposed in this paper were reasonable.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  keywords	= {Machine Learning, Machine Translation, Binary Semantic
		  Pattern Rules, Association Rules, Chinese and English
		  Corpora}
}

@Article{	  10.1145/3624988,
  author	= {Bassani, Elias and Tonellotto, Nicola and Pasi,
		  Gabriella},
  title		= {Personalized Query Expansion with Contextual Word
		  Embeddings},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3624988},
  doi		= {10.1145/3624988},
  abstract	= {Personalized Query Expansion, the task of expanding
		  queries with additional terms extracted from the
		  user-related vocabulary, is a well-known solution to
		  improve the retrieval performance of a system w.r.t. short
		  queries. Recent approaches rely on word embeddings to
		  select expansion terms from user-related texts. Although
		  promising results have been delivered with former word
		  embedding techniques, we argue that these methods are not
		  suited for contextual word embeddings, which produce a
		  unique vector representation for each term occurrence.In
		  this article, we propose a Personalized Query Expansion
		  method designed to solve the issues arising from the use of
		  contextual word embeddings with the current Personalized
		  Query Expansion approaches based on word embeddings.
		  Specifically, we employ a clustering-based procedure to
		  identify the terms that better represent the user interests
		  and to improve the diversity of those selected for
		  expansion, achieving improvements of up to 4% w.r.t. the
		  best-performing baseline in terms of MAP@100. Moreover, our
		  approach outperforms previous ones in terms of efficiency,
		  allowing us to achieve sub-millisecond expansion times even
		  in data-rich scenarios. Finally, we introduce a novel
		  metric to evaluate the expansion terms’ diversity and
		  empirically show the unsuitability of previous approaches
		  based on word embeddings when employed along with
		  contextual word embeddings, which cause the selection of
		  semantically overlapping expansion terms.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= dec,
  articleno	= {61},
  numpages	= {35},
  keywords	= {Personalization, Query Expansion, contextual word
		  embeddings, dense retrieval}
}

@InProceedings{	  10.1109/icse48619.2023.00161,
  author	= {Nam, Daye and Myers, Brad and Vasilescu, Bogdan and
		  Hellendoorn, Vincent},
  title		= {Improving API Knowledge Discovery with ML: A Case Study of
		  Comparable API Methods},
  year		= {2023},
  isbn		= {9781665457019},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ICSE48619.2023.00161},
  doi		= {10.1109/ICSE48619.2023.00161},
  abstract	= {Developers constantly learn new APIs, but often lack
		  necessary information from documentation, resorting instead
		  to popular question-and-answer platforms such as Stack
		  Overflow. In this paper, we investigate how to use recent
		  machine-learning-based knowledge extraction techniques to
		  automatically identify pairs of comparable API methods and
		  the sentences describing the comparison from Stack Overflow
		  answers. We first built a prototype that can be stocked
		  with a dataset of comparable API methods and provides
		  tool-tips to users in search results and in API
		  documentation. We conducted a user study with this tool
		  based on a dataset of TensorFlow comparable API methods
		  spanning 198 hand-annotated facts from Stack Overflow
		  posts. This study confirmed that providing comparable API
		  methods can be useful for helping developers understand the
		  design space of APIs: developers using our tool were
		  significantly more aware of the comparable API methods and
		  better understood the differences between them. We then
		  created SOREL, an comparable API methods knowledge
		  extraction tool trained on our hand-annotated corpus, which
		  achieves a 71% precision and 55% recall at discovering our
		  manually extracted facts and discovers 433 pairs of
		  comparable API methods from thousands of unseen Stack
		  Overflow posts. This work highlights the merit of jointly
		  studying programming assistance tools and constructing
		  machine learning techniques to power them.},
  booktitle	= {Proceedings of the 45th International Conference on
		  Software Engineering},
  pages		= {1890–1906},
  numpages	= {17},
  location	= {Melbourne, Victoria, Australia},
  series	= {ICSE '23}
}

@Proceedings{	  10.1145/3545801,
  title		= {ICBDC '22: Proceedings of the 7th International Conference
		  on Big Data and Computing},
  year		= {2022},
  isbn		= {9781450396097},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shenzhen, China}
}

@Proceedings{	  10.1145/3614008,
  title		= {SPML '23: Proceedings of the 2023 6th International
		  Conference on Signal Processing and Machine Learning},
  year		= {2023},
  isbn		= {9798400707575},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tianjin, China}
}

@InProceedings{	  10.1145/3490725.3490737,
  author	= {Huang, Chao and Di, Hui and Wang, Lina and Ouchi,
		  Kazushige},
  title		= {ECO-DST: An Efficient Cross-lingual Dialogue State
		  Tracking Framework},
  year		= {2022},
  isbn		= {9781450384247},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3490725.3490737},
  doi		= {10.1145/3490725.3490737},
  abstract	= {Data efficiency is a critical challenge for cross-lingual
		  task-oriented dialogue state tracking (DST) due to high
		  cost of collecting large amount of task-related labeled
		  training set for specific language. Therefore, we focus on
		  adapting high-performance source language DST to target
		  language by using only bilingual dictionary, without
		  accessing labeled target data. We propose a novel data
		  efficient cross-lingual DST framework (ECO-DST), which
		  consists of cross-lingual encoder and language independent
		  decoder. To support cross-lingual zero-shot adaptation, we
		  leverage two advanced methods in encoder: 1) pre-trained
		  cross-lingual model XLM-RoBERTa (XLM-R), 2) dynamic local
		  phrase code-switching data augmentation for cross-lingual
		  representation alignment. We evaluate the proposed method
		  on The Ninth Dialogue System Technology Challenge (DSTC9)
		  cross-lingual tasks. For target language DST, we compare
		  our proposed framework with submitted systems in DSTC9, our
		  model achieves state-of-the-art result on CrossWOZ dataset
		  and promising result on MultiWOZ 2.1 dataset. Meanwhile on
		  source language DST, the same model keeps competitive
		  performance compared with original source DST model.},
  booktitle	= {Proceedings of the 2021 4th International Conference on
		  Machine Learning and Machine Intelligence},
  pages		= {77–82},
  numpages	= {6},
  keywords	= {Cross-Lingual Transfer, Data Efficiency, Dialogue State
		  Tracking, Dynamic Local Phrase Code-Switching,
		  Task-oriented Dialogue},
  location	= {Hangzhou, China},
  series	= {MLMI '21}
}

@Article{	  10.1109/taslp.2022.3181350,
  author	= {An, Jinwon and Cho, Sungzoon and Bang, Junseong and Kim,
		  Misuk},
  title		= {Domain-Slot Relationship Modeling Using a Pre-Trained
		  Language Encoder for Multi-Domain Dialogue State Tracking},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3181350},
  doi		= {10.1109/TASLP.2022.3181350},
  abstract	= {Dialogue state tracking for multi-domain dialogues is
		  challenging because the model should be able to track
		  dialogue states across multiple domains and slots. As using
		  pre-trained language models is the de facto standard for
		  natural language processing tasks, many recent studies use
		  them to encode the dialogue context for predicting the
		  dialogue states. Model architectures that have certain
		  inductive biases for modeling the relationship among
		  different domain-slot pairs are also emerging. Our work is
		  based on these research approaches on multi-domain dialogue
		  state tracking. We propose a model architecture that
		  effectively models the relationship among domain-slot pairs
		  using a pre-trained language encoder. Inspired by the way
		  the special &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$[CLS]$&lt;/tex-math&gt;&lt;/inline-formula&gt;
		  token in BERT is used to aggregate the information of the
		  whole sequence, we use multiple special tokens for each
		  domain-slot pair that encodes information corresponding to
		  its domain and slot. The special tokens are run together
		  with the dialogue context through the pre-trained language
		  encoder, which effectively models the relationship among
		  different domain-slot pairs. Our experimental results on
		  the datasets MultiWOZ-2.0 and MultiWOZ-2.1 show that our
		  model outperforms other models with the same setting. Our
		  ablation studies incorporate three main parts. The first
		  component shows the effectiveness of our approach
		  exploiting the relationship modeling. The second component
		  compares the effect of using different pre-trained language
		  encoders. The final component involves comparing different
		  initialization methods that could be used for the special
		  tokens. Qualitative analysis of the attention map of the
		  pre-trained language encoder shows that our special tokens
		  encode relevant information through the encoding process by
		  attending to each other.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jun,
  pages		= {2091–2102},
  numpages	= {12}
}

@Article{	  10.1145/3450315,
  author	= {Joaristi, Mikel and Serra, Edoardo},
  title		= {SIR-GN: A Fast Structural Iterative Representation
		  Learning Approach For Graph Nodes},
  year		= {2021},
  issue_date	= {June 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {6},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3450315},
  doi		= {10.1145/3450315},
  abstract	= {Graph representation learning methods have attracted an
		  increasing amount of attention in recent years. These
		  methods focus on learning a numerical representation of the
		  nodes in a graph. Learning these representations is a
		  powerful instrument for tasks such as graph mining,
		  visualization, and hashing. They are of particular interest
		  because they facilitate the direct use of standard machine
		  learning models on graphs. Graph representation learning
		  methods can be divided into two main categories: methods
		  preserving the connectivity information of the nodes and
		  methods preserving nodes’ structural information.
		  Connectivity-based methods focus on encoding relationships
		  between nodes, with connected nodes being closer together
		  in the resulting latent space. While methods preserving
		  structure generate a latent space where nodes serving a
		  similar structural function in the network are encoded
		  close to each other, independently of them being connected
		  or even close to each other in the graph. While there are a
		  lot of works that focus on preserving node connectivity,
		  only a few works focus on preserving nodes’ structure.
		  Properly encoding nodes’ structural information is
		  fundamental for many real-world applications as it has been
		  demonstrated that this information can be leveraged to
		  successfully solve many tasks where connectivity-based
		  methods usually fail. A typical example is the task of node
		  classification, i.e., the assignment or prediction of a
		  particular label for a node. Current limitations of
		  structural representation methods are their scalability,
		  representation meaning, and no formal proof that guaranteed
		  the preservation of structural properties. We propose a new
		  graph representation learning method, called Structural
		  Iterative Representation learning approach for Graph Nodes
		  (SIR-GN). In this work, we propose two variations
		  (SIR-GN:&nbsp;GMM and SIR-GN:&nbsp;K-Means) and show how
		  our best variation SIR-GN:&nbsp;K-Means: (1) theoretically
		  guarantees the preservation of graph structural
		  similarities, (2) provides a clear meaning about its
		  representation and a way to interpret it with a
		  specifically designed attribution procedure, and (3) is
		  scalable and fast to compute. In addition, from our
		  experiment, we show that SIR-GN:&nbsp;K-Means is often
		  better or, in the worst-case comparable than the existing
		  structural graph representation learning methods present in
		  the literature. Also, we empirically show its superior
		  scalability and computational performance when compared to
		  other existing approaches.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= may,
  articleno	= {100},
  numpages	= {39},
  keywords	= {Datasets, neural networks, gaze detection, text tagging}
}

@InProceedings{	  10.1145/3586183.3606822,
  author	= {Pu, Kevin and Yang, Jim and Yuan, Angel and Ma, Minyi and
		  Dong, Rui and Wang, Xinyu and Chen, Yan and Grossman,
		  Tovi},
  title		= {DiLogics: Creating Web Automation Programs with Diverse
		  Logics},
  year		= {2023},
  isbn		= {9798400701320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3586183.3606822},
  doi		= {10.1145/3586183.3606822},
  abstract	= {Knowledge workers frequently encounter repetitive web data
		  entry tasks, like updating records or placing orders. Web
		  automation increases productivity, but translating tasks to
		  web actions accurately and extending to new specifications
		  is challenging. Existing tools can automate tasks that
		  perform the same logical trace of UI actions (e.g., input
		  text in each field in order), but do not support tasks
		  requiring different executions based on varied input
		  conditions. We present DiLogics, a
		  programming-by-demonstration system that utilizes NLP to
		  assist users in creating web automation programs that
		  handle diverse specifications. DiLogics first semantically
		  segments input data to structured task steps. By recording
		  user demonstrations for each step, DiLogics generalizes the
		  web macros to novel but semantically similar task
		  requirements. Our evaluation showed that non-experts can
		  effectively use DiLogics to create automation programs that
		  fulfill diverse input instructions. DiLogics provides an
		  efficient, intuitive, and expressive method for developing
		  web automation programs satisfying diverse
		  specifications.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {74},
  numpages	= {15},
  keywords	= {PBD, Web automation, neurosymbolic programming},
  location	= {San Francisco, CA, USA},
  series	= {UIST '23}
}

@InProceedings{	  10.1145/3555041.3589406,
  author	= {Chai, Chengliang and Tang, Nan and Fan, Ju and Luo, Yuyu},
  title		= {Demystifying Artificial Intelligence for Data
		  Preparation},
  year		= {2023},
  isbn		= {9781450395076},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3555041.3589406},
  doi		= {10.1145/3555041.3589406},
  abstract	= {Data preparation -- the process of discovering,
		  integrating, transforming, cleaning, and annotating data --
		  is one of the oldest, hardest, yet inevitable data
		  management problems. Unfortunately, data preparation is
		  known to be iterative, requires high human cost, and is
		  error-prone. Recent advances in artificial intelligence
		  (AI) have shown very promising results on many data
		  preparation tasks. At a high level, AI for data preparation
		  (AI4DP) should have the following abilities. First, the AI
		  model should capture real-world knowledge so as to solve
		  various tasks. Second, it is important to easily adapt to
		  new datasets/tasks. Third, data preparation is a
		  complicated pipeline with many operations, which results in
		  a large number of candidates to select the optimum, and
		  thus it is crucial to effectively and efficiently explore
		  the large space of possible pipelines.In this tutorial, we
		  will cover three important topics to address the above
		  issues: demystifying foundation models to inject knowledge
		  for data preparation, tuning and adapting pre-trained
		  language models for data preparation, and orchestrating
		  data preparation pipelines for different downstream
		  applications.},
  booktitle	= {Companion of the 2023 International Conference on
		  Management of Data},
  pages		= {13–20},
  numpages	= {8},
  keywords	= {artificial intelligence, data preparation, foundation
		  models},
  location	= {Seattle, WA, USA},
  series	= {SIGMOD '23}
}

@Proceedings{	  10.1145/3627377,
  title		= {ICBDT '23: Proceedings of the 2023 6th International
		  Conference on Big Data Technologies},
  year		= {2023},
  isbn		= {9798400707667},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Qingdao, China}
}

@Proceedings{	  10.1145/3584376,
  title		= {RICAI '22: Proceedings of the 2022 4th International
		  Conference on Robotics, Intelligent Control and Artificial
		  Intelligence},
  year		= {2022},
  isbn		= {9781450398343},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dongguan, China}
}

@InProceedings{	  10.1145/3447548.3469053,
  author	= {Wang, Yu and Li, Jinchao and Naumann, Tristan and Xiong,
		  Chenyan and Cheng, Hao and Tinn, Robert and Wong, Cliff and
		  Usuyama, Naoto and Rogahn, Richard and Shen, Zhihong and
		  Qin, Yang and Horvitz, Eric and Bennett, Paul N. and Gao,
		  Jianfeng and Poon, Hoifung},
  title		= {Domain-Specific Pretraining for Vertical Search: Case
		  Study on Biomedical Literature},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3469053},
  doi		= {10.1145/3447548.3469053},
  abstract	= {Information overload is a prevalent challenge in many
		  high-value domains. A prominent case in point is the
		  explosion of the biomedical literature on COVID-19, which
		  swelled to hundreds of thousands of papers in a matter of
		  months. In general, biomedical literature expands by two
		  papers every minute, totalling over a million new papers
		  every year. Search in the biomedical realm, and many other
		  vertical domains is challenging due to the scarcity of
		  direct supervision from click logs. Self-supervised
		  learning has emerged as a promising direction to overcome
		  the annotation bottleneck. We propose a general approach
		  for vertical search based on domain-specific pretraining
		  and present a case study for the biomedical domain. Despite
		  being substantially simpler and not using any relevance
		  labels for training or development, our method performs
		  comparably or better than the best systems in the official
		  TREC-COVID evaluation, a COVID-related biomedical search
		  competition. Using distributed computing in modern cloud
		  infrastructure, our system can scale to tens of millions of
		  articles on PubMed and has been deployed as Microsoft
		  Biomedical Search, a new search experience for biomedical
		  literature: https://aka.ms/biomedsearch.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {3717–3725},
  numpages	= {9},
  keywords	= {COVID-19, NLP, biomedical, domain-specific pretraining,
		  search},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@Proceedings{	  10.1145/3570773,
  title		= {ISAIMS '22: Proceedings of the 3rd International Symposium
		  on Artificial Intelligence for Medicine Sciences},
  year		= {2022},
  isbn		= {9781450398442},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Amsterdam, Netherlands}
}

@InProceedings{	  10.1145/3539618.3591911,
  author	= {Kaur, Simerjot and Smiley, Charese and Gupta, Akshat and
		  Sain, Joy and Wang, Dongsheng and Siddagangappa, Suchetha
		  and Aguda, Toyin and Shah, Sameena},
  title		= {REFinD: Relation Extraction Financial Dataset},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591911},
  doi		= {10.1145/3539618.3591911},
  abstract	= {A number of datasets for Relation Extraction (RE) have
		  been created to aide downstream tasks such as information
		  retrieval, semantic search, question answering and textual
		  entailment. However, these datasets fail to capture
		  financial-domain specific challenges since most of these
		  datasets are compiled using general knowledge sources such
		  as Wikipedia, web-based text and news articles, hindering
		  real-life progress and adoption within the financial world.
		  To address this limitation, we propose REFinD, the first
		  large-scale annotated dataset of relations, with ~29K
		  instances and 22 relations amongst 8 types of entity pairs,
		  generated entirely over financial documents. We also
		  provide an empirical evaluation with various
		  state-of-the-art models as benchmarks for the RE task and
		  highlight the challenges posed by our dataset. We observed
		  that various state-of-the-art deep learning models struggle
		  with numeric inference, relational and directional
		  ambiguity. To encourage further research in this direction,
		  REFinD is available at
		  https://www.jpmorgan.com/technology/artificial-intelligence/initiatives/refind-dataset/problem-motivation-outcome.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3054–3063},
  numpages	= {10},
  keywords	= {annotation datasets, benchmarking, finance, information
		  retrieval, natural language processing, relation
		  extraction},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Proceedings{	  10.1145/3594315,
  title		= {ICCAI '23: Proceedings of the 2023 9th International
		  Conference on Computing and Artificial Intelligence},
  year		= {2023},
  isbn		= {9781450399029},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tianjin, China}
}

@InProceedings{	  10.1145/3563657.3596002,
  author	= {Cho, Hyungjun and Lee, Jiyeon and Ku, Bonhee and Jeong,
		  Yunwoo and Yadgarova, Shakhnozakhon and Nam, Tek-Jin},
  title		= {ARECA: A Design Speculation on Everyday Products Having
		  Minds},
  year		= {2023},
  isbn		= {9781450398930},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3563657.3596002},
  doi		= {10.1145/3563657.3596002},
  abstract	= {An increasing number of everyday products are being
		  designed to possess qualities such as intelligence,
		  consciousness, and emotion. However, there is a need for
		  more understanding on how to design for these properties of
		  mind. To address this, we present the design of Areca, an
		  air purifier that keeps a diary. This paper outlines our
		  design process, focusing on how the diary generation
		  process gives Areca properties of mind and how its
		  appearance and interaction design support this concept.
		  Through exhibiting Areca in a design exhibition, we
		  gathered people's initial reactions and perceptions to
		  further evaluate the effectiveness of our design
		  intentions. Finally, based on these experiences, we engage
		  in discussions on the design of products having minds.},
  booktitle	= {Proceedings of the 2023 ACM Designing Interactive Systems
		  Conference},
  pages		= {31–44},
  numpages	= {14},
  location	= {Pittsburgh, PA, USA},
  series	= {DIS '23}
}

@InProceedings{	  10.1145/3459637.3482328,
  author	= {Wu, Junda and Zhao, Canzhe and Yu, Tong and Li, Jingyang
		  and Li, Shuai},
  title		= {Clustering of Conversational Bandits for User Preference
		  Learning and Elicitation},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482328},
  doi		= {10.1145/3459637.3482328},
  abstract	= {Conversational recommender systems elicit user preference
		  via interactive conversational interactions. By introducing
		  conversational key-terms, existing conversational
		  recommenders can effectively reduce the need for extensive
		  exploration in a traditional interactive recommender.
		  However, there are still limitations of existing
		  conversational recommender approaches eliciting user
		  preference via key-terms. First, the key-term data of the
		  items needs to be carefully labeled, which requires a lot
		  of human efforts. Second, the number of the human labeled
		  key-terms is limited and the granularity of the key-terms
		  is fixed, while the elicited user preference is usually
		  from coarse-grained to fine-grained during the
		  conversations. In this paper, we propose a clustering of
		  conversational bandits algorithm. To avoid the human
		  labeling efforts and automatically learn the key-terms with
		  the proper granularity, we online cluster the items and
		  generate meaningful key-terms for the items during the
		  conversational interactions. Our algorithm is general and
		  can also be used in the user clustering when the feedback
		  from multiple users is available, which further leads to
		  more accurate learning and generations of conversational
		  key-terms. We analyze the regret bound of our learning
		  algorithm. In the empirical evaluations, without using any
		  human labeled key-terms, our algorithm effectively
		  generates meaningful coarse-to-fine grained key-terms and
		  performs as well as or better than the state-of-the-art
		  baseline.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {2129–2139},
  numpages	= {11},
  keywords	= {clustering of bandits, conversational recommender, online
		  learning},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3510858.3511425,
  author	= {Liu, Nan},
  title		= {Intelligent English automatic translation system based on
		  computer corpus},
  year		= {2022},
  isbn		= {9781450390422},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3510858.3511425},
  doi		= {10.1145/3510858.3511425},
  abstract	= {Machine translation system, also known as automatic
		  translation system, generally refers to a relatively
		  complex process of automatically by computer, and generally
		  refers to the translation of sentences or full texts in
		  this process. At present, systems are generally divided
		  into two categories: one is statistical-based machine
		  translation systems, and the other is case-based machine
		  translation systems. Corpus-based machine translation
		  system involves many disciplines, among which linguistics,
		  mathematics and computer science are the most basic.
		  Multi-disciplinary cooperation constitutes a rapidly
		  developing machine translation system based on corpus. The
		  rapid development of machine translation system has
		  accelerated the cultural, economic and other exchanges
		  between different countries. Looking at the current
		  development situation, we can see that the rapid
		  development of corpus linguistics and computer linguistics
		  has greatly improved the performance of today's machine
		  translation system.},
  booktitle	= {2021 International Conference on Aviation Safety and
		  Information Technology},
  pages		= {925–929},
  numpages	= {5},
  location	= {Changsha, China},
  series	= {ICASIT 2021}
}

@Proceedings{	  10.1145/3569192,
  title		= {ICBRA '22: Proceedings of the 9th International Conference
		  on Bioinformatics Research and Applications},
  year		= {2022},
  isbn		= {9781450396868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Berlin, Germany}
}

@Article{	  10.1145/3589786,
  author	= {Langenecker, Sven and Sturm, Christoph and Schalles,
		  Christian Schalles and Binnig, Carsten},
  title		= {Steered Training Data Generation for Learned Semantic Type
		  Detection},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {2},
  url		= {https://doi.org/10.1145/3589786},
  doi		= {10.1145/3589786},
  abstract	= {In this paper, we introduce STEER to adapt learned
		  semantic type extraction approaches to a new, unseen data
		  lake. STEER provides a data programming framework for
		  semantic labeling which is used to generate new labeled
		  training data with minimal overhead. At its core, STEER
		  comes with a novel training data generation procedure
		  called Steered-Labeling that can generate high quality
		  training data not only for non-numeric but also for
		  numerical columns. With this generated training data STEER
		  is able to fine-tune existing learned semantic type
		  extraction models. We evaluate our approach on four
		  different data lakes and show that we can significantly
		  improve the performance of two different types of learned
		  models across all data lakes.},
  journal	= {Proc. ACM Manag. Data},
  month		= jun,
  articleno	= {201},
  numpages	= {25},
  keywords	= {data discovery, data lakes, semantic type detection}
}

@InProceedings{	  10.1145/3528588.3528658,
  author	= {Alchokr, Rand and Borkar, Manoj and Thotadarya, Sharanya
		  and Saake, Gunter and Leich, Thomas},
  title		= {Supporting systematic literature reviews using
		  deep-learning-based language models},
  year		= {2023},
  isbn		= {9781450393430},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3528588.3528658},
  doi		= {10.1145/3528588.3528658},
  abstract	= {Background: Systematic Literature Reviews are an important
		  research method for gathering and evaluating the available
		  evidence regarding a specific research topic. However, the
		  process of conducting a Systematic Literature Review
		  manually can be difficult and time-consuming. For this
		  reason, researchers aim to semi-automate this process or
		  some of its phases. Aim: We aimed at using a deep-learning
		  based contextualized embeddings clustering technique
		  involving transformer-based language models and a weighted
		  scheme to accelerate the conduction phase of Systematic
		  Literature Reviews for efficiently scanning the initial set
		  of retrieved publications. Method: We performed an
		  experiment using two manually conducted SLRs to evaluate
		  the performance of two deep-learning-based clustering
		  models. These models build on transformer-based deep
		  language models (i.e., BERT and S-BERT) to extract
		  contextualized embeddings on different text levels along
		  with a weighted scheme to cluster similar publications.
		  Results: Our primary results show that clustering based on
		  embedding at paragraph-level using S-BERT-paragraph
		  represents the best performing model setting in terms of
		  optimizing the required parameters such as correctly
		  identifying primary studies, number of additional documents
		  identified as part of the relevant cluster and the
		  execution time of the experiments. Conclusions: The
		  findings indicate that using natural-language-based
		  deep-learning architectures for semi-automating the
		  selection of primary studies can accelerate the scanning
		  and identification process. While our results represent
		  first insights only, such a technique seems to enhance SLR
		  process, promising to help researchers identify the most
		  relevant publications more quickly and efficiently.},
  booktitle	= {Proceedings of the 1st International Workshop on Natural
		  Language-Based Software Engineering},
  pages		= {67–74},
  numpages	= {8},
  keywords	= {BERT, deep learning, language models, systematic
		  literature review},
  location	= {Pittsburgh, Pennsylvania},
  series	= {NLBSE '22}
}

@InProceedings{	  10.1145/3437963.3441730,
  author	= {Zhang, Yu and Chen, Xiusi and Meng, Yu and Han, Jiawei},
  title		= {Hierarchical Metadata-Aware Document Categorization under
		  Weak Supervision},
  year		= {2021},
  isbn		= {9781450382977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3437963.3441730},
  doi		= {10.1145/3437963.3441730},
  abstract	= {Categorizing documents into a given label hierarchy is
		  intuitively appealing due to the ubiquity of hierarchical
		  topic structures in massive text corpora. Although related
		  studies have achieved satisfying performance in fully
		  supervised hierarchical document classification, they
		  usually require massive human-annotated training data and
		  only utilize text information. However, in many domains,
		  (1) annotations are quite expensive where very few training
		  samples can be acquired; (2) documents are accompanied by
		  metadata information. Hence, this paper studies how to
		  integrate the label hierarchy, metadata, and text signals
		  for document categorization under weak supervision. We
		  develop HiMeCat, an embedding-based generative framework
		  for our task. Specifically, we propose a novel joint
		  representation learning module that allows simultaneous
		  modeling of category dependencies, metadata information and
		  textual semantics, and we introduce a data augmentation
		  module that hierarchically synthesizes training documents
		  to complement the original, small-scale training set. Our
		  experiments demonstrate a consistent improvement of HiMeCat
		  over competitive baselines and validate the contribution of
		  our representation learning and data augmentation
		  modules.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {770–778},
  numpages	= {9},
  keywords	= {hierarchical text categorization, metadata-aware text
		  categorization, weak supervision},
  location	= {Virtual Event, Israel},
  series	= {WSDM '21}
}

@InProceedings{	  10.1145/3491102.3517434,
  author	= {Hope, Tom and Tamari, Ronen and Hershcovich, Daniel and
		  Kang, Hyeonsu B and Chan, Joel and Kittur, Aniket and
		  Shahaf, Dafna},
  title		= {Scaling Creative Inspiration with Fine-Grained Functional
		  Aspects of Ideas},
  year		= {2022},
  isbn		= {9781450391573},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3491102.3517434},
  doi		= {10.1145/3491102.3517434},
  abstract	= {Large repositories of products, patents and scientific
		  papers offer an opportunity for building systems that scour
		  millions of ideas and help users discover inspirations.
		  However, idea descriptions are typically in the form of
		  unstructured text, lacking key structure that is required
		  for supporting creative innovation interactions. Prior work
		  has explored idea representations that were either limited
		  in expressivity, required significant manual effort from
		  users, or dependent on curated knowledge bases with poor
		  coverage. We explore a novel representation that
		  automatically breaks up products into fine-grained
		  functional aspects capturing the purposes and mechanisms of
		  ideas, and use it to support important creative innovation
		  interactions: functional search for ideas, and exploration
		  of the design space around a focal problem by viewing
		  related problem perspectives pooled from across many
		  products. In user studies, our approach boosts the quality
		  of creative search and inspirations, substantially
		  outperforming strong baselines by 50-60%.},
  booktitle	= {Proceedings of the 2022 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {12},
  numpages	= {15},
  location	= {New Orleans, LA, USA},
  series	= {CHI '22}
}

@Proceedings{	  10.1145/3574318,
  title		= {FIRE '22: Proceedings of the 14th Annual Meeting of the
		  Forum for Information Retrieval Evaluation},
  year		= {2022},
  isbn		= {9798400700231},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Kolkata, India}
}

@Article{	  10.1109/taslp.2022.3153254,
  author	= {Chen, Xiaofeng and Wang, Guohua and Ren, Haopeng and Cai,
		  Yi and Leung, Ho-fung and Wang, Tao},
  title		= {Task-Adaptive Feature Fusion for Generalized Few-Shot
		  Relation Classification in an Open World Environment},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3153254},
  doi		= {10.1109/TASLP.2022.3153254},
  abstract	= {Relation Classification (RC) is an important task in
		  information extraction. In most real-world scenarios, the
		  frequency of relations often follows a long-tailed and
		  open-ended distribution. However, current efforts mainly
		  focus on the partial frequency distribution of relations,
		  which is limited in real-world applications. Meanwhile,
		  prototypical network achieves remarkable performance among
		  fields of deep supervised learning, few-shot learning and
		  open set learning. Nevertheless, in the open world
		  environment, it still suffers from the incompatible feature
		  embedding problem as the novel and unknown relations come
		  in. To address these problems, we propose an Open
		  Generalized Prototypical Network with task-adaptive feature
		  fusion for the open generalized few-shot relation
		  classification. Extensive experiments are conducted on
		  public large-scale datasets and our proposed model obtains
		  the better performances.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= feb,
  pages		= {1003–1015},
  numpages	= {13}
}

@InProceedings{	  10.1145/3460231.3474272,
  author	= {Polignano, Marco and Musto, Cataldo and de Gemmis, Marco
		  and Lops, Pasquale and Semeraro, Giovanni},
  title		= {Together is Better: Hybrid Recommendations Combining Graph
		  Embeddings and Contextualized Word Representations},
  year		= {2021},
  isbn		= {9781450384582},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460231.3474272},
  doi		= {10.1145/3460231.3474272},
  abstract	= {In this paper, we present a hybrid recommendation
		  framework based on the combination of graph embeddings and
		  contextual word representations. Our approach is based on
		  the intuition that each of the above mentioned
		  representation models heterogeneous (and equally important)
		  information, that is worth to be taken into account to
		  generate a recommendation. Accordingly, we propose a
		  strategy to combine both the features, which is based on
		  the following steps: first, we separately generate graph
		  embeddings and contextual word representations by
		  exploiting state-of-the-art techniques. Next, these
		  embeddings are used to feed a deep architecture that learns
		  a hybrid representation based on the combination of the
		  single groups of features. Finally, we exploit the
		  resulting embedding to identify suitable recommendations.
		  In the experimental session, we evaluate the effectiveness
		  of our strategy on two datasets and results show that the
		  use of a hybrid representation leads to an improvement of
		  the predictive accuracy. Moreover, our approach overcomes
		  several competitive baselines, thus confirming the validity
		  of this work.},
  booktitle	= {Proceedings of the 15th ACM Conference on Recommender
		  Systems},
  pages		= {187–198},
  numpages	= {12},
  keywords	= {BERT embeddings, USE embeddings, deep learning, graph
		  embeddings, recommender systems},
  location	= {Amsterdam, Netherlands},
  series	= {RecSys '21}
}

@InProceedings{	  10.1145/3540250.3549123,
  author	= {Cao, Junming and Chen, Bihuan and Sun, Chao and Hu,
		  Longjie and Wu, Shuaihong and Peng, Xin},
  title		= {Understanding performance problems in deep learning
		  systems},
  year		= {2022},
  isbn		= {9781450394130},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3540250.3549123},
  doi		= {10.1145/3540250.3549123},
  abstract	= {Deep learning (DL) has been widely applied to many
		  domains. Unique challenges in engineering DL systems are
		  posed by the programming paradigm shift from traditional
		  systems to DL systems, and performance is one of the
		  challenges. Performance problems (PPs) in DL systems can
		  cause severe consequences such as excessive resource
		  consumption and financial loss. While bugs in DL systems
		  have been extensively investigated, PPs in DL systems have
		  hardly been explored. To bridge this gap, we present the
		  first comprehensive study to i) characterize symptoms, root
		  causes, and introducing and exposing stages of PPs in DL
		  systems developed in TensorFLow and Keras, with 224 PPs
		  collected from 210 StackOverflow posts, and to ii) assess
		  the capability of existing performance analysis approaches
		  in tackling PPs, with a constructed benchmark of 58 PPs in
		  DL systems. Our findings shed light on the implications on
		  developing high-performance DL systems, and detecting and
		  localizing PPs in DL systems. To demonstrate the usefulness
		  of our findings, we develop a static checker DeepPerf to
		  detect three types of PPs. It has detected 488 new PPs in
		  130 GitHub projects. 105 and 27 PPs have been confirmed and
		  fixed.},
  booktitle	= {Proceedings of the 30th ACM Joint European Software
		  Engineering Conference and Symposium on the Foundations of
		  Software Engineering},
  pages		= {357–369},
  numpages	= {13},
  keywords	= {deep learning, performance analysis, performance
		  problems},
  location	= {Singapore, Singapore},
  series	= {ESEC/FSE 2022}
}

@InProceedings{	  10.1145/3597926.3598085,
  author	= {Xu, Sihan and Gao, Ya and Fan, Lingling and Li, Linyu and
		  Cai, Xiangrui and Liu, Zheli},
  title		= {LiResolver: License Incompatibility Resolution for Open
		  Source Software},
  year		= {2023},
  isbn		= {9798400702211},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3597926.3598085},
  doi		= {10.1145/3597926.3598085},
  abstract	= {Open source software (OSS) licenses regulate the
		  conditions under which OSS can be legally reused,
		  distributed, and modified. However, a common issue arises
		  when incorporating third-party OSS accompanied with
		  licenses, i.e., license incompatibility, which occurs when
		  multiple licenses exist in one project and there are
		  conflicts between them. Despite being problematic, fixing
		  license incompatibility issues requires substantial efforts
		  due to the lack of license understanding and complex
		  package dependency. In this paper, we propose LiResolver, a
		  fine-grained, scalable, and flexible tool to resolve
		  license incompatibility issues for open source software.
		  Specifically, it first understands the semantics of
		  licenses through fine-grained entity extraction and
		  relation extraction. Then, it detects and resolves license
		  incompatibility issues by recommending official licenses in
		  priority. When no official licenses can satisfy the
		  constraints, it generates a custom license as an
		  alternative solution. Comprehensive experiments demonstrate
		  the effectiveness of LiResolver, with 4.09% false positive
		  (FP) rate and 0.02% false negative (FN) rate for
		  incompatibility issue localization, and 62.61% of 230
		  real-world incompatible projects resolved by LiResolver. We
		  discuss the feedback from OSS developers and the lessons
		  learned from this work. All the datasets and the
		  replication package of LiResolver have been made publicly
		  available to facilitate follow-up research.},
  booktitle	= {Proceedings of the 32nd ACM SIGSOFT International
		  Symposium on Software Testing and Analysis},
  pages		= {652–663},
  numpages	= {12},
  keywords	= {License, License Incompatibility Resolution, Open Source
		  Software},
  location	= {Seattle, WA, USA},
  series	= {ISSTA 2023}
}

@Proceedings{	  10.1145/3608298,
  title		= {ICMHI '23: Proceedings of the 2023 7th International
		  Conference on Medical and Health Informatics},
  year		= {2023},
  isbn		= {9798400700712},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Kyoto, Japan}
}

@Article{	  10.1109/taslp.2022.3161146,
  author	= {Su, Fangfang and Zhang, Yue and Li, Fei and Ji, Donghong},
  title		= {Balancing Precision and Recall for Neural Biomedical Event
		  Extraction},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3161146},
  doi		= {10.1109/TASLP.2022.3161146},
  abstract	= {Biomedicalevent extraction is an essential task in the
		  biomedical research. Existing models suffer from the issue
		  of low recall due to the large proportion of unrecognized
		  events and inflexible event argument combination. To
		  address this issue, we propose an end-to-end multi-task
		  approach for biomedical event extraction. Our model is able
		  to achieve balanced precision and recall with several
		  nichetargeting designs. First, neural encoders with rich
		  lexical and syntactic features are used and shared by
		  multiple subtasks such as event trigger recognition and
		  argument relation extraction, in order to enhance the
		  generalizability of the model. Second, a novel auxiliary
		  subtask is added to identify the proteins that participate
		  in the events, which helps decreasing the challenge of
		  mining event-related proteins from the large candidate
		  space. Third, event argument combination is performed using
		  a strong neural network rather than inflexible rules or
		  templates, to further increase the recall, especially for
		  complex nested events. To demonstrate the effectiveness of
		  our model, we evaluate it on two widely-used biomedical
		  event extraction datasets used in the BioNLP 2011 and 2013
		  shared tasks. Our model achieves the state-of-the-art
		  results (63.15% and 55.67% in F1 score) by significantly
		  improving the recalls (compared with
		  &lt;italic&gt;DeepEvnetMine&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$_{SciBERT}$&lt;/tex-math&gt;&lt;/inline-formula&gt;&lt;/italic&gt;,
		  4.65% and 5.0%) on the two datasets. Further experiments
		  and analyses show the effectiveness of our proposed
		  features and modules in the model.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {1637–1649},
  numpages	= {13}
}

@InProceedings{	  10.1145/3404835.3462977,
  author	= {Liao, Jinzhi and Zhao, Xiang and Li, Xinyi and Zhang,
		  Lingling and Tang, Jiuyang},
  title		= {Learning Discriminative Neural Representations for Event
		  Detection},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3462977},
  doi		= {10.1145/3404835.3462977},
  abstract	= {Retrieving event instances from texts is pivotal to
		  various natural language processing applications (e.g.,
		  automatic question answering and dialogue systems), and the
		  first task to perform is event detection. There are two
		  related sub-tasks therein-trigger identification and type
		  classification, and the former is considered to play a
		  dominant role. Nevertheless, it is notoriously challenging
		  to predict event triggers right. To handle the task,
		  existing work has made tremendous progress by incorporating
		  manual features, data augmentation and neural networks,
		  etc. Due to the scarcity of data and insufficient
		  representation of trigger words, however, they still fail
		  to precisely determine the spans of triggers (coined as
		  trigger span detection problem). To address the challenge,
		  we propose to learn discriminative neural representations
		  (DNR) from texts. Specifically, our DNR model tackles the
		  trigger span detection problem by exploiting two novel
		  techniques: 1) a contrastive learning strategy, which
		  enlarges the discrepancy between representations of words
		  inside and outside triggers; and 2) a Mixspan strategy,
		  which better trains the model to differentiate words nearby
		  triggers' span boundaries. Extensive experiments on
		  benchmarks-ACE2005 and TAC2015-demonstrate the superiority
		  of our DNR model, leading to state-of-the-art
		  performance.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {644–653},
  numpages	= {10},
  keywords	= {contrastive learning, event detection, mixed
		  representation},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3404835.3462862,
  author	= {Wu, Jiancan and Wang, Xiang and Feng, Fuli and He,
		  Xiangnan and Chen, Liang and Lian, Jianxun and Xie, Xing},
  title		= {Self-supervised Graph Learning for Recommendation},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3462862},
  doi		= {10.1145/3404835.3462862},
  abstract	= {Representation learning on user-item graph for
		  recommendation has evolved from using single ID or
		  interaction history to exploiting higher-order neighbors.
		  This leads to the success of graph convolution networks
		  (GCNs) for recommendation such as PinSage and LightGCN.
		  Despite effectiveness, we argue that they suffer from two
		  limitations: (1) high-degree nodes exert larger impact on
		  the representation learning, deteriorating the
		  recommendations of low-degree (long-tail) items; and (2)
		  representations are vulnerable to noisy interactions, as
		  the neighborhood aggregation scheme further enlarges the
		  impact of observed edges.In this work, we explore
		  self-supervised learning on user-item graph, so as to
		  improve the accuracy and robustness of GCNs for
		  recommendation. The idea is to supplement the classical
		  supervised task of recommendation with an auxiliary
		  self-supervised task, which reinforces node representation
		  learning via self-discrimination. Specifically, we generate
		  multiple views of a node, maximizing the agreement between
		  different views of the same node compared to that of other
		  nodes. We devise three operators to generate the views ---
		  node dropout, edge dropout, and random walk --- that change
		  the graph structure in different manners. We term this new
		  learning paradigm asSelf-supervised Graph Learning (SGL),
		  implementing it on the state-of-the-art model LightGCN.
		  Through theoretical analyses, we find that SGL has the
		  ability of automatically mining hard negatives. Empirical
		  studies on three benchmark datasets demonstrate the
		  effectiveness of SGL, which improves the recommendation
		  accuracy, especially on long-tail items, and the robustness
		  against interaction noises. Our implementations are
		  available at urlhttps://github.com/wujcan/SGL.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {726–735},
  numpages	= {10},
  keywords	= {collaborative filtering, graph neural network, long-tail
		  recommendation, self-supervised learning},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@Article{	  10.1145/3604605,
  author	= {Ibrohim, Muhammad Okky and Bosco, Cristina and Basile,
		  Valerio},
  title		= {Sentiment Analysis for the Natural Environment: A
		  Systematic Review},
  year		= {2023},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3604605},
  doi		= {10.1145/3604605},
  abstract	= {In this systematic review, Kitchenham’s framework is
		  used to explore what tasks, techniques, and benchmarks for
		  Sentiment Analysis have been developed for addressing
		  topics about the natural environment. We comprehensively
		  analyze seven dimensions including contribution, topical
		  focus, data source and query, annotation, language, detail
		  of the task, and technology/algorithm used. By showing how
		  this research area has grown during the last few years, our
		  investigation provides important findings about the results
		  achieved and the challenges that need to be still addressed
		  for making this technology actually helpful for
		  stakeholders such as policymakers and governments.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {88},
  numpages	= {37},
  keywords	= {Natural environment, data-driven policy, sentiment
		  analysis, natural language processing (NLP), systematic
		  review}
}

@InProceedings{	  10.1145/3442381.3450134,
  author	= {Liao, Lizi and Zhu, Tongyao and Long, Le Hong and Chua,
		  Tat Seng},
  title		= {Multi-domain Dialogue State Tracking with Recursive
		  Inference},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3450134},
  doi		= {10.1145/3442381.3450134},
  abstract	= {Multi-domain dialogue state tracking (DST) is a critical
		  component for monitoring user goals during the course of an
		  interaction. Existing approaches have relied on dialogue
		  history indiscriminately or updated on the most recent
		  turns incrementally. However, in spite of modeling it based
		  on fixed ontology or open vocabulary, the former setting
		  violates the interactive and progressing nature of
		  dialogue, while the later easily gets affected by the error
		  accumulation conundrum. Here, we propose a Recursive
		  Inference mechanism (ReInf) to resolve DST in multi-domain
		  scenarios that call for more robust and accurate tracking
		  capability. Specifically, our agent reversely reviews the
		  dialogue history until the agent has pinpointed sufficient
		  turns confidently for slot value prediction. It also
		  recursively factors in potential dependencies among domains
		  and slots to further solve the co-reference and value
		  sharing problems. The quantitative and qualitative
		  experimental results on the MultiWOZ 2.1 corpus demonstrate
		  that the proposed ReInf not only outperforms the
		  state-of-the-art methods, but also achieves reasonable turn
		  reference and interpretable slot co-reference.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {2568–2577},
  numpages	= {10},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Proceedings{	  10.1145/3599589,
  title		= {ICMIP '23: Proceedings of the 2023 8th International
		  Conference on Multimedia and Image Processing},
  year		= {2023},
  isbn		= {9781450399586},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tianjin, China}
}

@Article{	  10.1109/tcbb.2022.3233856,
  author	= {Chen, Peng and Wang, Jian and Lin, Hongfei and Zhang,
		  Yijia and Yang, Zhihao},
  title		= {Knowledge Adaptive Multi-Way Matching Network for
		  Biomedical Named Entity Recognition via Machine Reading
		  Comprehension},
  year		= {2023},
  issue_date	= {May-June 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {3},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2022.3233856},
  doi		= {10.1109/TCBB.2022.3233856},
  abstract	= {Rapid and effective utilization of biomedical literature
		  is paramount to combat diseases like COVID19. Biomedical
		  named entity recognition (BioNER) is a fundamental task in
		  text mining that can help physicians accelerate knowledge
		  discovery to curb the spread of the COVID-19 epidemic.
		  Recent approaches have shown that casting entity extraction
		  as the machine reading comprehension task can significantly
		  improve model performance. However, two major drawbacks
		  impede higher success in identifying entities (1) ignoring
		  the use of domain knowledge to capture the context beyond
		  sentences and (2) lacking the ability to deeper understand
		  the intent of questions. In this paper, to remedy this, we
		  introduce and explore external domain knowledge which
		  cannot be implicitly learned in text sequence. Previous
		  works have focused more on text sequence and explored
		  little of the domain knowledge. To better incorporate
		  domain knowledge, a multi-way matching reader mechanism is
		  devised to model representations of interaction between
		  sequence, question and knowledge retrieved from Unified
		  Medical Language System (UMLS). Benefiting from these, our
		  model can better understand the intent of questions in
		  complex contexts. Experimental results indicate that
		  incorporating domain knowledge can help to obtain
		  competitive results across 10 BioNER datasets, achieving
		  absolute improvement of up to 2.02% in the f1 score.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jan,
  pages		= {2101–2111},
  numpages	= {11}
}

@Article{	  10.1109/taslp.2022.3153256,
  author	= {Zhang, Mi and Qian, Tieyun and Liu, Bing},
  title		= {Exploit Feature and Relation Hierarchy for Relation
		  Extraction},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3153256},
  doi		= {10.1109/TASLP.2022.3153256},
  abstract	= {Existing methods in relation extraction have leveraged the
		  lexical features in the word sequence and the syntactic
		  features in the parse tree. Though effective, the lexical
		  features extracted from the successive word sequence may
		  introduce some noise that has little or no meaningful
		  content. Meanwhile, the syntactic features are usually
		  encoded via graph convolutional networks which have
		  restricted receptive field. In addition, the relation
		  between lexical and syntactic features in the
		  representation space has been largely neglected. To address
		  the above limitations, we propose a multi-scale
		  representation and metric learning framework to exploit the
		  feature and relation hierarchy for RE tasks.
		  Methodologically, we &lt;italic&gt;build a lexical and
		  syntactic feature and relation hierarchy&lt;/italic&gt; in
		  text data. Technically, we first develop &lt;italic&gt;a
		  multi-scale convolutional neural network&lt;/italic&gt; to
		  aggregate the non-successive lexical patterns in the word
		  sequence. We also design &lt;italic&gt;a multi-scale graph
		  convolutional network&lt;/italic&gt; to increase the
		  receptive field via the coarsened syntactic graph.
		  Moreover, we present &lt;italic&gt;a multi-scale metric
		  learning&lt;/italic&gt; paradigm to exploit both the
		  feature-level relation between lexical and syntactic
		  features and the sample-level relation between instances
		  with the same or different classes. Extensive experiments
		  on three public datasets for two RE tasks prove that our
		  model achieves a new state-of-the-art performance.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= feb,
  pages		= {917–930},
  numpages	= {14}
}

@Proceedings{	  10.1145/3573428,
  title		= {EITCE '22: Proceedings of the 2022 6th International
		  Conference on Electronic Information Technology and
		  Computer Engineering},
  year		= {2022},
  isbn		= {9781450397148},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@InProceedings{	  10.1109/icse43902.2021.00040,
  author	= {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang,
		  Meng and Cleland-Huang, Jane},
  title		= {Traceability Transformed: Generating more Accurate Links
		  with Pre-Trained BERT Models},
  year		= {2021},
  isbn		= {9781450390859},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ICSE43902.2021.00040},
  doi		= {10.1109/ICSE43902.2021.00040},
  abstract	= {Software traceability establishes and leverages
		  associations between diverse development artifacts.
		  Researchers have proposed the use of deep learning trace
		  models to link natural language artifacts, such as
		  requirements and issue descriptions, to source code;
		  however, their effectiveness has been restricted by
		  availability of labeled data and efficiency at runtime. In
		  this study, we propose a novel framework called Trace BERT
		  (T-BERT) to generate trace links between source code and
		  natural language artifacts. To address data sparsity, we
		  leverage a three-step training strategy to enable trace
		  models to transfer knowledge from a closely related
		  Software Engineering challenge, which has a rich dataset,
		  to produce trace links with much higher accuracy than has
		  previously been achieved. We then apply the T-BERT
		  framework to recover links between issues and commits in
		  Open Source Projects. We comparatively evaluated accuracy
		  and efficiency of three BERT architectures. Results show
		  that a Single-BERT architecture generated the most accurate
		  links, while a Siamese-BERT architecture produced
		  comparable results with significantly less execution time.
		  Furthermore, by learning and transferring knowledge, all
		  three models in the framework outperform classical IR trace
		  models. On the three evaluated real-word OSS projects, the
		  best T-BERT stably outperformed the VSM model with average
		  improvements of 60.31% measured using Mean Average
		  Precision (MAP). RNN severely underper-formed on these
		  projects due to insufficient training data, while T-BERT
		  overcame this problem by using pretrained language models
		  and transfer learning.},
  booktitle	= {Proceedings of the 43rd International Conference on
		  Software Engineering},
  pages		= {324–335},
  numpages	= {12},
  keywords	= {Software traceability, deep learning, language models},
  location	= {Madrid, Spain},
  series	= {ICSE '21}
}

@Article{	  10.1109/taslp.2023.3240661,
  author	= {Liu, Hong and Cai, Yucheng and Lin, Zhenru and Ou, Zhijian
		  and Huang, Yi and Feng, Junlan},
  title		= {Variational Latent-State GPT for Semi-Supervised
		  Task-Oriented Dialog Systems},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3240661},
  doi		= {10.1109/TASLP.2023.3240661},
  abstract	= {Recently, two approaches, fine-tuning large pre-trained
		  language models and variational training, have attracted
		  significant interests, separately, for semi-supervised
		  end-to-end task-oriented dialog (TOD) systems. In this
		  paper, we propose Variational Latent-State GPT model
		  (VLS-GPT), which is the first to combine the strengths of
		  the two approaches. Among many options of models, we
		  propose the generative model and the inference model for
		  variational learning of the end-to-end TOD system, both as
		  auto-regressive language models based on GPT-2, which can
		  be further trained over a mix of labeled and unlabeled
		  dialog data in a semi-supervised manner. Variational
		  training of VLS-GPT is both statistically and
		  computationally more challenging than previous variational
		  learning works for sequential latent variable models, which
		  use turn-level first-order Markovian. The inference model
		  in VLS-GPT is non-Markovian due to the use of the
		  Transformer architecture. In this work, we establish
		  Recursive Monte Carlo Approximation (RMCA) to the
		  variational objective with non-Markovian inference model
		  and prove its unbiasedness. Further, we develop the
		  computational strategy of sampling-then-forward-computation
		  to realize RMCA, which successfully overcomes the memory
		  explosion issue of using GPT in variational learning and
		  speeds up training. Semi-supervised TOD experiments are
		  conducted on two benchmark multi-domain datasets of
		  different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is
		  shown to significantly outperform both supervised-only and
		  semi-supervised self-training baselines.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jan,
  pages		= {970–984},
  numpages	= {15}
}

@InProceedings{	  10.1145/3459637.3482197,
  author	= {Zhou, Yiwei and Singh, Siffi and Christodoulopoulos,
		  Christos},
  title		= {Tabular Data Concept Type Detection Using
		  Star-Transformers},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482197},
  doi		= {10.1145/3459637.3482197},
  abstract	= {Tabular data is an invaluable information resource for
		  search, in-formation extraction and question answering
		  about the world. It is critical to understand the semantic
		  concept types for table columns in order to fully exploit
		  the information in tabular data. In this paper, we focus on
		  learning-based approaches for column concept type detection
		  without relying on any metadata or queries to existing
		  knowledge bases. We propose a model that employs both
		  statistical and semantic features of table columns, and use
		  Star-Transformers to gather and scatter information across
		  the whole table to boost the performance on individual
		  columns. We apply distant supervision to construct a
		  tabular dataset with columns annotated with DBpedia
		  classes. Our experiment results show that our model
		  achieves 93.57 accuracy on the dataset, exceeding that of
		  the state-of-the-art baselines.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3677–3681},
  numpages	= {5},
  keywords	= {column classification, concept type detection, neural
		  networks, tabular data},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Article{	  10.1613/jair.1.12228,
  author	= {Burkart, Nadia and Huber, Marco F.},
  title		= {A Survey on the Explainability of Supervised Machine
		  Learning},
  year		= {2021},
  issue_date	= {May 2021},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {70},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.12228},
  doi		= {10.1613/jair.1.12228},
  abstract	= {Predictions obtained by, e.g., artificial neural networks
		  have a high accuracy but humans often perceive the models
		  as black boxes. Insights about the decision making are
		  mostly opaque for humans. Particularly understanding the
		  decision making in highly sensitive areas such as
		  healthcare or finance, is of paramount importance. The
		  decision-making behind the black boxes requires it to be
		  more transparent, accountable, and understandable for
		  humans. This survey paper provides essential definitions,
		  an overview of the different principles and methodologies
		  of explainable Supervised Machine Learning (SML). We
		  conduct a state-of-the-art survey that reviews past and
		  recent explainable SML approaches and classifies them
		  according to the introduced definitions. Finally, we
		  illustrate principles by means of an explanatory case study
		  and discuss important future directions.},
  journal	= {J. Artif. Int. Res.},
  month		= may,
  pages		= {245–317},
  numpages	= {73}
}

@InProceedings{	  10.1145/3549737.3549752,
  author	= {Zoupanos, Spyros and Kolovos, Stratis and Kanavos,
		  Athanasios and Papadimitriou, Orestis and Maragoudakis,
		  Manolis},
  title		= {Efficient comparison of sentence embeddings},
  year		= {2022},
  isbn		= {9781450395977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3549737.3549752},
  doi		= {10.1145/3549737.3549752},
  abstract	= {The evolution of natural language processing (NLP) has
		  drastically improved numerous applications in terms of
		  quality of results and speed, like the use of semantic
		  search in modern search engines. NLP has highly benefited
		  from the recent developments in word and sentence
		  embeddings which enable the transformation of complex NLP
		  tasks, such as semantic similarity or Question and
		  Answering (Q&amp;A), into much simpler to perform vector
		  comparisons. However, the new problems resulting from such
		  transformations have also challenging tasks to address like
		  the efficient comparison of embeddings and their
		  manipulation. In this work, we will discuss about various
		  word and sentence embeddings algorithms, we will select a
		  sentence embedding algorithm, BERT, as our algorithm of
		  choice and we will evaluate the performance of two vector
		  comparison approaches, FAISS and Elasticsearch, in the
		  specific problem of sentence embeddings. According to the
		  results, FAISS outperforms Elasticsearch when used in a
		  centralized environment with only one node, especially when
		  big datasets are included.},
  booktitle	= {Proceedings of the 12th Hellenic Conference on Artificial
		  Intelligence},
  articleno	= {11},
  numpages	= {6},
  keywords	= {Elasticsearch, FAISS, sentence embeddings, vector
		  performance comparison},
  location	= {Corfu, Greece},
  series	= {SETN '22}
}

@InProceedings{	  10.1145/3485447.3512074,
  author	= {Manotumruksa, Jarana and Dalton, Jeffrey and Meij, Edgar
		  and Yilmaz, Emine},
  title		= {Similarity-based Multi-Domain Dialogue State Tracking with
		  Copy Mechanisms for Task-based Virtual Personal
		  Assistants},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3512074},
  doi		= {10.1145/3485447.3512074},
  abstract	= {Task-based Virtual Personal Assistants (VPAs) rely on
		  multi-domain Dialogue State Tracking (DST) models to
		  monitor goals throughout a conversation. Previously
		  proposed models show promising results on established
		  benchmarks, but they have difficulty adapting to unseen
		  domains due to domain-specific parameters in their model
		  architectures. We propose a new Similarity-based
		  Multi-domain Dialogue State Tracking model (SM-DST) that
		  uses retrieval-inspired and fine-grained contextual
		  token-level similarity approaches to efficiently and
		  effectively track dialogue state. The key difference with
		  state-of-the-art DST models is that SM-DST has a single
		  model with shared parameters across domains and slots.
		  Because we base SM-DST on similarity it allows the transfer
		  of tracking information between semantically related
		  domains as well as to unseen domains without retraining.
		  Furthermore, we leverage copy mechanisms that consider the
		  system’s response and the dialogue state from previous
		  turn predictions, allowing it to more effectively track
		  dialogue state for complex conversations. We evaluate
		  SM-DST on three variants of the MultiWOZ DST benchmark
		  datasets. The results demonstrate that SM-DST significantly
		  and consistently outperforms state-of-the-art models across
		  all datasets by absolute 5-18% and 3-25% in the few- and
		  zero-shot settings, respectively.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {2006–2014},
  numpages	= {9},
  keywords	= {copy mechanisms, dialogue state tracking, task-based
		  virtual personal assistants},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Proceedings{	  10.1145/3626686,
  title		= {ICDTE '23: Proceedings of the 7th International Conference
		  on Digital Technology in Education},
  year		= {2023},
  isbn		= {9798400708527},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Article{	  10.1145/3603109,
  author	= {Zhang, Jingxuan and Luo, Junpeng and Liang, Jiahui and
		  Gong, Lina and Huang, Zhiqiu},
  title		= {An Accurate Identifier Renaming Prediction and Suggestion
		  Approach},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {32},
  number	= {6},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3603109},
  doi		= {10.1145/3603109},
  abstract	= {Identifiers play an important role in helping developers
		  analyze and comprehend source code. However, many
		  identifiers exist that are inconsistent with the
		  corresponding code conventions or semantic functions,
		  leading to flawed identifiers. Hence, identifiers need to
		  be renamed regularly. Even though researchers have proposed
		  several approaches to identify identifiers that need
		  renaming and further suggest correct identifiers for them,
		  these approaches only focus on a single or a limited number
		  of granularities of identifiers without universally
		  considering all the granularities and suggest a series of
		  sub-tokens for composing identifiers without completely
		  generating new identifiers. In this article, we propose a
		  novel identifier renaming prediction and suggestion
		  approach. Specifically, given a set of training source
		  code, we first extract all the identifiers in multiple
		  granularities. Then, we design and extract five groups of
		  features from identifiers to capture inherent properties of
		  identifiers themselves and the relationships between
		  identifiers and code conventions, as well as other related
		  code entities, enclosing files, and change history. By
		  parsing the change history of identifiers, we can figure
		  out whether specific identifiers have been renamed or not.
		  These identifier features and their renaming history are
		  used to train a Random Forest classifier, which can be
		  further used to predict whether a given new identifier
		  needs to be renamed or not. Subsequently, for the
		  identifiers that need renaming, we extract all the related
		  code entities and their renaming change history. Based on
		  the intuition that identifiers are co-evolved as their
		  relevant code entities with similar patterns and renaming
		  sequences, we could suggest and recommend a series of new
		  identifiers for those identifiers. We conduct extensive
		  experiments to validate our approach in both the Java
		  projects and the Android projects. Experimental results
		  demonstrate that our approach could identify identifiers
		  that need renaming with an average F-measure of more than
		  89%, which outperforms the state-of-the-art approach by
		  8.30% in the Java projects and 21.38% in the Android
		  projects. In addition, our approach achieves a Hit@10 of
		  48.58% and 40.97% in the Java and Android projects in
		  suggesting correct identifiers and outperforms the
		  state-of-the-art approach by 29.62% and 15.75%,
		  respectively.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= sep,
  articleno	= {148},
  numpages	= {51},
  keywords	= {Identifier renaming, source code analysis, code
		  refactoring, mining code repository}
}

@InProceedings{	  10.1145/3487553.3524703,
  author	= {Saeidi, Mozhgan and Milios, Evangelos and Zeh, Norbert},
  title		= {Biomedical Word Sense Disambiguation with Contextualized
		  Representation Learning},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524703},
  doi		= {10.1145/3487553.3524703},
  abstract	= {Representation learning is an important component in
		  solving most Natural Language Processing&nbsp;(NLP)
		  problems, including Word Sense Disambiguation&nbsp;(WSD).
		  The WSD task tries to find the best meaning in a knowledge
		  base for a word with multiple meanings&nbsp;(ambiguous
		  word). WSD methods choose this best meaning based on the
		  context, i.e., the words around the ambiguous word in the
		  input text document. Thus, word representations may improve
		  the effectiveness of the disambiguation models if they
		  carry useful information from the context and the knowledge
		  base. Most of the current representation learning
		  approaches are that they are mostly trained on the general
		  English text and are not domain specified. In this paper,
		  we present a novel contextual-knowledge base aware sense
		  representation method in the biomedical domain. The novelty
		  in our representation is the integration of the knowledge
		  base and the context. This representation lies in a space
		  comparable to that of contextualized word vectors, thus
		  allowing a word occurrence to be easily linked to its
		  meaning by applying a simple nearest neighbor approach.
		  Comparing our approach with state-of-the-art methods shows
		  the effectiveness of our method in terms of text
		  coherence.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {843–848},
  numpages	= {6},
  keywords	= {Biomedical Text, Neural Networks, Representation Learning,
		  Transformers, Word Sense Disambiguation},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Proceedings{	  10.1145/3604951,
  title		= {HIP '23: Proceedings of the 7th International Workshop on
		  Historical Document Imaging and Processing},
  year		= {2023},
  isbn		= {9798400708411},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {San Jose, CA, USA}
}

@Proceedings{	  10.1145/3617023,
  title		= {WebMedia '23: Proceedings of the 29th Brazilian Symposium
		  on Multimedia and the Web},
  year		= {2023},
  isbn		= {9798400709081},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Ribeir\~{a}o Preto, Brazil}
}

@Proceedings{	  10.1145/3587716,
  title		= {ICMLC '23: Proceedings of the 2023 15th International
		  Conference on Machine Learning and Computing},
  year		= {2023},
  isbn		= {9781450398411},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Zhuhai, China}
}

@Article{	  10.1145/3611008,
  author	= {Sun, Yatong and Yang, Xiaochun and Sun, Zhu and Wang,
		  Bin},
  title		= {BERD+: A Generic Sequential Recommendation Framework by
		  Eliminating Unreliable Data with Item- and Attribute-level
		  Signals},
  year		= {2023},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3611008},
  doi		= {10.1145/3611008},
  abstract	= {Most sequential recommendation systems (SRSs) predict the
		  next item as the target for users given its preceding items
		  as input, assuming the target is definitely related to its
		  input. However, users may unintentionally click items that
		  are inconsistent with their preference due to external
		  factors, causing unreliable instances whose target
		  mismatches the input. We, for the first time, verify SRSs
		  can be misguided by such unreliable instances and design a
		  generic SRS framework By Eliminating unReliable Data
		  (BERD+), which can be flexibly plugged into existing SRSs.
		  Specifically, BRED+ is guided with observations on the
		  training process of instances: Unreliable instances
		  generally have high training loss; high-loss instances are
		  not necessarily unreliable but uncertain ones caused by
		  blurry sequential patterns; and item attributes help
		  rectify instance loss and uncertainty, but may also
		  introduce disturbance. Accordingly, BERD+ models both the
		  loss and uncertainty of each instance via a Gaussian
		  distribution, whereby a heterogeneous uncertainty-aware
		  graph convolution network is designed to learn accurate
		  embeddings for different entities while reducing the
		  disturbance caused by uncertain attribute values.
		  Thereafter, an explicit preference extractor rectifies
		  instance loss and uncertainty and reduces the disturbance
		  caused by less-focused attribute types. Finally, instances
		  with high loss and low uncertainty are eliminated as
		  unreliable data. Extensive experiments verify the efficacy
		  of BERD+.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= nov,
  articleno	= {41},
  numpages	= {33},
  keywords	= {Sequential recommender systems, unreliable instances,
		  heterogeneous graph, graph convolution network}
}

@InProceedings{	  10.1145/3468264.3468618,
  author	= {Liu, Mingwei and Peng, Xin and Marcus, Andrian and Treude,
		  Christoph and Bai, Xuefang and Lyu, Gang and Xie, Jiazhan
		  and Zhang, Xiaoxin},
  title		= {Learning-based extraction of first-order logic
		  representations of API directives},
  year		= {2021},
  isbn		= {9781450385626},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3468264.3468618},
  doi		= {10.1145/3468264.3468618},
  abstract	= {Developers often rely on API documentation to learn API
		  directives, i.e., constraints and guidelines related to API
		  usage. Failing to follow API directives may cause defects
		  or improper implementations. Since there are no
		  industry-wide standards on how to document API directives,
		  they take many forms and are often hard to understand by
		  developers or challenging to parse with tools. In this
		  paper, we propose a learning based approach for extracting
		  first-order logic representations of API directives (FOL
		  directives for short). The approach, called LEADFOL, uses a
		  joint learning method to extract atomic formulas by
		  identifying the predicates and arguments involved in
		  directive sentences, and recognizes the logical relations
		  between atomic formulas, by parsing the sentence
		  structures. It then parses the arguments and uses a
		  learning based method to link API references to their
		  corresponding API elements. Finally, it groups the formulas
		  of the same class or method together and transforms them
		  into conjunctive normal form. Our evaluation shows that
		  LEADFOL can accurately extract more FOL directives than a
		  state-of-the-art approach and that the extracted FOL
		  directives are useful in supporting code reviews.},
  booktitle	= {Proceedings of the 29th ACM Joint Meeting on European
		  Software Engineering Conference and Symposium on the
		  Foundations of Software Engineering},
  pages		= {491–502},
  numpages	= {12},
  keywords	= {API Documentation, Directive, First Order Logic},
  location	= {Athens, Greece},
  series	= {ESEC/FSE 2021}
}

@Article{	  10.1109/tcbb.2021.3089195,
  author	= {Sun, Sunny and Chen, Yi-Ping Phoebe},
  title		= {Editorial},
  year		= {2022},
  issue_date	= {Jan.-Feb. 2022},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {19},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2021.3089195},
  doi		= {10.1109/TCBB.2021.3089195},
  abstract	= {Presents the introductory editorial for this issue of the
		  publication.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= feb,
  pages		= {1–2},
  numpages	= {2}
}

@Article{	  10.1145/3564156,
  author	= {Alqahtani, Fatimah and Dohler, Mischa},
  title		= {Survey of Authorship Identification Tasks on Arabic
		  Texts},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3564156},
  doi		= {10.1145/3564156},
  abstract	= {Authorship identification is the process of extracting and
		  analysing the writing styles of authors to identify the
		  authorship. From the writing style, the author and his/her
		  different characteristics can be recognised, which is very
		  useful in digital forensics and cyber investigations. In
		  the literature, authorship identification tasks were
		  addressed on both long and short documents and performed on
		  different languages, such as English, Arabic, Chinese, and
		  Greek. This survey has reviewed the authorship
		  identification tasks for the Arabic language to contribute
		  to this area of research by exploring Arabic language
		  performance and challenges. A total of 27 prominent Arabic
		  studies of each authorship identification domain were
		  reviewed considering the used data, selected features,
		  utilised methods, and results. After a review of the
		  various studies, it was concluded that the results of
		  authorship identification tasks vary based on mostly the
		  selected features and used dataset. Furthermore, the
		  effective features differ from one dataset to another based
		  on the various types of the&nbsp;Arabic language. However,
		  all authorship identification tasks involving the Arabic
		  language face considerable challenges with data
		  pre-processing due to the challenging Arabic concatenative
		  morphology.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {93},
  numpages	= {24},
  keywords	= {Authorship identification, authorship attribution,
		  authorship verification, Arabic texts, stylometry}
}

@Proceedings{	  10.1145/3551349,
  title		= {ASE '22: Proceedings of the 37th IEEE/ACM International
		  Conference on Automated Software Engineering},
  year		= {2022},
  isbn		= {9781450394758},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rochester, MI, USA}
}

@InProceedings{	  10.1145/3585967.3585994,
  author	= {Jiang, Haoyu},
  title		= {Short-Text Semantic Similarity Model of BERT-Based Siamese
		  Network},
  year		= {2023},
  isbn		= {9781450398466},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3585967.3585994},
  doi		= {10.1145/3585967.3585994},
  abstract	= {People convey their emotions and thoughts through words,
		  the medium of human thoughts. Up against the vigorous
		  development of streaming media, the calculation of text
		  similarity is imperative in the field of natural language
		  processing. Any text-related field is inseparable from text
		  semantic similarity. The calculation of text semantic
		  similarity plays a key role in document management,
		  document classification, and document relevance. Besides,
		  popular natural language processing tasks in some trendy
		  fields, such as artificial intelligence, human-machine
		  translation, problem system, intelligent chat system, and
		  nomenclature recognition, are intertwined with text
		  semantic similarity calculation. In recent years, many
		  excellent researchers have studied the algorithms and
		  models of text semantic similarity from different
		  dimensions. In this paper, a new short-text cosine
		  similarity calculation model of the BERT-based Siamese
		  network is proposed.},
  booktitle	= {Proceedings of the 2023 10th International Conference on
		  Wireless Communication and Sensor Networks},
  pages		= {145–149},
  numpages	= {5},
  keywords	= {BERT, Cosine Similarity, Short-Text Semantic Similarity,
		  Siamese Network},
  location	= {Chengdu, China},
  series	= {icWCSN '23}
}

@Proceedings{	  10.1145/3603719,
  title		= {SSDBM '23: Proceedings of the 35th International
		  Conference on Scientific and Statistical Database
		  Management},
  year		= {2023},
  isbn		= {9798400707469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Los Angeles, CA, USA}
}

@InProceedings{	  10.1145/3511808.3557068,
  author	= {Li, Sen and Lv, Fuyu and Jin, Taiwei and Li, Guiyang and
		  Zheng, Yukun and Zhuang, Tao and Liu, Qingwen and Zeng,
		  Xiaoyi and Kwok, James and Ma, Qianli},
  title		= {Query Rewriting in TaoBao Search},
  year		= {2022},
  isbn		= {9781450392365},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511808.3557068},
  doi		= {10.1145/3511808.3557068},
  abstract	= {In e-commerce search engines, query rewriting (QR) is a
		  crucial technique that improves shopping experience by
		  reducing the vocabulary gap between user queries and
		  product catalog. Recent works have mainly adopted the
		  generative paradigm. However, they hardly ensure
		  high-quality generated rewrites and do not consider
		  personalization, which leads to degraded search relevance.
		  In this work, we present Contrastive Learning Enhanced
		  Query Rewriting (CLE-QR), the solution used in Taobao
		  product search. It uses a novel contrastive learning
		  enhanced architecture based on "query retrieval-semantic
		  relevance ranking-online ranking". It finds the rewrites
		  from hundreds of millions of historical queries while
		  considering relevance and personalization. Specifically, we
		  first alleviate the representation degeneration problem
		  during the query retrieval stage by using an unsupervised
		  contrastive loss, and then further propose an
		  interaction-aware matching method to find the beneficial
		  and incremental candidates, thus improving the quality and
		  relevance of candidate queries. We then present a
		  relevance-oriented contrastive pre-training paradigm on the
		  noisy user feedback data to improve semantic ranking
		  performance. Finally, we rank these candidates online with
		  the user profile to model personalization for the retrieval
		  of more relevant products. We evaluate CLE-QR on Taobao
		  Product Search, one of the largest e-commerce platforms in
		  China. Significant metrics gains are observed in online A/B
		  tests. CLE-QR has been deployed to our large-scale
		  commercial retrieval system and serviced hundreds of
		  millions of users since December 2021. We also introduce
		  its online deployment scheme, and share practical lessons
		  and optimization tricks of our lexical match system.},
  booktitle	= {Proceedings of the 31st ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3262–3271},
  numpages	= {10},
  keywords	= {e-commerce search, lexical match, query rewriting},
  location	= {Atlanta, GA, USA},
  series	= {CIKM '22}
}

@Proceedings{	  10.1145/3597512,
  title		= {TAS '23: Proceedings of the First International Symposium
		  on Trustworthy Autonomous Systems},
  year		= {2023},
  isbn		= {9798400707346},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Edinburgh, United Kingdom}
}

@Article{	  10.1145/3462207,
  author	= {Xu, Ruijian and Tao, Chongyang and Feng, Jiazhan and Wu,
		  Wei and Yan, Rui and Zhao, Dongyan},
  title		= {Response Ranking with Multi-types of Deep Interactive
		  Representations in Retrieval-based Dialogues},
  year		= {2021},
  issue_date	= {October 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {39},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3462207},
  doi		= {10.1145/3462207},
  abstract	= {Building an intelligent dialogue system with the ability
		  to select a proper response according to a multi-turn
		  context is challenging in three aspects: (1) the meaning of
		  a context–response pair is built upon language units from
		  multiple granularities (e.g., words, phrases, and
		  sub-sentences, etc.); (2) local (e.g., a small window
		  around a word) and long-range (e.g., words across the
		  context and the response) dependencies may exist in
		  dialogue data; and (3) the relationship between the context
		  and the response candidate lies in multiple relevant
		  semantic clues or relatively implicit semantic clues in
		  some real cases. However, existing approaches usually
		  encode the dialogue with mono-type representation and the
		  interaction processes between the context and the response
		  candidate are executed in a rather shallow manner, which
		  may lead to an inadequate understanding of dialogue content
		  and hinder the recognition of the semantic relevance
		  between the context and response. To tackle these
		  challenges, we propose a
		  representation[K]-interaction[L]-matching framework that
		  explores multiple types of deep interactive representations
		  to build context-response matching models for response
		  selection. Particularly, we construct different types of
		  representations for utterance–response pairs and deepen
		  them via alternate encoding and interaction. By this means,
		  the model can handle the relation of neighboring elements,
		  phrasal pattern, and long-range dependencies during the
		  representation and make a more accurate prediction through
		  multiple layers of interactions between the
		  context–response pair. Experiment results on three public
		  benchmarks indicate that the proposed model significantly
		  outperforms previous conventional context-response matching
		  models and achieve slightly better results than the BERT
		  model for multi-turn response selection in retrieval-based
		  dialogue systems.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= aug,
  articleno	= {44},
  numpages	= {28},
  keywords	= {Retrieval-based dialogue systems, response selection,
		  context-response matching}
}

@Article{	  10.1145/3617371,
  author	= {Banerjee, Anasua and Kumar, Vinay and Shankar, Achyut and
		  Jhaveri, Rutvij H. and Banik, Debajyoty},
  title		= {Automatic Resource Augmentation for Machine Translation in
		  Low Resource Language: EnIndic Corpus},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3617371},
  doi		= {10.1145/3617371},
  abstract	= {Parallel corpus is the primary ingredient of machine
		  translation. It is required to train the statistical
		  machine translation (SMT) and neural machine translation
		  (NMT) systems. There is a lack of good quality parallel
		  corpus for Hindi to English. Comparable corpora for a given
		  language pair are comparatively easy to find, but this
		  cannot be used directly in SMT or NMT systems. As a result,
		  we generate a parallel corpus from the comparable corpus.
		  For this purpose, the sentences (which are translations of
		  each other) are mined from the comparable corpus to prepare
		  the parallel corpus. The proposed algorithm uses the length
		  of the sentence and word translation model to align
		  sentence pairs that are translations of each other. Then,
		  the sentence pairs that are poor translations of each other
		  (measured by a similarity score based on IBM model 1
		  translation probability) are filtered out. We apply this
		  algorithm to comparable corpora, which are crawled from
		  speeches of the President and Vice-President of India, and
		  mined parallel corpora out of them. The prepared parallel
		  corpus contains good quality aligned sentences (with
		  96.338% f-score). Subsequently, incorrect sentence pairs
		  are filtered out manually to make the corpus in qualitative
		  practical use. Finally, we gather various sentences from
		  different sources to prepare the EnIndic corpus, which
		  comprises 1,656,207 English-Hindi sentence pairs
		  (miscellaneous domain). We have deployed this prepared
		  largest English-Hindi parallel corpus at
		  https://github.com/debajyoty/EnIndic.git and the source
		  code at
		  https://github.com/debajyoty/EnIndicSourceCode.git.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  keywords	= {Parallel Corpus, Comparable Corpus, Machine Translation,
		  Linguistic Resources and Natural Language Processing}
}

@Proceedings{	  10.1145/3572549,
  title		= {ICETC '22: Proceedings of the 14th International
		  Conference on Education Technology and Computers},
  year		= {2022},
  isbn		= {9781450397766},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Barcelona, Spain}
}

@InProceedings{	  10.1145/3584371.3612998,
  author	= {Theodorou, Brandon Philip and Xiao, Cao and Sun, Jimeng},
  title		= {TREEMENT: Interpretable Patient-Trial Matching via
		  Personalized Dynamic Tree-based Memory Network},
  year		= {2023},
  isbn		= {9798400701269},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3584371.3612998},
  doi		= {10.1145/3584371.3612998},
  abstract	= {Clinical trials are critical for drug development but
		  often suffer from expensive and inefficient patient
		  recruitment. In recent years, machine learning models have
		  been proposed for speeding up patient recruitment via
		  automatically matching patients with clinical trials based
		  on longitudinal patient electronic health records (EHRs)
		  and eligibility criteria of trials. However, they either
		  depend on trial-specific expert rules that cannot be
		  generalized or perform matching more generally with a
		  black-box model where the lack of interpretability makes
		  the model results difficult to be adopted.To provide
		  accurate and interpretable patient trial matching, we
		  introduce a personalized dynamic tree-based memory network,
		  TREEMENT. It utilizes hierarchical clinical ontologies to
		  expand the personalized patient representation learned from
		  sequential EHR data, and then uses an attentional
		  beam-search query learned from eligibility criteria
		  embedding to offer a granular level of alignment for
		  improved performance and interpretability. We evaluate
		  TREEMENT against existing models on real-world datasets and
		  show that TREEMENT outperforms the top baseline by 7% in
		  terms of error reduction in criteria-level matching and
		  achieves state-of-the-art results at the trial-level too.
		  Furthermore, we show TREEMENT offers good interpretability
		  to make the model results easier for adoption.},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Bioinformatics, Computational Biology, and Health
		  Informatics},
  articleno	= {55},
  numpages	= {9},
  location	= {Houston, TX, USA},
  series	= {BCB '23}
}

@InProceedings{	  10.1109/ase51524.2021.9678670,
  author	= {Chen, Songqiang and Jin, Shuo and Xie, Xiaoyuan},
  title		= {Testing your question answering software via asking
		  recursively},
  year		= {2022},
  isbn		= {9781665403375},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE51524.2021.9678670},
  doi		= {10.1109/ASE51524.2021.9678670},
  abstract	= {Question Answering (QA) is an attractive and challenging
		  area in NLP community. There are diverse algorithms being
		  proposed and various benchmark datasets with different
		  topics and task formats being constructed. QA software has
		  also been widely used in daily human life now. However,
		  current QA software is mainly tested in a reference-based
		  paradigm, in which the expected outputs (labels) of test
		  cases need to be annotated with much human effort before
		  testing. As a result, neither the just-in-time test during
		  usage nor the extensible test on massive unlabeled
		  real-life data is feasible, which keeps the current testing
		  of QA software from being flexible and sufficient. In this
		  paper, we propose a method, QAAskeR, with three novel
		  Metamorphic Relations for testing QA software. qaAskeR does
		  not require the annotated labels but tests QA software by
		  checking its behaviors on multiple recursively asked
		  questions that are related to the same knowledge.
		  Experimental results show that qaAskeR can reveal
		  violations at over 80% of valid cases without using any
		  pre-annotated labels. Diverse answering issues, especially
		  the limited generalization on question types across
		  datasets, are revealed on a state-of-the-art QA
		  algorithm.},
  booktitle	= {Proceedings of the 36th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {104–116},
  numpages	= {13},
  keywords	= {natural language processing, question answering, recursive
		  metamorphic testing, testing and validation},
  location	= {Melbourne, Australia},
  series	= {ASE '21}
}

@Article{	  10.1109/taslp.2023.3302232,
  author	= {Lim, Jungwoo and Whang, Taesun and Lee, Dongyub and Lim,
		  Heuiseok},
  title		= {Adaptive Multi-Domain Dialogue State Tracking on Spoken
		  Conversations},
  year		= {2023},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3302232},
  doi		= {10.1109/TASLP.2023.3302232},
  abstract	= {The main objective of the task-oriented dialogue system is
		  to identify the intent and needs of human dialogue. Many
		  existing studies are conducted under the setting of written
		  dialogue, but there always exists a difficulty in coping
		  with real-world spoken dialogues. To this end, DSTC10
		  challenge organizers propose the task of building robust
		  dialogue state tracking (DST) models on spoken dialogues.
		  With the powerful existing DST model (i.e., MinTL), this
		  article suggests integral components for building a
		  dialogue state tracker; 1) Data augmentation effectively
		  enhances the capability of the model to catch the entities
		  that exist in the evaluation dataset. 2) Levenshtein
		  post-processing aims to prevent the distortion in model
		  prediction caused by automatic speech recognition errors.
		  To validate the effectiveness of our methods, we evaluate
		  our model on DSTC10 datasets and conduct qualitative
		  analysis by ablating each component of the model.
		  Experimental results show that our model significantly
		  outperforms baselines in all evaluation metrics and took
		  3rd place in the challenge.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= aug,
  pages		= {727–732},
  numpages	= {6}
}

@InProceedings{	  10.5555/3507788.3507811,
  author	= {Kici, Derya and Bozanta, Aysun and Cevik, Mucahit and
		  Parikh, Devang and Ba\c{s}ar, Ay\c{s}e},
  title		= {Text classification on software requirements
		  specifications using transformer models},
  year		= {2021},
  publisher	= {IBM Corp.},
  address	= {USA},
  abstract	= {Text classification in Software Requirements
		  Specifications (SRS) documents is an essential task for
		  various purposes including automatically extracting
		  requirements and their types as well as identification of
		  duplicate or conflicting information, which all contribute
		  to avoiding potential issues in the later stages of the
		  software development life cycle. While a variety of machine
		  learning approaches have been considered for text
		  classification over SRS documents, many of these fail to
		  provide adequate performance as they often ignore the
		  meaning of software artifacts or integrate domain knowledge
		  for the classification task. Recent advances in deep
		  learning methodology have significantly contributed to
		  Natural Language Processing (NLP) and text classification.
		  One of the main challenges in using deep learning models
		  for various NLP tasks in the software engineering domain is
		  the scarcity of labeled textual data. In addition, even
		  with sufficient data, training from the scratch still
		  requires significant training time and computational
		  resources. Transfer learning is a novel approach that
		  proposes a solution to such reservations by providing
		  pre-trained models that enable fine-tuning with the
		  customized data. In this research, we conduct an empirical
		  analysis on multi-class text classification over SRS
		  documents using different pre-trained transformer models
		  including BERT, DistilBERT, Roberta, AlBERT, and XLNet, and
		  compare their performance. We test the performance of these
		  models using three SRS datasets: DOORS, NFR-PROMISE, and
		  PURE. Our numerical study shows that the transformer models
		  are able to generate highly accurate results to classify
		  all categories except Priority of the requirements. While
		  all models provide a 80% or higher accuracy for other
		  classification tasks, the accuracy of the models to
		  classify the Priority does not exceed 60%.},
  booktitle	= {Proceedings of the 31st Annual International Conference on
		  Computer Science and Software Engineering},
  pages		= {163–172},
  numpages	= {10},
  keywords	= {BERT, NLP, software requirement specifications, text
		  classification, transfer learning},
  location	= {Toronto, Canada},
  series	= {CASCON '21}
}

@InProceedings{	  10.1145/3482632.3482752,
  author	= {Chai, Jinlian},
  title		= {Design of English Translation Computer Intelligent
		  Proofreading System Based on Fuzzy Decision Algorithm},
  year		= {2021},
  isbn		= {9781450390255},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3482632.3482752},
  doi		= {10.1145/3482632.3482752},
  booktitle	= {2021 4th International Conference on Information Systems
		  and Computer Aided Education},
  pages		= {568–571},
  numpages	= {4},
  location	= {Dalian, China},
  series	= {ICISCAE 2021}
}

@Proceedings{	  10.1145/3605390,
  title		= {CHItaly '23: Proceedings of the 15th Biannual Conference
		  of the Italian SIGCHI Chapter},
  year		= {2023},
  isbn		= {9798400708060},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Torino, Italy}
}

@Article{	  10.1145/3468889,
  author	= {Zhang, Ruqing and Guo, Jiafeng and Chen, Lu and Fan,
		  Yixing and Cheng, Xueqi},
  title		= {A Review on Question Generation from Natural Language
		  Text},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3468889},
  doi		= {10.1145/3468889},
  abstract	= {Question generation is an important yet challenging
		  problem in Artificial Intelligence (AI), which aims to
		  generate natural and relevant questions from various input
		  formats, e.g., natural language text, structure database,
		  knowledge base, and image. In this article, we focus on
		  question generation from natural language text, which has
		  received tremendous interest in recent years due to the
		  widespread applications such as data augmentation for
		  question answering systems. During the past decades, many
		  different question generation models have been proposed,
		  from traditional rule-based methods to advanced neural
		  network-based methods. Since there have been a large
		  variety of research works proposed, we believe it is the
		  right time to summarize the current status, learn from
		  existing methodologies, and gain some insights for future
		  development. In contrast to existing reviews, in this
		  survey, we try to provide a more comprehensive taxonomy of
		  question generation tasks from three different
		  perspectives, i.e., the types of the input context text,
		  the target answer, and the generated question. We take a
		  deep look into existing models from different dimensions to
		  analyze their underlying ideas, major design principles,
		  and training strategies We compare these models through
		  benchmark tasks to obtain an empirical understanding of the
		  existing techniques. Moreover, we discuss what is missing
		  in the current literature and what are the promising and
		  desired future directions.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= sep,
  articleno	= {14},
  numpages	= {43},
  keywords	= {Question generation, natural language generation, survey}
}

@InProceedings{	  10.1145/3575879.3575985,
  author	= {Kostis, Ioannis Aris and Sarafis, Dimitrios and
		  Karamitsios, Konstantinos and Kotrotsios, Konstantinos and
		  Kravari, Kalliopi and Badica, Costin and Chatzimisios,
		  Periklis},
  title		= {Towards an Integrated Retrieval System to Semantically
		  Match CVs, Job Descriptions and Curricula},
  year		= {2023},
  isbn		= {9781450398541},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3575879.3575985},
  doi		= {10.1145/3575879.3575985},
  abstract	= {The job market is continuously evolving. The specific
		  occupations, skills, competences and qualifications that
		  people need change over time, as does their description. To
		  deal with this, effective and intelligent communication and
		  information exchange between the job market and the
		  education and training sector is vital. On the other hand,
		  and from the perspective of the individual (job seeker),
		  especially the less privileged there is a need for
		  approaches that combine practical tools with motivation and
		  mentoring support since skill-matching it is not enough,
		  skill-building is also needed. In this context, the current
		  approach follows a bottom-up methodology investigating the
		  problem of formalizing the lifelong learning process in a
		  dynamic and flexible way. On the other hand, this proposal
		  utilizes a parallel top-down approach in applying semantics
		  and standards upon data in order to alleviate the gap among
		  individuals, workplaces and educational contexts for the
		  benefit of all in a transparent way. More specifically,
		  this article reports towards an approach on tackling the
		  complex task of interconnecting job seekers, employers and
		  educational agents in the current European labor market. To
		  perform this task, we implement an end-to-end service to
		  parse resumes, job descriptions and open courses
		  descriptions, retrieve information on the qualifications
		  associated with the aforementioned, and semantically match
		  them. The proposed implementation effectively detects the
		  underlying information associated with those sources, and
		  manages to interlink job seekers’ resumes to occupations
		  and job vacancies, while being able to assign skill
		  deficits to courses provided by educational agents. The
		  performance of our implementation on CVs, job descriptions
		  and course descriptions in English, Greek, Romanian and
		  Bulgarian, indicate that our approach yields results on par
		  with the state-of-the-art, however on a much larger scale:
		  to the best of our knowledge, this is the first research
		  work that engages with this task on three stakeholders (job
		  seekers, employers, educational agents) and in four
		  European languages.},
  booktitle	= {Proceedings of the 26th Pan-Hellenic Conference on
		  Informatics},
  pages		= {151–157},
  numpages	= {7},
  keywords	= {ESCO, Information Retrieval, Linked Data, Semantic Web},
  location	= {Athens, Greece},
  series	= {PCI '22}
}

@Article{	  10.1145/3445965,
  author	= {Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad
		  Kamran},
  title		= {Named Entity Recognition and Relation Extraction:
		  State-of-the-Art},
  year		= {2021},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3445965},
  doi		= {10.1145/3445965},
  abstract	= {With the advent of Web 2.0, there exist many online
		  platforms that result in massive textual-data production.
		  With ever-increasing textual data at hand, it is of immense
		  importance to extract information nuggets from this data.
		  One approach towards effective harnessing of this
		  unstructured textual data could be its transformation into
		  structured text. Hence, this study aims to present an
		  overview of approaches that can be applied to extract key
		  insights from textual data in a structured way. For this,
		  Named Entity Recognition and Relation Extraction are being
		  majorly addressed in this review study. The former deals
		  with identification of named entities, and the latter deals
		  with problem of extracting relation between set of
		  entities. This study covers early approaches as well as the
		  developments made up till now using machine learning
		  models. Survey findings conclude that deep-learning-based
		  hybrid and joint models are currently governing the
		  state-of-the-art. It is also observed that annotated
		  benchmark datasets for various textual-data generators such
		  as Twitter and other social forums are not available. This
		  scarcity of dataset has resulted into relatively less
		  progress in these domains. Additionally, the majority of
		  the state-of-the-art techniques are offline and
		  computationally expensive. Last, with increasing focus on
		  deep-learning frameworks, there is need to understand and
		  explain the under-going processes in deep architectures.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {20},
  numpages	= {39},
  keywords	= {Information extraction, deep learning, joint modeling,
		  named entity recognition, relation extraction}
}

@Article{	  10.1145/3453185,
  author	= {Tan, Minghuan and Jiang, Jing and Dai, Bing Tian},
  title		= {A BERT-Based Two-Stage Model for Chinese Chengyu
		  Recommendation},
  year		= {2021},
  issue_date	= {November 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3453185},
  doi		= {10.1145/3453185},
  abstract	= {In Chinese, Chengyu are fixed phrases consisting of four
		  characters. As a type of idioms, their meanings usually
		  cannot be derived from their component characters. In this
		  article, we study the task of recommending a Chengyu given
		  a textual context. Observing some of the limitations with
		  existing work, we propose a two-stage model, where during
		  the first stage we re-train a Chinese BERT model by masking
		  out Chengyu from a large Chinese corpus with a wide
		  coverage of Chengyu. During the second stage, we fine-tune
		  the re-trained, Chengyu-oriented BERT on a specific Chengyu
		  recommendation dataset. We evaluate this method on ChID and
		  CCT datasets and find that it can achieve the state of the
		  art on both datasets. Ablation studies show that both
		  stages of training are critical for the performance gain.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  articleno	= {92},
  numpages	= {18},
  keywords	= {Question answering, Chengyu recommendation, idiom
		  understanding}
}

@InProceedings{	  10.1145/3586183.3606806,
  author	= {Setlur, Vidya and Kanyuka, Andriy and Srinivasan, Arjun},
  title		= {Olio: A Semantic Search Interface for Data Repositories},
  year		= {2023},
  isbn		= {9798400701320},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3586183.3606806},
  doi		= {10.1145/3586183.3606806},
  abstract	= {Search and information retrieval systems are becoming more
		  expressive in interpreting user queries beyond the
		  traditional weighted bag-of-words model of document
		  retrieval. For example, searching for a flight status or a
		  game score returns a dynamically generated response along
		  with supporting, pre-authored documents contextually
		  relevant to the query. In this paper, we extend this hybrid
		  search paradigm to data repositories that contain curated
		  data sources and visualization content. We introduce a
		  semantic search interface, Olio, that provides a hybrid set
		  of results comprising both auto-generated visualization
		  responses and pre-authored charts to blend analytical
		  question-answering with content discovery search goals. We
		  specifically explore three search scenarios -
		  question-and-answering, exploratory search, and design
		  search over data repositories. The interface also provides
		  faceted search support for users to refine and filter the
		  conventional best-first search results based on parameters
		  such as author name, time, and chart type. A preliminary
		  user evaluation of the system demonstrates that Olio’s
		  interface and the hybrid search paradigm collectively
		  afford greater expressivity in how users discover insights
		  and visualization content in data repositories.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {95},
  numpages	= {16},
  keywords	= {Hybrid search, curated data sources., design search,
		  dynamic and static content, exploratory search, federated
		  querying, question and answering, visualizations},
  location	= {San Francisco, CA, USA},
  series	= {UIST '23}
}

@InProceedings{	  10.1145/3487553.3524935,
  author	= {Zhang, Dongyu and Zhang, Minghao and Peng, Ciyuan and Xia,
		  Feng},
  title		= {Expressing Metaphorically, Writing Creatively: Metaphor
		  Identification for Creativity Assessment in Writing},
  year		= {2022},
  isbn		= {9781450391306},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487553.3524935},
  doi		= {10.1145/3487553.3524935},
  abstract	= {Metaphor, which can implicitly express profound meanings
		  and emotions, is a unique writing technique frequently used
		  in human language. In writing, meaningful metaphorical
		  expressions can enhance the literariness and creativity of
		  texts. Therefore, the usage of metaphor is a significant
		  impact factor when assessing the creativity and
		  literariness of writing. However, little to no automatic
		  writing assessment system considers metaphorical
		  expressions when giving the score of creativity. For
		  improving the accuracy of automatic writing assessment,
		  this paper proposes a novel creativity assessment model
		  that imports a token-level metaphor identification method
		  to extract metaphors as the indicators for creativity
		  scoring. The experimental results show that our model can
		  accurately assess the creativity of different texts with
		  precise metaphor identification. To the best of our
		  knowledge, we are the first to apply automatic metaphor
		  identification to assess writing creativity. Moreover,
		  identifying features (e.g., metaphors) that influence
		  writing creativity using computational approaches can offer
		  fair and reliable assessment methods for educational
		  settings.},
  booktitle	= {Companion Proceedings of the Web Conference 2022},
  pages		= {1198–1205},
  numpages	= {8},
  keywords	= {Metaphor identification, Metaphorical feature analytics.,
		  Textual data mining, Writing analytics, Writing creativity
		  assessment},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Proceedings{	  10.1145/3628797,
  title		= {SOICT '23: Proceedings of the 12th International Symposium
		  on Information and Communication Technology},
  year		= {2023},
  isbn		= {9798400708916},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Ho Chi Minh, Vietnam}
}

@Proceedings{	  10.1145/3543434,
  title		= {dg.o '22: Proceedings of the 23rd Annual International
		  Conference on Digital Government Research},
  year		= {2022},
  isbn		= {9781450397490},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, Republic of Korea}
}

@InProceedings{	  10.1145/3617695.3617702,
  author	= {Le, Quang Ba Minh and Vo, Chau Thi Ngoc},
  title		= {Patient Information Retrieval Based on BERT Variants and
		  Clinical Texts in Electronic Medical Records},
  year		= {2023},
  isbn		= {9798400708015},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3617695.3617702},
  doi		= {10.1145/3617695.3617702},
  abstract	= {Information retrieval is a task related to search a
		  database for the most relevant similar objects to a given
		  query object. In medicine, patient information retrieval is
		  important to get the patients that are the most similar to
		  a patient being considered. The resulting data can be
		  further used for physicians to produce adaptive treatment
		  plans as well as for other applications such as disease
		  classification, re-admission prediction, and stay-length
		  prediction. Due to its significance, several traditional
		  information retrieval approaches were applied on medical
		  databases. However, there is still a growing need for an
		  effective solution to patient information retrieval in the
		  context where more and more electronic medical records and
		  their clinical texts are captured. In this paper, we focus
		  on this task and propose to perform local learning on the
		  BERT-based embeddings from clinical texts of patients to
		  achieve an effective solution. The advanced properties of
		  BERT variants help better represent each patient using the
		  clinical texts instead of other data types like medication
		  codes and demographic data. The experimental results on
		  MIMIC III database have confirmed the effectiveness of our
		  proposed solution. Above all, the better differences
		  between our solution and the others in F-measure are
		  statistically significant in all the cases.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Big Data and Internet of Things},
  pages		= {188–194},
  numpages	= {7},
  location	= {Beijing, China},
  series	= {BDIOT '23}
}

@Article{	  10.1145/3569576,
  author	= {Abdelrahman, Ghodai and Wang, Qing and Nunes, Bernardo},
  title		= {Knowledge Tracing: A Survey},
  year		= {2023},
  issue_date	= {November 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {11},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3569576},
  doi		= {10.1145/3569576},
  abstract	= {Humans’ ability to transfer knowledge through teaching
		  is one of the essential aspects for human intelligence. A
		  human teacher can track the knowledge of students to
		  customize the teaching on students’ needs. With the rise
		  of online education platforms, there is a similar need for
		  machines to track the knowledge of students and tailor
		  their learning experience. This is known as the Knowledge
		  Tracing (KT) problem in the literature. Effectively solving
		  the KT problem would unlock the potential of computer-aided
		  education applications such as intelligent tutoring
		  systems, curriculum learning, and learning materials’
		  recommendation. Moreover, from a more general viewpoint, a
		  student may represent any kind of intelligent agents
		  including both human and artificial agents. Thus, the
		  potential of KT can be extended to any machine teaching
		  application scenarios which seek for customizing the
		  learning experience for a student agent (i.e., a machine
		  learning model). In this paper, we provide a comprehensive
		  survey for the KT literature. We cover a broad range of
		  methods starting from the early attempts to the recent
		  state-of-the-art methods using deep learning, while
		  highlighting the theoretical aspects of models and the
		  characteristics of benchmark datasets. Besides these, we
		  shed light on key modelling differences between closely
		  related methods and summarize them in an easy-to-understand
		  format. Finally, we discuss current research gaps in the KT
		  literature and possible future research and application
		  directions.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {224},
  numpages	= {37},
  keywords	= {Knowledge tracing, memory networks, deep learning,
		  sequence modelling, key-value memory, Bayesian knowledge
		  tracing (BKT), intelligent education, factor analysis,
		  survey}
}

@InProceedings{	  10.1145/3583780.3615109,
  author	= {Wang, Tianle and Wang, Zihan and Liu, Weitang and Shang,
		  Jingbo},
  title		= {WOT-Class: Weakly Supervised Open-world Text
		  Classification},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3615109},
  doi		= {10.1145/3583780.3615109},
  abstract	= {State-of-the-art weakly supervised text classification
		  methods, while significantly reduced the required human
		  supervision, still requires the supervision to cover all
		  the classes of interest. This is never easy to meet in
		  practice when human explore new, large corpora without
		  complete pictures. In this paper, we work on a novel yet
		  important problem of weakly supervised open-world text
		  classification, where supervision is only needed for a few
		  examples from a few known classes and the machine should
		  handle both known and unknown classes in test time. General
		  open-world classification has been studied mostly using
		  image classification; however, existing methods typically
		  assume the availability of sufficient known-class
		  supervision and strong unknown-class prior knowledge (e.g.,
		  the number and/or data distribution). We propose a novel
		  framework \o{}ur that lifts those strong assumptions.
		  Specifically, it follows an iterative process of (a)
		  clustering text to new classes, (b) mining and ranking
		  indicative words for each class, and (c) merging redundant
		  classes by using the overlapped indicative words as a
		  bridge. Extensive experiments on 7 popular text
		  classification datasets demonstrate that \o{}ur outperforms
		  strong baselines consistently with a large margin,
		  attaining 23.33% greater average absolute macro-F1 over
		  existing approaches across all datasets. Such competent
		  accuracy illuminates the practical potential of further
		  reducing human effort for text classification.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2666–2675},
  numpages	= {10},
  keywords	= {open-world learning, text classification, weak
		  supervision},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@InProceedings{	  10.1145/3617233.3617242,
  author	= {Neptune, Nathalie and Mothe, Josiane},
  title		= {Annotating Satellite Images of Forests with Keywords from
		  a Specialized Corpus in the Context of Change Detection},
  year		= {2023},
  isbn		= {9798400709128},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3617233.3617242},
  doi		= {10.1145/3617233.3617242},
  abstract	= {The Amazon rain forest is a vital ecosystem that plays a
		  crucial role in regulating the Earth’s climate and
		  providing habitat for countless species. Deforestation in
		  the Amazon is a major concern as it has a significant
		  impact on global carbon emissions and biodiversity. In this
		  paper, we present a method for detecting deforestation in
		  the Amazon using image pairs from Earth observation
		  satellites. Our method leverages deep learning techniques
		  to compare the images of the same area at different dates
		  and identify changes in the forest cover. We also propose a
		  visual semantic model that automatically annotates the
		  detected changes with relevant keywords. The candidate
		  annotation for images are extracted from scientific
		  documents related to the Amazon region. We evaluate our
		  approach on a dataset of Amazon image pairs and demonstrate
		  its effectiveness in detecting deforestation and generating
		  relevant annotations. Our method provides a useful tool for
		  monitoring and studying the impact of deforestation in the
		  Amazon. While we focus on environment applications of our
		  work by using images of deforestation in the Amazon rain
		  forest to demonstrate the effectiveness of our proposed
		  approach, it is generic enough to be applied to other
		  domains.},
  booktitle	= {Proceedings of the 20th International Conference on
		  Content-Based Multimedia Indexing},
  pages		= {14–20},
  numpages	= {7},
  keywords	= {Image annotation, change detection, deforestation
		  detection.},
  location	= {Orleans, France},
  series	= {CBMI '23}
}

@InProceedings{	  10.1145/3534678.3539449,
  author	= {Shen, Wei and Yang, Yang and Liu, Yinan},
  title		= {Multi-View Clustering for Open Knowledge Base
		  Canonicalization},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539449},
  doi		= {10.1145/3534678.3539449},
  abstract	= {Open information extraction (OIE) methods extract plenty
		  of OIE triples &lt;noun phrase, relation phrase, noun
		  phrase&gt; from unstructured text, which compose large open
		  knowledge bases (OKBs). Noun phrases and relation phrases
		  in such OKBs are not canonicalized, which leads to
		  scattered and redundant facts. It is found that two views
		  of knowledge (i.e., a fact view based on the fact triple
		  and a context view based on the fact triple's source
		  context) provide complementary information that is vital to
		  the task of OKB canonicalization, which clusters synonymous
		  noun phrases and relation phrases into the same group and
		  assigns them unique identifiers. However, these two views
		  of knowledge have so far been leveraged in isolation by
		  existing works. In this paper, we propose CMVC, a novel
		  unsupervised framework that leverages these two views of
		  knowledge jointly for canonicalizing OKBs without the need
		  of manually annotated labels. To achieve this goal, we pro-
		  pose a multi-view CH K-Means clustering algorithm to
		  mutually reinforce the clustering of view-specific
		  embeddings learned from each view by considering their
		  different clustering qualities. In order to further enhance
		  the canonicalization performance, we propose a training
		  data optimization strategy in terms of data quantity and
		  data quality respectively in each particular view to refine
		  the learned view-specific embeddings in an iterative
		  manner. Additionally, we propose a Log-Jump algorithm to
		  predict the optimal number of clusters in a data-driven way
		  without requiring any labels. We demonstrate the
		  superiority of our framework through extensive experiments
		  on multiple real-world OKB data sets against
		  state-of-the-art methods.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1578–1588},
  numpages	= {11},
  keywords	= {multi-view clustering, open knowledge base
		  canonicalization, training data optimization},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@Article{	  10.1145/3569927,
  author	= {Broy, Manfred and Rumpe, Bernhard},
  title		= {Development Use Cases for Semantics-Driven Modeling
		  Languages},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {66},
  number	= {5},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3569927},
  doi		= {10.1145/3569927},
  abstract	= {Choosing underlying semantic theories and definition
		  techniques must closely follow intended use cases for the
		  modeling language.},
  journal	= {Commun. ACM},
  month		= apr,
  pages		= {62–71},
  numpages	= {10}
}

@InProceedings{	  10.1145/3476887.3476895,
  author	= {Nagy, George},
  title		= {Generalized Template Matching for Semi-structured Text},
  year		= {2021},
  isbn		= {9781450386906},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3476887.3476895},
  doi		= {10.1145/3476887.3476895},
  abstract	= {Conventional template matching for named entity
		  recognition on book-length text strings is generalized by
		  allowing search phrases to capture distant tokens. Combined
		  with word-type tagging and format variants (alternative
		  name/date formats), a few initial templates
		  (class—search-phrase—extract-phrase triples) can label
		  most of the significant tokens. The program then uses its
		  book-length statistics of tag-label associations to suggest
		  candidate text for further template construction. The
		  method serves as a preprocessor for error-free extraction
		  of semantic relations from text obeying explicit
		  semi-structure constraints. On three sample books of
		  genealogical records, an F-measure of over 0.99 was
		  achieved with less than 3 hours’ user time on each
		  book.},
  booktitle	= {Proceedings of the 6th International Workshop on
		  Historical Document Imaging and Processing},
  pages		= {55–60},
  numpages	= {6},
  keywords	= {mask matching, tokenization, tagging, data formats,
		  semi-structure},
  location	= {Lausanne, Switzerland},
  series	= {HIP '21}
}

@InProceedings{	  10.1145/3464509.3464892,
  author	= {G\"{u}nther, Michael and Thiele, Maik and Gonsior, Julius
		  and Lehner, Wolfgang},
  title		= {Pre-Trained Web Table Embeddings for Table Discovery},
  year		= {2021},
  isbn		= {9781450385350},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3464509.3464892},
  doi		= {10.1145/3464509.3464892},
  abstract	= {Pre-trained word embedding models have become the de-facto
		  standard to model text in state-of-the-art analysis tools
		  and frameworks. However, while there are massive amounts of
		  textual data stored in tables, word embedding models are
		  usually pre-trained on large documents. This mismatch can
		  lead to narrowed performance on tasks where text values in
		  tables are analyzed. To improve analysis and retrieval
		  tasks working with tabular data, we propose a novel
		  embedding technique to be pre-trained directly on a large
		  Web table corpus. In an experimental evaluation, we employ
		  our models for various data analysis tasks on different
		  data sources. Our evaluation shows that models using
		  pre-trained Web table embeddings outperform the same models
		  when applied to embeddings pre-trained on text. Moreover,
		  we show that by using Web table embeddings state-of-the-art
		  models for the investigated tasks can be outperformed.},
  booktitle	= {Proceedings of the Fourth International Workshop on
		  Exploiting Artificial Intelligence Techniques for Data
		  Management},
  pages		= {24–31},
  numpages	= {8},
  keywords	= {Web tables, learned representations, table discovery},
  location	= {Virtual Event, China},
  series	= {aiDM '21}
}

@InProceedings{	  10.1145/3462757.3466081,
  author	= {Hamdani, Rajaa El and Mustapha, Majd and Amariles, David
		  Restrepo and Troussel, Aurore and Mee\`{u}s, S\'{e}bastien
		  and Krasnashchok, Katsiaryna},
  title		= {A combined rule-based and machine learning approach for
		  automated GDPR compliance checking},
  year		= {2021},
  isbn		= {9781450385268},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3462757.3466081},
  doi		= {10.1145/3462757.3466081},
  abstract	= {The General Data Protection Regulation (GDPR) requires
		  data controllers to implement end-to-end compliance.
		  Controllers must therefore ensure that the terms agreed
		  with the data subject and their own obligations under GDPR
		  are respected in the data flows from data subject to
		  controllers, processors and sub processors (i.e. data
		  supply chain). This paper seeks to contribute to bridge
		  both ends of compliance checking through a two-pronged
		  study. First, we conceptualize a framework to implement a
		  document-centric approach to compliance checking in the
		  data supply chain. Second, we develop specific methods to
		  automate compliance checking of privacy policies. We test a
		  two-modules system, where the first module relies on NLP to
		  extract data practices from privacy policies. The second
		  module encodes GDPR rules to check the presence of
		  mandatory information. The results show that the
		  text-to-text approach outperforms local classifiers and
		  enables the extraction of both coarse-grained and
		  fine-grained information with only one model. We implement
		  full evaluation of our system on a dataset of 30 privacy
		  policies annotated by legal experts. We conclude that this
		  approach could be generalized to other documents in the
		  data supply as a means to improve end-to-end compliance.},
  booktitle	= {Proceedings of the Eighteenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {40–49},
  numpages	= {10},
  location	= {S\~{a}o Paulo, Brazil},
  series	= {ICAIL '21}
}

@InProceedings{	  10.1145/3570991.3571008,
  author	= {Bar-Haim, Roy and Eden, Lilach and Kantor, Yoav and
		  Agarwal, Vikas and Devereux, Mark and Gupta, Nisha and
		  Kumar, Arun and Orbach, Matan and Zan, Michael},
  title		= {Towards Automated Assessment of Organizational
		  Cybersecurity Posture in Cloud},
  year		= {2023},
  isbn		= {9781450397971},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3570991.3571008},
  doi		= {10.1145/3570991.3571008},
  abstract	= {In a world where reliance on digital services becomes more
		  critical every year with billions of dollars in penalties
		  being levied annually by regulators and the impacts from
		  security control failures growing, the potential
		  consequence of organizations being unable to determine the
		  completeness of their cybersecurity strategy and control
		  environment are worsening. Established standards such as
		  NIST 800-53, Cloud Security Alliance Cloud Controls Matrix
		  (CSA-CCM) and CIS 20 Security Controls offer baselines
		  against which organizations can mandate compliance, in the
		  support of managing their security control environment and
		  meeting risk and regulatory expectations. While there is
		  increased security and compliance automation, it is
		  hampered by the fact that control requirements are
		  expressed in natural language text. With large
		  organizations often needing to comply with several thousand
		  security requirements across their IT enterprise, it
		  becomes humanly impossible to assess coverage and identify
		  potential gaps. In this paper, we present a system that
		  enables performing a coarse-grained assessment of an
		  organization’s security posture, against a standard
		  control framework. We propose an AI-based model for
		  performing the mapping automatically and evaluate its
		  performance empirically. We further develop the idea and
		  employ a novel domain-specific taxonomy that enhances the
		  granularity of the coverage assessment while providing
		  explainability. We also describe how this system is being
		  used in production.},
  booktitle	= {Proceedings of the 6th Joint International Conference on
		  Data Science &amp; Management of Data (10th ACM IKDD CODS
		  and 28th COMAD)},
  pages		= {167–175},
  numpages	= {9},
  keywords	= {cloud security, mapping, regulations, security and
		  compliance},
  location	= {Mumbai, India},
  series	= {CODS-COMAD '23}
}

@Article{	  10.1145/3624013,
  author	= {Kumari, Namrata and Singh, Pardeep},
  title		= {Hindi Text Summarization Using Sequence to Sequence Neural
		  Network},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {10},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3624013},
  doi		= {10.1145/3624013},
  abstract	= {Text summarizing reduces a large block of text data to a
		  precise, short, and intelligible text that conveys the
		  whole meaning of the actual text in a few words while
		  maintaining the original context. Due to a lack of relevant
		  summaries, it is hard to understand the main idea of the
		  document. Text summarization using the abstractive
		  technique is well-studied in English, although it is still
		  in its infancy in Indian regional languages. In this study,
		  we investigate the effectiveness of using a
		  sequence-to-sequence (Seq2Seq) neural network based on
		  attention and its optimization for text summarization for
		  the Hindi language (HiATS), explicitly comparing the Adam
		  and RMSprop optimizers. Our method allows the model to take
		  the Hindi language dataset and, as output, provides a
		  concise summary that accurately reflects the gist of the
		  original text. The performance of the models will be
		  evaluated using Rouge-1 and Rouge-2 metrics.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  articleno	= {239},
  numpages	= {18},
  keywords	= {Abstractive text summarization, optimizers, word
		  embedding, neural network}
}

@InProceedings{	  10.1145/3404835.3462970,
  author	= {Liao, Lizi and Long, Le Hong and Zhang, Zheng and Huang,
		  Minlie and Chua, Tat-Seng},
  title		= {MMConv: An Environment for Multimodal Conversational
		  Search across Multiple Domains},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3462970},
  doi		= {10.1145/3404835.3462970},
  abstract	= {Although conversational search has become a hot topic in
		  both dialogue research and IR community, the real
		  breakthrough has been limited by the scale and quality of
		  datasets available. To address this fundamental obstacle,
		  we introduce the Multimodal Multi-domain Conversational
		  dataset (MMConv), a fully annotated collection of
		  human-to-human role-playing dialogues spanning over
		  multiple domains and tasks. The contribution is two-fold.
		  First, beyond the task-oriented multimodal dialogues among
		  user and agent pairs, dialogues are fully annotated with
		  dialogue belief states and dialogue acts. More importantly,
		  we create a relatively comprehensive environment for
		  conducting multimodal conversational search with real user
		  settings, structured venue database, annotated image
		  repository as well as crowd-sourced knowledge database. A
		  detailed description of the data collection procedure along
		  with a summary of data structure and analysis is provided.
		  Second, a set of benchmark results for dialogue state
		  tracking, conversational recommendation, response
		  generation as well as a unified model for multiple tasks
		  are reported. We adopt the state-of-the-art methods for
		  these tasks respectively to demonstrate the usability of
		  the data, discuss limitations of current methods and set
		  baselines for future studies.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {675–684},
  numpages	= {10},
  keywords	= {conversational search, datasets, multimodal dialogue},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@Proceedings{	  10.1145/3582935,
  title		= {ICITEE '22: Proceedings of the 5th International
		  Conference on Information Technologies and Electrical
		  Engineering},
  year		= {2022},
  isbn		= {9781450396806},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Changsha, China}
}

@InProceedings{	  10.1145/3594536.3595124,
  author	= {van Drie, Romy A. N. and de Boer, Maaike H. T. and Bakker,
		  Roos M. and Tolios, Ioannis and Vos, Daan},
  title		= {The Dutch Law as a Semantic Role Labeling Dataset},
  year		= {2023},
  isbn		= {9798400701979},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3594536.3595124},
  doi		= {10.1145/3594536.3595124},
  abstract	= {Legal documents, and specifically law texts, are not easy
		  to understand by humans. The specific terminology and
		  sentence constructions are particular, which also makes it
		  a difficult machine understanding task. In this paper, we
		  present a publicly available benchmark dataset containing
		  Dutch law texts which can be used to train AI models that
		  assist humans equipped with the task of interpreting legal
		  texts. However, the dataset can be used in a broader
		  context, such as semantic role labeling of Dutch (legal)
		  texts. Our dataset contains 4463 annotated sentences from
		  55 different Dutch laws, in which four roles are annotated
		  by human annotators: action, actor, object and recipient.
		  The inter-annotator agreement is substantial (κ=0.75). In
		  experiments with a rule-based and a transformer-based
		  method, results show that the transformer-based method
		  performs quite well on the dataset (accuracy &gt; 0.8).
		  These results indicate that we can reliably predict
		  actions, actors, objects and recipients in legal texts.
		  This can help people equipped with the task of formal
		  interpretation of legal texts.},
  booktitle	= {Proceedings of the Nineteenth International Conference on
		  Artificial Intelligence and Law},
  pages		= {316–322},
  numpages	= {7},
  keywords	= {datasets, legal interpretation, legal text, natural
		  language processing, norms, semantic role labeling},
  location	= {Braga, Portugal},
  series	= {ICAIL '23}
}

@Proceedings{	  10.1145/3573381,
  title		= {IMX '23: Proceedings of the 2023 ACM International
		  Conference on Interactive Media Experiences},
  year		= {2023},
  isbn		= {9798400700286},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Nantes, France}
}

@Proceedings{	  10.1145/3607199,
  title		= {RAID '23: Proceedings of the 26th International Symposium
		  on Research in Attacks, Intrusions and Defenses},
  year		= {2023},
  isbn		= {9798400707650},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hong Kong, China}
}

@Proceedings{	  10.5555/3606010,
  title		= {ICSE '23: Proceedings of the 45th International Conference
		  on Software Engineering},
  year		= {2023},
  isbn		= {9781665457019},
  publisher	= {IEEE Press},
  abstract	= {ICSE is the leading and by far the largest conference in
		  Software Engineering, attracting researchers, practitioners
		  and students from around the world. ICSE2023 is co-located
		  with 10 conferences and symposia this year, many
		  long-established and prestigious venues in their own
		  right.},
  location	= {Melbourne, Victoria, Australia}
}

@InProceedings{	  10.1145/3404835.3463058,
  author	= {Song, Liqiang and Yao, Mengqiu and Bi, Ye and Wu, Zhenyu
		  and Wang, Jianming and Xiao, Jing and Wen, Juan and Yu,
		  Xin},
  title		= {LS-DST: Long and Sparse Dialogue State Tracking with Smart
		  History Collector in Insurance Marketing},
  year		= {2021},
  isbn		= {9781450380379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3404835.3463058},
  doi		= {10.1145/3404835.3463058},
  abstract	= {Different from traditional task-oriented and open-domain
		  dialogue systems, insurance agents aim to engage customers
		  for helping them satisfy specific demands and emotional
		  companionship. As a result, customer-to-agent dialogues are
		  usually very long, and many turns of them are pure
		  chit-chat without any useful marketing clues. This brings
		  challenges to dialogue state tracking task in insurance
		  marketing. To deal with these long and sparse dialogues, we
		  propose a new dialogue state tracking architecture
		  containing three components: dialogue encoder, Smart
		  History Collector (SHC) and dialogue state classifier. SHC,
		  a deliberately designed memory network, effectively selects
		  relevant dialogue history via slot-attention, and then
		  updates dialogue history memory. With SHC, our model is
		  able to keep track of the vital information and filter out
		  pure chit-chat. Experimental results demonstrate that our
		  proposed LS-DST significantly outperforms the
		  state-of-the-art baselines on real insurance dialogue
		  dataset.},
  booktitle	= {Proceedings of the 44th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1960–1964},
  numpages	= {5},
  keywords	= {dialogue state tracking, insurance marketing, memory
		  network, smart history collector},
  location	= {Virtual Event, Canada},
  series	= {SIGIR '21}
}

@InProceedings{	  10.1145/3442381.3449827,
  author	= {Nguyen, Tuan-Phong and Razniewski, Simon and Weikum,
		  Gerhard},
  title		= {Advanced Semantics for Commonsense Knowledge Extraction},
  year		= {2021},
  isbn		= {9781450383127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3442381.3449827},
  doi		= {10.1145/3442381.3449827},
  abstract	= {Commonsense knowledge (CSK) about concepts and their
		  properties is useful for AI applications such as robust
		  chatbots. Prior works like ConceptNet, TupleKB and others
		  compiled large CSK collections, but are restricted in their
		  expressiveness to subject-predicate-object (SPO) triples
		  with simple concepts for S and monolithic strings for P and
		  O. Also, these projects have either prioritized precision
		  or recall, but hardly reconcile these complementary goals.
		  This paper presents a methodology, called Ascent, to
		  automatically build a large-scale knowledge base (KB) of
		  CSK assertions, with advanced expressiveness and both
		  better precision and recall than prior works. Ascent goes
		  beyond triples by capturing composite concepts with
		  subgroups and aspects, and by refining assertions with
		  semantic facets. The latter are important to express
		  temporal and spatial validity of assertions and further
		  qualifiers. Ascent combines open information extraction
		  with judicious cleaning using language models. Intrinsic
		  evaluation shows the superior size and quality of the
		  Ascent KB, and an extrinsic evaluation for QA-support tasks
		  underlines the benefits of Ascent. A web interface, data
		  and code can be found at
		  https://www.mpi-inf.mpg.de/ascent.},
  booktitle	= {Proceedings of the Web Conference 2021},
  pages		= {2636–2647},
  numpages	= {12},
  location	= {Ljubljana, Slovenia},
  series	= {WWW '21}
}

@Proceedings{	  10.1145/3624062,
  title		= {SC-W '23: Proceedings of the SC '23 Workshops of the
		  International Conference on High Performance Computing,
		  Network, Storage, and Analysis},
  year		= {2023},
  isbn		= {9798400707858},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Denver, CO, USA}
}

@Article{	  10.1109/tcbb.2023.3247634,
  author	= {Dhanuka, Richa and Singh, Jyoti Prakash and Tripathi,
		  Anushree},
  title		= {A Comprehensive Survey of Deep Learning Techniques in
		  Protein Function Prediction},
  year		= {2023},
  issue_date	= {May-June 2023},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {20},
  number	= {3},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2023.3247634},
  doi		= {10.1109/TCBB.2023.3247634},
  abstract	= {Protein function prediction is a major challenge in the
		  field of bioinformatics which aims at predicting the
		  functions performed by a known protein. Many protein data
		  forms like protein sequences, protein structures,
		  protein-protein interaction networks, and micro-array data
		  representations are being used to predict functions. During
		  the past few decades, abundant protein sequence data has
		  been generated using high throughput techniques making them
		  a suitable candidate for predicting protein functions using
		  deep learning techniques. Many such advanced techniques
		  have been proposed so far. It becomes necessary to
		  comprehend all these works in a survey to provide a
		  systematic view of all the techniques along with the
		  chronology in which the techniques have advanced. This
		  survey provides comprehensive details of the latest
		  methodologies, their pros and cons as well as predictive
		  accuracy, and a new direction in terms of interpretability
		  of the predictive models needed to be ventured by protein
		  function prediction systems.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= feb,
  pages		= {2291–2301},
  numpages	= {11}
}

@InProceedings{	  10.1145/3503823.3503903,
  author	= {Tsimpiris, Alkiviadis and Varsamis, Dimitrios and
		  Strouthopoulos, Charalampos and Pavlidis, George and
		  Chairi, Kiourt},
  title		= {Open-source OCR Engine Integration with Greek Dictionary},
  year		= {2022},
  isbn		= {9781450395557},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3503823.3503903},
  doi		= {10.1145/3503823.3503903},
  booktitle	= {Proceedings of the 25th Pan-Hellenic Conference on
		  Informatics},
  pages		= {436–441},
  numpages	= {6},
  keywords	= {Greek, OCR, Tesseract, dictionary},
  location	= {Volos, Greece},
  series	= {PCI '21}
}

@InProceedings{	  10.1145/3527188.3561941,
  author	= {Saund, Carolyn and Matuszak, Haley and Weinstein, Anna and
		  Marsella, Stacy},
  title		= {Motion and Meaning: Data-Driven Analyses of The
		  Relationship Between Gesture and Communicative Semantics},
  year		= {2022},
  isbn		= {9781450393232},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3527188.3561941},
  doi		= {10.1145/3527188.3561941},
  abstract	= {Gestures convey critical information within social
		  interactions. As such, the success of virtual agents (VA)
		  in both building social relationships and achieving their
		  goals is heavily dependent on the information conveyed
		  within their gestures. Because of the precision required
		  for effective gesture behavior, it is prudent to retain
		  some designer control over these conversational gestures.
		  However, in order to exercise that control practically we
		  must first understand how gestural motion conveys meaning.
		  One consideration in this relationship between motion and
		  meaning is the notion of Ideational Units, meaning that
		  only parts of a gesture’s motion at a point in time may
		  convey meaning, while other parts may be held from the
		  previous gesture. In this paper, we develop, demonstrate,
		  and release a set of tools that help quantify the
		  relationship between the semantics conveyed in a
		  gesture’s co-speech utterance and the fine-grained motion
		  of that gesture. This allows us to explore insights into
		  the complex relationship between motion and meaning. In
		  particular, we use spectral motion clustering to discern
		  patterns of motion that tend to be associated with semantic
		  concepts, on both an aggregate and individual-speaker
		  level. We then discuss the potential for these tools to
		  serve as a framework for both automated gesture generation
		  and interpretation in virtual agents. These tools can
		  ideally be used within approaches to automating VA gesture
		  performances as well as serve as an analysis framework for
		  fundamental gesture research.},
  booktitle	= {Proceedings of the 10th International Conference on
		  Human-Agent Interaction},
  pages		= {227–235},
  numpages	= {9},
  keywords	= {Analysis Techniques, Animation, Clustering, Gesture,
		  Human-Agent Interaction, Motion Capture, Virtual Humans},
  location	= {Christchurch, New Zealand},
  series	= {HAI '22}
}

@Proceedings{	  10.1145/3631991,
  title		= {WSSE '23: Proceedings of the 2023 5th World Symposium on
		  Software Engineering},
  year		= {2023},
  isbn		= {9798400708053},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tokyo, Japan}
}

@InProceedings{	  10.1145/3485447.3511933,
  author	= {Hamzei, Ehsan and Tomko, Martin and Winter, Stephan},
  title		= {Translating Place-Related Questions to GeoSPARQL Queries},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3511933},
  doi		= {10.1145/3485447.3511933},
  abstract	= {Many place-related questions can only be answered by
		  complex spatial reasoning, a task poorly supported by
		  factoid question retrieval. Such reasoning using
		  combinations of spatial and non-spatial criteria pertinent
		  to place-related questions is increasingly possible on
		  linked data knowledge bases. Yet, to enable question
		  answering based on linked knowledge bases, natural language
		  questions must first be re-formulated as formal queries.
		  Here, we first present an enhanced version of YAGO2geo, the
		  geospatially-enabled variant of the YAGO2 knowledge base,
		  by linking and adding more than one million places from
		  OpenStreetMap data to YAGO2. We then propose a novel
		  approach to translate the place-related questions into
		  logical representations, theoretically grounded in the core
		  concepts of spatial information. Next, we use a dynamic
		  template-based approach to generate fully executable
		  GeoSPARQL queries from the logical representations. We test
		  our approach using the Geospatial Gold Standard dataset and
		  report substantial improvements over existing methods.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {902–911},
  numpages	= {10},
  keywords	= {geographic question answering, geospatial knowledge bases,
		  place-based search, query generation},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Proceedings{	  10.1145/3556223,
  title		= {ICCCM '22: Proceedings of the 10th International
		  Conference on Computer and Communications Management},
  year		= {2022},
  isbn		= {9781450396349},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Okayama, Japan}
}

@Article{	  10.1109/taslp.2023.3313415,
  author	= {Wang, Ante and Song, Linfeng and Jin, Lifeng and Yao,
		  Junfeng and Mi, Haitao and Lin, Chen and Su, Jinsong and
		  Yu, Dong},
  title		= {D&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$^{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt;PSG:
		  Multi-Party Dialogue Discourse Parsing as Sequence
		  Generation},
  year		= {2023},
  issue_date	= {2023},
  publisher	= {IEEE Press},
  volume	= {31},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2023.3313415},
  doi		= {10.1109/TASLP.2023.3313415},
  abstract	= {Conversational discourse analysis aims to extract the
		  interactions between dialogue turns, which is crucial for
		  modeling complex multi-party dialogues. As the benchmarks
		  are still limited in size and human annotations are costly,
		  the current standard approaches apply pretrained language
		  models, but they still require randomly initialized
		  classifiers to make predictions. These classifiers usually
		  require massive data to work smoothly with the pretrained
		  encoder, causing severe data hunger issue. We propose two
		  convenient strategies to formulate this task as a sequence
		  generation problem, where classifier decisions are
		  carefully converted into sequence of tokens. We then adopt
		  a pretrained T5 [C. Raffel et al., 2020] model to solve
		  this task so that no parameters are randomly initialized.
		  We also leverage the descriptions of the discourse
		  relations to help model understand their meanings.
		  Experiments on two popular benchmarks show that our
		  approach outperforms previous state-of-the-art models by a
		  large margin, and it is also more robust in zero-shot and
		  few-shot settings.&lt;sup&gt;1&lt;/sup&gt;},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= sep,
  pages		= {4004–4013},
  numpages	= {10}
}

@Proceedings{	  10.1145/3623462,
  title		= {KUI '23: Proceedings of the 20th International Conference
		  on Culture and Computer Science: Code and Materiality},
  year		= {2023},
  isbn		= {9798400708367},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3539618.3591748,
  author	= {Pang, Ning and Zhao, Xiang and Zeng, Weixin and Wang, Ji
		  and Xiao, Weidong},
  title		= {Personalized Federated Relation Classification over
		  Heterogeneous Texts},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591748},
  doi		= {10.1145/3539618.3591748},
  abstract	= {Relation classification detects the semantic relation
		  between two annotated entities from a piece of text, which
		  is a useful tool for structurization of knowledge.
		  Recently, federated learning has been introduced to train
		  relation classification models in decentralized settings.
		  Current methods strive for a strong server model by
		  decoupling the model training at server from direct access
		  to texts at clients while taking advantage of them.
		  Nevertheless, they overlook the fact that clients have
		  heterogeneous texts (i.e., texts with diversely skewed
		  distribution of relations), which renders existing methods
		  less practical. In this paper, we propose to investigate
		  personalized federated relation classification, in which
		  strong client models adapted to their own data are desired.
		  To further meet the challenges brought by heterogeneous
		  texts, we present a novel framework, namely pf-RC, with
		  several optimized designs. It features a knowledge
		  aggregation method that exploits a relation-wise weighting
		  mechanism, and a feature augmentation method that leverages
		  prototypes to adaptively enhance the representations of
		  instances of long-tail relations. We experimentally
		  validate the superiority of pf-RC against competing
		  baselines in various settings, and the results suggest that
		  the tailored techniques mitigate the challenges.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {973–982},
  numpages	= {10},
  keywords	= {federated learning, heterogeneous texts, relation
		  classification},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Article{	  10.1145/3564769,
  author	= {Kumar Attar, Rakesh and Goyal, Vishal and Goyal, Lalit},
  title		= {State of the Art of Automation in Sign Language: A
		  Systematic Review},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3564769},
  doi		= {10.1145/3564769},
  abstract	= {Sign language is the fundamental communication language of
		  deaf people. Efforts to develop sign language generation
		  systems can make the life of these people smooth and
		  effortless. Despite the importance of sign language
		  generation systems, there is a paucity of a systematic
		  literature review. This is the foremost recognizable
		  scholastic literature review of sign language generation
		  systems. It presents a scholastic database of the
		  literature between 1998 and 2020 and suggests
		  classification criteria to systematize research studies.
		  Four hundred fourteen research studies were recognized and
		  reviewed for their direct pertinence to sign language
		  generation systems. One hundred sixty-two research studies
		  were subsequently chosen, examined, and classified. Each of
		  the 162 chosen research papers was categorized based on 30
		  sign languages and was further comparatively analyzed based
		  on seven comparison parameters (input form, translation
		  technologies, application domain, use of parsers/grammars,
		  manual/non-manual features, accuracy, and output form). It
		  is evident from our research findings that the majority of
		  research on sign language generation was carried out using
		  data-driven approaches in the absence of proper grammar
		  rules and generated only manual signs. This research study
		  may provide researchers a roadmap toward future research
		  directions and facilitate the compilation of information in
		  the field of sign language generation.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {94},
  numpages	= {80},
  keywords	= {Machine translation, Interlingua, virtual avatar, SiGML,
		  HamNoSys}
}

@Article{	  10.1145/3453156,
  author	= {Alhussain, Arwa I. and Azmi, Aqil M.},
  title		= {Automatic Story Generation: A Survey of Approaches},
  year		= {2021},
  issue_date	= {June 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {5},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3453156},
  doi		= {10.1145/3453156},
  abstract	= {Computational generation of stories is a subfield of
		  computational creativity where artificial intelligence and
		  psychology intersect to teach computers how to mimic
		  humans’ creativity. It helps generate many stories with
		  minimum effort and customize the stories for the users’
		  education and entertainment needs. Although the automatic
		  generation of stories started to receive attention many
		  decades ago, advances in this field to date are less than
		  expected and suffer from many limitations. This survey
		  presents an extensive study of research in the area of
		  non-interactive textual story generation, as well as
		  covering resources, corpora, and evaluation methods that
		  have been used in those studies. It also shed light on
		  factors of story interestingness.},
  journal	= {ACM Comput. Surv.},
  month		= may,
  articleno	= {103},
  numpages	= {38},
  keywords	= {Text generation, datasets, evaluation, story generation,
		  survey}
}

@Article{	  10.14778/3494124.3494131,
  author	= {Jin, Di and Sisman, Bunyamin and Wei, Hao and Dong, Xin
		  Luna and Koutra, Danai},
  title		= {Deep transfer learning for multi-source entity linkage via
		  domain adaptation},
  year		= {2021},
  issue_date	= {November 2021},
  publisher	= {VLDB Endowment},
  volume	= {15},
  number	= {3},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3494124.3494131},
  doi		= {10.14778/3494124.3494131},
  abstract	= {Multi-source entity linkage focuses on integrating
		  knowledge from multiple sources by linking the records that
		  represent the same real world entity. This is critical in
		  high-impact applications such as data cleaning and user
		  stitching. The state-of-the-art entity linkage pipelines
		  mainly depend on supervised learning that requires abundant
		  amounts of training data. However, collecting well-labeled
		  training data becomes expensive when the data from many
		  sources arrives incrementally over time. Moreover, the
		  trained models can easily overfit to specific data sources,
		  and thus fail to generalize to new sources due to
		  significant differences in data and label distributions. To
		  address these challenges, we present AdaMEL, a deep
		  transfer learning framework that learns generic high-level
		  knowledge to perform multi-source entity linkage. AdaMEL
		  models the attribute importance that is used to match
		  entities through an attribute-level self-attention
		  mechanism, and leverages the massive unlabeled data from
		  new data sources through domain adaptation to make it
		  generic and data-source agnostic. In addition, AdaMEL is
		  capable of incorporating an additional set of labeled data
		  to more accurately integrate data sources with different
		  attribute importance. Extensive experiments show that our
		  framework achieves state-of-the-art results with 8.21%
		  improvement on average over methods based on supervised
		  learning. Besides, it is more stable in handling different
		  sets of data sources in less runtime.},
  journal	= {Proc. VLDB Endow.},
  month		= nov,
  pages		= {465–477},
  numpages	= {13}
}

@Proceedings{	  10.1145/3542637,
  title		= {APNet '22: Proceedings of the 6th Asia-Pacific Workshop on
		  Networking},
  year		= {2022},
  isbn		= {9781450397483},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Fuzhou, China}
}

@InProceedings{	  10.1145/3447548.3467308,
  author	= {Zeng, Qingkai and Lin, Jinfeng and Yu, Wenhao and
		  Cleland-Huang, Jane and Jiang, Meng},
  title		= {Enhancing Taxonomy Completion with Concept Generation via
		  Fusing Relational Representations},
  year		= {2021},
  isbn		= {9781450383325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3447548.3467308},
  doi		= {10.1145/3447548.3467308},
  abstract	= {Automatic construction of a taxonomy supports many
		  applications in e-commerce, web search, and question
		  answering. Existing taxonomy expansion or completion
		  methods assume that new concepts have been accurately
		  extracted and their embedding vectors learned from the text
		  corpus. However, one critical and fundamental challenge in
		  fixing the incompleteness of taxonomies is the
		  incompleteness of the extracted concepts, especially for
		  those whose names have multiple words and consequently low
		  frequency in the corpus. To resolve the limitations of
		  extraction-based methods, we propose GenTaxo to enhance
		  taxonomy completion by identifying positions in existing
		  taxonomies that need new concepts and then generating
		  appropriate concept names. Instead of relying on the corpus
		  for concept embeddings, GenTaxo learns the contextual
		  embeddings from their surrounding graph-based and
		  language-based relational information, and leverages the
		  corpus for pre-training a concept name generator.
		  Experimental results demonstrate that GenTaxo improves the
		  completeness of taxonomies over existing methods.},
  booktitle	= {Proceedings of the 27th ACM SIGKDD Conference on Knowledge
		  Discovery &amp; Data Mining},
  pages		= {2104–2113},
  numpages	= {10},
  keywords	= {concept generation, taxonomy completion},
  location	= {Virtual Event, Singapore},
  series	= {KDD '21}
}

@InProceedings{	  10.1145/3539618.3591897,
  author	= {Kartchner, David and Al-Hussaini, Irfan and Turner, Haydn
		  and Deng, Jennifer and Lohiya, Shubham and Bathala,
		  Prasanth and Mitchell, Cassie},
  title		= {BioSift: A Dataset for Filtering Biomedical Abstracts for
		  Drug Repurposing and Clinical Meta-Analysis},
  year		= {2023},
  isbn		= {9781450394086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3539618.3591897},
  doi		= {10.1145/3539618.3591897},
  abstract	= {This work presents a new, original document classification
		  dataset, BioSift, to expedite the initial selection and
		  labeling of studies for drug repurposing. The dataset
		  consists of 10,000 human-annotated abstracts from
		  scientific articles in PubMed. Each abstract is labeled
		  with up to eight attributes necessary to perform
		  meta-analysis utilizing the popular
		  patient-intervention-comparator-outcome (PICO) method: has
		  human subjects, is clinical trial/cohort, has population
		  size, has target disease, has study drug, has comparator
		  group, has a quantitative outcome, and an "aggregate"
		  label. Each abstract was annotated by 3 different
		  annotators (i.e., biomedical students) and randomly sampled
		  abstracts were reviewed by senior annotators to ensure
		  quality. Data statistics such as reviewer agreement, label
		  co-occurrence, and confidence are shown. Robust benchmark
		  results illustrate neither PubMed advanced filters nor
		  state-of-the-art document classification schemes (e.g.,
		  active learning, weak supervision, full supervision) can
		  efficiently replace human annotation. In short, BioSift is
		  a pivotal but challenging document classification task to
		  expedite drug repurposing. The full annotated dataset is
		  publicly available and enables research development of
		  algorithms for document classification that enhance drug
		  repurposing.},
  booktitle	= {Proceedings of the 46th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2913–2923},
  numpages	= {11},
  keywords	= {document classification, document filtering, drug
		  repurposing, meta analysis, natural language processing},
  location	= {Taipei, Taiwan},
  series	= {SIGIR '23}
}

@Article{	  10.1145/3606370,
  author	= {Chan, Chia-Pang and Yang, Jun-He},
  title		= {Instagram Text Sentiment Analysis Combining Machine
		  Learning and NLP},
  year		= {2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3606370},
  doi		= {10.1145/3606370},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  keywords	= {Instagram, natural language processing, machine learning,
		  deep learning, word embedding technology}
}

@Article{	  10.1145/3418208,
  author	= {Ni, Pin and Li, Yuming and Li, Gangmin and Chang, Victor},
  title		= {A Hybrid Siamese Neural Network for Natural Language
		  Inference in Cyber-Physical Systems},
  year		= {2021},
  issue_date	= {June 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {21},
  number	= {2},
  issn		= {1533-5399},
  url		= {https://doi.org/10.1145/3418208},
  doi		= {10.1145/3418208},
  abstract	= {Cyber-Physical Systems (CPS), as a multi-dimensional
		  complex system that connects the physical world and the
		  cyber world, has a strong demand for processing large
		  amounts of heterogeneous data. These tasks also include
		  Natural Language Inference (NLI) tasks based on text from
		  different sources. However, the current research on natural
		  language processing in CPS does not involve exploration in
		  this field. Therefore, this study proposes a Siamese
		  Network structure that combines Stacked Residual Long
		  Short-Term Memory (bidirectional) with the Attention
		  mechanism and Capsule Network for the NLI module in CPS,
		  which is used to infer the relationship between
		  text/language data from different sources. This model is
		  mainly used to implement NLI tasks and conduct a detailed
		  evaluation in three main NLI benchmarks as the basic
		  semantic understanding module in CPS. Comparative
		  experiments prove that the proposed method achieves
		  competitive performance, has a certain generalization
		  ability, and can balance the performance and the number of
		  trained parameters.},
  journal	= {ACM Trans. Internet Technol.},
  month		= mar,
  articleno	= {33},
  numpages	= {25},
  keywords	= {Cyber-physical systems, Natural language inference,
		  Siamese neural networks}
}

@InProceedings{	  10.1145/3511616.3513115,
  author	= {Thapa, Nischay Bikram and Seifollahi, Sattar and Taheri,
		  Sona},
  title		= {Hospital Readmission Prediction Using Clinical Admission
		  Notes},
  year		= {2022},
  isbn		= {9781450396066},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511616.3513115},
  doi		= {10.1145/3511616.3513115},
  abstract	= {Clinical notes contain contextualised information beyond
		  structured data relating to patients’ past and current
		  health conditions. Despite the richness, their
		  unstructured, long, and high dimensional nature presents
		  challenges to traditional text representation techniques.
		  The advancement of deep contextual representation
		  techniques in natural language processing (NLP) has shown
		  remarkable performance in the biomedical and clinical
		  domains for various information extraction and predictive
		  tasks, including hospital readmission. However, most
		  previous works have proposed discharge summary models where
		  on-site medical intervention is impossible, and readmission
		  could still occur. This paper utilises clinical notes
		  recorded during admissions to study the risk of 30-day
		  hospital readmissions. We employ clinical notes from
		  MIMIC-III and consider competing baselines for clinical
		  text representation, where a set of machine learning and
		  deep learning algorithms are used to classify hospital
		  readmission. The study demonstrates that notes captured
		  during admissions play a crucial role to recognise
		  potential readmission risk supporting healthcare
		  practitioners for practical therapeutic intervention and
		  discharge planning.},
  booktitle	= {Proceedings of the 2022 Australasian Computer Science
		  Week},
  pages		= {193–199},
  numpages	= {7},
  keywords	= {Electronic health records, Embedding techniques, Hospital
		  readmission, Natural language processing},
  location	= {Brisbane, Australia},
  series	= {ACSW '22}
}

@InProceedings{	  10.1145/3544548.3581252,
  author	= {Xiao, Ziang and Li, Tiffany Wenting and Karahalios, Karrie
		  and Sundaram, Hari},
  title		= {Inform the Uninformed: Improving Online Informed Consent
		  Reading with an AI-Powered Chatbot},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3544548.3581252},
  doi		= {10.1145/3544548.3581252},
  abstract	= {Informed consent is a core cornerstone of ethics in human
		  subject research. Through the informed consent process,
		  participants learn about the study procedure, benefits,
		  risks, and more to make an informed decision. However,
		  recent studies showed that current practices might lead to
		  uninformed decisions and expose participants to unknown
		  risks, especially in online studies. Without the
		  researcher’s presence and guidance, online participants
		  must read a lengthy form on their own with no answers to
		  their questions. In this paper, we examined the role of an
		  AI-powered chatbot in improving informed consent online. By
		  comparing the chatbot with form-based interaction, we found
		  the chatbot improved consent form reading, promoted
		  participants’ feelings of agency, and closed the power
		  gap between the participant and the researcher. Our
		  exploratory analysis further revealed the altered power
		  dynamic might eventually benefit study response quality. We
		  discussed design implications for creating AI-powered
		  chatbots to offer effective informed consent in broader
		  settings.},
  booktitle	= {Proceedings of the 2023 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {112},
  numpages	= {17},
  keywords	= {AI-powered chatbot, conversational agents, human-AI
		  interaction, informed consent, power dynamic},
  location	= {Hamburg, Germany},
  series	= {CHI '23}
}

@Proceedings{	  10.1145/3536221,
  title		= {ICMI '22: Proceedings of the 2022 International Conference
		  on Multimodal Interaction},
  year		= {2022},
  isbn		= {9781450393904},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bengaluru, India}
}

@InProceedings{	  10.1145/3485447.3512002,
  author	= {Lee, Dongha and Shen, Jiaming and Kang, Seongku and Yoon,
		  Susik and Han, Jiawei and Yu, Hwanjo},
  title		= {TaxoCom: Topic Taxonomy Completion with Hierarchical
		  Discovery of Novel Topic Clusters},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3512002},
  doi		= {10.1145/3485447.3512002},
  abstract	= {Topic taxonomies, which represent the latent topic (or
		  category) structure of document collections, provide
		  valuable knowledge of contents in many applications such as
		  web search and information filtering. Recently, several
		  unsupervised methods have been developed to automatically
		  construct the topic taxonomy from a text corpus, but it is
		  challenging to generate the desired taxonomy without any
		  prior knowledge. In this paper, we study how to leverage
		  the partial (or incomplete) information about the topic
		  structure as guidance to find out the complete topic
		  taxonomy. We propose a novel framework for topic taxonomy
		  completion, named TaxoCom, which recursively expands the
		  topic taxonomy by discovering novel sub-topic clusters of
		  terms and documents. To effectively identify novel topics
		  within a hierarchical topic structure, TaxoCom devises its
		  embedding and clustering techniques to be closely-linked
		  with each other: (i) locally discriminative embedding
		  optimizes the text embedding space to be discriminative
		  among known (i.e., given) sub-topics, and (ii) novelty
		  adaptive clustering assigns terms into either one of the
		  known sub-topics or novel sub-topics. Our comprehensive
		  experiments on two real-world datasets demonstrate that
		  TaxoCom not only generates the high-quality topic taxonomy
		  in terms of term coherency and topic coverage but also
		  outperforms all other baselines for a downstream task.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {2819–2829},
  numpages	= {11},
  keywords	= {Hierarchical topic discovery, Novelty detection, Text
		  clustering, Text embedding, Topic taxonomy completion},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Proceedings{	  10.1145/3528588,
  title		= {NLBSE '22: Proceedings of the 1st International Workshop
		  on Natural Language-based Software Engineering},
  year		= {2022},
  isbn		= {9781450393430},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 1st edition of the International Workshop
		  on Natural Language-Based Software Engineering (NLBSE). The
		  potential of Natural Language Processing (NLP) and Natural
		  Language Generation (NLG) to support developers and
		  engineers in a wide number of software engineering-related
		  tasks (e.g., requirements engineering, extraction of
		  knowledge and patterns from the software artifacts,
		  summarization and prioritization of development and
		  maintenance activities, etc.) is increasingly evident.
		  Furthermore, the current availability of libraries (e.g.,
		  NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that
		  allow efficiently and easily dealing with low-level aspects
		  of natural language processing and representation, pushed
		  more and more researchers to closely work with industry to
		  attempt to solve software engineers' real-world problems.},
  location	= {Pittsburgh, Pennsylvania}
}

@InProceedings{	  10.1145/3534678.3539077,
  author	= {Yang, Jiuding and Guo, Weidong and Liu, Bang and Yu, Yakun
		  and Wang, Chaoyue and Luo, Jinwen and Kong, Linglong and
		  Niu, Di and Wen, Zhen},
  title		= {TAG: Toward Accurate Social Media Content Tagging with a
		  Concept Graph},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539077},
  doi		= {10.1145/3534678.3539077},
  abstract	= {Although conceptualization has been widely studied in
		  semantics and knowledge representation, it is still
		  challenging to find the most accurate concept terms to tag
		  fast-growing social media content. This is partly
		  attributed to the fact that most traditional knowledge
		  bases contain general terms of the world, such as trees and
		  cars, which are not interesting to users, and do not have
		  the defining power for social media content. Another reason
		  is that the intricate use of tense, negation and grammar in
		  social media content may change the logic or emphasis of
		  the content, thus focusing on different main ideas. In this
		  paper, we present TAG, a high-quality concept matching
		  dataset consisting of 10,000 labeled pairs of fine-grained
		  concepts and web-styled natural language sentences, mined
		  from open-domain social media content. The concepts we
		  provide are the trending terms on social media and have the
		  right granularity to define user interests, e.g., highly
		  educated actors instead of just actors. In the meantime,
		  TAG offers a concept graph which interconnects these
		  fine-grained concepts and entities to provide contextual
		  information. We evaluate a wide range of neural text
		  matching models as well as pre-trained language models for
		  the concept matching task on TAG, and point out their
		  insufficiency to tag social media content to characterize
		  its main idea. We further propose a novel graph-graph
		  matching framework that demonstrates superior abstraction
		  and generalization performance by better utilizing both the
		  structural information in the concept graph and logic
		  interactions between semantic units in the natural language
		  sentence via syntactic dependency parsing.},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4332–4341},
  numpages	= {10},
  keywords	= {concept-sentence matching, datasets, graph-graph matching,
		  heterogeneous graph},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@InProceedings{	  10.1145/3412841.3441957,
  author	= {Meijer, Lisa and Frasincar, Flavius and Tru\c{s}c\u{a},
		  Maria Mihaela},
  title		= {Explaining a neural attention model for aspect-based
		  sentiment classification using diagnostic classification},
  year		= {2021},
  isbn		= {9781450381048},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3412841.3441957},
  doi		= {10.1145/3412841.3441957},
  abstract	= {Many high performance machine learning models for
		  Aspect-Based Sentiment Classification (ABSC) produce black
		  box models, and therefore barely explain how they classify
		  a certain sentiment value towards an aspect. In this paper,
		  we propose explanation models, that inspect the internal
		  dynamics of a state-of-the-art neural attention model, the
		  LCR-Rot-hop, by using a technique called Diagnostic
		  Classification. Our diagnostic classifier is a simple
		  neural network, which evaluates whether the internal layers
		  of the LCR-Rot-hop model encode useful word information for
		  classification, i.e., the part of speech, the sentiment
		  value, the presence of aspect relation, and the
		  aspect-related sentiment value of words. We conclude that
		  the lower layers in the LCR-Rot-hop model encode the part
		  of speech and the sentiment value, whereas the higher
		  layers represent the presence of a relation with the aspect
		  and the aspect-related sentiment value of words.},
  booktitle	= {Proceedings of the 36th Annual ACM Symposium on Applied
		  Computing},
  pages		= {821–827},
  numpages	= {7},
  keywords	= {aspect-based sentiment classification, diagnostic
		  classification, neural rotatory attention model},
  location	= {Virtual Event, Republic of Korea},
  series	= {SAC '21}
}

@InProceedings{	  10.1145/3477314.3507256,
  author	= {Kanwal, Neel and Rizzo, Giuseppe},
  title		= {Attention-based clinical note summarization},
  year		= {2022},
  isbn		= {9781450387132},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477314.3507256},
  doi		= {10.1145/3477314.3507256},
  abstract	= {In recent years, the trend of deploying digital systems in
		  numerous industries has hiked. The health sector has
		  observed an extensive adoption of digital systems and
		  services that generate significant medical records.
		  Electronic health records contain valuable information for
		  prospective and retrospective analysis that is often not
		  entirely exploited because of the complicated dense
		  information storage. The crude purpose of condensing health
		  records is to select the information that holds most
		  characteristics of the original documents based on a
		  reported disease. These summaries may boost diagnosis and
		  save a doctor's time during a saturated workload situation
		  like the COVID-19 pandemic. In this paper, we are applying
		  a multi-head attention-based mechanism to perform
		  extractive summarization of meaningful phrases on clinical
		  notes. Our method finds major sentences for a summary by
		  correlating tokens, segments, and positional embeddings of
		  sentences in a clinical note. The model outputs attention
		  scores that are statistically transformed to extract
		  critical phrases for visualization on the heat-mapping tool
		  and for human use.},
  booktitle	= {Proceedings of the 37th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {813–820},
  numpages	= {8},
  keywords	= {ICD-9, MIMIC-III, clinical notes, deep learning,
		  electronic health records, extractive summarization,
		  information extraction, medical records, multi-head
		  attention, natural language processing, transformer
		  models},
  location	= {Virtual Event},
  series	= {SAC '22}
}

@InProceedings{	  10.1145/3460231.3474260,
  author	= {Jiang, Jyun-Yu and Lee, Chia-Jung and Yang, Longqi and
		  Sarrafzadeh, Bahareh and Hecht, Brent and Teevan, Jaime},
  title		= {Learning to Represent Human Motives for Goal-directed Web
		  Browsing},
  year		= {2021},
  isbn		= {9781450384582},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460231.3474260},
  doi		= {10.1145/3460231.3474260},
  abstract	= {Motives or goals are recognized in psychology literature
		  as the most fundamental drive that explains and predicts
		  why people do what they do, including when they browse the
		  web. Although providing enormous value, these
		  higher-ordered goals are often unobserved, and little is
		  known about how to leverage such goals to assist people’s
		  browsing activities. This paper proposes to take a new
		  approach to address this problem, which is fulfilled
		  through a novel neural framework, Goal-directed Web
		  Browsing &nbsp;(GoWeB). We adopt a psychologically-sound
		  taxonomy of higher-ordered goals and learn to build their
		  representations in a structure-preserving manner. Then we
		  incorporate the resulting representations for enhancing the
		  experiences of common activities people perform on the web.
		  Experiments on large-scale data from Microsoft Edge web
		  browser show that GoWeB significantly outperforms
		  competitive baselines for in-session web page
		  recommendation, re-visitation classification, and
		  goal-based web page grouping. A follow-up analysis further
		  characterizes how the variety of human motives can affect
		  the difference observed in human behavioral patterns.},
  booktitle	= {Proceedings of the 15th ACM Conference on Recommender
		  Systems},
  pages		= {361–371},
  numpages	= {11},
  keywords	= {Goal Representation Learning, User Behavior, User Goals,
		  Web Browser Session Modeling},
  location	= {Amsterdam, Netherlands},
  series	= {RecSys '21}
}

@Proceedings{	  10.1145/3607505,
  title		= {CSET '23: Proceedings of the 16th Cyber Security
		  Experimentation and Test Workshop},
  year		= {2023},
  isbn		= {9798400707889},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Marina del Rey, CA, USA}
}

@Proceedings{	  10.1145/3564746,
  title		= {ACMSE '23: Proceedings of the 2023 ACM Southeast
		  Conference},
  year		= {2023},
  isbn		= {9781450399210},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, USA}
}

@InProceedings{	  10.1145/3583780.3614984,
  author	= {Wang, Siyuan and Zheng, Jianming and Chen, Wanyu and Cai,
		  Fei and Luo, Xueshan},
  title		= {MultiPLe: Multilingual Prompt Learning for Relieving
		  Semantic Confusions in Few-shot Event Detection},
  year		= {2023},
  isbn		= {9798400701245},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3583780.3614984},
  doi		= {10.1145/3583780.3614984},
  abstract	= {Event detection (ED) is a challenging task in the field of
		  information extraction. Due to the monolingual text and
		  rampant confusing triggers, traditional ED models suffer
		  from semantic confusions in terms of polysemy and synonym,
		  leading to severe detection mistakes. Such semantic
		  confusions can be further exacerbated in a practical
		  situation where scarce labeled data cannot provide
		  sufficient semantic clues. To mitigate such bottleneck, we
		  propose a multilingual prompt learning (MultiPLe) framework
		  for few-shot event detection (FSED), including three
		  components, i.e., a multilingual prompt, a hierarchical
		  prototype and a quadruplet contrastive learning module. In
		  detail, to ease the polysemy confusion, the multilingual
		  prompt module develops the in-context semantics of triggers
		  via the multilingual disambiguation and prior knowledge in
		  pretrained language models. Then, the hierarchical
		  prototype module is adopted to diminish the synonym
		  confusion by connecting the captured inmost semantics of
		  fuzzy triggers with labels at a fine granularity. Finally,
		  we employ the quadruplet contrastive learning module to
		  tackle the insufficient label representation and potential
		  noise. Experiments on two public datasets show that
		  MultiPLe outperforms the state-of-the-art baselines in
		  weighted F1-score, presenting a maximum improvement of
		  13.63% for FSED.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2676–2685},
  numpages	= {10},
  keywords	= {few-shot event detection, prompt learning, semantic
		  confusions},
  location	= {Birmingham, United Kingdom},
  series	= {CIKM '23}
}

@Article{	  10.1145/3450273,
  author	= {Qi, Shanshan and Zheng, Limin and Shang, Feiyu},
  title		= {Dependency Parsing-based Entity Relation Extraction over
		  Chinese Complex Text},
  year		= {2021},
  issue_date	= {July 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3450273},
  doi		= {10.1145/3450273},
  abstract	= {Open Relation Extraction (ORE) plays a significant role in
		  the field of Information Extraction. It breaks the
		  limitation that traditional relation extraction must
		  pre-define relational types in the annotated corpus and
		  specific domains restrictions, to realize the goal of
		  extracting entities and the relation between entities in
		  the open domain. However, with the increase of sentence
		  complexity, the precision and recall of Entity Relation
		  Extraction will be significantly reduced. To solve this
		  problem, we present an unsupervised Clause_CORE method
		  based on Chinese grammar and dependency parsing features.
		  Clause_CORE is used for complex sentences processing,
		  including decomposing complex sentence and dynamically
		  complementing sentence components, which can reduce
		  sentences complexity and maintain the integrity of
		  sentences at the same time. Then, we perform dependency
		  parsing for complete sentences and implement open entity
		  relation extraction based on the model constructed by
		  Chinese grammar rules. The experimental results show that
		  the performance of Clause_CORE method is better than that
		  of other advanced Chinese ORE systems on Wikipedia and Sina
		  news datasets, which proves the correctness and
		  effectiveness of the method. The results on mixed datasets
		  of news data and encyclopedia data prove the generalization
		  and portability of the method.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {67},
  numpages	= {34},
  keywords	= {Open entity relation extraction, dependency parsing,
		  complex sentences processed, Chinese grammar rules,
		  unsupervised}
}

@Proceedings{	  10.1145/3565472,
  title		= {UMAP '23: Proceedings of the 31st ACM Conference on User
		  Modeling, Adaptation and Personalization},
  year		= {2023},
  isbn		= {9781450399326},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Limassol, Cyprus}
}

@Proceedings{	  10.1145/3584748,
  title		= {EBIMCS '22: Proceedings of the 2022 5th International
		  Conference on E-Business, Information Management and
		  Computer Science},
  year		= {2022},
  isbn		= {9781450397827},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hong Kong, Hong Kong}
}

@InProceedings{	  10.1145/3487664.3487706,
  author	= {Odakura, Fumimaro and Kobayashi, Koga and Wakabayashi,
		  Kei},
  title		= {Active Learning for Extracting Technical Terms Covering
		  Multiword Phrases},
  year		= {2022},
  isbn		= {9781450395564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3487664.3487706},
  doi		= {10.1145/3487664.3487706},
  abstract	= {Automatic extraction of technical terms is an important
		  task for organizing a set of documents. While the sequence
		  labeling formulation is the major approach, we explore a
		  method that takes examples of terms as input and outputs
		  phrases in the same category as the given terms to avoid
		  the heavy cost for building a training dataset. The
		  existing methods in this direction are template-based,
		  which the user cannot give any feedback to the system even
		  if some of the extracted terms are not intended. This paper
		  proposes a framework for extracting technical terms that
		  considers the user’s feedback by adopting active learning
		  approach. The proposed method can extract terms consisting
		  of multiple words by dynamically accessing an inverted
		  index created in advance. We empirically show the
		  effectiveness of the proposed method in comparison to the
		  straightforward application of active learning to an
		  existing method.},
  booktitle	= {The 23rd International Conference on Information
		  Integration and Web Intelligence},
  pages		= {311–318},
  numpages	= {8},
  keywords	= {active learning, inverted index, multiword phrases, part
		  of speech, terminology extraction},
  location	= {Linz, Austria},
  series	= {iiWAS2021}
}

@InProceedings{	  10.1145/3511095.3531279,
  author	= {Vitiugin, Fedor and Castillo, Carlos},
  title		= {Cross-Lingual Query-Based Summarization of Crisis-Related
		  Social Media: An Abstractive Approach Using Transformers},
  year		= {2022},
  isbn		= {9781450392334},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3511095.3531279},
  doi		= {10.1145/3511095.3531279},
  abstract	= {Relevant and timely information collected from social
		  media during crises can be an invaluable resource for
		  emergency management. However, extracting this information
		  remains a challenging task, particularly when dealing with
		  social media postings in multiple languages. This work
		  proposes a cross-lingual method for retrieving and
		  summarizing crisis-relevant information from social media
		  postings. We describe a uniform way of expressing various
		  information needs through structured queries and a way of
		  creating summaries answering those information needs. The
		  method is based on multilingual transformers embeddings.
		  Queries are written in one of the languages supported by
		  the embeddings, and the extracted sentences can be in any
		  of the other languages supported. Abstractive summaries are
		  created by transformers. The evaluation, done by
		  crowdsourcing evaluators and emergency management experts,
		  and carried out on collections extracted from Twitter
		  during five large-scale disasters spanning ten languages,
		  shows the flexibility of our approach. The generated
		  summaries are regarded as more focused, structured, and
		  coherent than existing state-of-the-art methods, and
		  experts compare them favorably against summaries created by
		  existing, state-of-the-art methods.},
  booktitle	= {Proceedings of the 33rd ACM Conference on Hypertext and
		  Social Media},
  pages		= {21–31},
  numpages	= {11},
  keywords	= {abstractive summarization, emergency management,
		  multilingual retrieval, social media},
  location	= {Barcelona, Spain},
  series	= {HT '22}
}

@InProceedings{	  10.1145/3460120.3484536,
  author	= {Bui, Duc and Yao, Yuan and Shin, Kang G. and Choi,
		  Jong-Min and Shin, Junbum},
  title		= {Consistency Analysis of Data-Usage Purposes in Mobile
		  Apps},
  year		= {2021},
  isbn		= {9781450384544},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3460120.3484536},
  doi		= {10.1145/3460120.3484536},
  abstract	= {While privacy laws and regulations require apps and
		  services to disclose the purposes of their data collection
		  to the users (i.e., why do they collect my data?), the data
		  usage in an app's actual behavior does not always comply
		  with the purposes stated in its privacy policy. Automated
		  techniques have been proposed to analyze apps' privacy
		  policies and their execution behavior, but they often
		  overlooked the purposes of the apps' data collection, use
		  and sharing. To mitigate this oversight, we propose
		  PurPliance, an automated system that detects the
		  inconsistencies between the data-usage purposes stated in a
		  natural language privacy policy and those of the actual
		  execution behavior of an Android app. PurPliance analyzes
		  the predicate-argument structure of policy sentences and
		  classifies the extracted purpose clauses into a taxonomy of
		  data purposes. Purposes of actual data usage are inferred
		  from network data traffic. We propose a formal model to
		  represent and verify the data usage purposes in the
		  extracted privacy statements and data flows to detect
		  policy contradictions in a privacy policy and
		  flow-to-policy inconsistencies between network data flows
		  and privacy statements. Our evaluation results of
		  end-to-end contradiction detection have shown PurPliance to
		  improve detection precision from 19% to 95% and recall from
		  10% to 50% compared to a state-of-the-art method. Our
		  analysis of 23.1k Android apps has also shown PurPliance to
		  detect contradictions in 18.14% of privacy policies and
		  flow-to-policy inconsistencies in 69.66% of apps,
		  indicating the prevalence of inconsistencies of data
		  practices in mobile apps.},
  booktitle	= {Proceedings of the 2021 ACM SIGSAC Conference on Computer
		  and Communications Security},
  pages		= {2824–2843},
  numpages	= {20},
  keywords	= {consistency analysis, data flow, data-usage purposes,
		  mobile apps, privacy policies},
  location	= {Virtual Event, Republic of Korea},
  series	= {CCS '21}
}

@Article{	  10.1145/3483424,
  author	= {Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael},
  title		= {A Survey of AIOps Methods for Failure Management},
  year		= {2021},
  issue_date	= {December 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {12},
  number	= {6},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3483424},
  doi		= {10.1145/3483424},
  abstract	= {Modern society is increasingly moving toward complex and
		  distributed computing systems. The increase in scale and
		  complexity of these systems challenges O&amp;M teams that
		  perform daily monitoring and repair operations, in contrast
		  with the increasing demand for reliability and scalability
		  of modern applications. For this reason, the study of
		  automated and intelligent monitoring systems has recently
		  sparked much interest across applied IT industry and
		  academia. Artificial Intelligence for IT Operations (AIOps)
		  has been proposed to tackle modern IT administration
		  challenges thanks to Machine Learning, AI, and Big Data.
		  However, AIOps as a research topic is still largely
		  unstructured and unexplored, due to missing conventions in
		  categorizing contributions for their data requirements,
		  target goals, and components. In this work, we focus on
		  AIOps for Failure Management (FM), characterizing and
		  describing 5 different categories and 14 subcategories of
		  contributions, based on their time intervention window and
		  the target problem being solved. We review 100 FM
		  solutions, focusing on applicability requirements and the
		  quantitative results achieved, to facilitate an effective
		  application of AIOps solutions. Finally, we discuss current
		  development problems in the areas covered by AIOps and
		  delineate possible future trends for AI-based failure
		  management.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= nov,
  articleno	= {81},
  numpages	= {45},
  keywords	= {AIOps, IT operations and maintenance, failure management,
		  artificial intelligence}
}

@Article{	  10.1145/3584861,
  author	= {Das, Ringki and Singh, Thoudam Doren},
  title		= {Image–Text Multimodal Sentiment Analysis Framework of
		  Assamese News Articles Using Late Fusion},
  year		= {2023},
  issue_date	= {June 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3584861},
  doi		= {10.1145/3584861},
  abstract	= {Before the arrival of the web as a corpus, people detected
		  positive and negative news based on the understanding of
		  the textual content from physical newspaper rather than an
		  automatic identification approach from readily available
		  e-newspapers. Thus, the earlier sentiment analysis approach
		  is based on unimodal data, and less effort is paid to the
		  multimodal data. However, the presence of multimodal
		  information helps us to get a clearer understanding of the
		  sentiment. To the best of our knowledge, less work has been
		  introduced on the image–text multimodal sentiment
		  analysis framework of Assamese, a low-resource Indian
		  language mostly spoken in the northeast part of India. We
		  built an Assamese news articles dataset consisting of news
		  text and associated images and one image caption to conduct
		  an experimental study. Focusing on important words and
		  discriminative regions of the images mostly related to
		  sentiment, two individual unimodal such as textual and
		  visual models are proposed. The visual model is developed
		  using an encoder-decoder–based image caption generation
		  system. An image–text multimodal approach is proposed to
		  explore the internal correlation between textual and visual
		  features for joint sentiment classification. Finally, we
		  propose the multimodal sentiment analysis framework, i.e.,
		  Textual Visual Multimodal Fusion, by employing a late
		  fusion scheme to merge the three different modalities for
		  the final sentiment prediction. Experimental results
		  conducted on the Assamese dataset built in-house
		  demonstrate that the contextual integration of multimodal
		  features delivers better performance than unimodal
		  features.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {161},
  numpages	= {30},
  keywords	= {Multimodal sentiment analysis, low resource language,
		  caption generation, machine learning classifier, late
		  fusion}
}

@Proceedings{	  10.1145/3568562,
  title		= {SoICT '22: Proceedings of the 11th International Symposium
		  on Information and Communication Technology},
  year		= {2022},
  isbn		= {9781450397254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hanoi, Vietnam}
}

@Proceedings{	  10.1145/3580219,
  title		= {CCEAI '23: Proceedings of the 7th International Conference
		  on Control Engineering and Artificial Intelligence},
  year		= {2023},
  isbn		= {9781450397513},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sanya, China}
}

@Article{	  10.1109/taslp.2022.3155281,
  author	= {Wu, Junshuang and Zhang, Richong and Mao, Yongyi and Huai,
		  Jinpeng},
  title		= {Dealing With Hierarchical Types and Label Noise in
		  Fine-Grained Entity Typing},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3155281},
  doi		= {10.1109/TASLP.2022.3155281},
  abstract	= {Fine-Grained entity typing is complicated by the fact that
		  type labels form a hierarchical structure, and those
		  training examples usually contain noisy type labels. This
		  paper addresses these two issues by proposing a novel
		  framework that simultaneously models the correlation among
		  hierarchical types and the noise within the training data.
		  Additionally, the framework contains an innovative training
		  approach during which the noise in the training data is
		  progressively removed. Experiments on standard benchmarking
		  datasets validate the proposed framework and establish it
		  as a new state of the art for this problem.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {1305–1318},
  numpages	= {14}
}

@InProceedings{	  10.1145/3531146.3533161,
  author	= {Lu, Christina and Kay, Jackie and McKee, Kevin},
  title		= {Subverting machines, fluctuating identities: Re-learning
		  human categorization},
  year		= {2022},
  isbn		= {9781450393522},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3531146.3533161},
  doi		= {10.1145/3531146.3533161},
  abstract	= {Most machine learning systems that interact with humans
		  construct some notion of a person’s “identity,” yet
		  the default paradigm in AI research envisions identity with
		  essential attributes that are discrete and static. In stark
		  contrast, strands of thought within critical theory present
		  a conception of identity as malleable and constructed
		  entirely through interaction; a doing rather than a being.
		  In this work, we distill some of these ideas for machine
		  learning practitioners and introduce a theory of identity
		  as autopoiesis, circular processes of formation and
		  function. We argue that the default paradigm of identity
		  used by the field immobilizes existing identity categories
		  and the power differentials that co-occur, due to the
		  absence of iterative feedback to our models. This includes
		  a critique of emergent AI fairness practices that continue
		  to impose the default paradigm. Finally, we apply our
		  theory to sketch approaches to autopoietic identity through
		  multilevel optimization and relational learning. While
		  these ideas raise many open questions, we imagine the
		  possibilities of machines that are capable of expressing
		  human identity as a relationship perpetually in flux.},
  booktitle	= {Proceedings of the 2022 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {1005–1015},
  numpages	= {11},
  keywords	= {algorithmic fairness, identity systems, social
		  construction, theories of identity},
  location	= {Seoul, Republic of Korea},
  series	= {FAccT '22}
}

@Proceedings{	  10.1145/3614321,
  title		= {ICEGOV '23: Proceedings of the 16th International
		  Conference on Theory and Practice of Electronic
		  Governance},
  year		= {2023},
  isbn		= {9798400707421},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Belo Horizonte, Brazil}
}

@Article{	  10.1145/3590152,
  author	= {Wagner, Isabel},
  title		= {Privacy Policies across the Ages: Content of Privacy
		  Policies 1996–2021},
  year		= {2023},
  issue_date	= {August 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {26},
  number	= {3},
  issn		= {2471-2566},
  url		= {https://doi.org/10.1145/3590152},
  doi		= {10.1145/3590152},
  abstract	= {It is well known that most users do not read privacy
		  policies but almost always tick the box to agree with them.
		  While the length and readability of privacy policies have
		  been well studied and many approaches for policy analysis
		  based on natural language processing have been proposed,
		  existing studies are limited in their depth and scope,
		  often focusing on a small number of data practices at
		  single point in time. In this article, we fill this gap by
		  analyzing the 25-year history of privacy policies using
		  machine learning and natural language processing and
		  presenting a comprehensive analysis of policy contents.
		  Specifically, we collect a large-scale longitudinal corpus
		  of privacy policies from 1996 to 2021 and analyze their
		  content in terms of the data practices they describe, the
		  rights they grant to users, and the rights they reserve for
		  their organizations. We pay particular attention to changes
		  in response to recent privacy regulations such as the GDPR
		  and CCPA. We observe some positive changes, such as
		  reductions in data collection post-GDPR, but also a range
		  of concerning data practices, such as widespread implicit
		  data collection for which users have no meaningful choices
		  or access rights. Our work is an important step toward
		  making privacy policies machine readable on the user side,
		  which would help users match their privacy preferences
		  against the policies offered by web services.},
  journal	= {ACM Trans. Priv. Secur.},
  month		= may,
  articleno	= {32},
  numpages	= {32},
  keywords	= {Privacy policy, longitudinal study, natural language
		  processing, machine learning, neural networks, BERT}
}

@Proceedings{	  10.1145/3523227,
  title		= {RecSys '22: Proceedings of the 16th ACM Conference on
		  Recommender Systems},
  year		= {2022},
  isbn		= {9781450392785},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Seattle, WA, USA}
}

@Article{	  10.1145/3612921,
  author	= {Bensalem, Raja and Haddar, Kais and Blache, Philippe},
  title		= {An Arabic Probabilistic Parser Based on a Property
		  Grammar},
  year		= {2023},
  issue_date	= {October 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {10},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3612921},
  doi		= {10.1145/3612921},
  abstract	= {The specificities of Arabic parsing, such as
		  agglutination, vocalization, and the relatively order-free
		  words in Arabic sentences, remain major issues to consider.
		  To promote its robustness, such parseing should define
		  different types of constraints. Property Grammar (PG)
		  formalism verifies the satisfiability of the constraints
		  directly on the units of the structure, thanks to its
		  properties (or relations). In this context, we propose to
		  build a probabilistic parser with syntactic properties,
		  using a PG, and we measure the production rules in terms of
		  different implicit information and in particular the
		  syntactic properties. We experimented with our parser on
		  the treebank ATB, using the parsing algorithm CYK, and we
		  obtained encouraging results. Our method is also automatic
		  for implementation of most property types. Its
		  generalization for other languages or corpus domains (using
		  treebanks) could be a good perspective. Its combination
		  with pre-trained models of BERT may also make our parser
		  faster.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  articleno	= {237},
  numpages	= {25},
  keywords	= {Probabilistic parser, property grammar formalism, Arabic
		  language, lexicalized grammar}
}

@Proceedings{	  10.1145/3626705,
  title		= {MUM '23: Proceedings of the 22nd International Conference
		  on Mobile and Ubiquitous Multimedia},
  year		= {2023},
  isbn		= {9798400709210},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Vienna, Austria}
}

@Proceedings{	  10.1145/3557915,
  title		= {SIGSPATIAL '22: Proceedings of the 30th International
		  Conference on Advances in Geographic Information Systems},
  year		= {2022},
  isbn		= {9781450395298},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The conference started as a series of workshops and
		  symposia back in 1993 with the aim of promoting
		  interdisciplinary discussions among researchers,
		  developers, users, and practitioners and fostering research
		  in all aspects of geographic information systems,
		  especially in relation to novel systems based on geospatial
		  data and knowledge. It continues to provide a forum for
		  original research contributions covering all conceptual,
		  design and implementation aspects of geospatial data
		  ranging from applications, user interfaces and
		  visualization, to data storage, query processing, indexing,
		  machine learning and data mining. The conference is the
		  premier annual event of the ACM Special Interest Group on
		  Spatial Information (ACM SIGSPATIAL).},
  location	= {Seattle, Washington}
}

@Proceedings{	  10.1145/3560470,
  title		= {ICHMI '22: Proceedings of the 2022 International
		  Conference on Human Machine Interaction},
  year		= {2022},
  isbn		= {9781450396615},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@Article{	  10.14778/3611540.3611634,
  author	= {Halevy, Alon and Choi, Yejin and Floratou, Avrilia and
		  Franklin, Michael J. and Noy, Natasha and Wang, Haixun},
  title		= {Will LLMs Reshape, Supercharge, or Kill Data Science?
		  (VLDB 2023 Panel)},
  year		= {2023},
  issue_date	= {August 2023},
  publisher	= {VLDB Endowment},
  volume	= {16},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3611540.3611634},
  doi		= {10.14778/3611540.3611634},
  abstract	= {Large language models (LLMs) have recently taken the world
		  by storm, promising potentially game changing opportunities
		  in multiple fields. Naturally, there is significant promise
		  in applying LLMs to the management of structured data, or
		  more generally, to the processes involved in data science.
		  At the very least, LLMs have the potential to provide
		  substantial advancements in long-standing challenges that
		  our community has been tackling for decades. On the other
		  hand, they may introduce completely new capabilities that
		  we have only dreamed of thus far. This panel will bring
		  together a few leading experts who have been thinking about
		  these opportunities from various perspectives and fielding
		  them in research prototypes and even in commercial
		  applications.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {4114–4115},
  numpages	= {2}
}

@Article{	  10.1145/3473337,
  author	= {Pan, Yaoxin and Liang, Shangsong and Ren, Jiaxin and Meng,
		  Zaiqiao and Zhang, Qiang},
  title		= {Personalized, Sequential, Attentive, Metric-Aware Product
		  Search},
  year		= {2021},
  issue_date	= {April 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3473337},
  doi		= {10.1145/3473337},
  abstract	= {The task of personalized product search aims at retrieving
		  a ranked list of products given a user’s input query and
		  his/her purchase history. To address this task, we propose
		  the PSAM model, a Personalized, Sequential, Attentive and
		  Metric-aware (PSAM) model, that learns the semantic
		  representations of three different categories of entities,
		  i.e., users, queries, and products, based on user
		  sequential purchase historical data and the corresponding
		  sequential queries. Specifically, a query-based attentive
		  LSTM (QA-LSTM) model and an attention mechanism are
		  designed to infer users dynamic embeddings, which is able
		  to capture their short-term and long-term preferences. To
		  obtain more fine-grained embeddings of the three categories
		  of entities, a metric-aware objective is deployed in our
		  model to force the inferred embeddings subject to the
		  triangle inequality, which is a more realistic distance
		  measurement for product search. Experiments conducted on
		  four benchmark datasets show that our PSAM model
		  significantly outperforms the state-of-the-art product
		  search baselines in terms of effectiveness by up to 50.9%
		  improvement under NDCG@20. Our visualization experiments
		  further illustrate that the learned product embeddings are
		  able to distinguish different types of products.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= nov,
  articleno	= {36},
  numpages	= {29},
  keywords	= {Product search, personalized web search, neural networks,
		  LSTM, metric learning}
}

@InProceedings{	  10.1145/3581641.3584049,
  author	= {Prakash, Yash and Sunkara, Mohan and Lee, Hae-Na and
		  Jayarathna, Sampath and Ashok, Vikas},
  title		= {AutoDesc: Facilitating Convenient Perusal of Web Data
		  Items for Blind Users},
  year		= {2023},
  isbn		= {9798400701061},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3581641.3584049},
  doi		= {10.1145/3581641.3584049},
  abstract	= {Web data items such as shopping products, classifieds, and
		  job listings are indispensable components of most
		  e-commerce websites. The information on the data items are
		  typically distributed over two or more webpages, e.g., a
		  ‘Query-Results’ page showing the summaries of the
		  items, and ‘Details’ pages containing full information
		  about the items. While this organization of data mitigates
		  information overload and visual cluttering for sighted
		  users, it however increases the interaction overhead and
		  effort for blind users, as back-and-forth navigation
		  between webpages using screen reader assistive technology
		  is tedious and cumbersome. Existing usability-enhancing
		  solutions are unable to provide adequate support in this
		  regard as they predominantly focus on enabling efficient
		  content access within a single webpage, and as such are not
		  tailored for content distributed across multiple webpages.
		  As an initial step towards addressing this issue, we
		  developed AutoDesc, a browser extension that leverages a
		  custom extraction model to automatically detect and pull
		  out additional item descriptions from the ‘details’
		  pages, and then proactively inject the extracted
		  information into the ‘Query-Results’ page, thereby
		  reducing the amount of back-and-forth screen reader
		  navigation between the two webpages. In a study with 16
		  blind users, we observed that within the same time
		  duration, the participants were able to peruse
		  significantly more data items on average with AutoDesc,
		  compared to that with their preferred screen readers as
		  well as with a state-of-the-art solution.},
  booktitle	= {Proceedings of the 28th International Conference on
		  Intelligent User Interfaces},
  pages		= {32–45},
  numpages	= {14},
  keywords	= {Blind, Screen reader, Visual impairment, Web
		  accessibility},
  location	= {Sydney, NSW, Australia},
  series	= {IUI '23}
}

@Article{	  10.1145/3505243,
  author	= {Yang, Yanming and Xia, Xin and Lo, David and Grundy,
		  John},
  title		= {A Survey on Deep Learning for Software Engineering},
  year		= {2022},
  issue_date	= {January 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {10s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3505243},
  doi		= {10.1145/3505243},
  abstract	= {In 2006, Geoffrey Hinton proposed the concept of training
		  “Deep Neural Networks (DNNs)” and an improved model
		  training method to break the bottleneck of neural network
		  development. More recently, the introduction of AlphaGo in
		  2016 demonstrated the powerful learning ability of deep
		  learning and its enormous potential. Deep learning has been
		  increasingly used to develop state-of-the-art software
		  engineering (SE) research tools due to its ability to boost
		  performance for various SE tasks. There are many factors,
		  e.g., deep learning model selection, internal structure
		  differences, and model optimization techniques, that may
		  have an impact on the performance of DNNs applied in SE.
		  Few works to date focus on summarizing, classifying, and
		  analyzing the application of deep learning techniques in
		  SE. To fill this gap, we performed a survey to analyze the
		  relevant studies published since 2006. We first provide an
		  example to illustrate how deep learning techniques are used
		  in SE. We then conduct a background analysis (BA) of
		  primary studies and present four research questions to
		  describe the trend of DNNs used in SE (BA), summarize and
		  classify different deep learning techniques (RQ1), and
		  analyze the data processing including data collection, data
		  classification, data pre-processing, and data
		  representation (RQ2). In RQ3, we depicted a range of key
		  research topics using DNNs and investigated the
		  relationships between DL-based model adoption and multiple
		  factors (i.e., DL architectures, task types, problem types,
		  and data types). We also summarized commonly used datasets
		  for different SE tasks. In RQ4, we summarized the widely
		  used optimization algorithms and provided important
		  evaluation metrics for different problem types, including
		  regression, classification, recommendation, and generation.
		  Based on our findings, we present a set of current
		  challenges remaining to be investigated and outline a
		  proposed research road map highlighting key opportunities
		  for future work.},
  journal	= {ACM Comput. Surv.},
  month		= sep,
  articleno	= {206},
  numpages	= {73},
  keywords	= {Deep learning, neural network, machine learning, software
		  engineering, survey}
}

@InProceedings{	  10.1145/3485447.3512135,
  author	= {Chen, Zhendong and Hui, Siu Cheung and Zhuang, Fuzhen and
		  Liao, Lejian and Li, Fei and Jia, Meihuizi and Li, Jiaqi},
  title		= {EvidenceNet: Evidence Fusion Network for Fact
		  Verification},
  year		= {2022},
  isbn		= {9781450390965},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3485447.3512135},
  doi		= {10.1145/3485447.3512135},
  abstract	= {Fact verification is a challenging task that requires the
		  retrieval of multiple pieces of evidence from a reliable
		  corpus for verifying the truthfulness of a claim. Although
		  the current methods have achieved satisfactory performance,
		  they still suffer from one or more of the following three
		  problems: (1) unable to extract sufficient contextual
		  information from the evidence sentences; (2) containing
		  redundant evidence information and (3) incapable of
		  capturing the interaction between claim and evidence. To
		  tackle the problems, we propose an evidence fusion network
		  called EvidenceNet. The proposed EvidenceNet model captures
		  global contextual information from various levels of
		  evidence information for deep understanding. Moreover, a
		  gating mechanism is designed to filter out redundant
		  information in evidence. In addition, a symmetrical
		  interaction attention mechanism is also proposed for
		  identifying the interaction between claim and evidence. We
		  conduct extensive experiments based on the FEVER dataset.
		  The experimental results have shown that the proposed
		  EvidenceNet model outperforms the current fact verification
		  methods and achieves the state-of-the-art performance.},
  booktitle	= {Proceedings of the ACM Web Conference 2022},
  pages		= {2636–2645},
  numpages	= {10},
  keywords	= {fact verification, gating mechanism, symmetrical
		  interaction attention mechanism},
  location	= {Virtual Event, Lyon, France},
  series	= {WWW '22}
}

@Article{	  10.1145/3439800,
  author	= {Bi, Mingwen and Zhang, Qingchuan and Zuo, Min and Xu,
		  Zelong and Jin, Qingyu},
  title		= {Bi-directional Long Short-Term Memory Model with Semantic
		  Positional Attention for the Question Answering System},
  year		= {2021},
  issue_date	= {September 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {20},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3439800},
  doi		= {10.1145/3439800},
  abstract	= {The intelligent question answering system aims to provide
		  quick and concise feedback on the questions of users.
		  Although the performance of phrase-level and numerous
		  attention models have been improved, the sentence
		  components and position information are not emphasized
		  enough. This article combines Ci-Lin and word2vec to divide
		  all of the words in the question-answer pairs into groups
		  according to the semantics and select one kernel word in
		  each group. The remaining words are common words and
		  realize the semantic mapping mechanism between kernel words
		  and common words. With this Chinese semantic mapping
		  mechanism, the common words in all questions and answers
		  are replaced by the semantic kernel words to realize the
		  normalization of the semantic representation. Meanwhile,
		  based on the bi-directional LSTM model, this article
		  introduces a method of the combination of semantic role
		  labeling and positional context, dividing the sentence into
		  multiple semantic segments according to semantic logic. The
		  weight is given to the neighboring words in the same
		  semantic segment and propose semantic role labeling
		  position attention based on the bi-directional LSTM model
		  (BLSTM-SRLP). The good performance of the BLSTM-SRLP model
		  has been demonstrated in comparative experiments on the
		  food safety field dataset (FS-QA).},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {77},
  numpages	= {13},
  keywords	= {Question answering, BLSTM model, semantic positional-based
		  attention, Chinese semantic mapping mechanism}
}

@Proceedings{	  10.1145/3570945,
  title		= {IVA '23: Proceedings of the 23rd ACM International
		  Conference on Intelligent Virtual Agents},
  year		= {2023},
  isbn		= {9781450399944},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {This volume contains the papers presented at the 23nd
		  International Conference on Intelligent Virtual Agents (IVA
		  2023) located in W\"{u}rzburg, Germany, from 19. to
		  22.09.2023.},
  location	= {W\"{u}rzburg, Germany}
}

@Proceedings{	  10.1145/3594806,
  title		= {PETRA '23: Proceedings of the 16th International
		  Conference on PErvasive Technologies Related to Assistive
		  Environments},
  year		= {2023},
  isbn		= {9798400700699},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Corfu, Greece}
}

@InProceedings{	  10.1145/3483207.3483231,
  author	= {Lin, Yiquan and Xie, Hongtu},
  title		= {Learning Dense Entity-Aware Dialogue Intentions with
		  Rewritten Utterance for External Knowledge Documents
		  Retrieval},
  year		= {2021},
  isbn		= {9781450390170},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3483207.3483231},
  doi		= {10.1145/3483207.3483231},
  abstract	= {External knowledge-enhanced task-oriented dialogue systems
		  aim to cover user requests beyond pre-defined DBs/APIs.
		  Recently, existing dialogue systems have focused more on
		  retrieving external knowledge sources relevant to dialogue
		  contexts, achieving competitive results. However, due to
		  the lack of modeling entity-aware dialogue intention, such
		  dialogue systems are hard to accurately and efficiently
		  link the out-of-API functions in real-world scenarios. To
		  tackle this problem, this paper investigates learning dense
		  entity-aware dialogue intentions for external knowledge
		  documents retrieval in task-oriented dialogues. To this
		  end, we propose an intention-guided two-stage training
		  approach that includes intention-guided training and
		  knowledge transfer stages. This approach, which leverages
		  rewritten utterances that explicitly convey entity-aware
		  user intentions, can improve the performance of existing
		  Bi-Encoder retrievers such as DPR (Deep Passage Retriever).
		  In intention-guided training stage, a posterior history
		  encoder is initialized and guided by inputting rewritten
		  utterances for learning discriminative dense
		  representations. In knowledge transfer stage, these
		  representations are transferred to a newly initialized
		  prior encoder for inference via an extra intent consistency
		  loss. In addition, negative sampling in test knowledge
		  documents is used to learn more discriminative dense
		  representations of the unseen domain. The advantages of our
		  approach are no need for response annotations and extra
		  response generator, additionally, it provides great
		  scalability. The experimental results on augmented MultiWOZ
		  2.1 dataset show that our approach outperforms baseline
		  models except for relevance classifiers in retrieval
		  accuracy and has reasonably high efficiency.},
  booktitle	= {Proceedings of the 2021 4th International Conference on
		  Signal Processing and Machine Learning},
  pages		= {142–151},
  numpages	= {10},
  keywords	= {Bi-Encoder retrievers, External knowledge documents
		  retrieval, Intention-guided two-stage training,
		  Task-oriented dialogue systems},
  location	= {Beijing, China},
  series	= {SPML '21}
}

@InProceedings{	  10.1145/3477495.3531729,
  author	= {Lin, Tengteng and Chen, Qiaosheng and Cheng, Gong and
		  Soylu, Ahmet and Ell, Basil and Zhao, Ruoqi and Shi, Qing
		  and Wang, Xiaxia and Gu, Yu and Kharlamov, Evgeny},
  title		= {ACORDAR: A Test Collection for Ad Hoc Content-Based (RDF)
		  Dataset Retrieval},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531729},
  doi		= {10.1145/3477495.3531729},
  abstract	= {Ad hoc dataset retrieval is a trending topic in IR
		  research. Methods and systems are evolving from
		  metadata-based to content-based ones which exploit the data
		  itself for improving retrieval accuracy but thus far lack a
		  specialized test collection. In this paper, we build and
		  release the first test collection for ad hoc content-based
		  dataset retrieval, where content-oriented dataset queries
		  and content-based relevance judgments are annotated by
		  human experts who are assisted with a dashboard designed
		  specifically for comprehensively and conveniently browsing
		  both the metadata and data of a dataset. We conduct
		  extensive experiments on the test collection to analyze its
		  difficulty and provide insights into the underlying task.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2981–2991},
  numpages	= {11},
  keywords	= {ad hoc dataset retrieval, dataset browsing, dataset
		  search, rdf, test collection},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@Article{	  10.1145/3572905,
  author	= {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis,
		  Diomidis},
  title		= {Machine Learning for Software Engineering: A Tertiary
		  Study},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {12},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3572905},
  doi		= {10.1145/3572905},
  abstract	= {Machine learning (ML) techniques increase the
		  effectiveness of software engineering (SE) lifecycle
		  activities. We systematically collected, quality-assessed,
		  summarized, and categorized 83 reviews in ML for SE
		  published between 2009 and 2022, covering 6,117 primary
		  studies. The SE areas most tackled with ML are software
		  quality and testing, while human-centered areas appear more
		  challenging for ML. We propose a number of ML for SE
		  research challenges and actions, including conducting
		  further empirical validation and industrial studies on ML,
		  reconsidering deficient SE methods, documenting and
		  automating data collection and pipeline processes,
		  reexamining how industrial practitioners distribute their
		  proprietary data, and implementing incremental ML
		  approaches.},
  journal	= {ACM Comput. Surv.},
  month		= mar,
  articleno	= {256},
  numpages	= {39},
  keywords	= {Tertiary study, machine learning, software engineering,
		  systematic literature review}
}

@Article{	  10.5555/3586589.3586815,
  author	= {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan
		  and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and
		  Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob
		  and Hoffman, Matthew D. and Hormozdiari, Farhad and
		  Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and
		  Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and
		  McLean, Cory and Mincu, Diana and Mitani, Akinori and
		  Montanari, Andrea and Nado, Zachary and Natarajan, Vivek
		  and Nielson, Christopher and Osborne, Thomas F. and Raman,
		  Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff,
		  Jessica and Seneviratne, Martin and Sequeira, Shannon and
		  Suresh, Harini and Veitch, Victor and Vladymyrov, Max and
		  Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and
		  Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  title		= {Underspecification presents challenges for credibility in
		  modern machine learning},
  year		= {2022},
  issue_date	= {January 2022},
  publisher	= {JMLR.org},
  volume	= {23},
  number	= {1},
  issn		= {1532-4435},
  abstract	= {Machine learning (ML) systems often exhibit unexpectedly
		  poor behavior when they are deployed in real-world domains.
		  We identify underspecification in ML pipelines as a key
		  reason for these failures. An ML pipeline is the full
		  procedure followed to train and validate a predictor. Such
		  a pipeline is underspecified when it can return many
		  distinct predictors with equivalently strong test
		  performance. Underspecification is common in modern ML
		  pipelines that primarily validate predictors on held-out
		  data that follow the same distribution as the training
		  data. Predictors returned by underspecified pipelines are
		  often treated as equivalent based on their training domain
		  performance, but we show here that such predictors can
		  behave very differently in deployment domains. This
		  ambiguity can lead to instability and poor model behavior
		  in practice, and is a distinct failure mode from previously
		  identified issues arising from structural mismatch between
		  training and deployment domains. We provide evidence that
		  underspecfication has substantive implications for
		  practical ML pipelines, using examples from computer
		  vision, medical imaging, natural language processing,
		  clinical risk prediction based on electronic health
		  records, and medical genomics. Our results show the need to
		  explicitly account for underspecification in modeling
		  pipelines that are intended for real-world deployment in
		  any domain.},
  journal	= {J. Mach. Learn. Res.},
  month		= jan,
  articleno	= {226},
  numpages	= {61},
  keywords	= {distribution shift, spurious correlation, fairness,
		  identifiability, computer vision, natural language
		  processing, medical imaging, electronic health records,
		  genomics}
}

@Proceedings{	  10.1145/3558489,
  title		= {PROMISE 2022: Proceedings of the 18th International
		  Conference on Predictive Models and Data Analytics in
		  Software Engineering},
  year		= {2022},
  isbn		= {9781450398602},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our pleasure to welcome you to the 18th ACM
		  International Conference on Predictive Models and Data
		  Analytics in Software Engineering (PROMISE 2022), to be
		  held in hybrid mode (physically and virtually) on November
		  18th, 2022, co-located with the ACM Joint European Software
		  Engineering Conference and Symposium on the Foundations of
		  Software Engineering (ESEC/FSE 2022). PROMISE is an annual
		  forum for researchers and practitioners to present, discuss
		  and exchange ideas, results, expertise and experiences in
		  the construction and/or application of predictive models
		  and data analytics in software engineering. Such models and
		  analyses could be targeted at planning, design,
		  implementation, testing, maintenance, quality assurance,
		  evaluation, process improvement, management, decision
		  making, and risk assessment in software and systems
		  development. This year PROMISE received a total of 18 paper
		  submissions. The review process was double blind and each
		  paper was reviewed by at least three members of the program
		  committee. An online discussion was also held for 8 days.
		  Based on this procedure, we accepted a total of 10 full
		  papers, which will be presented in 3 technical sessions.
		  The acceptance criteria were entirely based on the quality
		  of the papers, without imposing any constraint on the
		  number of papers to be accepted.
		  
		  We are delighted to announce an outstanding keynote:
		  Release Engineering in the AI World: How can Analytics
		  Help? By Prof. Bram Adams, Queen’s University, Canada
		  
		  We would like to thank all authors for submitting high
		  quality papers, and program committee members for their
		  timely and accurate reviewing activity. Last, but not
		  least, we would like to thank the FSE 2022 organizers for
		  hosting PROMISE 2022 as a co-located event and for their
		  logistic support in the organization of the conference.
		  
		  We hope you will enjoy PROMISE 2022. We certainly will!
		  
		  Many thanks from Shane McIntosh (General Chair), Gema
		  Rodriguez-Perez and Weiyi Shang (Program Chairs).},
  location	= {Singapore, Singapore}
}

@InProceedings{	  10.1145/3459637.3482440,
  author	= {Sheng, Qiang and Zhang, Xueyao and Cao, Juan and Zhong,
		  Lei},
  title		= {Integrating Pattern- and Fact-based Fake News Detection
		  via Model Preference Learning},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3482440},
  doi		= {10.1145/3459637.3482440},
  abstract	= {To defend against fake news, researchers have developed
		  various methods based on texts. These methods can be
		  grouped as 1) pattern-based methods, which focus on shared
		  patterns among fake news posts rather than the claim
		  itself; and 2) fact-based methods, which retrieve from
		  external sources to verify the claim's veracity without
		  considering patterns. The two groups of methods, which have
		  different preferences of textual clues, actually play
		  complementary roles in detecting fake news. However, few
		  works consider their integration. In this paper, we study
		  the problem of integrating pattern- and fact-based models
		  into one framework via modeling their preference
		  differences, i.e., making the pattern- and fact-based
		  models focus on respective preferred parts in a post and
		  mitigate interference from non-preferred parts as possible.
		  To this end, we build a Preference-aware Fake News
		  Detection Framework (Pref-FEND), which learns the
		  respective preferences of pattern- and fact-based models
		  for joint detection. We first design a heterogeneous
		  dynamic graph convolutional network to generate the
		  respective preference maps, and then use these maps to
		  guide the joint learning of pattern- and fact-based models
		  for final prediction. Experiments on two real-world
		  datasets show that Pref-FEND effectively captures model
		  preferences and improves the performance of models based on
		  patterns, facts, or both.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {1640–1650},
  numpages	= {11},
  keywords	= {fact-checking, fake news detection, graph neural networks,
		  pattern mining, preference learning},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@InProceedings{	  10.1145/3477495.3531737,
  author	= {Alexander, Daria and Kusa, Wojciech and P. de Vries,
		  Arjen},
  title		= {ORCAS-I: Queries Annotated with Intent using Weak
		  Supervision},
  year		= {2022},
  isbn		= {9781450387323},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477495.3531737},
  doi		= {10.1145/3477495.3531737},
  abstract	= {User intent classification is an important task in
		  information retrieval. In this work, we introduce a revised
		  taxonomy of user intent. We take the widely used
		  differentiation between navigational, transactional and
		  informational queries as a starting point, and identify
		  three different sub-classes for the informational queries:
		  instrumental, factual and abstain. The resulting
		  classification of user queries is more fine-grained,
		  reaches a high level of consistency between annotators, and
		  can serve as the basis for an effective automatic
		  classification process. The newly introduced categories
		  help distinguish between types of queries that a retrieval
		  system could act upon, for example by prioritizing
		  different types of results in the ranking. We have used a
		  weak supervision approach based on Snorkel to annotate the
		  ORCAS dataset according to our new user intent taxonomy,
		  utilising established heuristics and keywords to construct
		  rules for the prediction of the intent category. We then
		  present a series of experiments with a variety of machine
		  learning models, using the labels from the weak supervision
		  stage as training data, but find that the results produced
		  by Snorkel are not outperformed by these competing
		  approaches and can be considered state-of-the-art. The
		  advantage of a rule-based approach like Snorkel's is its
		  efficient deployment in an actual system, where intent
		  classification would be executed for every query issued.
		  The resource released with this paper is the ORCAS-I
		  dataset: a labelled version of the ORCAS click-based
		  dataset of Web queries, which provides 18 million
		  connections to 10 million distinct queries. We anticipate
		  the usage of this resource in a scenario where the
		  retrieval system would change its internal workings and
		  search user interface to match the type of information
		  request. For example, a navigational query could trigger
		  just a short result list; and, for instrumental intent the
		  system could rank tutorials and instructions higher than
		  for other types of queries.},
  booktitle	= {Proceedings of the 45th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3057–3066},
  numpages	= {10},
  keywords	= {click data, intent labelling, snorkel, weak supervision,
		  web search},
  location	= {Madrid, Spain},
  series	= {SIGIR '22}
}

@InProceedings{	  10.1145/3558489.3559074,
  author	= {Mohamad, Mazen and Stegh\"{o}fer, Jan-Philipp and
		  \r{A}str\"{o}m, Alexander and Scandariato, Riccardo},
  title		= {Identifying security-related requirements in regulatory
		  documents based on cross-project classification},
  year		= {2022},
  isbn		= {9781450398602},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3558489.3559074},
  doi		= {10.1145/3558489.3559074},
  abstract	= {Security is getting substantial focus in many industries,
		  especially safety-critical ones. When new regulations and
		  standards which can run to hundreds of pages are
		  introduced, it is necessary to identify the requirements in
		  those documents which have an impact on security.
		  Additionally, it is necessary to revisit the requirements
		  of existing systems and identify the security related ones.
		  We investigate the feasibility of using a classifier for
		  security-related requirements trained on requirement
		  specifications available online. We base our investigation
		  on 15 requirement documents, randomly selected and
		  partially pre-labelled, with a total of 3,880 requirements.
		  To validate the model, we run a cross-project prediction on
		  the data where each specification constitutes a group. We
		  also test the model on three different United Nations (UN)
		  regulations from the automotive domain with different
		  magnitudes of security relevance. Our results indicate the
		  feasibility of training a model from a heterogeneous data
		  set including specifications from multiple domains and in
		  different styles. Additionally, we show the ability of such
		  a classifier to identify security requirements in real-life
		  regulations and discuss scenarios in which such a
		  classification becomes useful to practitioners.},
  booktitle	= {Proceedings of the 18th International Conference on
		  Predictive Models and Data Analytics in Software
		  Engineering},
  pages		= {82–91},
  numpages	= {10},
  keywords	= {Automated Requirements Engineering, Machine Learning,
		  Requirements Classification, Security Requirements},
  location	= {Singapore, Singapore},
  series	= {PROMISE 2022}
}

@InProceedings{	  10.1145/3459637.3481909,
  author	= {Liu, Peiyang and Wang, Xi and Wang, Lin and Ye, Wei and
		  Xi, Xiangyu and Zhang, Shikun},
  title		= {Distilling Knowledge from BERT into Simple Fully Connected
		  Neural Networks for Efficient Vertical Retrieval},
  year		= {2021},
  isbn		= {9781450384469},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3459637.3481909},
  doi		= {10.1145/3459637.3481909},
  abstract	= {Distilled BERT models are more suitable for efficient
		  vertical retrieval in online sponsored vertical search with
		  low-latency requirements than BERT due to fewer parameters
		  and faster inference. Unfortunately, most of these models
		  are still far from ideal inference speed. This paper
		  presents a novel and effective method to distill knowledge
		  from BERT into simple fully connected neural networks
		  (FNN). Results of extensive experiments on English and
		  Chinese datasets demonstrate that our method achieves
		  comparable results with existing distilled BERT models
		  while the inference is accelerated by more than ten times.
		  We have successfully applied our method on our online
		  sponsored vertical search engine and get remarkable
		  improvements.},
  booktitle	= {Proceedings of the 30th ACM International Conference on
		  Information &amp; Knowledge Management},
  pages		= {3965–3975},
  numpages	= {11},
  keywords	= {bert, knowledge distillation, sponsored search, vertical
		  retrieval},
  location	= {Virtual Event, Queensland, Australia},
  series	= {CIKM '21}
}

@Proceedings{	  10.1145/3623809,
  title		= {HAI '23: Proceedings of the 11th International Conference
		  on Human-Agent Interaction},
  year		= {2023},
  isbn		= {9798400708244},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Gothenburg, Sweden}
}

@Proceedings{	  10.1145/3613372,
  title		= {SBES '23: Proceedings of the XXXVII Brazilian Symposium on
		  Software Engineering},
  year		= {2023},
  isbn		= {9798400707872},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Campo Grande, Brazil}
}

@Article{	  10.1145/3476106,
  author	= {Zhao, Jiashu and Huang, Jimmy Xiangji and Deng, Hongbo and
		  Chang, Yi and Xia, Long},
  title		= {Are Topics Interesting or Not? An LDA-based Topic-graph
		  Probabilistic Model for Web Search Personalization},
  year		= {2022},
  issue_date	= {July 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {40},
  number	= {3},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3476106},
  doi		= {10.1145/3476106},
  abstract	= {In this article, we propose a Latent Dirichlet
		  Allocation– (LDA) based topic-graph probabilistic
		  personalization model for Web search. This model represents
		  a user graph in a latent topic graph and simultaneously
		  estimates the probabilities that the user is interested in
		  the topics, as well as the probabilities that the user is
		  not interested in the topics. For a given query issued by
		  the user, the webpages that have higher relevancy to the
		  interested topics are promoted, and the webpages more
		  relevant to the non-interesting topics are penalized. In
		  particular, we simulate a user’s search intent by
		  building two profiles: A positive user profile for the
		  probabilities of the user is interested in the topics and a
		  corresponding negative user profile for the probabilities
		  of being not interested in the the topics. The profiles are
		  estimated based on the user’s search logs. A clicked
		  webpage is assumed to include interesting topics. A skipped
		  (viewed but not clicked) webpage is assumed to cover some
		  non-interesting topics to the user. Such estimations are
		  performed in the latent topic space generated by LDA.
		  Moreover, a new approach is proposed to estimate the
		  correlation between a given query and the user’s search
		  history so as to determine how much personalization should
		  be considered for the query. We compare our proposed models
		  with several strong baselines including state-of-the-art
		  personalization approaches. Experiments conducted on a
		  large-scale real user search log collection illustrate the
		  effectiveness of the proposed models.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= dec,
  articleno	= {51},
  numpages	= {24},
  keywords	= {Personalization, probabilistic model, Web search, Latent
		  Dirichlet Allocation (LDA), topic-graph}
}

@InProceedings{	  10.1145/3534678.3539187,
  author	= {Srivastava, Aseem and Suresh, Tharun and Lord, Sarah P.
		  and Akhtar, Md Shad and Chakraborty, Tanmoy},
  title		= {Counseling Summarization Using Mental Health Knowledge
		  Guided Utterance Filtering},
  year		= {2022},
  isbn		= {9781450393850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3534678.3539187},
  doi		= {10.1145/3534678.3539187},
  abstract	= {The psychotherapy intervention technique is a multifaceted
		  conversation between a therapist and a patient. Unlike
		  general clinical discussions, psychotherapy's core
		  components (viz. symptoms) are hard to distinguish, thus
		  becoming a complex problem to summarize later. A structured
		  counseling conversation may contain discussions about
		  symptoms, history of mental health issues, or the discovery
		  of the patient's behavior. It may also contain discussion
		  filler words irrelevant to a clinical summary. We refer to
		  these elements of structured psychotherapy as counseling
		  components. In this paper, the aim is mental health
		  counseling summarization to build upon domain knowledge and
		  to help clinicians quickly glean meaning. We create a new
		  dataset after annotating 12.9K utterances of counseling
		  components and reference summaries for each dialogue.
		  Further, we propose ConSum, a novel counseling-component
		  guided summarization model. ConSum undergoes three
		  independent modules. First, to assess the presence of
		  depressive symptoms, it filters utterances utilizing the
		  Patient Health Questionnaire (PHQ-9), while the second and
		  third modules aim to classify counseling components. At
		  last, we propose a problem-specific Mental Health
		  Information Capture (MHIC) evaluation metric for counseling
		  summaries. Our comparative study shows that we improve on
		  performance and generate cohesive, semantic, and coherent
		  summaries. We comprehensively analyze the generated
		  summaries to investigate the capturing of psychotherapy
		  elements. Human and clinical evaluations on the summary
		  show that ConSum generates quality summary. Further, mental
		  health experts validate the clinical acceptability of the
		  ConSum. Lastly, we discuss the uniqueness in mental health
		  counseling summarization in the real world and show
		  evidences of its deployment on an online application with
		  the support of mpathic.ai},
  booktitle	= {Proceedings of the 28th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3920–3930},
  numpages	= {11},
  keywords	= {dialogue summarization, natural language processing},
  location	= {Washington DC, USA},
  series	= {KDD '22}
}

@Article{	  10.1145/3512768,
  author	= {Sworna, Zarrin Tasnim and Islam, Chadni and Babar,
		  Muhammad Ali},
  title		= {APIRO: A Framework for Automated Security Tools API
		  Recommendation},
  year		= {2023},
  issue_date	= {January 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {32},
  number	= {1},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3512768},
  doi		= {10.1145/3512768},
  abstract	= {Security Orchestration, Automation, and Response (SOAR)
		  platforms integrate and orchestrate a wide variety of
		  security tools to accelerate the operational activities of
		  Security Operation Center (SOC). Integration of security
		  tools in a SOAR platform is mostly done manually using
		  APIs, plugins, and scripts. SOC teams need to navigate
		  through API calls of different security tools to find a
		  suitable API to define or update an incident response
		  action. Analyzing various types of API documentation with
		  diverse API format and presentation structure involves
		  significant challenges such as data availability, data
		  heterogeneity, and semantic variation for automatic
		  identification of security tool APIs specific to a
		  particular task. Given these challenges can have negative
		  impact on SOC team’s ability to handle security incident
		  effectively and efficiently, we consider it important to
		  devise suitable automated support solutions to address
		  these challenges. We propose a novel learning-based
		  framework for automated security tool API Recommendation
		  for security Orchestration, automation, and response,
		  APIRO. To mitigate data availability constraint, APIRO
		  enriches security tool API description by applying a wide
		  variety of data augmentation techniques. To learn data
		  heterogeneity of the security tools and semantic variation
		  in API descriptions, APIRO consists of an API-specific word
		  embedding model and a Convolutional Neural Network (CNN)
		  model that are used for prediction of top three relevant
		  APIs for a task. We experimentally demonstrate the
		  effectiveness of APIRO in recommending APIs for different
		  tasks using three security tools and 36 augmentation
		  techniques. Our experimental results demonstrate the
		  feasibility of APIRO for achieving 91.9% Top-1 Accuracy.
		  Compared to the state-of-the-art baseline, APIRO is 26.93%,
		  23.03%, and 20.87% improved in terms of Top-1, Top-2, and
		  Top-3 Accuracy and outperforms the baseline by 23.7% in
		  terms of Mean Reciprocal Rank (MRR).},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= feb,
  articleno	= {24},
  numpages	= {42},
  keywords	= {Security Orchestration, Incident Response Plan, security
		  tool API, Security Operation Center, API Recommendation,
		  SOAR}
}

@InProceedings{	  10.1145/3524481.3527229,
  author	= {Liu, Yu and Yandrapally, Rahulkrishna and Kalia, Anup K.
		  and Sinha, Saurabh and Tzoref-Brill, Rachel and Mesbah,
		  Ali},
  title		= {CrawLabel: computing natural-language labels for UI test
		  cases},
  year		= {2022},
  isbn		= {9781450392860},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3524481.3527229},
  doi		= {10.1145/3524481.3527229},
  abstract	= {End-to-end test cases that exercise the application under
		  test via its user interface (UI) are known to be hard for
		  developers to read and understand; consequently, diagnosing
		  failures in these tests and maintaining them can be
		  tedious. Techniques for computing natural-language
		  descriptions of test cases can help increase test
		  readability. However, so far, such techniques have been
		  developed for unit test cases; they are not applicable to
		  end-to-end test cases.In this paper, we focus on the
		  problem of computing natural-language labels for the steps
		  of end-to-end UI test cases for web applications. We
		  present two techniques that apply natural-language
		  processing to information available in the browser document
		  object model (DOM). The first technique is an instance of a
		  supervised approach in which labeling-relevant DOM
		  attributes are ranked via manual analysis and fed into
		  label computation. However, supervised approach requires a
		  training dataset. So we propose the second technique, which
		  is unsupervised: it leverages probabilistic context-free
		  grammar learning to compute dominant DOM attributes
		  automatically. We implemented these techniques, along with
		  two simpler baseline techniques, in a tool called CrawLabel
		  (available as a plugin to Crawljax, a state-of-the-art UI
		  test-generation tool for web applications) and evaluated
		  their effectiveness on open-source web applications. Our
		  results indicate that the supervised approach can achieve
		  precision, recall, and Fl-score of 83.38, 60.64, and 66.40,
		  respectively. The unsupervised approach, although less
		  effective, is competitive, achieving scores of 72.37,
		  58.12, and 59.77. We highlight key results and discuss the
		  implications of our findings.},
  booktitle	= {Proceedings of the 3rd ACM/IEEE International Conference
		  on Automation of Software Test},
  pages		= {103–114},
  numpages	= {12},
  location	= {Pittsburgh, Pennsylvania},
  series	= {AST '22}
}

@Proceedings{	  10.1145/3568364,
  title		= {WSSE '22: Proceedings of the 4th World Symposium on
		  Software Engineering},
  year		= {2022},
  isbn		= {9781450396950},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@Proceedings{	  10.1145/3582197,
  title		= {ICIT '22: Proceedings of the 2022 10th International
		  Conference on Information Technology: IoT and Smart City},
  year		= {2022},
  isbn		= {9781450397438},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shanghai, China}
}

@Article{	  10.1109/taslp.2022.3210442,
  author	= {Liu, Jian and Chen, Yufeng and Xu, Jinan},
  title		= {MRCAug: Data Augmentation via Machine Reading
		  Comprehension for Document-Level Event Argument
		  Extraction},
  year		= {2023},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2022.3210442},
  doi		= {10.1109/TASLP.2022.3210442},
  abstract	= {Document-level event argument extraction (EAE) is a
		  critical event semantic understanding task that requires a
		  model to identify an event's global arguments beyond the
		  sentence level. Existing approaches to this problem are
		  based on supervised learning, which require a large amount
		  of labeled data for model training. However, due to the
		  complicated structure of an event, human annotation for
		  this task is costly, and the issue of inadequacy of
		  training data has long hampered the study. In this study,
		  we propose a novel approach to mitigating the data sparsity
		  problem faced by document-level EAE, by linking the task
		  with machine reading comprehension (MRC). Particularly, we
		  devise two data augmentation regimes via MRC, including an
		  implicit knowledge transfer method, which enables knowledge
		  transfer from other tasks to the document-level EAE task,
		  and an explicit data generation method, which can
		  explicitly generate new training examples by treating a
		  pre-trained MRC model as an annotator. Furthermore, we
		  propose a self-training based noise reduction strategy that
		  can effectively addresses the out-of-domain noise
		  introduced by the data augmentation methods. The extensive
		  assessments on three benchmarks have validated the
		  effectiveness of our approach — it not only achieves
		  state-of-the-art performance but also demonstrates superior
		  results in the data-low scenario.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {3160–3172},
  numpages	= {13}
}

@Article{	  10.1109/taslp.2021.3065234,
  author	= {Xie, Huang and Virtanen, Tuomas},
  title		= {Zero-Shot Audio Classification Via Semantic Embeddings},
  year		= {2021},
  issue_date	= {2021},
  publisher	= {IEEE Press},
  volume	= {29},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3065234},
  doi		= {10.1109/TASLP.2021.3065234},
  abstract	= {In this paper, we study zero-shot learning in audio
		  classification via semantic embeddings extracted from
		  textual labels and sentence descriptions of sound classes.
		  Our goal is to obtain a classifier that is capable of
		  recognizing audio instances of sound classes that have no
		  available training samples, but only semantic side
		  information. We employ a bilinear compatibility framework
		  to learn an acoustic-semantic projection between
		  intermediate-level representations of audio instances and
		  sound classes, i.e., acoustic embeddings and semantic
		  embeddings. We use VGGish to extract deep acoustic
		  embeddings from audio clips, and pre-trained language
		  models (Word2Vec, GloVe, BERT) to generate either label
		  embeddings from textual labels or sentence embeddings from
		  sentence descriptions of sound classes. Audio
		  classification is performed by a linear compatibility
		  function that measures how compatible an acoustic embedding
		  and a semantic embedding are. We evaluate the proposed
		  method on a small balanced dataset ESC-50 and a large-scale
		  unbalanced audio subset of AudioSet. The experimental
		  results show that classification performance is
		  significantly improved by involving sound classes that are
		  semantically close to the test classes in training.
		  Meanwhile, we demonstrate that both label embeddings and
		  sentence embeddings are useful for zero-shot learning.
		  Classification performance is improved by concatenating
		  label/sentence embeddings generated with different language
		  models. With their hybrid concatenations, the results are
		  improved further.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {1233–1242},
  numpages	= {10}
}

@Article{	  10.1145/3616017,
  author	= {Smith, Ronnie and Dragone, Mauro},
  title		= {Generalisable Dialogue-based Approach for Active Learning
		  of Activities of Daily Living},
  year		= {2023},
  issue_date	= {September 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {13},
  number	= {3},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3616017},
  doi		= {10.1145/3616017},
  abstract	= {While Human Activity Recognition systems may benefit from
		  Active Learning by allowing users to self-annotate their
		  Activities of Daily Living (ADLs), many proposed methods
		  for collecting such annotations are for short-term data
		  collection campaigns for specific datasets. We present a
		  reusable dialogue-based approach to user interaction for
		  active learning in activity recognition systems, which
		  utilises semantic similarity measures and a dataset of
		  natural language descriptions of common activities (which
		  we make publicly available). Our approach involves
		  system-initiated dialogue, including follow-up questions to
		  reduce ambiguity in user responses where appropriate. We
		  apply this approach to two active learning scenarios: (i)
		  using an existing CASAS dataset, demonstrating long-term
		  usage; and (ii) using an online activity recognition
		  system, which tackles the issue of online segmentation and
		  labelling. We demonstrate our work in context, in which a
		  natural language interface provides knowledge that can help
		  interpret other multi-modal sensor data. We provide results
		  highlighting the potential of our dialogue- and semantic
		  similarity-based approach. We evaluate our work: (i)
		  quantitatively, as an efficient way to seek users’ input
		  for active learning of ADLs; and (ii) qualitatively,
		  through a user study in which users were asked to compare
		  our approach and an established method. Results show the
		  potential of our approach as a hands-free interface for
		  annotation of sensor data as part of an active learning
		  system. We provide insights into the challenges of active
		  learning for activity recognition under real-world
		  conditions and identify potential ways to address them.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= sep,
  articleno	= {18},
  numpages	= {37},
  keywords	= {Human-in-the-Loop (HITL) annotation, Active Learning (AL),
		  natural language, semantic similarity, Human Activity
		  Recognition (HAR) labelling}
}

@Proceedings{	  10.1145/3589462,
  title		= {IDEAS '23: Proceedings of the 27th International Database
		  Engineered Applications Symposium},
  year		= {2023},
  isbn		= {9798400707445},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Heraklion, Crete, Greece}
}

@InProceedings{	  10.1145/3626705.3627775,
  author	= {Gallo, Simone and Paterno, Fabio and Malizia, Alessio},
  title		= {Conversational Interfaces in IoT Ecosystems: Where We Are,
		  What Is Still Missing},
  year		= {2023},
  isbn		= {9798400709210},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626705.3627775},
  doi		= {10.1145/3626705.3627775},
  abstract	= {In the last few years, text and voice-based conversational
		  agents have become more and more popular all over the world
		  as virtual assistants for a variety of tasks. In addition,
		  the deployment on the market of many smart objects
		  connected with these agents has introduced the possibility
		  of controlling and personalising the behaviour of several
		  connected objects using natural language. This has the
		  potential to allow people, also those without a technical
		  background, to effectively control and use the wide variety
		  of connected objects and services. In this paper, we
		  present an analysis of how conversational agents have been
		  used to interact with smart environments (such as smart
		  homes). For this purpose, we have carried out a systematic
		  literature review considering publications selected from
		  the ACM and IEEE digital libraries to investigate the
		  technologies used to design and develop conversational
		  agents for IoT settings, including Artificial Intelligence
		  techniques, the purpose that they have been used for, and
		  the level of user involvement in such studies. The
		  resulting analysis is useful to better understand how this
		  field is evolving and indicate the challenges still open in
		  this area that should be addressed in future research work
		  to allow people to completely benefit from this type of
		  solution.},
  booktitle	= {Proceedings of the 22nd International Conference on Mobile
		  and Ubiquitous Multimedia},
  pages		= {279–293},
  numpages	= {15},
  keywords	= {Conversational Agents, Internet of Things, User
		  Experience},
  location	= {Vienna, Austria},
  series	= {MUM '23}
}

@InProceedings{	  10.1145/3490099.3511130,
  author	= {Smith, Ronnie and Dragone, Mauro},
  title		= {A Dialogue-Based Interface for Active Learning of
		  Activities of Daily Living},
  year		= {2022},
  isbn		= {9781450391443},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3490099.3511130},
  doi		= {10.1145/3490099.3511130},
  abstract	= {While Human Activity Recognition (HAR) systems may benefit
		  from Active Learning (AL) by allowing users to
		  self-annotate their Activities of Daily Living (ADLs), many
		  proposed methods for collecting such annotations are for
		  short-term data collection campaigns for specific datasets.
		  We present a reusable dialogue-based approach to user
		  interaction for active learning in HAR systems, which
		  utilises a dataset of natural language descriptions of
		  common activities (which we make publicly available) and
		  semantic similarity measures. Our approach involves
		  system-initiated dialogue, including follow-up questions to
		  reduce ambiguity in user responses where appropriate. We
		  apply our work to an existing CASAS dataset in an active
		  learning scenario, to demonstrate our work in context, in
		  which a natural language interface provides knowledge that
		  can help interpret other multi-modal sensor data. We
		  provide results highlighting the potential of our dialogue-
		  and semantic similarity-based approach. We evaluate our
		  work: (i) technically, as an effective way to seek users’
		  input for active learning of ADLs; and (ii) qualitatively,
		  through a user study in which users were asked to use our
		  approach and an established method, and to subsequently
		  compare the two. Results show the potential of our approach
		  as a user-friendly mechanism for annotation of sensor data
		  as part of an active learning system.},
  booktitle	= {Proceedings of the 27th International Conference on
		  Intelligent User Interfaces},
  pages		= {820–831},
  numpages	= {12},
  keywords	= {Active Learning (AL), Human Activity Recognition (HAR)
		  labelling, Human-in-the-Loop (HITL) annotation, natural
		  language, semantic similarity},
  location	= {Helsinki, Finland},
  series	= {IUI '22}
}

@Article{	  10.1145/3533020,
  author	= {Sun, Kai and Zhang, Richong and Mensah, Samuel and Mao,
		  Yongyi and Liu, Xudong},
  title		= {Learning Implicit and Explicit Multi-task Interactions for
		  Information Extraction},
  year		= {2023},
  issue_date	= {April 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {41},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3533020},
  doi		= {10.1145/3533020},
  abstract	= {Information extraction aims at extracting entities,
		  relations, and so on, in text to support information
		  retrieval systems. To extract information, researchers have
		  considered multitask learning (ML) approaches. The
		  conventional ML approach learns shared features across
		  tasks, with the assumption that these features capture
		  sufficient task interactions to learn expressive shared
		  representations for task classification. However, such an
		  assumption is flawed in different perspectives. First, the
		  shared representation may contain noise introduced by
		  another task; tasks coupled for multitask learning may have
		  different complexities but this approach treats all tasks
		  equally; the conventional approach has a flat structure
		  that hinders the learning of explicit interactions. This
		  approach, however, learns implicit interactions across
		  tasks and often has a generalization ability that has
		  benefited the learning of multitasks. In this article, we
		  take advantage of implicit interactions learned by
		  conventional approaches while alleviating the issues
		  mentioned above by developing a Recurrent Interaction
		  Network with an effective Early Prediction Integration
		  (RIN-EPI) for multitask learning. Specifically, RIN-EPI
		  learns implicit and explicit interactions across two
		  different but related tasks. To effectively learn explicit
		  interactions across tasks, we consider the correlations
		  among the outputs of related tasks. It is, however, obvious
		  that task outputs are unobservable during training, so we
		  leverage the predictions at intermediate layers (referred
		  to as early predictions) as proxies as well as shared
		  features across tasks to learn explicit interactions
		  through attention mechanisms and sequence learning models.
		  By recurrently learning explicit interactions, we gradually
		  improve predictions for the individual tasks in the
		  multitask learning. We demonstrate the effectiveness of
		  RIN-EPI on the learning of two mainstream multitasks for
		  information extraction: (1) entity recognition and relation
		  classification and (2) aspect and opinion term
		  co-extraction. Extensive experiments demonstrate the
		  effectiveness of the RIN-EPI architecture, where we achieve
		  state-of-the-art results on several benchmark datasets.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= apr,
  articleno	= {27},
  numpages	= {29},
  keywords	= {Multitask learning, information extraction}
}

@Proceedings{	  10.1145/3540250,
  title		= {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European
		  Software Engineering Conference and Symposium on the
		  Foundations of Software Engineering},
  year		= {2022},
  isbn		= {9781450394130},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {On behalf of all members of the organizing committee, we
		  are delighted to welcome everyone to the ACM Joint European
		  Software Engineering Conference and Symposium on the
		  Foundations of Software Engineering (ESEC/FSE) 2022. The
		  event continues the long, distinguished ESEC/FSE tradition
		  of presenting the most innovative research, and
		  facilitating interactions between scientists and engineers
		  who are passionate about advancing the theory and practice
		  of software engineering.},
  location	= {Singapore, Singapore}
}

@Article{	  10.1109/taslp.2021.3138670,
  author	= {Li, Qian and Peng, Hao and Li, Jianxin and Wu, Jia and
		  Ning, Yuanxing and Wang, Lihong and Yu, Philip S. and Wang,
		  Zheng},
  title		= {Reinforcement Learning-Based Dialogue Guided Event
		  Extraction to Exploit Argument Relations},
  year		= {2022},
  issue_date	= {2022},
  publisher	= {IEEE Press},
  volume	= {30},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2021.3138670},
  doi		= {10.1109/TASLP.2021.3138670},
  abstract	= {Event extraction is a ftask for natural language
		  processing. Finding the roles of event arguments like event
		  participants is essential for event extraction. However,
		  doing so for real-life event descriptions is challenging
		  because an argument’s role often varies in different
		  contexts. While the relationship and interactions between
		  multiple arguments are useful for settling the argument
		  roles, such information is largely ignored by existing
		  approaches. This paper presents a better approach for event
		  extraction by explicitly utilizing the relationships of
		  event arguments. We achieve this through a carefully
		  designed task-oriented dialogue system. To model the
		  argument relation, we employ reinforcement learning and
		  incremental learning to extract multiple arguments via a
		  multi-turned, iterative process. Our approach leverages
		  knowledge of the already extracted arguments of the same
		  sentence to determine the role of arguments that would be
		  difficult to decide individually. It then uses the newly
		  obtained information to improve the decisions of previously
		  extracted arguments. This two-way feedback process allows
		  us to exploit the argument relations to effectively settle
		  argument roles, leading to better sentence understanding
		  and event extraction. Experimental results show that our
		  approach consistently outperforms seven state-of-the-art
		  event extraction methods for the classification of events
		  and argument role and argument identification.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= dec,
  pages		= {520–533},
  numpages	= {14}
}

@Article{	  10.1145/3588722,
  author	= {Genossar, Bar and Shraga, Roee and Gal, Avigdor},
  title		= {FlexER: Flexible Entity Resolution for Multiple Intents},
  year		= {2023},
  issue_date	= {May 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {1},
  url		= {https://doi.org/10.1145/3588722},
  doi		= {10.1145/3588722},
  abstract	= {Entity resolution, a longstanding problem of data cleaning
		  and integration, aims at identifying data records that
		  represent the same real-world entity. Existing approaches
		  treat entity resolution as a universal task, assuming the
		  existence of a single interpretation of a real-world entity
		  and focusing only on finding matched records, separating
		  corresponding from non-corresponding ones, with respect to
		  this single interpretation. However, in real-world
		  scenarios, where entity resolution is part of a more
		  general data project, downstream applications may have
		  varying interpretations of real-world entities relating,
		  for example, to various user needs. In what follows, we
		  introduce the problem of multiple intents entity resolution
		  (MIER), an extension to the universal (single intent)
		  entity resolution task. As a solution, we propose FlexER,
		  utilizing contemporary solutions to universal entity
		  resolution tasks to solve MIER. FlexER addresses the
		  problem as a multi-label classification problem. It
		  combines intent-based representations of tuple pairs using
		  a multiplex graph representation that serves as an input to
		  a graph neural network (GNN). FlexER learns intent
		  representations and improves the outcome to multiple
		  resolution problems. A large-scale empirical evaluation
		  introduces a new benchmark and, using also two well-known
		  benchmarks, shows that FlexER effectively solves the MIER
		  problem and outperforms the state-of-the-art for a
		  universal entity resolution.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {42},
  numpages	= {27},
  keywords	= {entity matching, entity resolution, graph neural networks,
		  supervised learning}
}

@Proceedings{	  10.1145/3549555,
  title		= {CBMI '22: Proceedings of the 19th International Conference
		  on Content-based Multimedia Indexing},
  year		= {2022},
  isbn		= {9781450397209},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Graz, Austria}
}

@Article{	  10.1145/3440755,
  author	= {Chandrasekaran, Dhivya and Mago, Vijay},
  title		= {Evolution of Semantic Similarity—A Survey},
  year		= {2021},
  issue_date	= {March 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {54},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3440755},
  doi		= {10.1145/3440755},
  abstract	= {Estimating the semantic similarity between text data is
		  one of the challenging and open research problems in the
		  field of Natural Language Processing (NLP). The versatility
		  of natural language makes it difficult to define rule-based
		  methods for determining semantic similarity measures. To
		  address this issue, various semantic similarity methods
		  have been proposed over the years. This survey article
		  traces the evolution of such methods beginning from
		  traditional NLP techniques such as kernel-based methods to
		  the most recent research work on transformer-based models,
		  categorizing them based on their underlying principles as
		  knowledge-based, corpus-based, deep neural network–based
		  methods, and hybrid methods. Discussing the strengths and
		  weaknesses of each method, this survey provides a
		  comprehensive view of existing systems in place for new
		  researchers to experiment and develop innovative ideas to
		  address the issue of semantic similarity.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {41},
  numpages	= {37},
  keywords	= {Semantic similarity, corpus-based methods, knowledge-based
		  methods, linguistics, supervised and unsupervised methods,
		  word embeddings}
}

@Article{	  10.1109/tcbb.2019.2937771,
  author	= {Gao, Jianliang and Tian, Ling and Lv, Tengfei and Wang,
		  Jianxin and Song, Bo and Hu, Xiaohua},
  title		= {Protein2Vec: Aligning Multiple PPI Networks with
		  Representation Learning},
  year		= {2021},
  issue_date	= {Jan.-Feb. 2021},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {18},
  number	= {1},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2019.2937771},
  doi		= {10.1109/TCBB.2019.2937771},
  abstract	= {Research of Protein-Protein Interaction (PPI) Network
		  Alignment is playing an important role in understanding the
		  crucial underlying biological knowledge such as
		  functionally homologous proteins and conserved evolutionary
		  pathways across different species. Existing methods of PPI
		  network alignment often try to improve the coverage ratio
		  of the alignment result by aligning all proteins from
		  different species. However, there is a fundamental
		  biological premise that needs to be considered carefully:
		  not every protein in a species can, nor should, find its
		  homologous proteins in other species. In this work, we
		  propose a novel alignment method to map only those proteins
		  with the most similarity throughout the PPI networks of
		  multiple species. For the similarity features of the
		  protein in the networks, we integrate both topological
		  features with biological characteristics to provide
		  enhanced supports for the alignment procedures. For
		  topological features, we apply a representation learning
		  method on the networks and generate a low dimensional
		  vector embedding with its surrounding structural features
		  for each protein. The topological similarity of proteins
		  from different PPI networks can thus be transferred as the
		  similarity of their corresponding vector representations,
		  which provides a new way to comprehensively quantify the
		  topological similarities between proteins. We also propose
		  a new measure for the topological evaluation of the
		  alignment results which better uncover the structural
		  quality of the alignment across multiple networks. Both
		  biological and topological evaluations on the alignment
		  results of real datasets demonstrate our approach is
		  promising and preferable against previous multiple
		  alignment methods.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= feb,
  pages		= {240–249},
  numpages	= {10}
}

@InProceedings{	  10.1145/3628454.3629551,
  author	= {Nimpattanavong, Chollakorn and Taveekitworachai, Pittawat
		  and Khan, Ibrahim and Nguyen, Thai Van and Thawonmas, Ruck
		  and Choensawat, Worawat and Sookhanaphibarn, Kingkarn},
  title		= {Am I Fighting Well? Fighting Game Commentary Generation
		  With ChatGPT},
  year		= {2023},
  isbn		= {9798400708497},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3628454.3629551},
  doi		= {10.1145/3628454.3629551},
  abstract	= {This paper presents a new approach for leveraging ChatGPT
		  in fighting game commentary generation task. Commentary
		  generation often relies on deep learning techniques, which
		  typically demand extensive data to achieve effectiveness.
		  Large language models (LLMs) have become essential due to
		  their remarkable ability to process data efficiently,
		  thanks to their extensive training on vast datasets. Our
		  proposed approach integrates the use of LLMs, specifically
		  the GPT-3.5 model, for generating commentaries through the
		  utilization of various prompts with data from the
		  open-source fighting game, DareFightingICE. Four prompt
		  variants are employed to assess the effectiveness of each
		  prompt components. Objective evaluation using natural
		  language metrics reveals that different prompt components
		  significantly affect the generated commentaries.
		  Additionally, subjective evaluation through a questionnaire
		  reveals that prompts without parameter definitions received
		  the highest preference from human evaluators. These results
		  suggest that LLMs exhibit versatility in generating
		  fighting game commentaries and hold promise for broader
		  applications.},
  booktitle	= {Proceedings of the 13th International Conference on
		  Advances in Information Technology},
  articleno	= {14},
  numpages	= {7},
  keywords	= {ChatGPT, Commentary Generation, DareFightingICE, Fighting
		  Game, Prompt Engineering},
  location	= {Bangkok, Thailand},
  series	= {IAIT '23}
}

@InProceedings{	  10.1109/ase51524.2021.9678894,
  author	= {Shi, Lin and Jiang, Ziyou and Yang, Ye and Chen, Xiao and
		  Zhang, Yumin and Mu, Fangwen and Jiang, Hanzhi and Wang,
		  Qing},
  title		= {ISPY: automatic issue-solution pair extraction from
		  community live chats},
  year		= {2022},
  isbn		= {9781665403375},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE51524.2021.9678894},
  doi		= {10.1109/ASE51524.2021.9678894},
  abstract	= {Collaborative live chats are gaining popularity as a
		  development communication tool. In community live chatting,
		  developers are likely to post issues they encountered
		  (e.g., setup issues and compile issues), and other
		  developers respond with possible solutions. Therefore,
		  community live chats contain rich sets of information for
		  reported issues and their corresponding solutions, which
		  can be quite useful for knowledge sharing and future reuse
		  if extracted and restored in time. However, it remains
		  challenging to accurately mine such knowledge due to the
		  noisy nature of interleaved dialogs in live chat data. In
		  this paper, we first formulate the problem of
		  issue-solution pair extraction from developer live chat
		  data, and propose an automated approach, named ISPY, based
		  on natural language processing and deep learning techniques
		  with customized enhancements, to address the problem.
		  Specifically, ISPY automates three tasks: 1) Disentangle
		  live chat logs, employing a feedforward neural network to
		  disentangle a conversation history into separate dialogs
		  automatically; 2) Detect dialogs discussing issues, using a
		  novel convolutional neural network (CNN), which consists of
		  a BERT-based utterance embedding layer, a context-aware
		  dialog embedding layer, and an output layer; 3) Extract
		  appropriate utterances and combine them as corresponding
		  solutions, based on the same CNN structure but with
		  different feeding inputs. To evaluate ISPY, we compare it
		  with six baselines, utilizing a dataset with 750 dialogs
		  including 171 issue-solution pairs and evaluate ISPY from
		  eight open source communities. The results show that, for
		  issue-detection, our approach achieves the F1 of 76%, and
		  outperforms all baselines by 30%. Our approach achieves the
		  F1 of 63% for solution-extraction and outperforms the
		  baselines by 20%. Furthermore, we apply ISPY on three new
		  communities to extensively evaluate ISPY's practical usage.
		  Moreover, we publish over 30K issue-solution pairs
		  extracted from 11 communities. We believe that ISPY can
		  facilitate community-based software development by
		  promoting knowledge sharing and shortening the
		  issue-resolving process.},
  booktitle	= {Proceedings of the 36th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {142–154},
  numpages	= {13},
  location	= {Melbourne, Australia},
  series	= {ASE '21}
}

@Proceedings{	  10.1145/3600006,
  title		= {SOSP '23: Proceedings of the 29th Symposium on Operating
		  Systems Principles},
  year		= {2023},
  isbn		= {9798400702297},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the Proceedings of the 29th ACM Symposium on
		  Operating Systems Principles (SOSP 2023). This year's
		  program includes 43 papers that reflect today's broad range
		  of topics that comprise modern computer systems research.
		  The program committee carefully reviewed submitted papers
		  and worked closely with the authors of selected papers to
		  produce the collection of high-quality, readable papers
		  presented here. We hope that you enjoy the program!},
  location	= {Koblenz, Germany}
}

@Proceedings{	  10.1145/3584871,
  title		= {ICSIM '23: Proceedings of the 2023 6th International
		  Conference on Software Engineering and Information
		  Management},
  year		= {2023},
  isbn		= {9781450398237},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Palmerston North, New Zealand}
}

@InProceedings{	  10.1145/3397481.3450697,
  author	= {Karimi, Pegah and Plebani, Emanuele and Bolchini, Davide},
  title		= {Textflow: Screenless Access to Non-Visual Smart
		  Messaging},
  year		= {2021},
  isbn		= {9781450380171},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3397481.3450697},
  doi		= {10.1145/3397481.3450697},
  abstract	= {Texting relies on screen-centric prompts designed for
		  sighted users, still posing significant barriers to people
		  who are blind and visually impaired (BVI). Can we
		  re-imagine texting untethered from a visual display? In an
		  interview study, 20 BVI adults shared situations
		  surrounding their texting practices, recurrent topics of
		  conversations, and challenges. Informed by these insights,
		  we introduce TextFlow: a mixed-initiative context-aware
		  system that generates entirely auditory message options
		  relevant to the users’ location, activity, and time of
		  the day. Users can browse and select suggested aural
		  messages using finger-taps supported by an off-the-shelf
		  finger-worn device, without having to hold or attend to a
		  mobile screen. In an evaluative study, 10 BVI participants
		  successfully interacted with TextFlow to browse and send
		  messages in screen-free mode. The experiential response of
		  the users shed light on the importance of bypassing the
		  phone and accessing rapidly controllable messages at their
		  fingertips while preserving privacy and accuracy with
		  respect to speech or screen-based input. We discuss how
		  non-visual access to proactive, contextual messaging can
		  support the blind in a variety of daily scenarios.},
  booktitle	= {Proceedings of the 26th International Conference on
		  Intelligent User Interfaces},
  pages		= {186–196},
  numpages	= {11},
  keywords	= {Assistive technologies, Aural navigation, Intelligent
		  wearable and mobile interfaces, Text entry, Ubiquitous
		  smart environments},
  location	= {College Station, TX, USA},
  series	= {IUI '21}
}

@Proceedings{	  10.1145/3549737,
  title		= {SETN '22: Proceedings of the 12th Hellenic Conference on
		  Artificial Intelligence},
  year		= {2022},
  isbn		= {9781450395977},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Corfu, Greece}
}

@Proceedings{	  10.1145/3594739,
  title		= {UbiComp/ISWC '23 Adjunct: Adjunct Proceedings of the 2023
		  ACM International Joint Conference on Pervasive and
		  Ubiquitous Computing &amp; the 2023 ACM International
		  Symposium on Wearable Computing},
  year		= {2023},
  isbn		= {9798400702006},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Cancun, Quintana Roo, Mexico}
}

@Article{	  10.1145/3577925,
  author	= {Schiappa, Madeline C. and Rawat, Yogesh S. and Shah,
		  Mubarak},
  title		= {Self-Supervised Learning for Videos: A Survey},
  year		= {2023},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {55},
  number	= {13s},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3577925},
  doi		= {10.1145/3577925},
  abstract	= {The remarkable success of deep learning in various domains
		  relies on the availability of large-scale annotated
		  datasets. However, obtaining annotations is expensive and
		  requires great effort, which is especially challenging for
		  videos. Moreover, the use of human-generated annotations
		  leads to models with biased learning and poor domain
		  generalization and robustness. As an alternative,
		  self-supervised learning provides a way for representation
		  learning that does not require annotations and has shown
		  promise in both image and video domains. In contrast to the
		  image domain, learning video representations are more
		  challenging due to the temporal dimension, bringing in
		  motion and other environmental dynamics. This also provides
		  opportunities for video-exclusive ideas that advance
		  self-supervised learning in the video and multimodal
		  domains. In this survey, we provide a review of existing
		  approaches on self-supervised learning focusing on the
		  video domain. We summarize these methods into four
		  different categories based on their learning objectives:
		  (1) pretext tasks, (2) generative learning, (3) contrastive
		  learning, and (4) cross-modal agreement. We further
		  introduce the commonly used datasets, downstream evaluation
		  tasks, insights into the limitations of existing works, and
		  the potential future directions in this area.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {288},
  numpages	= {37},
  keywords	= {Self-supervised learning, deep learning, video
		  understanding, zero-shot learning, representation learning,
		  multimodal learning, visual-language models}
}

@Article{	  10.1145/3603499,
  author	= {Sangsavate, Suntarin and Sinthupinyo, Sukree and
		  Chandrachai, Achara},
  title		= {Experiments of Supervised Learning and Semi-Supervised
		  Learning in Thai Financial News Sentiment: A Comparative
		  Study},
  year		= {2023},
  issue_date	= {July 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {22},
  number	= {7},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3603499},
  doi		= {10.1145/3603499},
  abstract	= {Sentiment classification is an instrument of natural
		  language processing tasks in text analysis to measure
		  customer feedback from given documents such as product
		  reviews, news, and texts. This research aims to experiment
		  with Thai financial news sentiment classification and
		  evaluate sentiment classification performance. In this
		  research, we show financial news sentiment classification
		  experimental results when comparing supervised and
		  semi-supervised methods. In the research methodology, we
		  use PyThaiNLP to tokenize and remove stopwords and split
		  datasets into 85% of the training set and 15% of the
		  testing set. Next, we classify sentiment using machine
		  learning and deep learning approaches with feature
		  extraction such as bag-of-words, term frequency–inverse
		  document frequency, and word embedding (Word2Vec and
		  Bidirectional Encoder Representations from Transformers
		  (BERT)) in given texts. The results show that support
		  vector machine with the BERT model yields the best
		  performance at 83.38%; in contrast, the random forest
		  classifier with bag-of-words yields the worst performance
		  at 54.10% in the machine learning approach. Another
		  experiment reveals that long short-term memory with the
		  BERT model yields the best performance at 84.07% in
		  contrast to the convolutional neural network with
		  bag-of-words, which yields the worst performance at 69.80%
		  in the deep learning approach. The results imply that
		  support vector machine, convolutional neural network, and
		  long short-term memory are suitable for classifying
		  sentiment in complex structure language. From this study,
		  we observe the importance of sentiment classification tools
		  between supervised and semi-supervised learning, and we
		  look forward to furthering this work.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jul,
  articleno	= {197},
  numpages	= {36},
  keywords	= {Natural language processing, semi-supervised learning,
		  sentiment classification, supervised learning, Thai
		  language}
}

@InProceedings{	  10.1145/3324884.3416668,
  author	= {Nguyen, Hoang Lam and Nassar, Nebras and Kehrer, Timo and
		  Grunske, Lars},
  title		= {MoFuzz: a fuzzer suite for testing model-driven software
		  engineering tools},
  year		= {2021},
  isbn		= {9781450367684},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3324884.3416668},
  doi		= {10.1145/3324884.3416668},
  abstract	= {Fuzzing or fuzz testing is an established technique that
		  aims to discover unexpected program behavior (e.g., bugs,
		  security vulnerabilities, or crashes) by feeding
		  automatically generated data into a program under test.
		  However, the application of fuzzing to test Model-Driven
		  Software Engineering (MDSE) tools is still limited because
		  of the difficulty of existing fuzzers to provide
		  structured, well-typed inputs, namely models that conform
		  to typing and consistency constraints induced by a given
		  meta-model and underlying modeling framework. By drawing
		  from recent advances on both fuzz testing and automated
		  model generation, we present three different approaches for
		  fuzzing MDSE tools: A graph grammar-based fuzzer and two
		  variants of a coverage-guided mutation-based fuzzer working
		  with different sets of model mutation operators. Our
		  evaluation on a set of real-world MDSE tools shows that our
		  approaches can outperform both standard fuzzers and model
		  generators w.r.t. their fuzzing capabilities. Moreover, we
		  found that each of our approaches comes with its own
		  strengths and weaknesses in terms of fault finding
		  capabilities and the ability to cover different aspects of
		  the system under test. Thus the approaches complement each
		  other, forming a fuzzer suite for testing MDSE tools.},
  booktitle	= {Proceedings of the 35th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {1103–1115},
  numpages	= {13},
  keywords	= {automated model generation, eclipse modeling framework,
		  fuzzing, model-driven software engineering, modeling
		  tools},
  location	= {Virtual Event, Australia},
  series	= {ASE '20}
}

@Proceedings{	  10.1145/3544548,
  title		= {CHI '23: Proceedings of the 2023 CHI Conference on Human
		  Factors in Computing Systems},
  year		= {2023},
  isbn		= {9781450394215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hamburg, Germany}
}

@Proceedings{	  10.1145/3579375,
  title		= {ACSW '23: Proceedings of the 2023 Australasian Computer
		  Science Week},
  year		= {2023},
  isbn		= {9798400700057},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Melbourne, VIC, Australia}
}

@Article{	  10.1145/3519263,
  author	= {Karimi, Pegah and Plebani, Emanuele and Martin-Hammond,
		  Aqueasha and Bolchini, Davide},
  title		= {Textflow: Toward Supporting Screen-free Manipulation of
		  Situation-Relevant Smart Messages},
  year		= {2022},
  issue_date	= {December 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {12},
  number	= {4},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3519263},
  doi		= {10.1145/3519263},
  abstract	= {Texting relies on screen-centric prompts designed for
		  sighted users, still posing significant barriers to people
		  who are blind and visually impaired (BVI). Can we
		  re-imagine texting untethered from a visual display? In an
		  interview study, 20 BVI adults shared situations
		  surrounding their texting practices, recurrent topics of
		  conversations, and challenges. Informed by these insights,
		  we introduce TextFlow, a mixed-initiative context-aware
		  system that generates entirely auditory message options
		  relevant to the users’ location, activity, and time of
		  the day. Users can browse and select suggested aural
		  messages using finger-taps supported by an off-the-shelf
		  finger-worn device without having to hold or attend to a
		  mobile screen. In an evaluative study, 10 BVI participants
		  successfully interacted with TextFlow to browse and send
		  messages in screen-free mode. The experiential response of
		  the users shed light on the importance of bypassing the
		  phone and accessing rapidly controllable messages at their
		  fingertips while preserving privacy and accuracy with
		  respect to speech or screen-based input. We discuss how
		  non-visual access to proactive, contextual messaging can
		  support the blind in a variety of daily scenarios.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= nov,
  articleno	= {31},
  numpages	= {29},
  keywords	= {Text entry, assistive technologies, intelligent wearable
		  and mobile interfaces, aural navigation, ubiquitous smart
		  environments}
}

@InProceedings{	  10.1145/3607541.3616821,
  author	= {Zou, Jialing and Mei, Jiahao and Ye, Guangze and Huai,
		  Tianyu and Shen, Qiwei and Dong, Daoguo},
  title		= {EMID: An Emotional Aligned Dataset in Audio-Visual
		  Modality},
  year		= {2023},
  isbn		= {9798400702785},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3607541.3616821},
  doi		= {10.1145/3607541.3616821},
  abstract	= {In this paper, we propose Emotionally paired Music and
		  Image Dataset (EMID), a novel dataset designed for the
		  emotional matching of music and images, to facilitate
		  auditory-visual cross-modal tasks such as generation and
		  retrieval. Unlike existing approaches that primarily focus
		  on semantic correlations or roughly divided emotional
		  relations, EMID emphasizes the significance of emotional
		  consistency between music and images using an advanced
		  13-dimension emotional model. By incorporating emotional
		  alignment into the dataset, it aims to establish pairs that
		  closely align with human perceptual understanding, thereby
		  raising the performance of auditory-visual cross-modal
		  tasks. We also design a supplemental module named
		  EMI-Adapter to optimize existing cross-modal alignment
		  methods. To validate the effectiveness of the EMID, we
		  conduct a psychological experiment, which has demonstrated
		  that considering the emotional relationship between the two
		  modalities effectively improves the accuracy of matching in
		  abstract perspective. This research lays the foundation for
		  future cross-modal research in domains such as
		  psychotherapy and contributes to advancing the
		  understanding and utilization of emotions in cross-modal
		  alignment. The EMID dataset is available at
		  https://github.com/ecnu-aigc/EMID.},
  booktitle	= {Proceedings of the 1st International Workshop on
		  Multimedia Content Generation and Evaluation: New Methods
		  and Practice},
  pages		= {41–48},
  numpages	= {8},
  keywords	= {music-image dataset, emotional matching, cross-modal
		  alignment},
  location	= {Ottawa ON, Canada},
  series	= {McGE '23}
}

@Article{	  10.1145/3604550,
  author	= {Liu, Yaochen and Li, Qiuchi and Wang, Benyou and Zhang,
		  Yazhou and Song, Dawei},
  title		= {A Survey of Quantum-cognitively Inspired Sentiment
		  Analysis Models},
  year		= {2023},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3604550},
  doi		= {10.1145/3604550},
  abstract	= {Quantum theory, originally proposed as a physical theory
		  to describe the motions of microscopic particles, has been
		  applied to various non-physics domains involving human
		  cognition and decision-making that are inherently uncertain
		  and exhibit certain non-classical, quantum-like
		  characteristics. Sentiment analysis is a typical example of
		  such domains. In the last few years, by leveraging the
		  modeling power of quantum probability (a non-classical
		  probability stemming from quantum mechanics methodology)
		  and deep neural networks, a range of novel
		  quantum-cognitively inspired models for sentiment analysis
		  have emerged and performed well. This survey presents a
		  timely overview of the latest developments in this
		  fascinating cross-disciplinary area. We first provide a
		  background of quantum probability and quantum cognition at
		  a theoretical level, analyzing their advantages over
		  classical theories in modeling the cognitive aspects of
		  sentiment analysis. Then, recent quantum-cognitively
		  inspired models are introduced and discussed in detail,
		  focusing on how they approach the key challenges of the
		  sentiment analysis task. Finally, we discuss the
		  limitations of the current research and highlight future
		  research directions.},
  journal	= {ACM Comput. Surv.},
  month		= aug,
  articleno	= {15},
  numpages	= {37},
  keywords	= {emotion recognition, sarcasm detection, sentiment
		  analysis, non-classical probability from quantum mechanics
		  methodology, Quantum-cognitively inspired models}
}

@Proceedings{	  10.1145/3579370,
  title		= {SYSTOR '23: Proceedings of the 16th ACM International
		  Conference on Systems and Storage},
  year		= {2023},
  isbn		= {9781450399623},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Haifa, Israel}
}

@Proceedings{	  10.1145/3533271,
  title		= {ICAIF '22: Proceedings of the Third ACM International
		  Conference on AI in Finance},
  year		= {2022},
  isbn		= {9781450393768},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {New York, NY, USA}
}

@Proceedings{	  10.1145/3583678,
  title		= {DEBS '23: Proceedings of the 17th ACM International
		  Conference on Distributed and Event-based Systems},
  year		= {2023},
  isbn		= {9798400701221},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {DEBS 2023 is the seventeenth in a series that spans more
		  than 20 years of history, with 16 past editions as a
		  conference and five editions as a workshop co-located with
		  major conferences.The objectives of DEBS have been to
		  provide a forum dedicated to the dissemination of original
		  research, the discussion of practical insights, and the
		  reporting of experiences relevant to distributed systems
		  and event-based computing. The conference provides a forum
		  for academia and industry to exchange ideas through its
		  tutorials, research papers, and the grand challenge.
		  Recently, the ACM International Conference on Distributed
		  and Event-Based Systems, including DEBS 2022, has become
		  the premier venue for cutting-edge research in the
		  integration of distributed and event-based systems in
		  relevant domains such as Big Data, AI, ML, IoT, and
		  Blockchain.},
  location	= {Neuchatel, Switzerland}
}

@Proceedings{	  10.1145/3598469,
  title		= {dg.o '23: Proceedings of the 24th Annual International
		  Conference on Digital Government Research},
  year		= {2023},
  isbn		= {9798400708374},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Gda?sk, Poland}
}

@Proceedings{	  10.1145/3597638,
  title		= {ASSETS '23: Proceedings of the 25th International ACM
		  SIGACCESS Conference on Computers and Accessibility},
  year		= {2023},
  isbn		= {9798400702204},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {New York, NY, USA}
}

@Proceedings{	  10.1145/3611450,
  title		= {AI2A '23: Proceedings of the 2023 3rd International
		  Conference on Artificial Intelligence, Automation and
		  Algorithms},
  year		= {2023},
  isbn		= {9798400707605},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@Article{	  10.1145/3494560,
  author	= {Abulaish, Muhammad and Fazil, Mohd and Zaki, Mohammed J.},
  title		= {Domain-Specific Keyword Extraction Using Joint Modeling of
		  Local and Global Contextual Semantics},
  year		= {2022},
  issue_date	= {August 2022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {4},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3494560},
  doi		= {10.1145/3494560},
  abstract	= {Domain-specific keyword extraction is a vital task in the
		  field of text mining. There are various research tasks,
		  such as spam e-mail classification, abusive language
		  detection, sentiment analysis, and emotion mining, where a
		  set of domain-specific keywords (aka lexicon) is highly
		  effective. Existing works for keyword extraction list all
		  keywords rather than domain-specific keywords from a
		  document corpus. Moreover, most of the existing approaches
		  perform well on formal document corpuses but fail on noisy
		  and informal user-generated content in online social media.
		  In this article, we present a hybrid approach by jointly
		  modeling the local and global contextual semantics of
		  words, utilizing the strength of distributional word
		  representation and contrasting-domain corpus for
		  domain-specific keyword extraction. Starting with a seed
		  set of a few domain-specific keywords, we model the text
		  corpus as a weighted word-graph. In this graph, the initial
		  weight of a node (word) represents its semantic association
		  with the target domain calculated as a linear combination
		  of three semantic association metrics, and the weight of an
		  edge connecting a pair of nodes represents the
		  co-occurrence count of the respective words. Thereafter, a
		  modified PageRank method is applied to the word-graph to
		  identify the most relevant words for expanding the initial
		  set of domain-specific keywords. We evaluate our method
		  over both formal and informal text corpuses (comprising six
		  datasets), and show that it performs significantly better
		  in comparison to state-of-the-art methods. Furthermore, we
		  generalize our approach to handle the language-agnostic
		  case, and show that it outperforms existing
		  language-agnostic approaches.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= jan,
  articleno	= {70},
  numpages	= {30},
  keywords	= {language-agnostic keyword extraction, domain-specific
		  keyword extraction, information extraction, Text mining}
}

@Proceedings{	  10.1145/3538969,
  title		= {ARES '22: Proceedings of the 17th International Conference
		  on Availability, Reliability and Security},
  year		= {2022},
  isbn		= {9781450396707},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Vienna, Austria}
}

@InProceedings{	  10.1145/3587259.3627564,
  author	= {Jia, Yue-Bo and Johnson, Gavin and Arnold, Alex and
		  Heflin, Jeff},
  title		= {An Evaluation of Strategies to Train More Efficient
		  Backward-Chaining Reasoners},
  year		= {2023},
  isbn		= {9798400701412},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3587259.3627564},
  doi		= {10.1145/3587259.3627564},
  abstract	= {Knowledge bases traditionally require manual optimization
		  to ensure reasonable performance when answering queries. We
		  build on previous work on training a deep learning model to
		  learn heuristics for answering queries by comparing
		  different representations of the sentences contained in
		  knowledge bases. We decompose the problem into issues of
		  representation, training, and control and propose solutions
		  for each subproblem. We evaluate different configurations
		  on three synthetic knowledge bases. In particular we
		  compare a novel representation approach based on learning
		  to maximize similarity of logical atoms that unify and
		  minimize similarity of atoms that do not unify, to two
		  vectorization strategies taken from the automated theorem
		  proving literature: a chain-based and a 3-term-walk
		  strategy. We also evaluate the efficacy of pruning the
		  search by ignoring rules with scores below a threshold.},
  booktitle	= {Proceedings of the 12th Knowledge Capture Conference
		  2023},
  pages		= {206–213},
  numpages	= {8},
  keywords	= {backward chaining, efficient queries, knowledge bases,
		  machine learning, meta-reasoning, neurosymbolic AI},
  location	= {Pensacola, FL, USA},
  series	= {K-CAP '23}
}

@Proceedings{	  10.1145/3617694,
  title		= {EAAMO '23: Proceedings of the 3rd ACM Conference on Equity
		  and Access in Algorithms, Mechanisms, and Optimization},
  year		= {2023},
  isbn		= {9798400703812},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Boston, MA, USA}
}

@Proceedings{	  10.1145/3552326,
  title		= {EuroSys '23: Proceedings of the Eighteenth European
		  Conference on Computer Systems},
  year		= {2023},
  isbn		= {9781450394871},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rome, Italy}
}

@Proceedings{	  10.1145/3581784,
  title		= {SC '23: Proceedings of the International Conference for
		  High Performance Computing, Networking, Storage and
		  Analysis},
  year		= {2023},
  isbn		= {9798400701092},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Started in 1988, the SC Conference has become the annual
		  nexus for researchers and practitioners from academia,
		  industry and government to share information and foster
		  collaborations to advance the state of the art in High
		  Performance Computing (HPC), Networking, Storage, and
		  Analysis.},
  location	= {Denver, CO, USA}
}

@Proceedings{	  10.1145/3582437,
  title		= {FDG '23: Proceedings of the 18th International Conference
		  on the Foundations of Digital Games},
  year		= {2023},
  isbn		= {9781450398558},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3576842,
  title		= {IoTDI '23: Proceedings of the 8th ACM/IEEE Conference on
		  Internet of Things Design and Implementation},
  year		= {2023},
  isbn		= {9798400700378},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {San Antonio, TX, USA}
}

@Proceedings{	  10.1145/3543712,
  title		= {ICCTA '22: Proceedings of the 2022 8th International
		  Conference on Computer Technology Applications},
  year		= {2022},
  isbn		= {9781450396226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Vienna, Austria}
}

@Proceedings{	  10.1145/3597926,
  title		= {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT
		  International Symposium on Software Testing and Analysis},
  year		= {2023},
  isbn		= {9798400702211},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to ISSTA 2023, the
		  32nd edition of the International Symposium on Software
		  Testing and Analysis, to be held on July 18–20, 2023 in
		  Seattle, USA. The symposium has become a premier scientific
		  event in the expanding area of software testing and
		  analysis, with a strong appeal to researchers from all
		  continents.},
  location	= {Seattle, WA, USA}
}

@Proceedings{	  10.1145/3579142,
  title		= {BiDEDE '23: Proceedings of the International Workshop on
		  Big Data in Emergent Distributed Environments},
  year		= {2023},
  isbn		= {9798400700934},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Seattle, WA, USA}
}

@Proceedings{	  10.1145/3600160,
  title		= {ARES '23: Proceedings of the 18th International Conference
		  on Availability, Reliability and Security},
  year		= {2023},
  isbn		= {9798400707728},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Benevento, Italy}
}

@Proceedings{	  10.1145/3569951,
  title		= {PEARC '23: Practice and Experience in Advanced Research
		  Computing 2023: Computing for the Common Good},
  year		= {2023},
  isbn		= {9781450399852},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Portland, OR, USA}
}

@Proceedings{	  10.1145/3625403,
  title		= {ADMIT '23: Proceedings of the 2023 2nd International
		  Conference on Algorithms, Data Mining, and Information
		  Technology},
  year		= {2023},
  isbn		= {9798400707629},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Chengdu, China}
}

@Proceedings{	  10.1145/3569219,
  title		= {Academic Mindtrek '22: Proceedings of the 25th
		  International Academic Mindtrek Conference},
  year		= {2022},
  isbn		= {9781450399555},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tampere, Finland}
}

@Proceedings{	  10.1145/3558100,
  title		= {DocEng '22: Proceedings of the 22nd ACM Symposium on
		  Document Engineering},
  year		= {2022},
  isbn		= {9781450395441},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The symposium brings together experts in all areas of
		  document engineering, across academia and industry, with
		  the intention of presenting and discussing the most recent
		  advances in the field of Document Engineering.},
  location	= {San Jose, California}
}

@Proceedings{	  10.1145/3617233,
  title		= {CBMI '23: Proceedings of the 20th International Conference
		  on Content-based Multimedia Indexing},
  year		= {2023},
  isbn		= {9798400709128},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Orleans, France}
}

@Proceedings{	  10.1145/3577190,
  title		= {ICMI '23: Proceedings of the 25th International Conference
		  on Multimodal Interaction},
  year		= {2023},
  isbn		= {9798400700552},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Paris, France}
}

@Proceedings{	  10.1145/3568739,
  title		= {ICDTE '22: Proceedings of the 6th International Conference
		  on Digital Technology in Education},
  year		= {2022},
  isbn		= {9781450398091},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Proceedings{	  10.1145/3615522,
  title		= {VINCI '23: Proceedings of the 16th International Symposium
		  on Visual Information Communication and Interaction},
  year		= {2023},
  isbn		= {9798400707513},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guangzhou, China}
}

@Proceedings{	  10.1145/3606843,
  title		= {ITCC '23: Proceedings of the 2023 5th International
		  Conference on Information Technology and Computer
		  Communications},
  year		= {2023},
  isbn		= {9798400700583},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tianjin, China}
}

@Proceedings{	  10.1145/3603555,
  title		= {MuC '23: Proceedings of Mensch und Computer 2023},
  year		= {2023},
  isbn		= {9798400707711},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rapperswil, Switzerland}
}

@Proceedings{	  10.1145/3587281,
  title		= {W4A '23: Proceedings of the 20th International Web for All
		  Conference},
  year		= {2023},
  isbn		= {9798400707483},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Austin, TX, USA}
}

@Proceedings{	  10.1145/3584318,
  title		= {NSPW '22: Proceedings of the 2022 New Security Paradigms
		  Workshop},
  year		= {2022},
  isbn		= {9781450398664},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {North Conway, NH, USA}
}

@Proceedings{	  10.1145/3569966,
  title		= {CSSE '22: Proceedings of the 5th International Conference
		  on Computer Science and Software Engineering},
  year		= {2022},
  isbn		= {9781450397780},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guilin, China}
}

@Proceedings{	  10.1145/3625135,
  title		= {DLfM '23: Proceedings of the 10th International Conference
		  on Digital Libraries for Musicology},
  year		= {2023},
  isbn		= {9798400708336},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Milan, Italy}
}

@Proceedings{	  10.1145/3411764,
  title		= {CHI '21: Proceedings of the 2021 CHI Conference on Human
		  Factors in Computing Systems},
  year		= {2021},
  isbn		= {9781450380966},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Yokohama, Japan}
}

@Proceedings{	  10.1145/3563357,
  title		= {BuildSys '22: Proceedings of the 9th ACM International
		  Conference on Systems for Energy-Efficient Buildings,
		  Cities, and Transportation},
  year		= {2022},
  isbn		= {9781450398909},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Over the past thirteen years, BuildSys has been an
		  interdisciplinary conference that brings together various
		  stakeholders, including researchers, practitioners, and
		  policymakers from different disciplines, including civil
		  engineering, mechanical engineering, environmental science,
		  electrical and computer engineering, computer science,
		  system management and control, and many others. This year
		  is no exception, with papers and attendees from all these
		  disciplines and regions worldwide. The conference's focus
		  extends beyond building systems to the built environment
		  more generally.},
  location	= {Boston, Massachusetts}
}

@Proceedings{	  10.1145/3543829,
  title		= {CUI '22: Proceedings of the 4th Conference on
		  Conversational User Interfaces},
  year		= {2022},
  isbn		= {9781450397391},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Glasgow, United Kingdom}
}

@Proceedings{	  10.1145/3544902,
  title		= {ESEM '22: Proceedings of the 16th ACM / IEEE International
		  Symposium on Empirical Software Engineering and
		  Measurement},
  year		= {2022},
  isbn		= {9781450394277},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Helsinki, Finland}
}

@Proceedings{	  10.1145/3517428,
  title		= {ASSETS '22: Proceedings of the 24th International ACM
		  SIGACCESS Conference on Computers and Accessibility},
  year		= {2022},
  isbn		= {9781450392587},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Athens, Greece}
}

@Proceedings{	  10.1145/3583131,
  title		= {GECCO '23: Proceedings of the Genetic and Evolutionary
		  Computation Conference},
  year		= {2023},
  isbn		= {9798400701191},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {GECCO is the largest peer-reviewed conference in the field
		  of Evolutionary Computation, and the main conference of the
		  Special Interest Group on Genetic and Evolutionary
		  Computation (SIGEVO) of the Association for Computing
		  Machinery (ACM).},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3583133,
  title		= {GECCO '23 Companion: Proceedings of the Companion
		  Conference on Genetic and Evolutionary Computation},
  year		= {2023},
  isbn		= {9798400701207},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {GECCO is the largest peer-reviewed conference in the field
		  of Evolutionary Computation, and the main conference of the
		  Special Interest Group on Genetic and Evolutionary
		  Computation (SIGEVO) of the Association for Computing
		  Machinery (ACM).},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3524458,
  title		= {GoodIT '22: Proceedings of the 2022 ACM Conference on
		  Information Technology for Social Good},
  year		= {2022},
  isbn		= {9781450392846},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Limassol, Cyprus}
}

@Proceedings{	  10.1145/3545008,
  title		= {ICPP '22: Proceedings of the 51st International Conference
		  on Parallel Processing},
  year		= {2022},
  isbn		= {9781450397339},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bordeaux, France}
}

@Proceedings{	  10.1145/3592813,
  title		= {SBSI '23: Proceedings of the XIX Brazilian Symposium on
		  Information Systems},
  year		= {2023},
  isbn		= {9798400707599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Macei\'{o}, Brazil}
}

@Proceedings{	  10.1145/3593663,
  title		= {ECSEE '23: Proceedings of the 5th European Conference on
		  Software Engineering Education},
  year		= {2023},
  isbn		= {9781450399562},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Seeon/Bavaria, Germany}
}

@Proceedings{	  10.1145/3624007,
  title		= {GPCE 2023: Proceedings of the 22nd ACM SIGPLAN
		  International Conference on Generative Programming:
		  Concepts and Experiences},
  year		= {2023},
  isbn		= {9798400704062},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 22nd ACM SIGPLAN International Conference
		  on Generative Programming: Concepts &amp; Experiences
		  (GPCE’23). GPCE is the premiere venue for researchers and
		  practitioners interested in techniques that use program
		  generation to increase programmer productivity, improve
		  software quality, and shorten the time-to-market of
		  software products. In addition to exploring cutting-edge
		  techniques of generative software, GPCE seeks to foster
		  cross-fertilization between the programming languages
		  research communities.},
  location	= {Cascais, Portugal}
}

@Proceedings{	  10.1145/3555858,
  title		= {FDG '22: Proceedings of the 17th International Conference
		  on the Foundations of Digital Games},
  year		= {2022},
  isbn		= {9781450397957},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Athens, Greece}
}

@Proceedings{	  10.1145/3560905,
  title		= {SenSys '22: Proceedings of the 20th ACM Conference on
		  Embedded Networked Sensor Systems},
  year		= {2022},
  isbn		= {9781450398862},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to ACM SenSys 2022, the 20th ACM Conference on
		  Embedded Networked Sensor Systems, the premier computer
		  systems conference focused on networked sensing systems and
		  applications.},
  location	= {Boston, Massachusetts}
}

@Proceedings{	  10.1145/3564625,
  title		= {ACSAC '22: Proceedings of the 38th Annual Computer
		  Security Applications Conference},
  year		= {2022},
  isbn		= {9781450397599},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Austin, TX, USA}
}

@Proceedings{	  10.1145/3575879,
  title		= {PCI '22: Proceedings of the 26th Pan-Hellenic Conference
		  on Informatics},
  year		= {2022},
  isbn		= {9781450398541},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Athens, Greece}
}

@Proceedings{	  10.1145/3548785,
  title		= {IDEAS '22: Proceedings of the 26th International Database
		  Engineered Applications Symposium},
  year		= {2022},
  isbn		= {9781450397094},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Budapest, Hungary}
}

@Proceedings{	  10.1145/3573834,
  title		= {AISS '22: Proceedings of the 4th International Conference
		  on Advanced Information Science and System},
  year		= {2022},
  isbn		= {9781450397933},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sanya, China}
}

@Proceedings{	  10.1145/3556384,
  title		= {SPML '22: Proceedings of the 2022 5th International
		  Conference on Signal Processing and Machine Learning},
  year		= {2022},
  isbn		= {9781450396912},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dalian, China}
}

@Proceedings{	  10.5555/3571885,
  title		= {SC '22: Proceedings of the International Conference on
		  High Performance Computing, Networking, Storage and
		  Analysis},
  year		= {2022},
  isbn		= {9784665454445},
  publisher	= {IEEE Press},
  abstract	= {This volume, containing the accepted technical papers and
		  ACM Gordon Bell prize finalists, captures the best current
		  research in all aspects of High Performance Computing
		  (HPC). The SC22 Archive at the conference web site
		  sc22.supercomputing.org complements this volume by
		  collecting other high quality, peer-reviewed material
		  including research posters, the visualization &amp; data
		  analytics showcase, panels, birds of a feather, workshops,
		  and tutorials.},
  location	= {Dallas, Texas}
}

@Proceedings{	  10.1145/3527188,
  title		= {HAI '22: Proceedings of the 10th International Conference
		  on Human-Agent Interaction},
  year		= {2022},
  isbn		= {9781450393232},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Christchurch, New Zealand}
}

@Proceedings{	  10.1145/3500868,
  title		= {CSCW'22 Companion: Companion Publication of the 2022
		  Conference on Computer Supported Cooperative Work and
		  Social Computing},
  year		= {2022},
  isbn		= {9781450391900},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Virtual Event, Taiwan}
}

@Proceedings{	  10.1145/3577530,
  title		= {CSAI '22: Proceedings of the 2022 6th International
		  Conference on Computer Science and Artificial
		  Intelligence},
  year		= {2022},
  isbn		= {9781450397773},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@Book{		  10.1145/3563659,
  editor	= {Lugrin, Birgit and Pelachaud, Catherine and Traum, David},
  title		= {The Handbook on Socially Interactive Agents: 20 years of
		  Research on Embodied Conversational Agents, Intelligent
		  Virtual Agents, and Social Robotics Volume 2:
		  Interactivity, Platforms, Application},
  year		= {2022},
  isbn		= {9781450398961},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  volume	= {48},
  abstract	= {The Handbook on Socially Interactive Agents provides a
		  comprehensive overview of the research fields of Embodied
		  Conversational Agents, Intelligent Virtual Agents, and
		  Social Robotics. Socially Interactive Agents (SIAs),
		  whether virtually or physically embodied, are autonomous
		  agents that are able to perceive an environment including
		  people or other agents, reason, and decide how to interact,
		  and express attitudes such as emotions, engagement, or
		  empathy. They are capable of interacting with people and
		  each other in a socially intelligent manner using
		  multimodal communicative behaviors with the goal to support
		  humans in various domains.Written by international experts
		  in their respective fields, the book summarizes research in
		  the many important research communities pertinent for SIAs,
		  while discussing current challenges and future directions.
		  The handbook provides easy access to modeling and studying
		  SIAs for researchers and students and aims at further
		  bridging the gap between the research communities
		  involved.In two volumes, the book clearly structures the
		  vast body of research. The first volume starts by
		  introducing what is involved in SIAs research, in
		  particular research methodologies and ethical implications
		  of developing SIAs. It further examines research on
		  appearance and behavior, focusing on multimodality.
		  Finally, social cognition for SIAs is investigated by
		  different theoretical models and phenomena such as theory
		  of mind or pro-sociality. The second volume starts with
		  perspectives on interaction, examined from different angles
		  such as interaction in social space, group interaction, or
		  long-term interaction. It also includes an extensive
		  overview summarizing research and systems of human-agent
		  platforms and of some of the major application areas of
		  SIAs such as education, aging support, autism or games.}
}

@Proceedings{	  10.5555/3581644,
  title		= {CNSM '22: Proceedings of the 18th International Conference
		  on Network and Service Management},
  year		= {2022},
  isbn		= {9783903176515},
  publisher	= {International Federation for Information Processing},
  address	= {Laxenburg, AUT},
  abstract	= {CNSM 2022 focuses on the theme "Intelligent Management of
		  Disruptive Network Technologies and Services", that aims at
		  capturing emerging approaches and intelligent solutions for
		  dealing with disruptive network technologies, as well as
		  associated services and applications.},
  location	= {Thessaloniki, Greece}
}

@Proceedings{	  10.1145/3561613,
  title		= {ICCCV '22: Proceedings of the 5th International Conference
		  on Control and Computer Vision},
  year		= {2022},
  isbn		= {9781450397315},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@Proceedings{	  10.1145/3545948,
  title		= {RAID '22: Proceedings of the 25th International Symposium
		  on Research in Attacks, Intrusions and Defenses},
  year		= {2022},
  isbn		= {9781450397049},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Limassol, Cyprus}
}

@Proceedings{	  10.1145/3411763,
  title		= {CHI EA '21: Extended Abstracts of the 2021 CHI Conference
		  on Human Factors in Computing Systems},
  year		= {2021},
  isbn		= {9781450380959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Yokohama, Japan}
}

@Proceedings{	  10.1145/3582580,
  title		= {ICETM '22: Proceedings of the 2022 5th International
		  Conference on Education Technology Management},
  year		= {2022},
  isbn		= {9781450398015},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lincoln, United Kingdom}
}

@Proceedings{	  10.1145/3585967,
  title		= {icWCSN '23: Proceedings of the 2023 10th International
		  Conference on Wireless Communication and Sensor Networks},
  year		= {2023},
  isbn		= {9781450398466},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Chengdu, China}
}

@Proceedings{	  10.1145/3582099,
  title		= {AICCC '22: Proceedings of the 2022 5th Artificial
		  Intelligence and Cloud Computing Conference},
  year		= {2022},
  isbn		= {9781450398749},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Osaka, Japan}
}

@Proceedings{	  10.1145/3539637,
  title		= {WebMedia '22: Proceedings of the Brazilian Symposium on
		  Multimedia and the Web},
  year		= {2022},
  isbn		= {9781450394093},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Curitiba, Brazil}
}

@Proceedings{	  10.1145/3590837,
  title		= {ICIMMI '22: Proceedings of the 4th International
		  Conference on Information Management &amp; Machine
		  Intelligence},
  year		= {2022},
  isbn		= {9781450399937},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Jaipur, India}
}

@Proceedings{	  10.1145/3616712,
  title		= {ICEME '23: Proceedings of the 2023 14th International
		  Conference on E-business, Management and Economics},
  year		= {2023},
  isbn		= {9798400708022},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@Book{		  10.1145/3477355,
  editor	= {Jones, Cliff B. and Misra, Jayadev},
  title		= {Theories of Programming: The Life and Works of Tony
		  Hoare},
  year		= {2021},
  isbn		= {9781450387286},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  volume	= {39},
  abstract	= {Sir Tony Hoare has had an enormous influence on computer
		  science, from the Quicksort algorithm to the science of
		  software development, concurrency and program verification.
		  His contributions have been widely recognised: He was
		  awarded the ACM’s Turing Award in 1980, the Kyoto Prize
		  from the Inamori Foundation in 2000, and was knighted for
		  “services to education and computer science” by Queen
		  Elizabeth II of England in 2000.This book presents the
		  essence of his various works—the quest for effective
		  abstractions—both in his own words as well as chapters
		  written by leading experts in the field, including many of
		  his research collaborators. In addition, this volume
		  contains biographical material, his Turing award lecture,
		  the transcript of an interview and some of his seminal
		  papers.Hoare’s foundational paper “An Axiomatic Basis
		  for Computer Programming”, presented his approach,
		  commonly known as Hoare Logic, for proving the correctness
		  of programs by using logical assertions. Hoare Logic and
		  subsequent developments have formed the basis of a wide
		  variety of software verification efforts. Hoare was
		  instrumental in proposing the Verified Software Initiative,
		  a cooperative international project directed at the
		  scientific challenges of large-scale software verification,
		  encompassing theories, tools and experiments.Tony Hoare’s
		  contributions to the theory and practice of concurrent
		  software systems are equally impressive. The process
		  algebra called Communicating Sequential Processes (CSP) has
		  been one of the fundamental paradigms, both as a
		  mathematical theory to reason about concurrent computation
		  as well as the basis for the programming language occam.
		  CSP served as a framework for exploring several ideas in
		  denotational semantics such as powerdomains, as well as
		  notions of abstraction and refinement. It is the basis for
		  a series of industrial-strength tools which have been
		  employed in a wide range of applications.This book also
		  presents Hoare’s work in the last few decades. These
		  works include a rigorous approach to specifications in
		  software engineering practice, including procedural and
		  data abstractions, data refinement, and a modular theory of
		  designs. More recently, he has worked with collaborators to
		  develop Unifying Theories of Programming (UTP). Their goal
		  is to identify the common algebraic theories that lie at
		  the core of sequential, concurrent, reactive and
		  cyber-physical computations. Theories of Programming: The
		  Life and Works of Tony Hoare’ is available as a printed
		  book (DOI: ) and an on-line version. In addition to the
		  book itself, a number of on-line resources might be of
		  interest to readers: A bibliography of Tony Hoare’s
		  papers with clickable DOIs/URLs where available (ACM:
		  INSERT URL)Appendix E of the book provides links to talks
		  and interviews featuring Tony Hoare ()The Oxford archive of
		  Hoare’s manuscripts: Supplementary Material: Tony
		  Hoare’ is a PDF of additional material (not included in
		  the book) containing the following: Stories from a Life in
		  Interesting Times (A transcription by Jayadev Misra of Tony
		  Hoare’s acceptance speech for the 2000 Kyoto prize)Tony
		  Hoare’s Heidelberg comments: (A transcription by Margaret
		  Gray of Tony Hoare’s part in the 2020 Heidelberg
		  event)Milestones in Tony’s Life and Work: A ‘cv’ of
		  Tony Hoare prepared by Margaret GrayExtended version -
		  ’Bernard Sufrin: Teaching at Belfast and Oxford’}
}

@Proceedings{	  10.1145/3613424,
  title		= {MICRO '23: Proceedings of the 56th Annual IEEE/ACM
		  International Symposium on Microarchitecture},
  year		= {2023},
  isbn		= {9798400703294},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Toronto, ON, Canada}
}

@Proceedings{	  10.1145/3593743,
  title		= {C&amp;T '23: Proceedings of the 11th International
		  Conference on Communities and Technologies},
  year		= {2023},
  isbn		= {9798400707582},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lahti, Finland}
}

@Proceedings{	  10.1145/3568231,
  title		= {SIET '22: Proceedings of the 7th International Conference
		  on Sustainable Information Engineering and Technology},
  year		= {2022},
  isbn		= {9781450397117},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Malang, Indonesia}
}

@Article{	  10.1145/3641850,
  author	= {Bi, Zhen and Chen, Jing and Jiang, Yinuo and Xiong, Feiyu
		  and Guo, Wei and Chen, Huajun and Zhang, Ningyu},
  title		= {CodeKGC: Code Language Model for Generative Knowledge
		  Graph Construction},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {3},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3641850},
  doi		= {10.1145/3641850},
  abstract	= {Current generative knowledge graph construction approaches
		  usually fail to capture structural knowledge by simply
		  flattening natural language into serialized texts or a
		  specification language. However, large generative language
		  model trained on structured data such as code has
		  demonstrated impressive capability in understanding natural
		  language for structural prediction and reasoning tasks.
		  Intuitively, we address the task of generative knowledge
		  graph construction with code language model: given a
		  code-format natural language input, the target is to
		  generate triples which can be represented as code
		  completion tasks. Specifically, we develop schema-aware
		  prompts that effectively utilize the semantic structure
		  within the knowledge graph. As code inherently possesses
		  structure, such as class and function definitions, it
		  serves as a useful model for prior semantic structural
		  knowledge. Furthermore, we employ a rationale-enhanced
		  generation method to boost the performance. Rationales
		  provide intermediate steps, thereby improving knowledge
		  extraction abilities. Experimental results indicate that
		  the proposed approach can obtain better performance on
		  benchmark datasets compared with baselines.1},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= mar,
  articleno	= {45},
  numpages	= {16},
  keywords	= {Knowledge graph construction, code, language model}
}

@InProceedings{	  10.1145/3640457.3691703,
  author	= {Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni
		  and Marras, Mirko and Soccol, Alessandro},
  title		= {KGGLM: A Generative Language Model for Generalizable
		  Knowledge Graph Representation Learning in Recommendation},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3691703},
  doi		= {10.1145/3640457.3691703},
  abstract	= {Current recommendation methods based on knowledge graphs
		  rely on entity and relation representations for several
		  steps along the pipeline, with knowledge completion and
		  path reasoning being the most influential. Despite their
		  similarities, the most effective representation methods for
		  these steps differ, leading to inefficiencies, limited
		  representativeness, and reduced interpretability. In this
		  paper, we introduce KGGLM, a decoder-only Transformer model
		  designed for generalizable knowledge representation
		  learning to support recommendation. The model is trained on
		  generic paths sampled from the knowledge graph to capture
		  foundational patterns, and then fine-tuned on paths
		  specific of the downstream step (knowledge completion and
		  path reasoning in our case). Experiments on ML1M and LFM1M
		  show that KGGLM beats twenty-two baselines in effectiveness
		  under both knowledge completion and recommendation. Source
		  code and pre-processed data sets are available at
		  https://github.com/mirkomarras/kgglm.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {1079–1084},
  numpages	= {6},
  keywords	= {Generative Artificial Intelligence., Knowledge Completion,
		  Knowledge Graph, Knowledge Graph Embeddings, Knowledge
		  Representation Learning, Language Model, Recommendation},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3597503.3639157,
  author	= {Su, Yanqi and Liao, Dianshu and Xing, Zhenchang and Huang,
		  Qing and Xie, Mulong and Lu, Qinghua and Xu, Xiwei},
  title		= {Enhancing Exploratory Testing by Large Language Model and
		  Knowledge Graph},
  year		= {2024},
  isbn		= {9798400702174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3597503.3639157},
  doi		= {10.1145/3597503.3639157},
  abstract	= {Exploratory testing leverages the tester's knowledge and
		  creativity to design test cases for effectively uncovering
		  system-level bugs from the end user's perspective.
		  Researchers have worked on test scenario generation to
		  support exploratory testing based on a system knowledge
		  graph, enriched with scenario and oracle knowledge from bug
		  reports. Nevertheless, the adoption of this approach is
		  hindered by difficulties in handling bug reports of
		  inconsistent quality and varied expression styles, along
		  with the infeasibility of the generated test scenarios. To
		  overcome these limitations, we utilize the superior natural
		  language understanding (NLU) capabilities of Large Language
		  Models (LLMs) to construct a System KG of User Tasks and
		  Failures (SysKG-UTF). Leveraging the system and bug
		  knowledge from the KG, along with the logical reasoning
		  capabilities of LLMs, we generate test scenarios with high
		  feasibility and coherence. Particularly, we design
		  chain-of-thought (CoT) reasoning to extract human-like
		  knowledge and logical reasoning from LLMs, simulating a
		  developer's process of validating test scenario
		  feasibility. Our evaluation shows that our approach
		  significantly enhances the KG construction, particularly
		  for bug reports with low quality. Furthermore, our approach
		  generates test scenarios with high feasibility and
		  coherence. The user study further proves the effectiveness
		  of our generated test scenarios in supporting exploratory
		  testing. Specifically, 8 participants find 36 bugs from 8
		  seed bugs in two hours using our test scenarios, a
		  significant improvement over the 21 bugs found by the
		  state-of-the-art baseline.},
  booktitle	= {Proceedings of the IEEE/ACM 46th International Conference
		  on Software Engineering},
  articleno	= {98},
  numpages	= {12},
  keywords	= {exploratory testing, knowledge graph, AI chain, prompt
		  engineering},
  location	= {Lisbon, Portugal},
  series	= {ICSE '24}
}

@InProceedings{	  10.1145/3637528.3672503,
  author	= {Wang, Haixun},
  title		= {Generative AI in E-Commerce: What Can We Expect?},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3672503},
  doi		= {10.1145/3637528.3672503},
  abstract	= {The impact of generative AI on e-commerce is profound. It
		  has significantly improved the understanding of user intent
		  and serves as a comprehensive product knowledge graph.
		  However, the most substantial disruptions are yet to come,
		  partic- ularly through the rise of autonomous agents. In
		  this talk, I will outline a tentative path toward a future
		  where e-commerce not only offers an unparalleled customer
		  experience but also thrives in a world dominated by
		  generative AI and autonomous agents.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4739–4740},
  numpages	= {2},
  keywords	= {e-commerce, generative ai, information retrieval, large
		  language models},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3627673.3679087,
  author	= {Mane, Mansi Ranjit and Gligorijevic, Djordje and Wang,
		  Dingxian and Shahrasbi, Behzed and Biswas, Topojoy and
		  Korpeoglu, Evren and Savvides, Marios},
  title		= {Workshop on Generative AI for E-commerce},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679087},
  doi		= {10.1145/3627673.3679087},
  abstract	= {The "Gen AI for E-commerce" workshop explores the role of
		  Generative Artificial Intelligence in transforming
		  e-commerce through enhanced user experience and operational
		  efficiency. E-commerce companies grapple with multiple
		  challenges such as lack of quality content for products,
		  subpar user experience, sparse datasets etc. Gen AI offers
		  significant potential to address these complexities. Yet,
		  deploying these technologies at scale presents challenges
		  such as hallucination in data, excessive costs, increased
		  latency response, and limited generalization in sparse data
		  environments. This workshop will bring together experts
		  from academia and industry to discuss these challenges and
		  opportunities, aiming to showcase case studies,
		  breakthroughs, and insights into practical implementations
		  of Gen AI in e-commerce.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5592–5595},
  numpages	= {4},
  keywords	= {LLMs, e-commerce, generative AI},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3664647.3681327,
  author	= {Zhang, Yichi and Chen, Zhuo and Guo, Lingbing and Xu,
		  Yajing and Zhang, Wen and Chen, Huajun},
  title		= {Making Large Language Models Perform Better in Knowledge
		  Graph Completion},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681327},
  doi		= {10.1145/3664647.3681327},
  abstract	= {Large language model (LLM) based knowledge graph
		  completion (KGC) aims to predict the missing triples in the
		  KGs with LLMs. However, research about LLM-based KGC fails
		  to sufficiently harness LLMs' inference proficiencies,
		  overlooking critical structural information integral to
		  KGs. In this paper, we explore methods to incorporate
		  structural information into the LLMs, with the overarching
		  goal of facilitating structure-aware reasoning. We first
		  discuss on the existing LLM paradigms like in-context
		  learning and instruction tuning, proposing basic structural
		  information injection approaches. Then we propose a
		  Knowledge Prefix Adapter (KoPA) to fulfill this stated
		  goal. KoPA uses a structural pre-training phase to
		  comprehend the intricate entities and relations within KGs,
		  representing them as structural embeddings. Then KoPA
		  communicates such cross-modal structural information
		  understanding to the LLMs through a knowledge prefix
		  adapter which projects the structural embeddings into the
		  textual space and obtains virtual knowledge tokens
		  positioned as a prefix of the input prompt. We conduct
		  comprehensive experiments and provide incisive analysis.
		  Our code and data are available at
		  https://github.com/zjukg/KoPA.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {233–242},
  numpages	= {10},
  keywords	= {cross-modal adapter, graph-text fusion, knowledge graph
		  completion, knowledge graphs, large language models},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3637528.3671542,
  author	= {Baughman, Aaron and Morales, Eduardo and Agarwal, Rahul
		  and Akay, Gozde and Feris, Rogerio and Johnson, Tony and
		  Hammer, Stephen and Karlinsky, Leonid},
  title		= {Large Scale Generative AI Text Applied to Sports and
		  Music},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671542},
  doi		= {10.1145/3637528.3671542},
  abstract	= {We address the problem of scaling up the production of
		  media content, including commentary and personalized news
		  stories, for large-scale sports and music events worldwide.
		  Our approach relies on generative AI models to transform a
		  large volume of multimodal data (e.g., videos, articles,
		  real-time scoring feeds, statistics, and fact sheets) into
		  coherent and fluent text. Based on this approach, we
		  introduce, for the first time, an AI commentary system,
		  which was deployed to produce automated narrations for
		  highlight packages at the 2023 US Open, Wimbledon, and
		  Masters tournaments. In the same vein, our solution was
		  extended to create personalized content for ESPN Fantasy
		  Football and stories about music artists for the GRAMMY
		  awards. These applications were built using a common
		  software architecture achieved a 15x speed improvement with
		  an average Rouge-L of 82.00 and perplexity of 6.6. Our work
		  was successfully deployed at the aforementioned events,
		  supporting 90 million fans around the world with 8 billion
		  page views, continuously pushing the bounds on what is
		  possible at the intersection of sports, entertainment, and
		  AI.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4784–4792},
  numpages	= {9},
  keywords	= {applied computing, generative ai, large scale computing,
		  neural networks, sports and entertainment},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3644820,
  author	= {Yuan, Xiaowei and Liu, Kang and Wang, Yequan},
  title		= {Contrastive Language-knowledge Graph Pre-training},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3644820},
  doi		= {10.1145/3644820},
  abstract	= {Recent years have witnessed a surge of academic interest
		  in knowledge-enhanced pre-trained language models (PLMs)
		  that incorporate factual knowledge to enhance
		  knowledge-driven applications. Nevertheless, existing
		  studies primarily focus on shallow, static, and separately
		  pre-trained entity embeddings, with few delving into the
		  potential of deep contextualized knowledge representation
		  for knowledge incorporation. Consequently, the performance
		  gains of such models remain limited. In this article, we
		  introduce a simple yet effective knowledge-enhanced model,
		  College (Contrastive Language-Knowledge Graph
		  Pre-training), which leverages contrastive learning to
		  incorporate factual knowledge into PLMs. This approach
		  maintains the knowledge in its original graph structure to
		  provide the most available information and circumvents the
		  issue of heterogeneous embedding fusion. Experimental
		  results demonstrate that our approach achieves more
		  effective results on several knowledge-intensive tasks
		  compared to previous state-of-the-art methods. Our code and
		  trained models are available at .},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {51},
  numpages	= {21},
  keywords	= {Language Model, Knowledge Graph, Contrastive Learning}
}

@Article{	  10.1145/3686803,
  author	= {Li, Jialong and Zhang, Mingyue and Li, Nianyu and Weyns,
		  Danny and Jin, Zhi and Tei, Kenji},
  title		= {Generative AI for Self-Adaptive Systems: State of the Art
		  and Research Roadmap},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {3},
  issn		= {1556-4665},
  url		= {https://doi.org/10.1145/3686803},
  doi		= {10.1145/3686803},
  abstract	= {Self-adaptive systems (SASs) are designed to handle
		  changes and uncertainties through a feedback loop with four
		  core functionalities: monitoring, analyzing, planning, and
		  execution. Recently, generative artificial intelligence
		  (GenAI), especially the area of large language models, has
		  shown impressive performance in data comprehension and
		  logical reasoning. These capabilities are highly aligned
		  with the functionalities required in SASs, suggesting a
		  strong potential to employ GenAI to enhance SASs. However,
		  the specific benefits and challenges of employing GenAI in
		  SASs remain unclear. Yet, providing a comprehensive
		  understanding of these benefits and challenges is complex
		  due to several reasons: limited publications in the SAS
		  field, the technological and application diversity within
		  SASs, and the rapid evolution of GenAI technologies. To
		  that end, this article aims to provide researchers and
		  practitioners a comprehensive snapshot that outlines the
		  potential benefits and challenges of employing GenAI’s
		  within SAS. Specifically, we gather, filter, and analyze
		  literature from four distinct research fields and organize
		  them into two main categories to potential benefits: (i)
		  enhancements to the autonomy of SASs centered around the
		  specific functions of the MAPE-K feedback loop, and (ii)
		  improvements in the interaction between humans and SASs
		  within human-on-the-loop settings. From our study, we
		  outline a research roadmap that highlights the challenges
		  of integrating GenAI into SASs. The roadmap starts with
		  outlining key research challenges that need to be tackled
		  to exploit the potential for applying GenAI in the field of
		  SAS. The roadmap concludes with a practical reflection,
		  elaborating on current shortcomings of GenAI and proposing
		  possible mitigation strategies.†},
  journal	= {ACM Trans. Auton. Adapt. Syst.},
  month		= sep,
  articleno	= {13},
  numpages	= {60},
  keywords	= {Self-Adaptive Systems, MAPE, Generative AI, Large Language
		  Model, diffusion model, survey}
}

@InProceedings{	  10.1145/3639631.3639689,
  author	= {Qian, Jing and Li, Gangmin and Atkinson, Katie and Yue,
		  Yong},
  title		= {Enhancing Text Comprehension via Fusing Pre-trained
		  Language Model with Knowledge Graph},
  year		= {2024},
  isbn		= {9798400709203},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639631.3639689},
  doi		= {10.1145/3639631.3639689},
  abstract	= {Pre-trained language models (PLMs) such as BERT and GPTs
		  capture rich linguistic and syntactic knowledge from
		  pre-training over large-scale text corpora, which can be
		  further fine-tuned for specific downstream tasks. However,
		  these models still have limitations as they rely on
		  knowledge gained from plain text and ignore structured
		  knowledge such as knowledge graphs (KGs). Recently, there
		  has been a growing trend of explicitly integrating KGs into
		  PLMs to improve their performance. For instance, K-BERT
		  incorporates KG triples as domain-specific supplements into
		  input sentences. Nevertheless, we have observed that such
		  methods do not consider the semantic relevance between the
		  introduced knowledge and the original input sentence,
		  leading to the issue of knowledge impurities. To address
		  this issue, we propose a semantic matching-based approach
		  that enriches the input text with knowledge extracted from
		  an external KG. The architecture of our model comprises
		  three components: the knowledge retriever (KR), the
		  knowledge injector (KI), and the knowledge aggregator (KA).
		  The KR, built upon the sentence representation learning
		  model (i.e. CoSENT), retrieves triples with high semantic
		  relevance to the input sentence from an external KG to
		  alleviate the issue of knowledge impurities. The KI then
		  integrates the retrieved triples from the KR into the input
		  text by converting the original sentence into a knowledge
		  tree with multiple branches, the knowledge tree is
		  transformed into an accessible sequence of text that can be
		  fed into the KA. Finally, the KA takes the flattened
		  knowledge tree and passes it through an embedding layer and
		  a masked Transformer encoder. We conducted extensive
		  evaluations on eight datasets covering five text
		  comprehension tasks, and the experimental results
		  demonstrate that our approach exhibits competitive
		  advantages over popular knowledge-enhanced PLMs such as
		  K-BERT and ERNIE.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  pages		= {353–360},
  numpages	= {8},
  keywords	= {knowledge graphs, natural language understanding, sentence
		  representation learning},
  location	= {Sanya, China},
  series	= {ACAI '23}
}

@Article{	  10.1145/3708478,
  author	= {Meng, Zhixin and Zhan, Shaoxiong and Xu, Ruiqing and
		  Mayer, Wolfgang and Zhu, Ye and Zhang, Hong-Yu and He,
		  Chuan and He, Keqing and Cheng, Debo and Feng, Zaiwen},
  title		= {Domain Ontology-Driven Knowledge Graph Generation from
		  Text},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708478},
  doi		= {10.1145/3708478},
  abstract	= {A knowledge graph serves as a unified and standardized
		  representation for extracting and representing textual
		  information. In the field of knowledge extraction and
		  representation research, named entity recognition and
		  relation extraction provide effective solutions for
		  knowledge graph generation tasks. However, it is a
		  challenge that lies in extracting domain-specific knowledge
		  from the rich and general textual corpora and generating
		  corresponding domain knowledge graphs to support
		  domain-specific reasoning, question-answering, and
		  decision-making tasks. The hierarchical domain knowledge
		  representation model (i.e. domain ontology) provides a
		  solution for this problem. Therefore, we propose an
		  end-to-end approach based on domain ontology embedding and
		  pre-trained language models for domain knowledge graph
		  generation from text, which incorporates domain node
		  recognition and domain relation extraction phases. We
		  evaluated our domain ontology-driven model on the
		  Wikidata-TekGen dataset and the DBpedia-WebNLG dataset, and
		  the results indicate that our approach based on the
		  pre-trained language models with fewer parameters compared
		  with the baseline models has significantly contributed to
		  the domain knowledge graph generation without prompts.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Probab. Mach. Learn.},
  month		= dec,
  keywords	= {Domain Knowledge Graph, Domain Ontology, Ontology
		  Embedding, Domain Node Recognition, Domain Relation
		  Extraction}
}

@Article{	  10.1145/3688850,
  author	= {Yang, Guangqian and Zhang, Lei and Liu, Yi and Xie,
		  Hongtao and Mao, Zhendong},
  title		= {Exploiting Pre-Trained Language Models for Black-Box
		  Attack against Knowledge Graph Embeddings},
  year		= {2024},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {19},
  number	= {1},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3688850},
  doi		= {10.1145/3688850},
  abstract	= {Despite the emerging research on adversarial attacks
		  against knowledge graph embedding (KGE) models, most of
		  them focus on white-box attack settings. However, white-box
		  attacks are difficult to apply in practice compared to
		  black-box attacks since they require access to model
		  parameters that are unlikely to be provided. In this
		  article, we propose a novel black-box attack method that
		  only requires access to knowledge graph data, making it
		  more realistic in real-world attack scenarios.
		  Specifically, we utilize pre-trained language models (PLMs)
		  to encode text features of the knowledge graphs, an aspect
		  neglected by previous research. We then employ these
		  encoded text features to identify the most influential
		  triples for constructing corrupted triples for the attack.
		  To improve the transferability of the attack, we further
		  propose to fine-tune the PLM model by enriching triple
		  embeddings with structure information. Extensive
		  experiments conducted on two knowledge graph datasets
		  illustrate the effectiveness of our proposed method.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= nov,
  articleno	= {1},
  numpages	= {14},
  keywords	= {Knowledge Graph, Adversarial Attack, Language Model}
}

@InProceedings{	  10.1145/3650400.3650526,
  author	= {Li, Wenqing and Qi, Xiaoman and Zhao, Qi and Wang, Chen
		  and Wu, Qiongyu and Tang, Xue-song},
  title		= {Knowledge Graph-Based Credibility Evaluation Method for
		  Electric Grid Large Language Model Knowledge
		  Question-Answering},
  year		= {2024},
  isbn		= {9798400708305},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3650400.3650526},
  doi		= {10.1145/3650400.3650526},
  abstract	= {In the field of electricity, specialized terminology is
		  often intricate and complex, making it challenging for
		  non-experts to comprehend. However, with the advancement of
		  artificial intelligence technology, the emergence of large
		  language models provides a new technological solution to
		  address this issue. Large language models, based on deep
		  learning techniques, have the capability to quickly
		  understand and interpret specialized terminology in the
		  electricity domain through learning from a vast corpus of
		  professional literature and data. They can then be applied
		  to various domains, including question-answering systems.
		  However, existing large language models still face issues
		  of unreliable outputs, necessitating a method to evaluate
		  their results and improve the quality of their
		  applications. We propose a knowledge graph-based
		  credibility evaluation method for electric grid large
		  language model knowledge question-answering. This method
		  aligns the answers generated by large language models with
		  the knowledge graph of a local knowledge base and
		  calculates their cosine similarity and Pearson correlation
		  coefficient. We batch-process the answers from the large
		  language model into an electricity dataset and validate
		  them using this method. Experimental results demonstrate
		  that this method can accurately and efficiently reflect the
		  relevance between texts, providing a reliable scoring basis
		  for question-answering by large models in vertical domains.
		  Future research can focus on exploring other embedding
		  methods that can better extract semantic relationships
		  between texts and validating the feasibility of this method
		  in vertical domains other than electricity.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {754–759},
  numpages	= {6},
  location	= {Xiamen, China},
  series	= {EITCE '23}
}

@InProceedings{	  10.1145/3695719.3695726,
  author	= {Payne, Lucas and Xie, Mengjun},
  title		= {Log File Anomaly Detection Using Knowledge Graph
		  Completion},
  year		= {2024},
  isbn		= {9798400716867},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3695719.3695726},
  doi		= {10.1145/3695719.3695726},
  abstract	= {Log files can be vital in detecting anomalous behavior in
		  a computing system. However, the largely unstructured
		  format of log files makes it difficult for computers to
		  process them, and their large volume makes it infeasible
		  for large-scale manual analysis. Previous research has
		  suggested converting log messages into knowledge graph data
		  for querying but does not consider anomaly detection as the
		  downstream task. Other research has suggested using
		  knowledge graph completion for anomaly detection, but it
		  does not include the conversion of log messages to log
		  data. This study fills in the gaps by presenting an
		  end-to-end system that generates knowledge graph data from
		  log messages and applies the knowledge graph completion
		  task to binary classification for anomaly detection.
		  Results are reported using both knowledge graph completion
		  and classification metrics, and they demonstrate the
		  feasibility of the proposed method.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Deep Learning Technologies},
  pages		= {42–48},
  numpages	= {7},
  keywords	= {anomaly detection, knowledge graph, knowledge graph
		  completion, link prediction, log file},
  location	= { },
  series	= {ICDLT '24}
}

@InProceedings{	  10.1145/3627673.3680266,
  author	= {Ni, Bo},
  title		= {Reliable Knowledge Graph Reasoning with Uncertainty
		  Quantification},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680266},
  doi		= {10.1145/3627673.3680266},
  abstract	= {Recently, Knowledge Graphs (KGs) have been successfully
		  coupled with Large Language Models (LLMs) to mitigate their
		  hallucinations and enhance their reasoning capability,
		  e.g., KG-based retrieval-augmented framework for
		  question-answering. However, current KG-LLM frameworks lack
		  rigorous uncertainty estimation, limiting their reliable
		  deployment in high-stake applications where the cost of
		  errors is significant. To address this crucial gap, we
		  propose a new trustworthy KG-LLM framework,
		  UaG(&lt;u&gt;U&lt;/u&gt;ncertainty &lt;u&gt;A&lt;/u&gt;ware
		  &lt;u&gt;G&lt;/u&gt;raph Reasoning), which incorporates
		  uncertainty quantification into the KG-LLM framework. We
		  design an uncertainty-aware multi-step reasoning framework
		  that leverages conformal prediction to provide a
		  theoretical guarantee on the prediction set. To manage the
		  error rate of the multi-step process, we additionally
		  introduce an error rate control module to adjust the error
		  rate within the individual components. Our preliminary
		  results demonstrate that UaG can achieve the desired
		  theoretical coverage while maintaining a reasonable
		  prediction set size.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5463–5466},
  numpages	= {4},
  keywords	= {knowledge graph, question answering, trustworthy AI,
		  uncertainty quantification},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3677524,
  author	= {Cao, Yukun and Jin, Chengkun and Tang, Yijia and Wei,
		  ZiYue},
  title		= {Word Sense Disambiguation Combining Knowledge Graph and
		  Text Hierarchical Structure},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {12},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3677524},
  doi		= {10.1145/3677524},
  abstract	= {Current supervised word sense disambiguation models have
		  obtained high disambiguation results using annotated
		  information of different word senses and pre-trained
		  language models. However, the semantic data of the
		  supervised word sense disambiguation models are in the form
		  of short texts, and much of the corpus information is not
		  rich enough to distinguish the semantics in different
		  scenarios. This article proposes a bi-encoder word sense
		  disambiguation method combining a knowledge graph and text
		  hierarchy structure, by introducing structured knowledge
		  from the knowledge graph to supplement more extended
		  semantic information, using the hierarchy of contextual
		  input text to describe the meaning of words and phrases,
		  and constructing a BERT-based bi-encoder, introducing a
		  graph attention network to reduce the noise information in
		  the contextual input text, so as to improve the
		  disambiguation accuracy of the target words in phrase form
		  and ultimately improve the disambiguation effectiveness of
		  the method. By comparing the method with the latest nine
		  comparison algorithms in five test datasets, the
		  disambiguation accuracy of the method mostly outperformed
		  the comparison algorithms and achieved better results.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {161},
  numpages	= {16},
  keywords	= {Word sense disambiguation, knowledge graph, BERT, graph
		  attention network}
}

@InProceedings{	  10.1145/3700906.3700925,
  author	= {Liu, Tongtong},
  title		= {Multi-modal Knowledge Graph Completion: A Survey},
  year		= {2024},
  isbn		= {9798400707032},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3700906.3700925},
  doi		= {10.1145/3700906.3700925},
  abstract	= {In recent years, with the rapid development of artificial
		  intelligence, multi-modal knowledge graph completion
		  (MMKGC) has become increasingly important. Many scholars
		  have conducted in-depth research on multi-modal knowledge
		  graphs (MMKGs), leading to the proposal of numerous MMKGC
		  models. Summarizing the current state of research is
		  crucial for guiding future studies. This survey aims to
		  review the current advanced techniques for MMKGC. By
		  analyzing and elaborating on the value and categories of
		  MMKGs in detail, we summarize the challenges faced by
		  existing MMKGC methods. Our work provides valuable insights
		  and explorations for the research and application of
		  completing MMKGs.},
  booktitle	= {Proceedings of the International Conference on Image
		  Processing, Machine Learning and Pattern Recognition},
  pages		= {116–121},
  numpages	= {6},
  keywords	= {Explainable Artificial Intelligence, Few-shot Learning,
		  Multi-modal Knowledge Graph Completion},
  location	= { },
  series	= {IPMLP '24}
}

@Article{	  10.1145/3686806,
  author	= {Cheng, Kewei and Ahmed, Nesreen K. and Rossi, Ryan A. and
		  Willke, Theodore and Sun, Yizhou},
  title		= {Neural-Symbolic Methods for Knowledge Graph Reasoning: A
		  Survey},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {9},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3686806},
  doi		= {10.1145/3686806},
  abstract	= {Neural symbolic knowledge graph (KG) reasoning offers a
		  promising approach that combines the expressive power of
		  symbolic reasoning with the learning capabilities inherent
		  in neural networks. This survey provides a comprehensive
		  overview of advancements, techniques, and challenges in the
		  field of neural symbolic KG reasoning. The survey
		  introduces the fundamental concepts of KGs and symbolic
		  logic, followed by an exploration of three significant KG
		  reasoning tasks: KG completion, complex query answering,
		  and logical rule learning. For each task, we thoroughly
		  discuss three distinct categories of methods: pure symbolic
		  methods, pure neural approaches, and the integration of
		  neural networks and symbolic reasoning methods known as
		  neural-symbolic. We carefully analyze and compare the
		  strengths and limitations of each category of methods to
		  provide a comprehensive understanding. By synthesizing
		  recent research contributions and identifying open research
		  directions, this survey aims to equip researchers and
		  practitioners with a comprehensive understanding of the
		  state-of-the-art in neural symbolic KG reasoning, fostering
		  future advancements in this interdisciplinary domain.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= nov,
  articleno	= {225},
  numpages	= {44},
  keywords	= {Knowledge Graph, Neural Symbolic Reasoning}
}

@InProceedings{	  10.1145/3627673.3679753,
  author	= {Zhang, Zhiqiang and Wen, Liqiang and Zhao, Wen},
  title		= {A GAIL Fine-Tuned LLM Enhanced Framework for Low-Resource
		  Knowledge Graph Question Answering},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679753},
  doi		= {10.1145/3627673.3679753},
  abstract	= {Recent studies on knowledge graph question answering
		  (KGQA) have focused on tackling complex inquiries to
		  enhance the applicability of models in real-life settings.
		  Unfortunately, KGQA models encounter significant challenges
		  due to the lack of high-quality annotated data, making it
		  difficult to accurately answer the diverse range of complex
		  natural language questions posed by users. Inspired by the
		  recent success of Large Language Models (LLMs), the burden
		  associated with manual annotation can be mitigated by
		  utilizing LLMs. However, the data generated directly by
		  LLMs may exhibit a potential distribution discrepancy with
		  real user queries. In this paper, we present an enhancement
		  framework that utilizes Generative Adversarial Imitation
		  Learning (GAIL) to fine-tune LLMs, which can address the
		  challenges inherent in the low-resource KGQA task.
		  Specifically, based on GAIL, the LLMs act as the generator
		  aiming to output samples resembling expert demonstrations.
		  Meanwhile, we utilize a paired discriminator to assess the
		  authenticity of generated sequences and their relevance to
		  the input SPARQL queries. Additionally, proximal policy
		  optimization is leveraged to stabilize the training of the
		  generator. Furthermore, we employ an automated algorithm to
		  controllably sample various SPARQL queries from the
		  knowledge graph, subsequently transforming them into
		  corresponding natural language questions using fine-tuned
		  LLMs. The synthetic dataset can serve as supplementary data
		  for training lightweight KGQA models in real-world
		  scenarios. Experimental results on the WebQuestionsSP,
		  ComplexWebQuestions, and GrailQA show that our framework
		  achieves state-of-the-art performance in a low-resource
		  setting, even approaching the performance of supervised
		  models.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3300–3309},
  numpages	= {10},
  keywords	= {generative adversarial imitation learning, knowledge
		  graph, large language model, question answering},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3589334.3645451,
  author	= {Long, Xiao and Zhuang, Liansheng and Li, Aodi and Li,
		  Houqiang and Wang, Shafei},
  title		= {Fact Embedding through Diffusion Model for Knowledge Graph
		  Completion},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645451},
  doi		= {10.1145/3589334.3645451},
  abstract	= {Knowledge graph embedding (KGE) is an efficient and
		  scalable method for knowledge graph completion tasks.
		  Existing KGE models typically map entities and relations
		  into a unified continuous vector space and define a score
		  function to capture the connectivity patterns among the
		  elements (entities and relations) of facts. The score on a
		  fact measures its plausibility in a knowledge graph (KG).
		  However, since the connectivity patterns are very complex
		  in a real knowledge graph, it is difficult to define an
		  explicit and efficient score function to capture them,
		  which also limits their performance. This paper argues that
		  plausible facts in a knowledge graph come from a
		  distribution in the low-dimensional fact space. Inspired by
		  this insight, this paper proposes a novel framework called
		  Fact Embedding through Diffusion Model (FDM) to address the
		  knowledge graph completion task. Instead of defining a
		  score function to measure the plausibility of facts in a
		  knowledge graph, this framework directly learns the
		  distribution of plausible facts from the known knowledge
		  graph and casts the entity prediction task into the
		  conditional fact generation task. Specifically, we
		  concatenate the elements embedding in a fact as a whole and
		  take it as input. Then, we introduce a Conditional Fact
		  Denoiser to learn the reverse denoising diffusion process
		  and generate the target fact embedding from noised data.
		  Extensive experiments demonstrate that FDM significantly
		  outperforms existing state-of-the-art methods in three
		  benchmark datasets.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2020–2029},
  numpages	= {10},
  keywords	= {knowledge graph, knowledge graph embedding, link
		  prediction},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1145/3637216,
  author	= {Ren, Xuhui and Chen, Tong and Nguyen, Quoc Viet Hung and
		  Cui, Lizhen and Huang, Zi and Yin, Hongzhi},
  title		= {Explicit Knowledge Graph Reasoning for Conversational
		  Recommendation},
  year		= {2024},
  issue_date	= {August 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {4},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3637216},
  doi		= {10.1145/3637216},
  abstract	= {Traditional recommender systems estimate user preference
		  on items purely based on historical interaction records,
		  thus failing to capture fine-grained yet dynamic user
		  interests and letting users receive recommendation only
		  passively. Recent conversational recommender systems (CRSs)
		  tackle those limitations by enabling recommender systems to
		  interact with the user to obtain her/his current preference
		  through a sequence of clarifying questions. Recently, there
		  has been a rise of using knowledge graphs (KGs) for CRSs,
		  where the core motivation is to incorporate the abundant
		  side information carried by a KG into both the
		  recommendation and conversation processes. However,
		  existing KG-based CRSs are subject to two defects: (1)
		  there is a semantic gap between the learned representations
		  of utterances and KG entities, hindering the retrieval of
		  relevant KG information; (2) the reasoning over KG is
		  mostly performed with the implicitly learned user
		  interests, overlooking the explicit signals from the
		  entities actually mentioned in the conversation.To address
		  these drawbacks, we propose a new CRS framework, namely,
		  the Knowledge Enhanced Conversational Reasoning (KECR)
		  model. As a user can reflect her/his preferences via both
		  attribute- and item-level expressions, KECR jointly embeds
		  the structured knowledge from two levels in the KG. A
		  mutual information maximization constraint is further
		  proposed for semantic alignment between the embedding
		  spaces of utterances and KG entities. Meanwhile, KECR
		  utilizes the connectivity within the KG to conduct explicit
		  reasoning of the user demand, making the model less
		  dependent on the user’s feedback to clarifying questions.
		  As such, the semantic alignment and explicit KG reasoning
		  can jointly facilitate accurate recommendation and quality
		  dialogue generation. By comparing with strong baselines on
		  two real-world datasets, we demonstrate that KECR obtains
		  state-of-the-art recommendation effectiveness, as well as
		  competitive dialogue generation performance.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= jul,
  articleno	= {86},
  numpages	= {21},
  keywords	= {Conversational recommendation, knowledge graph, preference
		  mining}
}

@InProceedings{	  10.1145/3627673.3679602,
  author	= {Zhang, Tianli and Zheng, Tongya and Xiao, Zhenbang and
		  Chen, Zulong and Li, Liangyue and Feng, Zunlei and Zhang,
		  Dongxiang and Song, Mingli},
  title		= {Language Models-enhanced Semantic Topology Representation
		  Learning For Temporal Knowledge Graph Extrapolation},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679602},
  doi		= {10.1145/3627673.3679602},
  abstract	= {Temporal Knowledge Graph (TKG) extrapolation aims to
		  predict future missing facts based on historical
		  information, which has exhibited both semantics and
		  topology of events. The mainstream methods have advanced
		  the prediction performance by exploring the potential of
		  topology representations of TKGs based on dedicated
		  temporal Graph Neural Networks (GNNs). Until recently, few
		  Language Models (LM) based methods have attempted to model
		  the semantic representations of TKGs, however, lacking
		  specific designs for the topology information. Therefore,
		  we propose a Semantic TOpology REpresentation learning
		  (STORE) framework enhanced by LMs to bridge the gap between
		  the semantics and topology of TKGs. Firstly, we tackle the
		  challenge of long historical facts modeling by a time-aware
		  sampling based on semantic priors to extract concise yet
		  precise facts. Secondly, we handle the challenge of the
		  interaction between topology and semantics by transforming
		  graph representations into virtual tokens that are then
		  integrated with generated prompts and fed into LMs.
		  Finally, multi-head attention is adopted to obtain better
		  semantic topology representations, thereby achieving joint
		  optimization of both temporal GNNs and LMs. Extensive
		  experiments on five datasets show that our STORE
		  outperforms state-of-the-art GNNs- and LM-based methods.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3227–3236},
  numpages	= {10},
  keywords	= {knowledge graph reasoning, large language model, temporal
		  knowledge graph},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3627673.3680022,
  author	= {Zhao, Qian and Qian, Hao and Liu, Ziqi and Zhang, Gong-Duo
		  and Gu, Lihong},
  title		= {Breaking the Barrier: Utilizing Large Language Models for
		  Industrial Recommendation Systems through an Inferential
		  Knowledge Graph},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680022},
  doi		= {10.1145/3627673.3680022},
  abstract	= {Recommendation systems are widely used in e-commerce
		  websites and online platforms to address information
		  overload. However, existing systems primarily rely on
		  historical data and user feedback, making it difficult to
		  capture user intent transitions. Recently, Knowledge Base
		  (KB)-based models are proposed to incorporate expert
		  knowledge, but it struggle to adapt to new items and the
		  evolving e-commerce environment. To address these
		  challenges, we propose a novel Large Language Model based
		  Complementary Knowledge Enhanced Recommendation System
		  (LLM-KERec). It introduces an entity extractor that
		  extracts unified concept terms from item and user
		  information. To provide cost-effective and reliable prior
		  knowledge, entity pairs are generated based on entity
		  popularity and specific strategies. The large language
		  model determines complementary relationships in each entity
		  pair, constructing a complementary knowledge graph.
		  Furthermore, a new complementary recall module and an
		  Entity-Entity-Item (E-E-I) weight decision model refine the
		  scoring of the ranking model using real complementary
		  exposure-click samples. Extensive experiments conducted on
		  three industry datasets demonstrate the significant
		  performance improvement of our model compared to existing
		  approaches. Additionally, detailed analysis shows that
		  LLM-KERec enhances users' enthusiasm for consumption by
		  recommending complementary items. In summary, LLM-KERec
		  addresses the limitations of traditional recommendation
		  systems by incorporating complementary knowledge and
		  utilizing a large language model to capture user intent
		  transitions, adapt to new items, and enhance recommendation
		  efficiency in the evolving e-commerce landscape.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5086–5093},
  numpages	= {8},
  keywords	= {knowledge graph, large language model, recommendation
		  system},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3626772.3657665,
  author	= {Liu, Lingyuan and Du, Huifang and Zhang, Xiaolian and Guo,
		  Mengying and Wang, Haofen and Wang, Meng},
  title		= {A Question-Answering Assistant over Personal Knowledge
		  Graph},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657665},
  doi		= {10.1145/3626772.3657665},
  abstract	= {We develop a Personal Knowledge Graph Question-Answering
		  (PKGQA) assistant, seamlessly integrating information from
		  multiple mobile applications into a unified and
		  user-friendly query interface to offer users convenient
		  information retrieval and personalized knowledge services.
		  Based on a fine-grained schema customized for PKG, the
		  PKGQA system in this paper comprises Symbolic Semantic
		  Parsing, Frequently Asked Question (FAQ) Semantic Matching,
		  and Neural Semantic Parsing modules, which are designed to
		  take into account both accuracy and efficiency. The PKGQA
		  system achieves high accuracy on the constructed dataset
		  and demonstrates good performance in answering complex
		  questions. Our system is implemented through an Android
		  application, which is shown in
		  https://youtu.be/p732U5KPEq4.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2708–2712},
  numpages	= {5},
  keywords	= {intelligent personal assistant, knowledge graph, neural
		  networks, question answering, symbolic system},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3689218.3689221,
  author	= {Kong, Xiangxing and Li, Yangyang and Fan, Manyi and Shi,
		  Jiayi and Wei, Lingxiang and Qu, Shaojie},
  title		= {Automated Knowledge Mining and Knowledge Graph Reasoning
		  for Aircraft Engine Maintenance},
  year		= {2024},
  isbn		= {9798400718250},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689218.3689221},
  doi		= {10.1145/3689218.3689221},
  abstract	= {The maintenance process for aircraft engines is fraught
		  with significant challenges due to their inherent
		  complexity. Large Language Models excel in general Natural
		  Language Processing tasks, yet they lack domain-specific
		  knowledge, thereby compromising their performance in
		  specialized areas. The varied descriptions of engine faults
		  also render traditional text matching algorithms unsuitable
		  for this maintenance domain. In this paper, we construct a
		  knowledge graph integrated with fault diagnosis reasoning
		  ability with knowledge mined from aircraft engine
		  maintenance data. Firstly, we propose the Knowledge Mining
		  and Knowledge Graph Reasoning framework for aircraft engine
		  maintenance data knowledge mining and aircraft engine fault
		  diagnosis. Secondly, we utilize prompt with in-context
		  learning to mitigate the issue of the model lacking
		  expertise in the field of aircraft engine maintenance.
		  Finally, we adopt a sentence similarity calculation method
		  based on BERT, which enables more effective processing of
		  semantic information. We apply our method to Aircraft
		  Engine Fault dataset which is collected from maintenance
		  records of civil aircraft engine since 2007 to 2015, and
		  experimental results demonstrate the effectiveness of our
		  knowledge mining method and aircraft engine fault reasoning
		  algorithm.},
  booktitle	= {Proceedings of the 2024 6th International Conference on
		  Pattern Recognition and Intelligent Systems},
  pages		= {35–40},
  numpages	= {6},
  keywords	= {aircraft engine maintenance, knowledge graph reasoning,
		  large language model},
  location	= {Hong Kong, Hong Kong},
  series	= {PRIS '24}
}

@InProceedings{	  10.1145/3698587.3701538,
  author	= {Chu, Lei and Wu, Hongyan and Pan, Yi},
  title		= {ChatASD: A Dialogue Framework for LLMs Enhanced by Autism
		  Knowledge Graph Retrieval},
  year		= {2024},
  isbn		= {9798400713026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698587.3701538},
  doi		= {10.1145/3698587.3701538},
  abstract	= {Autism Spectrum Disorder (ASD) is a neurodevelopmental
		  disorder characterized by developmental delays,
		  communication difficulties, repetitive behaviors, and
		  restricted interests. Large Language Models (LLMs) have
		  demonstrated exceptional capabilities in various natural
		  language tasks, particularly in providing personalized
		  question-and-answer(Q&amp;A) services, making them
		  well-suited for constructing dialogue engines for autism
		  Q&amp;A systems. However, general LLMs often lack
		  integrated autism knowledge during training, limiting their
		  professional competency in autism consultation.
		  Additionally, the automatic evaluation of scientific
		  accuracy in autism medical knowledge Q&amp;A remains
		  underexplored. To address this gap, we propose ChatASD, an
		  autism knowledge Q&amp;A framework based on Graph
		  Retrieval-Augmented Generation (GraphRAG) technology. This
		  framework leverages LLMs and retrieves relevant information
		  from medical literature to generate an autism knowledge
		  graph, employing a combination of global and community
		  queries to produce reliable responses. Compared to
		  traditional methods, ChatASD effectively addresses the
		  sparse distribution of autism knowledge, providing more
		  accurate and comprehensive answersAutomatic efficacy
		  evaluations and competitive experiments on system responses
		  indicate our approach significantly improves reliability of
		  autism-related professional knowledge queries.},
  booktitle	= {Proceedings of the 15th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  articleno	= {36},
  numpages	= {8},
  keywords	= {Autism, Knowledge Graph, LLM, Question-and-Answer System,
		  Retrieval-Augmented Generation},
  location	= {Shenzhen, China},
  series	= {BCB '24}
}

@InProceedings{	  10.1145/3632410.3633292,
  author	= {Agarwal, Manoj},
  title		= {Building Knowledge Graph for Products at Web Scale},
  year		= {2024},
  isbn		= {9798400716348},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632410.3633292},
  doi		= {10.1145/3632410.3633292},
  abstract	= {A knowledge graph is the key to entity search as it can
		  store the factual entity related information in a
		  structured manner without the rigidity of a fixed schema.
		  Both Google and Bing have web scale knowledge graphs and
		  for a large fraction of web queries knowledge graph is
		  invoked. E-commerce search is primarily an entity search.
		  Therefore, building a Knowledge Graph is the key to improve
		  the eCommerce search in many ways. However, building it at
		  web scale is a highly challenging problem. It is an equally
		  or even more challenging problem to build the knowledge
		  graph for products. In this tutorial, we present
		  state-of-the-art work to address some of the key challenges
		  to build the knowledge graph as well as our methodology to
		  build a product graph at web scale for Microsoft-Shopping,
		  containing a few billion facts.},
  booktitle	= {Proceedings of the 7th Joint International Conference on
		  Data Science &amp; Management of Data (11th ACM IKDD CODS
		  and 29th COMAD)},
  pages		= {498–500},
  numpages	= {3},
  keywords	= {Product graph, faceted search, knowledge graph, semantic
		  search, taxonomy},
  location	= {Bangalore, India},
  series	= {CODS-COMAD '24}
}

@InProceedings{	  10.1145/3670105.3670139,
  author	= {Lin, Wangqun and Xu, Jing and Tian, Yu and Peng, Baoyun
		  and Li, Yan and Ge, Yawei},
  title		= {Cognitive Intelligence: Driven by Knowledge Graph and Big
		  Model Collaboration},
  year		= {2024},
  isbn		= {9798400716751},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670105.3670139},
  doi		= {10.1145/3670105.3670139},
  abstract	= {Abstract: Cognitive intelligence is primarily
		  characterized by the understanding, reasoning, cognition,
		  and decision-making of complex things. It is a higher-order
		  form of artificial intelligence development. This paper
		  provides an in-depth analysis of two representative
		  technologies, knowledge graph and big model, which promote
		  the development of cognitive intelligence. Firstly, we
		  systematically sorts out the characteristics, advantages,
		  and shortcomings of these two technologies. Secondly, we
		  proposes technical approaches and main methods for the
		  mutual enhancement of knowledge graph and big model.
		  Finally, we provides the main direction for the integrated
		  development of knowledge graph and big model to promote the
		  development of cognitive intelligence. We hope our work can
		  provide reference and inspiration for relevant engineers
		  and technical researchers.CCS Concepts: .Computing
		  methodologies → Artificial intelligence; Knowledge
		  representation and reasoning},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computing, Networks and Internet of Things},
  pages		= {204–209},
  numpages	= {6},
  keywords	= {artificial intelligence, big model, cognitive
		  intelligence, knowledge graph},
  location	= {Tokyo, Japan},
  series	= {CNIOT '24}
}

@Article{	  10.1145/3696664,
  author	= {Zhao, Yang and Kang, Xiaomian and Zhang, Yaping and Zhang,
		  Jiajun and Zhou, Yu and Zong, Chengqing},
  title		= {Knowledge Graph Guided Neural Machine Translation with
		  Dynamic Reinforce-selected Triples},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {12},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3696664},
  doi		= {10.1145/3696664},
  abstract	= {Previous methods incorporating knowledge graphs (KGs) into
		  neural machine translation (NMT) adopt a static knowledge
		  utilization strategy, that introduces many useless
		  knowledge triples and makes the useful triples difficult to
		  be utilized by NMT. To address this problem, we propose a
		  KG guided NMT model with dynamic reinforce-selected
		  triples. The proposed methods could dynamically select the
		  different useful knowledge triples for different source
		  sentences. Specifically, the proposed model contains two
		  components: (1) knowledge selector, that dynamically
		  selects useful knowledge triples for a source sentence, and
		  (2) knowledge guided NMT (KgNMT), that utilizes the
		  selected triples to guide the translation of NMT.
		  Meanwhile, to overcome the non-differentiable problem and
		  guide the training procedure, we propose a policy gradient
		  strategy to encourage the model to select useful triples
		  and improve the generation probability of gold target
		  sentence. Various experimental results show that the
		  proposed method can significantly outperform the baseline
		  models in both translation quality and handling the
		  entities.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {163},
  numpages	= {21},
  keywords	= {Neural machine translation, knowledge graph, reinforcement
		  learning}
}

@InProceedings{	  10.1145/3589334.3645564,
  author	= {Zhang, Honggen and Zhang, June and Molybog, Igor},
  title		= {HaSa: Hardness and Structure-Aware Contrastive Knowledge
		  Graph Embedding},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645564},
  doi		= {10.1145/3589334.3645564},
  abstract	= {We consider a contrastive learning approach to knowledge
		  graph embedding (KGE) via InfoNCE. For KGE, efficient
		  learning relies on augmenting the training data with
		  negative triples. However, most KGE works overlook the bias
		  from generating the negative triples- false negative
		  triples (factual triples missing from the knowledge graph).
		  We argue that generating high-quality (i.e., hard) negative
		  triples might lead to an increase in false negative
		  triples. To mitigate the impact of false negative triples
		  during the generation of hard negative triples, we propose
		  the Hardness and Structure-aware (HaSa) contrastive KGE
		  method, which alleviates the effect of false negative
		  triples while generating the hard negative triples.
		  Experiments show that HaSa improves the performance of
		  InfoNCE-based KGE approaches and achieves state-of-the-art
		  results in several metrics for WN18RR datasets and
		  competitive results for FB15k-237 datasets compared to
		  classic and pre-trained LM-based KGE methods.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2116–2127},
  numpages	= {12},
  keywords	= {contrastive learning, knowledge graph embedding, negative
		  sampling},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3597503.3639109,
  author	= {Wang, Jun and Li, Yanhui and Chen, Zhifei and Chen, Lin
		  and Zhang, Xiaofang and Zhou, Yuming},
  title		= {Knowledge Graph Driven Inference Testing for Question
		  Answering Software},
  year		= {2024},
  isbn		= {9798400702174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3597503.3639109},
  doi		= {10.1145/3597503.3639109},
  abstract	= {In the wake of developments in the field of Natural
		  Language Processing, Question Answering (QA) software has
		  penetrated our daily lives. Due to the data-driven
		  programming paradigm, QA software inevitably contains bugs,
		  i.e., misbehaving in real-world applications. Current
		  testing techniques for testing QA software include two
		  folds, reference-based testing and metamorphic testing.This
		  paper adopts a different angle to achieve testing for QA
		  software: we notice that answers to questions would have
		  inference relations, i.e., the answers to some questions
		  could be logically inferred from the answers to other
		  questions. If these answers on QA software do not satisfy
		  the inference relations, an inference bug is detected. To
		  generate the questions with the inference relations
		  automatically, we propose a novel testing method Knowledge
		  Graph driven Inference Testing (KGIT), which employs facts
		  in the Knowledge Graph (KG) as the seeds to logically
		  construct test cases containing questions and contexts with
		  inference relations. To evaluate the effectiveness of KGIT,
		  we conduct an extensive empirical study with more than 2.8
		  million test cases generated from the large-scale KG YAGO4
		  and three QA models based on the state-of-the-art QA model
		  structure. The experimental results show that our method
		  (a) could detect a considerable number of inference bugs in
		  all three studied QA models and (b) is helpful in
		  retraining QA models to improve their inference ability.},
  booktitle	= {Proceedings of the IEEE/ACM 46th International Conference
		  on Software Engineering},
  articleno	= {119},
  numpages	= {13},
  keywords	= {question answering, software testing, knowledge graph,
		  inference rules},
  location	= {Lisbon, Portugal},
  series	= {ICSE '24}
}

@InProceedings{	  10.1145/3686397.3686420,
  author	= {Sun, Yi and Yang, Wanru and Liu, Yin},
  title		= {The Application of Constructing Knowledge Graph of Oral
		  Historical Archives Resources Based on LLM-RAG},
  year		= {2024},
  isbn		= {9798400717345},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3686397.3686420},
  doi		= {10.1145/3686397.3686420},
  abstract	= {Oral historical archive resources are an emerging archive
		  resource with the rapid development of modern technology.
		  Its "bottom-up" approach to historical research has
		  received widespread attention in the fields of history,
		  archives, and libraries. Under the common knowledge
		  discovery mode, oral historical archives resources are
		  showing a dispersed state. Information technology
		  represented by knowledge graphs can break through the data
		  solidification of oral historical archives, reshape the
		  information stack of oral historical archives, and achieve
		  knowledge association and aggregation of oral historical
		  archive resources. The article attempts to construct a
		  knowledge graph of the oral historical archives resources
		  on the theme of "science and art" in the collection of T.D.
		  Lee Library of Shanghai Jiao Tong University. It uses Large
		  Language Model - Retrieval Augmented Generation (LLM-RAG)
		  for knowledge extraction, and then uses a semantic model
		  for knowledge organization and management. The article
		  attempts to empower humanities with technology, exploring
		  the possibility of combining "digital technology" and
		  "humanities research", extending traditional humanities
		  research methods, breaking down barriers between technology
		  and humanities resources, and providing a new path
		  reference for revealing resource content characteristics,
		  semantic deep correlation, and multi-dimensional knowledge
		  discovery.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Information System and Data Mining},
  pages		= {142–149},
  numpages	= {8},
  keywords	= {Knowledge Graph, LLM-RAG, Oral History Archives},
  location	= { },
  series	= {ICISDM '24}
}

@InProceedings{	  10.1145/3616855.3636507,
  author	= {Quintero-Narvaez, Carlos Efrain and Monroy, Raul},
  title		= {Integrating Knowledge Graph Data with Large Language
		  Models for Explainable Inference},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3636507},
  doi		= {10.1145/3616855.3636507},
  abstract	= {We propose a method to enable Large Language Models to
		  access Knowledge Graph (KG) data and justify their text
		  generation by showing the specific graph data the model
		  accessed during inference. For this, we combine Language
		  Models with methods from Neurosymbolic Artificial
		  Intelligence designed to answer queries on Knowledge
		  Graphs. This is done by modifying the model so that at
		  different stages of inference it outputs an Existential
		  Positive First-Order (EPFO) query, which is then processed
		  by an additional query appendix. In turn, the query
		  appendix uses neural link predictors along with description
		  aware embeddings to resolve these queries. After that, the
		  queries are logged and used as an explanation of the
		  inference process of the complete model. Lastly, we train
		  the model using a Linear Temporal Logic (LTL)
		  constraint-based loss function to measure the consistency
		  of the queries among each other and with the final model
		  output.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {1198–1199},
  numpages	= {2},
  keywords	= {advanced artificial intelligence, existential positive
		  first order query, explainable, knowledge graph, language
		  model, linear temporal logic, query},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@InProceedings{	  10.1145/3643479.3662055,
  author	= {Bui, Tuan and Tran, Oanh and Nguyen, Phuong and Ho, Bao
		  and Nguyen, Long and Bui, Thang and Quan, Tho},
  title		= {Cross-Data Knowledge Graph Construction for LLM-enabled
		  Educational Question-Answering System: A Case Study at
		  HCMUT},
  year		= {2024},
  isbn		= {9798400705472},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643479.3662055},
  doi		= {10.1145/3643479.3662055},
  abstract	= {In today's rapidly evolving landscape of Artificial
		  Intelligence, large language models (LLMs) have emerged as
		  a vibrant research topic. LLMs find applications in various
		  fields and contribute significantly. Despite their powerful
		  language capabilities, similar to pre-trained language
		  models (PLMs), LLMs still face challenges in remembering
		  events, incorporating new information, and addressing
		  domain-specific issues or hallucinations. To overcome these
		  limitations, researchers have proposed Retrieval-Augmented
		  Generation (RAG) techniques, some others have proposed the
		  integration of LLMs with Knowledge Graphs (KGs) to provide
		  factual context, thereby improving performance and
		  delivering more accurate feedback to user queries.Education
		  plays a crucial role in human development and progress.
		  With the technology transformation, traditional education
		  is being replaced by digital or blended education.
		  Therefore, educational data in the digital environment is
		  increasing day by day. Data in higher education
		  institutions are diverse, comprising various sources such
		  as unstructured/structured text, relational databases,
		  web/app-based API access, etc. Constructing a Knowledge
		  Graph from these cross-data sources is not a simple task.
		  This article proposes a method for automatically
		  constructing a Knowledge Graph from multiple data sources
		  and discusses some initial applications (experimental
		  trials) of KG in conjunction with LLMs for
		  question-answering tasks.},
  booktitle	= {Proceedings of the 1st ACM Workshop on AI-Powered Q&amp;A
		  Systems for Multimedia},
  pages		= {36–43},
  numpages	= {8},
  keywords	= {Education, Knowledge Graph, Large language model, Open
		  Intent Discovery, Question-Answering System},
  location	= {Phuket, Thailand},
  series	= {AIQAM '24}
}

@InProceedings{	  10.1145/3627673.3680262,
  author	= {Wang, Wenbo},
  title		= {The 'Path' to Clarity: Identifying False Claims Through a
		  Knowledge Graph Exploration},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680262},
  doi		= {10.1145/3627673.3680262},
  abstract	= {Automated fact-checking has emerged as a safeguard against
		  the spread of false information. Existing fact-checking
		  approaches aim to determine whether a news claim is true or
		  false, and they have achieved decent accuracy of veracity
		  prediction. However, the current state-of-the-art models
		  still face challenges, such as ambiguity in the claims and
		  lack of contextual information. This study introduces a
		  fact-checking model, Path-FC, which focuses on 1)
		  augmenting the representations of claims and evidence by
		  incorporating additional context using the Knowledge Paths
		  extracted from the external Knowledge Graph; 2) Identifying
		  false claims by learning the differences between claims and
		  evidence. The experimental results demonstrate that
		  Knowledge Path retrieval, combined with the multi-head
		  attention technique, contributes to improved performance of
		  fact-checking. The code is available at
		  https://anonymous.4open.science/r/Path-FC.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5487–5490},
  numpages	= {4},
  keywords	= {claim verification, deep learning, fact checking,
		  knowledge graph, natural language processing},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3660521,
  author	= {Zeng, Kaisheng and Jin, Hailong and Lv, Xin and Zhu,
		  Fangwei and Hou, Lei and Zhang, Yi and Pang, Fan and Qi, Yu
		  and Liu, Dingxiao and Li, Juanzi and Feng, Ling},
  title		= {XLORE 3: A Large-Scale Multilingual Knowledge Graph from
		  Heterogeneous Wiki Knowledge Resources},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3660521},
  doi		= {10.1145/3660521},
  abstract	= {In recent years, knowledge graph (KG) has attracted
		  significant attention from academia and industry, resulting
		  in the development of numerous technologies for KG
		  construction, completion, and application. XLORE is one of
		  the largest multilingual KGs built from Baidu Baike and
		  Wikipedia via a series of knowledge modeling and
		  acquisition methods. In this article, we utilize systematic
		  methods to improve XLORE's data quality and present its
		  latest version, XLORE 3, which enables the effective
		  integration and management of heterogeneous knowledge from
		  diverse resources. Compared with previous versions, XLORE 3
		  has three major advantages: (1) We design a comprehensive
		  and reasonable schema, namely XLORE ontology, which can
		  effectively organize and manage entities from various
		  resources. (2) We merge equivalent entities in different
		  languages to facilitate knowledge sharing. We provide a
		  large-scale entity linking system to establish the
		  associations between unstructured text and structured KG.
		  (3) We design a multi-strategy knowledge completion
		  framework, which leverages pre-trained language models and
		  vast amounts of unstructured text to discover missing and
		  new facts. The resulting KG contains 446 concepts, 2,608
		  properties, 66 million entities, and more than 2 billion
		  facts. It is available and downloadable online at ,
		  providing a valuable resource for researchers and
		  practitioners in various fields.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= aug,
  articleno	= {145},
  numpages	= {47},
  keywords	= {Knowledge graph, knowledge management, knowledge fusion,
		  knowledge completion, schema construction, entity typing,
		  entity alignment, entity linking}
}

@InProceedings{	  10.1145/3589334.3645592,
  author	= {Liu, Ben and Peng, Miao and Xu, Wenjie and Jia, Xu and
		  Peng, Min},
  title		= {UniLP: Unified Topology-aware Generative Framework for
		  Link Prediction in Knowledge Graph},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645592},
  doi		= {10.1145/3589334.3645592},
  abstract	= {Link prediction (LP) in knowledge graph (KG) is a crucial
		  task that has received increasing attention recently. Due
		  to the heterogeneous structures of KGs, various application
		  scenarios, and demand-specific downstream objectives, there
		  exist multiple subtasks in LP. Most studies only focus on
		  designing a dedicated architecture for a specific subtask,
		  which results in various complicated LP models. The
		  isolated architectures and chaotic situations make it
		  significant to construct a unified model that can handle
		  multiple LP subtasks simultaneously. However, unifying all
		  subtasks in LP presents numerous challenges, including
		  unified input forms, task-specific context modeling, and
		  topological information encoding. To address these
		  challenges, we propose a topology-aware generative
		  framework, namely UniLP, which utilizes a generative
		  pre-trained language model to accomplish different LP
		  subtasks universally. Specifically, we introduce a context
		  demonstration template to convert task-specific context
		  into a unified generative formulation. Based on the unified
		  formulation, to address the limitation of transformer
		  architecture that may overlook important structural signals
		  in KGs, we design novel topology-aware soft prompts to
		  deeply couple topology and text information in a
		  contextualized manner. Extensive experiment results
		  demonstrate that our framework achieves substantial
		  performance gain and provides a real unified end-to-end
		  solution for the whole LP subtasks. We also perform
		  comprehensive ablation studies to support in-depth analysis
		  of each component in UniLP.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2170–2180},
  numpages	= {11},
  keywords	= {knowledge graph, link prediction, unified generative
		  framework},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3686397.3686417,
  author	= {Alqaaidi, Sakher Khalil and Kochut, Krzysztof J.},
  title		= {Relations Prediction in Knowledge Graph Completion using
		  Large Language Models},
  year		= {2024},
  isbn		= {9798400717345},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3686397.3686417},
  doi		= {10.1145/3686397.3686417},
  abstract	= {Knowledge graphs have been widely used to represent facts
		  in a structured format. Due to their large-scale
		  applications, knowledge graphs suffer from being
		  incomplete. The relation prediction task obtains knowledge
		  graph completion by assigning one or more possible
		  relations to each pair of nodes. In this work, we make use
		  of the knowledge graph node names to fine-tune a large
		  language model for the relation prediction task. By
		  utilizing the node names only, we enable our model to
		  operate sufficiently in the inductive settings. Our
		  experiments show that we accomplish new scores on a widely
		  used knowledge graph benchmark.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Information System and Data Mining},
  pages		= {122–127},
  numpages	= {6},
  keywords	= {Knowledge Graphs, Large Language Models},
  location	= { },
  series	= {ICISDM '24}
}

@InProceedings{	  10.1145/3627673.3679711,
  author	= {Zhao, Kaichen and Song, Yaoxian and Zhao, Haiquan and Liu,
		  Haoyu and Li, Tiefeng and Li, Zhixu},
  title		= {Towards Coarse-grained Visual Language Navigation Task
		  Planning Enhanced by Event Knowledge Graph},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679711},
  doi		= {10.1145/3627673.3679711},
  abstract	= {Visual language navigation (VLN) is one of the important
		  research in embodied AI. It aims to enable an agent to
		  understand the surrounding environment and complete
		  navigation tasks. VLN instructions could be categorized
		  into coarse-grained and fine-grained commands. Fine-grained
		  command describes a whole task with subtasks step-by-step.
		  In contrast, coarse-grained command gives an abstract task
		  description, which more suites human habits. Most existing
		  work focuses on the former kind of instruction in VLN
		  tasks, ignoring the latter abstract instructions belonging
		  to daily life scenarios. To overcome the above challenge in
		  abstract instruction, we attempt to consider coarse-grained
		  instruction in VLN by event knowledge enhancement.
		  Specifically, we first propose a prompt-based framework to
		  extract an event knowledge graph (named VLN-EventKG =) for
		  VLN integrally over multiple mainstream benchmark datasets.
		  Through small and large language model collaboration, we
		  realize knowledge-enhanced navigation planning (named
		  EventNav) for VLN tasks with coarse-grained instruction
		  input. Additionally, we design a novel dynamic history
		  backtracking module to correct potential error action
		  planning in real time. Experimental results in various
		  public benchmarks show our knowledge-enhanced method has
		  superiority in coarse-grained-instruction VLN using our
		  proposed VLN-EventKG with over 5% improvement in success
		  rate. Our project is available at
		  https://sites.google.com/view/vln-eventkg},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3320–3330},
  numpages	= {11},
  keywords	= {dynamic backtracking, event knowledge graph, knowledge
		  retrieval, task planning, visual language navigation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3671148,
  author	= {Meng, Siyuan and Zhou, Jie and Chen, Xuxin and Liu, Yufei
		  and Lu, Fengyuan and Huang, Xinli},
  title		= {Structure-Information-Based Reasoning over the Knowledge
		  Graph: A Survey of Methods and Applications},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {8},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3671148},
  doi		= {10.1145/3671148},
  abstract	= {The knowledge graph (KG) is an efficient form of knowledge
		  organization and expression, providing prior knowledge
		  support for various downstream tasks, and has received
		  extensive attention in natural language processing.
		  However, existing large-scale KGs have many hidden facts
		  that need to be discovered. How to effectively use the
		  structure information of KG is an important research
		  direction of knowledge reasoning.
		  Structure-Information-based reasoning over the KG is a
		  technique used to find the missing facts by the structure
		  information of KG. This survey summarizes the methods and
		  applications of Structure-Information-based reasoning and
		  hopes to be helpful to the research in this field. First,
		  we introduced the definition of knowledge reasoning and the
		  conceptual description of related tasks. Then, we reviewed
		  the methods of Structure-Information-based reasoning.
		  Specifically, we categorized them into four representative
		  classes: PRA-based reasoning, Path-Embedding-based
		  reasoning, RL-based reasoning, and GNN-based reasoning. We
		  compared the motivations and details between practices in
		  the same category. After that, we described the application
		  of Structure-Information-based knowledge reasoning in the
		  KG Completion, Question Answering System, Recommendation
		  System, and other fields. Finally, we discussed the future
		  research directions of Structure-Information-based
		  reasoning.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= aug,
  articleno	= {210},
  numpages	= {42},
  keywords	= {Knowledge graph, knowledge reasoning, structure
		  information}
}

@InProceedings{	  10.1109/ase56229.2023.00075,
  author	= {Huang, Qing and Wan, Zhenyu and Xing, Zhenchang and Wang,
		  Changjing and Chen, Jieshan and Xu, Xiwei and Lu, Qinghua},
  title		= {Let's Chat to Find the APIs: Connecting Human, LLM and
		  Knowledge Graph through AI Chain},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00075},
  doi		= {10.1109/ASE56229.2023.00075},
  abstract	= {API recommendation methods have evolved from literal and
		  semantic keyword matching to query expansion and query
		  clarification. The latest query clarification method is
		  knowledge graph (KG)-based, but limitations include
		  out-of-vocabulary (OOV) failures and rigid question
		  templates. To address these limitations, we propose a novel
		  knowledge-guided query clarification approach for API
		  recommendation that leverages a large language model (LLM)
		  guided by KG. We utilize the LLM as a neural knowledge base
		  to overcome OOV failures, generating fluent and appropriate
		  clarification questions and options. We also leverage the
		  structured API knowledge and entity relationships stored in
		  the KG to filter out noise, and transfer the optimal
		  clarification path from KG to the LLM, increasing the
		  efficiency of the clarification process. Our approach is
		  designed as an AI chain that consists of five steps, each
		  handled by a separate LLM call, to improve accuracy,
		  efficiency, and fluency for query clarification in API
		  recommendation. We verify the usefulness of each unit in
		  our AI chain, which all received high scores close to a
		  perfect 5. When compared to the baselines, our approach
		  shows a significant improvement in MRR, with a maximum
		  increase of 63.9% higher when the query statement is
		  covered in KG and 37.2% when it is not. Ablation
		  experiments reveal that the guidance of knowledge in the KG
		  and the knowledge-guided pathfinding strategy are crucial
		  for our approach's performance, resulting in a 19.0% and
		  22.2% increase in MAP, respectively. Our approach
		  demonstrates a way to bridge the gap between KG and LLM,
		  effectively compensating for the strengths and weaknesses
		  of both.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {471–483},
  numpages	= {13},
  keywords	= {API recommendation, query clarification, knowledge graph,
		  large language model, out-of-vocabulary},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@InProceedings{	  10.1145/3605098.3635957,
  author	= {Bellan, Patrizio and Dragoni, Mauro and Ghidini, Chiara},
  title		= {Process Knowledge Extraction and Knowledge Graph
		  Construction Through Prompting: A Quantitative Analysis},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3635957},
  doi		= {10.1145/3605098.3635957},
  abstract	= {The automated construction of process knowledge graphs
		  from process description documents is a challenging
		  research area. Here, the lack of massive annotated data, as
		  well as raw text repositories describing real-world process
		  documents, makes it extremely difficult to adopt deep
		  learning approaches to perform this transformation. Indeed,
		  the main challenge is to extract conceptual elements
		  representing the actual entities or relations of the
		  process model described within its corresponding natural
		  language document. Large Language Models (LLMs) have shown
		  promising results in supporting the extraction of
		  structured knowledge from unstructured texts. Although
		  several works explored this strategy to build or complete
		  knowledge graphs, the exploitation of LLMs toward
		  domain-specific knowledge base construction from scratch
		  has not yet been investigated deeply. Our aim is to exploit
		  the LLM capabilities to extract process knowledge from
		  unseen natural language descriptions. In this work, we
		  present a prompt-based in-context learning strategy to
		  extract, from process descriptions, conceptual information
		  that can be converted into their equivalent knowledge
		  graphs. Such a strategy is performed in a multi-turn dialog
		  fashion. We validate the accuracy of the proposed approach
		  from a quantitative perspective. The results highlight the
		  feasibility of the proposed approach within our
		  low-resource scenarios and open interesting perspectives
		  for future activities.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1634–1641},
  numpages	= {8},
  keywords	= {process extraction from text, in-context learning,
		  knowledge graph, large language model, business process
		  management},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@InProceedings{	  10.1145/3631700.3665235,
  author	= {Carta, Salvatore and Giuliani, Alessandro and Manca, Marco
		  Manolo and Piano, Leonardo and Pompianu, Livio and Tiddia,
		  Sandro Gabriele},
  title		= {Towards Knowledge Graph Refinement: Misdirected Triple
		  Identification},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3665235},
  doi		= {10.1145/3631700.3665235},
  abstract	= {In the current digital transformation scenario, Knowledge
		  Graphs (KGs) represent an across-the-board instrument for
		  representing knowledge in a structured form. Such tools
		  allow to effectively enhance the performance of Artificial
		  Intelligence models in manifold contexts, such as reasoning
		  or information retrieval. Nevertheless, the effectiveness
		  of KGs is often affected by the incorrect directionality of
		  some of their edges, due in most cases to human error or
		  the inefficiency of automatic and semi-automatic graph
		  creation methods. This paper proposes a
		  classification-based approach to identify misdirected
		  triples within a KG, aiming to support and assist humans in
		  creating graph refinement. Triples are the main component
		  of KGs, and they model the connection between nodes with a
		  &lt;subject, predicate, object&gt; form. Our proposal
		  allows us to refine a KG by devising a classification-based
		  approach for recognizing whether the subjects and objects
		  are not compliant with the logic directionality of the
		  corresponding predicate, meaning that they should be
		  switched (e.g., the triple &lt;U.S.A., is capital,
		  Washington&gt; should be inverted as &lt;Washington, is
		  capital, U.S.A.&gt;). We compare traditional machine
		  learning techniques with cutting-edge advanced methods,
		  including pre-trained language models and large language
		  models. Extensive experiments have been performed across
		  several datasets, confirming the effectiveness of our
		  proposal.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {460–466},
  numpages	= {7},
  keywords	= {Artificial Intelligence, Digital Transformation, Large
		  Language Models},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@InProceedings{	  10.1145/3639233.3639357,
  author	= {Zhu, Ruiqi and Bundy, Alan and Pan, Jeff and Nuamah,
		  Kwabena and Wang, Fangrong and Li, Xue and Xu, Lei and
		  Mauceri, Stefano},
  title		= {Assessing the Quality of a Knowledge Graph via Link
		  Prediction Tasks},
  year		= {2024},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639233.3639357},
  doi		= {10.1145/3639233.3639357},
  abstract	= {Knowledge Graph (KG) Construction is the prerequisite for
		  all other KG research and applications. Researchers and
		  engineers have proposed various approaches to build KGs for
		  their use cases. However, how can we know whether our
		  constructed KG is good or bad? Is it correct and complete?
		  Is it consistent and robust? In this paper, we propose a
		  method called LP-Measure to assess the quality of a KG via
		  a link prediction tasks, without using a gold standard or
		  other human labour. Though theoretically, the LP-Measure
		  can only assess consistency and redundancy, instead of the
		  more desirable correctness and completeness, empirical
		  evidence shows that this measurement method can
		  quantitatively distinguish the good KGs from the bad ones,
		  even in terms of incorrectness and incompleteness. Compared
		  with the most commonly used manual assessment, our
		  LP-Measure is an automated evaluation, which saves time and
		  human labour.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {124–129},
  numpages	= {6},
  keywords	= {Knowledge Graph, Link Prediction, Quality Assessment},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '23}
}

@InProceedings{	  10.1145/3664647.3681112,
  author	= {Liang, Ke and Meng, Lingyuan and Liu, Yue and Liu, Meng
		  and Wei, Wei and Liu, Suyuan and Tu, Wenxuan and Wang,
		  Siwei and Zhou, Sihang and Liu, Xinwang},
  title		= {Simple Yet Effective: Structure Guided Pre-trained
		  Transformer for Multi-modal Knowledge Graph Reasoning},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681112},
  doi		= {10.1145/3664647.3681112},
  abstract	= {Various information in different modalities in an
		  intuitive way in multi-modal knowledge graphs (MKGs), which
		  are utilized in different downstream tasks, like
		  recommendation. However, most MKGs are still far from
		  complete, which motivates the flourishing of MKG reasoning
		  models. Recently, with the development of general
		  artificial intelligence, pre-trained transformers have
		  drawn increasing attention, especially in multi-modal
		  scenarios. However, the research of multi-modal pre-trained
		  transformers (MPT) for knowledge graph reasoning (KGR) is
		  still at an early stage. As the biggest difference between
		  MKG and other multi-modal data, the rich structural
		  information underlying the MKG is still not fully utilized
		  in previous MPT. Most of them only use the graph structure
		  as a retrieval map for matching images and texts connected
		  with the same entity, which hinders their reasoning
		  performances. To this end, the graph Structure Guided
		  Multi-modal Pre-trained Transformer is proposed for
		  knowledge graph reasoning (SGMPT). Specifically, the graph
		  structure encoder is adopted for structural feature
		  encoding. Then, a structure-guided fusion module with two
		  simple yet effective strategies, i.e., weighted summation
		  and alignment constraint, is designed to inject the
		  structural information into both the textual and visual
		  features. To the best of our knowledge, SGMPT is the first
		  MPT for multi-modal KGR, which mines structural information
		  underlying MKGs. Extensive experiments on FB15k-237-IMG and
		  WN18-IMG, demonstrate that our SGMPT outperforms existing
		  state-of-the-art models, and proves the effectiveness of
		  the designed strategies.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {1554–1563},
  numpages	= {10},
  keywords	= {knowledge graph reasoning, multimodal information fusion,
		  pretrained transformer model},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3627673.3679698,
  author	= {Li, Lijie and Wang, Hui and Li, Jiahang and Xu, Xiaodi and
		  Wang, Ye and Ren, Tao},
  title		= {Integrating Structure and Text for Enhancing
		  Hyper-relational Knowledge Graph Representation via
		  Structure Soft Prompt Tuning},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679698},
  doi		= {10.1145/3627673.3679698},
  abstract	= {Different from traditional knowledge graphs, where facts
		  are usually represented as (subject, relation, object),
		  hyper-relational knowledge graphs (HKGs) allow facts to be
		  associated with additional relation-entity pairs to
		  constrain the validity of facts. HKGs contain a substantial
		  amount of textual information, which plays a crucial role
		  in enriching representations. However, existing HKG
		  embedding methods mainly rely on structural information but
		  overlook textual information in HKGs, which are less
		  effective in representing entities with limited structural
		  information. To address this issue, the paper proposes HIST
		  (Hyper-relational Knowledge Graph Encoder Integrating
		  Structure and Text), which incorporates textual information
		  and structural information in HKGs to enhance
		  representations of entities and relations. HIST adopts the
		  graph convolutional network to extract structural
		  information and utilizes it to generate the Structure Soft
		  Prompt. During the Structure Soft Prompt Tuning process,
		  the textual information and structural information are
		  fully integrated to generate more comprehensive
		  representations. Additionally, an effective contrastive
		  learning method for HKG embedding is formulated to improve
		  the efficiency of negative sampling. Experimental results
		  show that HIST achieves state-of-the-art performance on
		  several public datasets. Our code is available at
		  https://github.com/QieFangBaiLuQingYaJian/HIST.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1226–1234},
  numpages	= {9},
  keywords	= {hyper-relational knowledge graph, hyper-relational
		  knowledge graph embedding, soft prompt},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3640457.3688171,
  author	= {Ali, Zafar and Qi, Guilin and Ullah, Irfan and Mohammed,
		  Adam A. Q. and Kefalas, Pavlos and Muhammad, Khan},
  title		= {GLAMOR: Graph-based LAnguage MOdel embedding for citation
		  Recommendation},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688171},
  doi		= {10.1145/3640457.3688171},
  abstract	= {Digital publishing’s exponential growth has created vast
		  scholarly collections. Guiding researchers to relevant
		  resources is crucial, and knowledge graphs (KGs) are key
		  tools for unlocking hidden knowledge. However, current
		  methods focus on external links between concepts, ignoring
		  the rich information within individual papers. Challenges
		  like insufficient multi-relational data, name ambiguity,
		  and cold-start issues further limit existing KG-based
		  methods, failing to capture the intricate attributes of
		  diverse entities. To solve these issues, we propose GLAMOR,
		  a robust KG framework encompassing entities e.g., authors,
		  papers, fields of study, and concepts, along with their
		  semantic interconnections. GLAMOR uses a novel random
		  walk-based KG text generation method and then fine-tunes
		  the language model using the generated text. Subsequently,
		  the acquired context-preserving embeddings facilitate
		  superior top@k predictions. Evaluation results on two
		  public benchmark datasets demonstrate our GLAMOR’s
		  superiority against state-of-the-art methods especially in
		  solving the cold-start problem.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {929–933},
  numpages	= {5},
  keywords	= {Attributed Graph Embedding, Citation Recommendation,
		  Cold-start, GLAMOR, Large Language Model, Recommender
		  Systems},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3627673.3679805,
  author	= {Zhao, Runhao and Tang, Jiuyang and Zeng, Weixin and Chen,
		  Ziyang and Zhao, Xiang},
  title		= {Zero-shot Knowledge Graph Question Generation via
		  Multi-agent LLMs and Small Models Synthesis},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679805},
  doi		= {10.1145/3627673.3679805},
  abstract	= {Knowledge Graph Question Generation (KGQG) is the task of
		  generating natural language questions based on the given
		  knowledge graph (KG). Although extensively explored in
		  recent years, prevailing models predominantly depend on
		  labelled data for training deep learning models or employ
		  large parametric frameworks, e.g., Large Language Models
		  (LLMs), which can incur significant deployment costs and
		  pose practical implementation challenges. To address these
		  issues, in this work, we put forward a zero-shot,
		  multi-agent KGQG framework. This framework integrates the
		  capabilities of LLMs with small models to facilitate
		  cost-effective, high-quality question generation. In
		  specific, we develop a professional editorial team
		  architecture accompanied by two workflow optimization tools
		  to reduce unproductive collaboration among LLMs-based
		  agents and enhance the robustness of the system. Extensive
		  experiments demonstrate that our proposed framework derives
		  the new state-of-the-art performance on the zero-shot KGQG
		  tasks, with relative gains of 20.24% and 13.57% on two KGQG
		  datasets, respectively, which rival fully supervised
		  state-of-the-art models.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3341–3351},
  numpages	= {11},
  keywords	= {large language models, multi-agents framework, zero-shot
		  knowledge graph question generation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.5555/3635637.3663238,
  author	= {Zhang, Shiyao and Dong, Yuji and Zhang, Yichuan and Payne,
		  Terry R. and Zhang, Jie},
  title		= {Large Language Model Assissted Multi-Agent Dialogue for
		  Ontology Alignment},
  year		= {2024},
  isbn		= {9798400704864},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Ontology alignment is critical in cross-domain
		  integration; however, it typically necessitates the
		  involvement of a human domain-expert, which can make the
		  task costly. Although a variety of machine-learning
		  approaches have been proposed that can simplify this task
		  by learning the patterns from experts, such techniques are
		  still susceptible to domain knowledge updates that could
		  potentially change the patterns and lead to extra expert
		  involvement. The use of Large Language Models (LLMs) has
		  demonstrated a general cognitive ability, which has the
		  potential to assist ontology alignment from the cognition
		  level, thus obviating the need for costly expert
		  involvement. However, the process by which the output of
		  LLMs is generated can be opaque and thus the reliability
		  and interpretability of such models is not always
		  predictable. This paper proposes a dialogue model, in which
		  multiple agents negotiate the correspondence between two
		  knowledge sets with the support from an LLM. We demonstrate
		  that this approach not only reduces the need for the
		  involvement of a domain expert for ontology alignment, but
		  that the results are interpretable despite the use of
		  LLMs.},
  booktitle	= {Proceedings of the 23rd International Conference on
		  Autonomous Agents and Multiagent Systems},
  pages		= {2594–2596},
  numpages	= {3},
  keywords	= {dialogue, large language model, multi-agent system,
		  negotiation, ontology alignment},
  location	= {Auckland, New Zealand},
  series	= {AAMAS '24}
}

@InProceedings{	  10.1145/3616855.3635738,
  author	= {Das, Sudeep and Saboo, Raghav and Vadrevu, Chaitanya S. K.
		  and Wang, Bruce and Xu, Steven},
  title		= {Applications of LLMs in E-Commerce Search and Product
		  Knowledge Graph: The DoorDash Case Study},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3635738},
  doi		= {10.1145/3616855.3635738},
  abstract	= {Extracting knowledge from unstructured or semi-structured
		  textual information is essential for the machine learning
		  applications that power DoorDash's search experience, and
		  the development and maintenance of its product knowledge
		  graph. Large language models (LLMs) have opened up new
		  possibilities for utilizing their power in these areas,
		  replacing or complementing traditional natural language
		  processing methods. LLMs are also proving to be useful in
		  the label and annotation generation process, which is
		  critical for these use cases. In this talk, we will provide
		  a high-level overview of how we incorporated LLMs for
		  search relevance and product understanding use cases, as
		  well as the key lessons learned and challenges faced during
		  their practical implementation.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {1163–1164},
  numpages	= {2},
  keywords	= {large language model, natural language processing, product
		  knowledge graph, search},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@InProceedings{	  10.1145/3665689.3665701,
  author	= {Chang, Xu},
  title		= {Research on Recommendation Algorithm Based on Knowledge
		  Graph},
  year		= {2024},
  isbn		= {9798400716645},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665689.3665701},
  doi		= {10.1145/3665689.3665701},
  abstract	= {In response to issues such as data explosion leading to
		  data overload and a subsequent decrease in the
		  effectiveness of information retrieval, this paper proposes
		  a Knowledge Graph Attention Network Splicing Semantics
		  (KGAT-SS) model based on attention mechanisms. The model
		  combines graph attention networks with a dual-tower model
		  framework, extending the breadth of recommendations through
		  the fusion of entity and text semantics. It enhances the
		  depth of recommendations based on graph attention networks
		  and adds constraints on the weights of transfer nodes in
		  the graph to facilitate more efficient learning of embedded
		  representations in nodes. The model consists of three
		  modules: text processing, graph representation, and
		  prediction. The main contributions include utilizing the
		  pre-trained natural language processing model BERT for
		  vectorizing user and item review texts, GRU encoding for
		  further hidden information exploration, TransR mapping of
		  instances in the dataset to vectors, and knowledge
		  representation through the knowledge graph. The graph
		  representation module employs graph attention networks to
		  differentiate weights between nodes, allowing nodes to
		  assess the importance of received information based on
		  neighboring node weights. A threshold is set during
		  propagation to filter out low-relevance entities. In the
		  prediction layer, multiple representations of entity nodes
		  are concatenated with semantic vectors of user and item
		  texts from the text processing module to obtain the final
		  vector representation. The matching degree is calculated
		  through attention scores. Experimental results indicate
		  that the proposed algorithm outperforms baseline models,
		  leading to an improvement in recommendation effectiveness
		  and enhancing the recommendation performance of the
		  system.},
  booktitle	= {Proceedings of the 2024 4th International Conference on
		  Bioinformatics and Intelligent Computing},
  pages		= {66–75},
  numpages	= {10},
  location	= {Beijing, China},
  series	= {BIC '24}
}

@InProceedings{	  10.1145/3675249.3675256,
  author	= {Zhou, Qian and Cao, Yanan and Wu, Ruiru and Tang,
		  Jinglei},
  title		= {Construction of Meteorological Disasters Knowledge Graph
		  Based on Deep Learning},
  year		= {2024},
  isbn		= {9798400718267},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3675249.3675256},
  doi		= {10.1145/3675249.3675256},
  abstract	= {Under the background of frequent meteorological disasters,
		  the science popularization and decision support of
		  meteorological disasters have snowballs. What's more, the
		  existing meteorological science popularization websites
		  have some defects such as document retrieval, low retrieval
		  efficiency and narrow knowledge coverage. Therefore, this
		  paper builds the meteorological disaster knowledge graph
		  based on deep learning model, which can be applied to the
		  fields of meteorological disaster science popularization
		  and decision support. This paper compares the application
		  effects of several deep learning models in the stage of
		  knowledge acquisition, and adopts the top-down method to
		  build the meteorological disaster knowledge graph. On this
		  basis, the data layer of knowledge graph is constructed
		  from bottom up. At the same time, this paper discusses the
		  application of meteorological disaster knowledge graph in
		  the field of meteorology.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Computer and Multimedia Technology},
  pages		= {37–42},
  numpages	= {6},
  location	= {Sanming, China},
  series	= {ICCMT '24}
}

@Article{	  10.14778/3685800.3685810,
  author	= {Yi, Peng and Liang, Lei and Zhang, Da and Chen, Yong and
		  Zhu, Jinye and Liu, Xiangyu and Tang, Kun and Chen, Jialin
		  and Lin, Hao and Qiu, Leijie and Zhou, Jun},
  title		= {KGFabric: A Scalable Knowledge Graph Warehouse for
		  Enterprise Data Interconnection},
  year		= {2024},
  issue_date	= {August 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {12},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3685800.3685810},
  doi		= {10.14778/3685800.3685810},
  abstract	= {Based on the diversified application scenarios at Ant
		  Group, we built the Ant Knowledge Graph Platform (AKGP). It
		  has constructed numerous domain-specific knowledge graphs
		  related to merchants, companies, accounts, products, and
		  more. AKGP manages trillions of structured knowledge
		  graphs, serving search, recommendation, risk control and
		  other businesses. However, as the demand increasing for
		  various workloads such as graph pattern matching, graph
		  representation learning, and cross-domain knowledge reuse,
		  the existing warehouse systems based on relational DBMS or
		  graph databases are unable to meet the requirements. To
		  address these issues, we propose KGFabric, an
		  industrial-scale knowledge graph management system built on
		  the distributed file system (DFS). KGFabric offers a
		  nearline knowledge storage engine that utilizes a
		  Semantic-enhanced Programmable Graph (SPG) model, which is
		  compatible with the Labeled Property Graph (LPG) model. The
		  data is persistently stored in DFS, such as HDFS, which
		  leverages the POSIX file system API, making it suitable for
		  deployment in multi-cloud environment at low cost. KGFabric
		  provides a native graph-based and hybrid storage format
		  that can serve as a shared backend for parallel graph
		  computing systems, significantly accelerating the analysis
		  of multi-workload. Additionally, KGFabric includes a graph
		  fabric framework that minimizes data duplication and
		  guarantees data security.KGFabric is able to manage
		  Peta-scale data and has supported graph fabric and analysis
		  with over 100 billion relations at Ant Group. We conduct
		  experiments on various datasets to evaluate the performance
		  of KGFabric. Compared with popular relational DBMS and
		  graph databases, the storage space for semantic relations
		  is reduced by over 90%. The performance of graph fabric
		  improves by 21\texttimes{} in real-world workloads. In
		  multi-hop semantic graph analysis, KGFabric enhances
		  performance by 100\texttimes{}.},
  journal	= {Proc. VLDB Endow.},
  month		= aug,
  pages		= {3841–3854},
  numpages	= {14}
}

@InProceedings{	  10.1145/3626772.3657762,
  author	= {Yang, Shenghao and Ma, Weizhi and Sun, Peijie and Ai,
		  Qingyao and Liu, Yiqun and Cai, Mingchen and Zhang, Min},
  title		= {Sequential Recommendation with Latent Relations based on
		  Large Language Model},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657762},
  doi		= {10.1145/3626772.3657762},
  abstract	= {Sequential recommender systems predict items that may
		  interest users by modeling their preferences based on
		  historical interactions. Traditional sequential
		  recommendation methods rely on capturing implicit
		  collaborative filtering signals among items. Recent
		  relation-aware sequential recommendation models have
		  achieved promising performance by explicitly incorporating
		  item relations into the modeling of user historical
		  sequences, where most relations are extracted from
		  knowledge graphs. However, existing methods rely on
		  manually predefined relations and suffer the sparsity
		  issue, limiting the generalization ability in diverse
		  scenarios with varied item relations.In this paper, we
		  propose a novel relation-aware sequential recommendation
		  framework with Latent Lelation Riscovery (LRD). Different
		  from previous relation-aware models that rely on predefined
		  rules, we propose to leverage the Large Language Model
		  (LLM) to provide new types of relations and connections
		  between items. The motivation is that LLM contains abundant
		  world knowledge, which can be adopted to mine latent
		  relations of items for recommendation. Specifically,
		  inspired by that humans can describe relations between
		  items using natural language, LRD harnesses the LLM that
		  has demonstrated human-like knowledge to obtain language
		  knowledge representations of items. These representations
		  are fed into a latent relation discovery module based on
		  the discrete state variational autoencoder (DVAE). Then the
		  self-supervised relation discovery tasks and recommendation
		  tasks are jointly optimized. Experimental results on
		  multiple public datasets demonstrate our proposed latent
		  relation discovery method can be incorporated with existing
		  relation-aware sequential recommendation models and
		  significantly improve the performance. Further analysis
		  experiments indicate the effectiveness and reliability of
		  the discovered latent relations.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {335–344},
  numpages	= {10},
  keywords	= {large language model, latent relation, sequential
		  recommendation},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3627673.3680094,
  author	= {Gubanov, Michael and Pyayt, Anna and Karolak, Aleksandra},
  title		= {CancerKG.ORG - A Web-scale, Interactive, Verifiable
		  Knowledge Graph-LLM Hybrid for Assisting with Optimal
		  Cancer Treatment and Care},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680094},
  doi		= {10.1145/3627673.3680094},
  abstract	= {Here, we describe one of the first Web-scale hybrid
		  Knowledge Graph (KG)-Large Language Model (LLM), populated
		  with the latest peer-reviewed medical knowledge on
		  colorectal Cancer. It is currently being evaluated to
		  assist with both medical research and clinical information
		  retrieval tasks at Moffitt Cancer Center and Research
		  Institute, which is one of the top Cancer centers in the
		  U.S. and in the world. Our hybrid is remarkable as it
		  serves the user needs better than just an LLM, KG or a
		  search-engine in isolation. LLMs as is are known to exhibit
		  hallucinations and catastrophic forgetting as well as are
		  trained on outdated corpora. The state of the art KGs, such
		  as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require
		  manual curation, hence are quickly getting stale. CancerKG
		  is unsupervised and is capable of automatically ingesting
		  and organizing the latest medical findings. To alleviate
		  the LLMs shortcomings, the verified KG serves as a
		  Retrieval Augmented Generation (RAG) guardrail. CancerKG
		  exhibits 5 different advanced user interfaces, each
		  tailored to serve different data modalities better and more
		  convenient for the user. We evaluated CancerKG on real user
		  queries and report a high NDCG score on a large-scale
		  corpora of approximately 44K publications.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4497–4505},
  numpages	= {9},
  keywords	= {LLM, artificial intelligence (AI), cancer, data
		  management},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3631700.3665234,
  author	= {Carta, Salvatore and Giuliani, Alessandro and Manca, Marco
		  Manolo and Piano, Leonardo and Tiddia, Sandro Gabriele},
  title		= {Towards Zero-shot Knowledge Graph building: Automated
		  Schema Inference},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3665234},
  doi		= {10.1145/3631700.3665234},
  abstract	= {In the current Digital Transformation scenario, Knowledge
		  Graphs are essential for comprehending, representing, and
		  exploiting complex information in a structured form. The
		  main paradigm in automatically generating proper Knowledge
		  Graphs relies on predefined schemas or ontologies. Such
		  schemas are typically manually constructed, requiring an
		  intensive human effort, and are often sensitive to
		  information loss due to negligence, incomplete analysis, or
		  human subjectivity or inclination. Limiting human bias and
		  the resulting information loss in creating proper Knowledge
		  Graphs is paramount, particularly for user modeling in
		  various sectors, such as education or healthcare. To this
		  end, we propose a novel approach to automatically
		  generating a proper entity schema. The devised methodology
		  combines the language understanding capabilities of LLM
		  with classical machine learning methods such as clustering
		  to properly build an entity schema from a set of documents.
		  This solution eliminates the need for human intervention
		  and fosters a more efficient and comprehensive knowledge
		  representation. The assessment of our proposal concerns
		  adopting a state-of-the-art entity extraction model
		  (UniNER) to estimate the relevance of the extracted
		  entities based on the generated schema. Results confirm the
		  potential of our approach, as we observed a negligible
		  difference between the topic similarity score obtained with
		  the ground truth and with the automatically generated
		  schema (less than 1% on average on three different
		  datasets). Such an outcome confirms that the proposed
		  approach may be valuable in automatically creating an
		  entity schema from a set of documents.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {467–473},
  numpages	= {7},
  keywords	= {Large Language Models, Named Entity Recognition, Ontology
		  Learning},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@InProceedings{	  10.1145/3673277.3673306,
  author	= {Peng, Zhen and Du, Ye and Chen, Qifang and Zheng,
		  Tianshuai},
  title		= {Research on Knowledge Graph Construction for Smart Grid
		  Cybersecurity},
  year		= {2024},
  isbn		= {9798400716959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3673277.3673306},
  doi		= {10.1145/3673277.3673306},
  abstract	= {This paper proposes a construction method for smart grid
		  cybersecurity knowledge graph and solves the difficulty of
		  multilingual entity extraction with a small amount of
		  labeled data. First, the construction method of smart grid
		  cybersecurity knowledge graph is proposed with the
		  multi-source heterogeneous data in the field of electric
		  power cybersecurity collected by subject crawlers. Then,
		  for the problems of insufficient labeled data and language
		  mixing in the electric power cybersecurity domain, a
		  DA-XLMR-BiLSTM-FC-CRF model based on a five-layer
		  architecture is proposed to realize the entity extraction
		  of multilingual unstructured text. Finally, comparative and
		  ablation experiments are designed to prove the
		  effectiveness of the proposed model, and the F1 value of
		  the model reaches 94.04% and the accuracy rate reaches
		  94.48%.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Cryptography, Network Security and Communication
		  Technology},
  pages		= {164–170},
  numpages	= {7},
  location	= {Harbin, China},
  series	= {CNSCT '24}
}

@InProceedings{	  10.1145/3627043.3659565,
  author	= {Petruzzelli, Alessandro and Martina, Alessandro Francesco
		  Maria and Spillo, Giuseppe and Musto, Cataldo and De
		  Gemmis, Marco and Lops, Pasquale and Semeraro, Giovanni},
  title		= {Improving Transformer-based Sequential Conversational
		  Recommendations through Knowledge Graph Embeddings},
  year		= {2024},
  isbn		= {9798400704338},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627043.3659565},
  doi		= {10.1145/3627043.3659565},
  abstract	= {Conversational Recommender Systems (CRS) have recently
		  drawn attention due to their capacity of delivering
		  personalized recommendations through multi-turn natural
		  language interactions. In this paper, we fit into this
		  research line and we introduce a Knowledge-Aware Sequential
		  Conversational Recommender System (KASCRS) that exploits
		  transformers and knowledge graph embeddings to provide
		  users with recommendations in a conversational setting. In
		  particular, KASCRS is able to predict a suitable
		  recommendation based on the elements that are mentioned in
		  a conversation between a user and a CRS. To do this, we
		  design a model that: (i) encodes each conversation as a
		  sequence of entities that are mentioned in the dialogue
		  (i.e., items and properties), and (ii) is trained on a
		  cloze task, that is to say, it learns to predict the final
		  element in the sequence - that corresponds to the item to
		  be recommended - based on the information it has previously
		  seen. The model has two main hallmarks: first, we exploit
		  Transformers and self-attention to capture the sequential
		  dependencies that exist among the entities that are
		  mentioned in the training dialogues, in a way similar to
		  session-based recommender systems [25]. Next, we used
		  knowledge graphs (KG) to improve the quality of the
		  representation of the elements mentioned in each sequence.
		  Indeed, we exploit knowledge graph embeddings techniques to
		  pre-train the representation of items and properties, and
		  we fed the input layer of our architecture with the
		  resulting embeddings. In this way, KASCRS integrates both
		  knowledge from the KGs as well as the dependencies and the
		  co-occurrences emerging from conversational data, resulting
		  in a more accurate representation of users and items. Our
		  experiments confirmed this intuition, since KASCRS overcame
		  several state-of-the-art baselines on two different
		  datasets.},
  booktitle	= {Proceedings of the 32nd ACM Conference on User Modeling,
		  Adaptation and Personalization},
  pages		= {172–182},
  numpages	= {11},
  keywords	= {Conversational Recommendations, Knowledge Graphs,
		  Recommender Systems, Transformers},
  location	= {Cagliari, Italy},
  series	= {UMAP '24}
}

@Article{	  10.1145/3635273,
  author	= {Liu, Jhih-Chen and Chen, Chiao-Ting and Lee, Chi and
		  Huang, Szu-Hao},
  title		= {Evolving Knowledge Graph Representation Learning with
		  Multiple Attention Strategies for Citation Recommendation
		  System},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3635273},
  doi		= {10.1145/3635273},
  abstract	= {The growing number of publications in the field of
		  artificial intelligence highlights the need for researchers
		  to enhance their efficiency in searching for relevant
		  articles. Most paper recommendation models either rely on
		  simplistic citation relationships among papers or focus on
		  content-based approaches, both of which overlook
		  interactions within academic networks. To address the
		  aforementioned problem, knowledge graph embedding (KGE)
		  methods have been used for citation recommendations because
		  recent research proves that graph representations can
		  effectively improve recommendation model accuracy. However,
		  academic networks are dynamic, leading to changes in the
		  representations of users and items over time. The majority
		  of KGE-based citation recommendations are primarily
		  designed for static graphs, thus failing to capture the
		  evolution of dynamic knowledge graph (DKG) structures. To
		  address these challenges, we introduced the evolving
		  knowledge graph embedding (EKGE) method. In this
		  methodology, evolving knowledge graphs are input into
		  time-series models to learn the patterns of structural
		  evolution. The model has the capability to generate
		  embeddings for each entity at various time points, thereby
		  overcoming limitation of static models that require
		  retraining to acquire embeddings at each specific time
		  point. To enhance the efficiency of feature extraction, we
		  employed a multiple attention strategy. This helped the
		  model find recommendation lists that are closely related to
		  a user’s needs, leading to improved recommendation
		  accuracy. Various experiments conducted on a citation
		  recommendation dataset revealed that the EKGE model
		  exhibits a 1.13% increase in prediction accuracy compared
		  to other KGE methods. Moreover, the model’s accuracy can
		  be further increased by an additional 0.84% through the
		  incorporation of an attention mechanism.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= mar,
  articleno	= {33},
  numpages	= {26},
  keywords	= {Multiple attention strategies, evolving knowledge graph
		  embedding, citation recommendation}
}

@InProceedings{	  10.1145/3661725.3661733,
  author	= {Anuyah, Sydney and Chakraborty, Sunandan},
  title		= {Can Deep Learning Large Language Models be Used to Unravel
		  Knowledge Graph Creation?},
  year		= {2024},
  isbn		= {9798400716393},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3661725.3661733},
  doi		= {10.1145/3661725.3661733},
  abstract	= {This research focuses on advancing RE methodologies by
		  employing and comparing various NLP models for analyzing
		  medical relationships, particularly concerning
		  Gastroesophageal Reflux Disease (GERD). Leveraging a
		  comprehensive dataset of GERD-related articles from PubMed,
		  the study explores the effectiveness of SpaCy for Named
		  Entity Recognition (NER) and BERT-based models (including
		  Bio-BERT and ELECTRA) for tokenization and deep learning
		  classification tasks. Unique to this study is the extensive
		  comparison across multiple advanced models, providing an
		  insightful evaluation of their performance in terms of
		  precision, recall, F1-score, and accuracy in the context of
		  biomedical text analysis. Significantly, Bio-BERT emerged
		  as the most effective model for this dataset, excelling
		  across all metrics compared to BERT-BASE and ELECTRA. This
		  performance underscores Bio-BERT’s specialized
		  pre-training on biomedical literature. The analysis
		  includes the application of these models in constructing a
		  comprehensive knowledge graph, which consolidates diverse
		  information about GERD. Additionally, the paper presents a
		  critical comparison between SpaCy’s automated annotation
		  and human annotators, utilizing the F-1 score for assessing
		  the reliability of BERT’s RE capabilities.},
  booktitle	= {Proceedings of the International Conference on Computing,
		  Machine Learning and Data Science},
  articleno	= {8},
  numpages	= {6},
  keywords	= {BERT, Bio-BERT, NER, RE, biomedical NLP., deep learning,
		  knowledge graph, medical text analysis, transformer
		  models},
  location	= {Singapore, Singapore},
  series	= {CMLDS '24}
}

@InProceedings{	  10.1145/3626772.3657706,
  author	= {Fang, Zhiyu and Lei, Shuai-Long and Zhu, Xiaobin and Yang,
		  Chun and Zhang, Shi-Xue and Yin, Xu-Cheng and Qin,
		  Jingyan},
  title		= {Transformer-based Reasoning for Learning Evolutionary
		  Chain of Events on Temporal Knowledge Graph},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657706},
  doi		= {10.1145/3626772.3657706},
  abstract	= {Temporal Knowledge Graph (TKG) reasoning often involves
		  completing missing factual elements along the timeline.
		  Although existing methods can learn good embeddings for
		  each factual element in quadruples by integrating temporal
		  information, they often fail to infer the evolution of
		  temporal facts. This is mainly because of (1)
		  insufficiently exploring the internal structure and
		  semantic relationships within individual quadruples and (2)
		  inadequately learning a unified representation of the
		  contextual and temporal correlations among different
		  quadruples. To overcome these limitations, we propose a
		  novel Transformer-based reasoning model (dubbed ECEformer)
		  for TKG to learn the Evolutionary Chain of Events (ECE).
		  Specifically, we unfold the neighborhood subgraph of an
		  entity node in chronological order, forming an evolutionary
		  chain of events as the input for our model. Subsequently,
		  we utilize a Transformer encoder to learn the embeddings of
		  intra-quadruples for ECE. We then craft a mixed-context
		  reasoning module based on the multi-layer perceptron (MLP)
		  to learn the unified representations of inter-quadruples
		  for ECE while accomplishing temporal knowledge reasoning.
		  In addition, to enhance the timeliness of the events, we
		  devise an additional time prediction task to complete
		  effective temporal information within the learned unified
		  representation. Extensive experiments on six benchmark
		  datasets verify the state-of-the-art performance and the
		  effectiveness of our method.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {70–79},
  numpages	= {10},
  keywords	= {context information mining, evolutionary chain of event,
		  link prediction, temporal knowledge graph completion},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Article{	  10.1145/3694979,
  author	= {Qiu, Jingyi and Song, Aibo and Jin, Jiahui and Chen,
		  Jiaoyan and Zhang, Xinyu and Fang, Xiaolin and Zhang,
		  Tianbo},
  title		= {Matching Tabular Data to Knowledge Graph with Effective
		  Core Column Set Discovery.},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {4},
  issn		= {1559-1131},
  url		= {https://doi.org/10.1145/3694979},
  doi		= {10.1145/3694979},
  abstract	= {Matching tabular data to a knowledge graph (KG) is
		  critical for understanding the semantic column types,
		  column relationships, and entities of a table. Existing
		  matching approaches rely heavily on core columns that
		  represent primary subject entities on which other columns
		  in the table depend. However, discovering these core
		  columns before understanding the table’s semantics is
		  challenging. Most prior works use heuristic rules, such as
		  the leftmost column, to discover a single core column,
		  while an insightful discovery of the core column set that
		  accurately captures the dependencies between columns is
		  often overlooked. To address these challenges, we introduce
		  Dependency-aware Core Column Set Discovery (DaCo), an
		  iterative method that uses a novel rough matching strategy
		  to identify both inter-column dependencies and the core
		  column set. Additionally, DaCo can be seamlessly integrated
		  with pre-trained language models, as proposed in the
		  optimization module. Unlike other methods, DaCo does not
		  require labeled data or contextual information, making it
		  suitable for real-world scenarios. In addition, it can
		  identify multiple core columns within a table, which is
		  common in real-world tables. We conduct experiments on six
		  datasets, including five datasets with single core columns
		  and one dataset with multiple core columns. Our
		  experimental results show that DaCo&nbsp; outperforms
		  existing core column set detection methods, further
		  improving the effectiveness of table understanding tasks.},
  journal	= {ACM Trans. Web},
  month		= oct,
  articleno	= {51},
  numpages	= {27},
  keywords	= {Table&nbsp;understanding, core column set, semantic
		  dependency}
}

@Article{	  10.1145/3639472,
  author	= {Wei, Wanxu and Song, Yitong and Yao, Bin},
  title		= {Enhancing Heterogeneous Knowledge Graph Completion with a
		  Novel GAT-based Approach},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {4},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3639472},
  doi		= {10.1145/3639472},
  abstract	= {Knowledge graphs (KGs) play a vital role in enhancing
		  search results and recommendation systems. With the rapid
		  increase in the size of KGs, they are becoming inaccurate
		  and incomplete. This problem can be solved by the KG
		  completion methods, of which graph attention network
		  (GAT)-based methods stand out because of their superior
		  performance. However, existing GAT-based KG completion
		  methods often suffer from overfitting issues when dealing
		  with heterogeneous KGs, primarily due to the unbalanced
		  number of samples. Additionally, these methods demonstrate
		  poor performance in predicting the tail (head) entity that
		  shares the same relation and head (tail) entity with
		  others. To solve these problems, we propose GATH, a novel
		  GAT-based method designed for Heterogeneous KGs. GATH
		  incorporates two separate attention network modules that
		  work synergistically to predict the missing entities. We
		  also introduce novel encoding and feature transformation
		  approaches, enabling the robust performance of GATH in
		  scenarios with imbalanced samples. Comprehensive
		  experiments are conducted to evaluate GATH’s performance.
		  Compared with the existing state-of-the-art GAT-based model
		  on Hits@10 and MRR metrics, our model improves performance
		  by 5.2% and 5.2% on the FB15K-237 dataset and by 4.5% and
		  14.6% on the WN18RR dataset, respectively.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= feb,
  articleno	= {104},
  numpages	= {20},
  keywords	= {Knowledge graph completion, graph attention network,
		  attention mechanism}
}

@InProceedings{	  10.1145/3652628.3652718,
  author	= {Zheng, Jianlong and Yu, Yan and Xiong, Xi},
  title		= {Joint Learning Framework of Semantics and Knowledge Graph
		  Reasoning},
  year		= {2024},
  isbn		= {9798400708831},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652628.3652718},
  doi		= {10.1145/3652628.3652718},
  abstract	= {The existing embedded Knowledge Graph Question Answering
		  (KGQA) methods based on relationship chain reasoning
		  primarily rely on explicit relationship chains in natural
		  language questions and implicit relationship chains in the
		  knowledge graph (KG) for reasoning. However, these methods
		  overlook the semantic contributions of entity semantics and
		  question context to the relationship chain, merely focusing
		  on the order of relationships within the chain. To address
		  this limitation, a joint learning model is proposed that
		  incorporates both text and KG reasoning using the Graph
		  Attention Network (GAT). In this approach, the semantics of
		  entities are integrated into the reasoning process of
		  multi-hop KGQA, thereby enhancing the semantic expression
		  within the relationship chain. Additionally, a fusion of
		  the question context with the KG is performed to achieve a
		  comprehensive understanding of the abundant semantic
		  information contained in the question. The experimental
		  results, obtained from three benchmark datasets, clearly
		  demonstrate the significant superiority of the proposed
		  model over the previous state-of-the-art methods, including
		  GraftNet, EmbedKGQA, and Rce-KGQA. This validation verifies
		  that the integration of question context and entity
		  semantics effectively enhances the expression ability of
		  incomplete KGs, leading to improved KGQA performance.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Artificial Intelligence and Computer Engineering},
  pages		= {535–541},
  numpages	= {7},
  location	= {Dalian, China},
  series	= {ICAICE '23}
}

@Article{	  10.1109/taslp.2024.3407575,
  author	= {Chen, Weize and Han, Xu and Lin, Yankai and He, Kaichen
		  and Xie, Ruobing and Zhou, Jie and Liu, Zhiyuan and Sun,
		  Maosong},
  title		= {Hyperbolic Pre-Trained Language Model},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3407575},
  doi		= {10.1109/TASLP.2024.3407575},
  abstract	= {In recent years, we have witnessed significant
		  improvements in pre-trained language models (PLM) brought
		  about by the scaling of parameter sizes and data amounts.
		  However, this also brings high computational and storage
		  costs. In this paper, we present a new direction to improve
		  PLMs without scaling parameters and data: adopting a
		  geometric feature space that is more suitable for encoding
		  the intrinsic structured features of text. Although text is
		  generally considered unstructured data, it possesses rich
		  intrinsic structured features that signify syntactic and
		  semantic relationships. Leveraging these structured
		  features is vital for text understanding. Given that
		  structured features are better encoded in hyperbolic spaces
		  than in the Euclidean spaces used by conventional PLMs, we
		  propose that PLMs should operate entirely within hyperbolic
		  spaces. Our experiments demonstrate the superiority of
		  hyperbolic PLMs over Euclidean PLMs across a wide variety
		  of tasks, using the same parameter and data settings. This
		  suggests that altering the geometry of model representation
		  is a promising direction for model enhancement.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= may,
  pages		= {3101–3112},
  numpages	= {12}
}

@InProceedings{	  10.1145/3677779.3677800,
  author	= {Xie, Yijie},
  title		= {Temporal Knowledge Graph Completion based on Historical
		  Constraints and Contemporaneous Subgraphs},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677779.3677800},
  doi		= {10.1145/3677779.3677800},
  abstract	= {Temporal knowledge graph (TKG) can model entities and
		  relations in the real world in the time dimension. However,
		  due to the problem of incomplete information in TKGs,
		  research on TKG completion techniques is needed. Most TKG
		  completion methods heavily rely on the repetitive
		  appearance of historical entities, which poses a challenge
		  for completing entities in the same period. Therefore, we
		  propose a TKG completion model based on historical
		  constraints and contemporaneous subgraphs, named HC-TKGC.
		  HC-TKGC divides candidate entities into historical
		  entities, non-historical entities, and contemporaneous
		  entities, and learns distribution vectors for different
		  types of entities. It also adjusts the final candidate
		  entity scores by using a binary classifier based on
		  external knowledge to determine the historical visibility
		  of candidate entities. We evaluate our proposed model on
		  three datasets. The results show that HC-TKGC outperforms
		  baseline models in most metrics, demonstrating the
		  effectiveness of the model in TKG completion tasks.},
  booktitle	= {Proceedings of the International Conference on Modeling,
		  Natural Language Processing and Machine Learning},
  pages		= {128–132},
  numpages	= {5},
  location	= {Xi'an, China},
  series	= {CMNM '24}
}

@InProceedings{	  10.1145/3589335.3651247,
  author	= {Bernard, Nolwenn and Kostric, Ivica and \L{}ajewska,
		  Weronika and Balog, Krisztian and Galus\v{c}\'{a}kov\'{a},
		  Petra and Setty, Vinay and Skj\ae{}veland, Martin G.},
  title		= {PKG API: A Tool for Personal Knowledge Graph Management},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651247},
  doi		= {10.1145/3589335.3651247},
  abstract	= {Personal knowledge graphs (PKGs) offer individuals a way
		  to store and consolidate their fragmented personal data in
		  a central place, improving service personalization while
		  maintaining full user control. Despite their potential,
		  practical PKG implementations with user-friendly interfaces
		  remain scarce. This work addresses this gap by proposing a
		  complete solution to represent, manage, and interface with
		  PKGs. Our approach includes (1) a user-facing PKG Client,
		  enabling end-users to administer their personal data easily
		  via natural language statements, and (2) a service-oriented
		  PKG API. To tackle the complexity of representing these
		  statements within a PKG, we present an RDF-based PKG
		  vocabulary that supports this, along with properties for
		  access rights and provenance.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1051–1054},
  numpages	= {4},
  keywords	= {knowledge representation, personal data management,
		  personal knowledge graphs, semantic technologies},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1145/3657305,
  author	= {Wu, Ling-I and Su, Yuxin and Li, Guoqiang},
  title		= {Zero-Shot Construction of Chinese Medical Knowledge Graph
		  with GPT-3.5-turbo and GPT-4},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3657305},
  doi		= {10.1145/3657305},
  abstract	= {Knowledge graphs have revolutionized the organization and
		  retrieval of real-world knowledge, prompting interest in
		  automatic NLP-based approaches for extracting medical
		  knowledge from texts. However, the availability of
		  high-quality Chinese medical knowledge remains limited,
		  posing challenges for constructing Chinese medical
		  knowledge graphs. As LLMs like ChatGPT show promise in
		  zero-shot learning for many NLP downstream tasks, their
		  potential on constructing Chinese medical knowledge graphs
		  is still uncertain. In this study, we create a Chinese
		  medical knowledge graph by manually annotating textual data
		  and using ChatGPT to automatically generate the graph. We
		  refine the results using filtering and mapping rules to
		  align with our schema. The manually generated graph serves
		  as the ground truth for evaluation, and we explore
		  different methods to enhance its accuracy through knowledge
		  graph completion techniques. As a result, we emphasize the
		  potential of employing ChatGPT for automated knowledge
		  graph construction within the Chinese medical domain. While
		  ChatGPT successfully identifies a larger number of
		  entities, further enhancements are required to improve its
		  performance in extracting more qualified relations.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= apr,
  keywords	= {Medical knowledge graph, ChatGPT, Nature language
		  processing, Named entity recognition, Relation extraction}
}

@InProceedings{	  10.1145/3660043.3660119,
  author	= {Chang, Xiaoyu and Liu, Yong and Huang, Liang and Li,
		  Jianbin and Liang, Yin and Li, Shike and Sun, Yifan},
  title		= {Blockchain Threat Intelligence Knowledge Graph Alignment
		  via Graph Convolutional Networks},
  year		= {2024},
  isbn		= {9798400716157},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3660043.3660119},
  doi		= {10.1145/3660043.3660119},
  abstract	= {The escalating prevalence of security incidents in the
		  blockchain sphere is posing sig- nificant challenges to its
		  future development. The integration of knowledge graphs
		  into blockchain security is being investigated as a
		  potential solution to offer a com- prehensive view of the
		  blockchain security landscape. Despite the promise, the di-
		  versity and subpar quality of existing blockchain threat
		  intelligence data complicate the use of knowledge graphs
		  for representing this information. The paper proposes the
		  use of knowledge graph fusion, particularly focusing on
		  entity alignment and en- tity linking, as an innovative
		  approach to reconcile knowledge graphs of blockchain threat
		  intelligence from disparate sources. Additionally, it
		  utilizes GCN to model the structural information and an
		  improved TransE to model the attribute information. By
		  combining both representations, the accuracy of blockchain
		  threat intelligence knowledge graph alignment is
		  significantly improved.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Information Education and Artificial Intelligence},
  pages		= {421–430},
  numpages	= {10},
  location	= {Xiamen, China},
  series	= {ICIEAI '23}
}

@InProceedings{	  10.1145/3674225.3674332,
  author	= {Xin, Rui and Zhang, Pengfei and Chen, Xi and Peng, Jiao
		  and Liu, Haifeng},
  title		= {Knowledge Graph Question-Answering Based on Link Reasoning
		  for Electrical Equipment},
  year		= {2024},
  isbn		= {9798400716638},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3674225.3674332},
  doi		= {10.1145/3674225.3674332},
  abstract	= {The construction of an intelligent question-answering
		  system based on a knowledge graph of electrical equipment
		  can facilitate the complex retrieval, querying, and
		  answering of questions related to electrical equipment
		  knowledge. However, existing knowledge graph
		  question-answering techniques encounter various challenges.
		  Traditional template-based approaches require substantial
		  human effort and time, limiting their scalability. On the
		  other hand, deep learning-based methods demand vast amounts
		  of data for model training. To address these issues, this
		  paper establishes a knowledge graph of electrical equipment
		  and creates a question-answering dataset based on this
		  graph. Subsequently, a knowledge graph question-answering
		  method grounded in link reasoning is proposed. Research
		  findings demonstrate that the method presented in this
		  paper outperforms three baseline methods in terms of
		  accuracy on the knowledge graph of electrical equipment.},
  booktitle	= {Proceedings of the 2024 International Conference on Power
		  Electronics and Artificial Intelligence},
  pages		= {594–600},
  numpages	= {7},
  location	= {Xiamen, China},
  series	= {PEAI '24}
}

@Article{	  10.1145/3643565,
  author	= {Rong, Huan and Qian, Minfeng and Ma, Tinghuai and Jin, Di
		  and Sheng, Victor S.},
  title		= {CoBjeason: Reasoning Covered Object in Image by
		  Multi-Agent Collaboration Based on Informed Knowledge
		  Graph},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {5},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3643565},
  doi		= {10.1145/3643565},
  abstract	= {Object detection is a widely studied problem in existing
		  works. However, in this paper, we turn to a more
		  challenging problem of “Covered Object Reasoning”,
		  aimed at reasoning the category label of target object in
		  the given image particularly when it has been totally
		  covered (or invisible). To resolve this problem, we propose
		  CoBjeason to seize the opportunity when visual reasoning
		  meets the knowledge graph, where “empirical cognition”
		  on common visual contexts have been incorporated as
		  knowledge graph to conduct reinforced multi-hop reasoning
		  via two collaborative agents. Such two agents, for one
		  thing, stand at the covered object (or unknown entity) to
		  observe the surrounding visual cues in the given image and
		  gradually select entities and relations from the global
		  gallery-level knowledge graph which contains entity-pairs
		  frequently occurring across the entire image-collection, so
		  as to infer the main structure of image-level knowledge
		  graph forward expanded from the unknown entity. In turn,
		  for another, based on the reasoned image-level knowledge
		  graph, the semantic context among entities will be
		  aggregated backward into unknown entity to select an
		  appropriate entity from the global gallery-level knowledge
		  graph as the reasoning result. Moreover, such two agents
		  will collaborate with each other, securing that the above
		  Forward &amp; Backward Reasoning will step towards the same
		  destination of the higher performance on covered object
		  reasoning. To our best knowledge, this is the first work on
		  Covered Object Reasoning with Knowledge Graphs and
		  reinforced Multi-Agent collaboration. Particularly, our
		  study on Covered Object Reasoning and the proposed model
		  CoBjeason could offer novel insights into more basic
		  Computer Vision (CV) tasks, such as Semantic Segmentation
		  with better understanding on the current scene when some
		  objects are blurred or covered, Visual Question Answering
		  with enhancement on the inference in more complicated
		  visual context when some objects are covered or invisible,
		  and Image Caption Generation with the augmentation on the
		  richness of visual context for images containing partially
		  visible objects. The improvement on the above basic CV
		  tasks can further refine more complicated ones involved
		  with nuanced visual interpretation like Autonomous Driving,
		  where the recognition and reasoning on partially visible or
		  covered object are critical. According to the experimental
		  results, our proposed CoBjeason can achieve the best
		  overall ranking performance on covered object reasoning
		  compared with other models, meanwhile enjoying the
		  advantage of lower “exploration cost”, with the
		  insensitivity against the long-tail covered objects and the
		  acceptable time complexity.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= feb,
  articleno	= {116},
  numpages	= {56},
  keywords	= {Covered object reasoning, visual reasoning, multi-hop
		  knowledge graph reasoning, multi-agent reinforcement
		  learning}
}

@InProceedings{	  10.1145/3589335.3651557,
  author	= {Venkatakrishnan, Radhakrishnan and Tanyildizi, Emrah and
		  Canbaz, M. Abdullah},
  title		= {Semantic interlinking of Immigration Data using LLMs for
		  Knowledge Graph Construction},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651557},
  doi		= {10.1145/3589335.3651557},
  abstract	= {The challenge of managing immigration data is exacerbated
		  by its reliance on paper-based, evidence-driven records
		  maintained by legal professionals, creating obstacles for
		  efficient processing and analysis due to inherent trust
		  issues with AI-based systems. This paper introduces a
		  cutting-edge framework to surmount these hurdles by
		  synergizing Large Language Models (LLMs) with Knowledge
		  Graphs (KGs), revolutionizing traditional data handling
		  methods. Our method transforms archaic, paper-based
		  immigration records into a structured, interconnected
		  knowledge network that intricately mirrors the legal and
		  procedural nuances of immigration, ensuring a dynamic and
		  trustworthy platform for data analysis. Utilizing LLMs, we
		  extract vital entities and relationships from diverse legal
		  documents to forge a comprehensive knowledge graph,
		  encapsulating the complex legalities and procedural
		  disparities in immigration processes and mapping the
		  multifaceted interactions among stakeholders like
		  applicants, sponsors, and legal experts. This graph not
		  only facilitates a deep dive into the legal stipulations
		  but also incorporates them, significantly boosting the
		  system's reliability and precision. With the integration of
		  Retrieval Augmented Generation (RAG) for exact,
		  context-aware data retrieval and Augmented Knowledge
		  Creation for developing a conversational interface via
		  LLMs, our framework offers a scalable, adaptable solution
		  to immigration data management. This innovative
		  amalgamation of LLMs, KGs, and RAG techniques marks a
		  paradigm shift towards more informed, efficient, and
		  trustworthy decision-making in the sphere of global
		  migration, setting a new benchmark for legal technology and
		  data source management.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {605–608},
  numpages	= {4},
  keywords	= {data restructuring, document processing, information
		  retrieval, knowledge graphs, large language models, legal
		  tech},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3691720.3691779,
  author	= {Liu, Qingqing and Wang, Zhiguo and Yang, Qiping},
  title		= {KGBL: A Study on the Design of a Knowledge Graph-based
		  Blended Learning Framework},
  year		= {2024},
  isbn		= {9798400710230},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691720.3691779},
  doi		= {10.1145/3691720.3691779},
  abstract	= {Blended learning has emerged as a significant trend in
		  global higher education due to its flexible and
		  personalized characteristics. From the literature, it can
		  be observed that almost every case of blended teaching
		  practice presents a blended teaching design model. This
		  indicates a lack of a universal framework for blended
		  learning. Additionally, the emergence of the Knowledge
		  Graph has introduced new challenges to traditional blended
		  learning scenarios. Faced with these challenges, this study
		  aims to utilize knowledge graphs to develop a generic
		  framework for blended learning through a literature
		  research approach. A review of the literature revealed that
		  knowledge graphs in the field of education are
		  micro-classes with learning resources. Based on this, a
		  general process for constructing knowledge graphs was
		  proposed with the use of MOOC platform. The constructed
		  knowledge graph serves as the foundation for the Knowledge
		  Graph Blended Learning framework. The online component
		  implements an adaptive learning cycle based on the
		  knowledge graph, while the offline component facilitates
		  students' deep learning. In future related research, the
		  Knowledge Graph Blended Learning framework can be used to
		  guide the practice of blended learning and further expand
		  and deepen it in terms of personalized and accurate
		  teaching decision-making and whole-process enhancement of
		  process evaluation.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Educational Knowledge and Informatization},
  pages		= {343–350},
  numpages	= {8},
  location	= {Shanghai, China},
  series	= {EKI '24}
}

@InProceedings{	  10.1145/3589334.3645676,
  author	= {Zamiri, Mona and Qiang, Yao and Nikolaev, Fedor and Zhu,
		  Dongxiao and Kotov, Alexander},
  title		= {Benchmark and Neural Architecture for Conversational
		  Entity Retrieval from a Knowledge Graph},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645676},
  doi		= {10.1145/3589334.3645676},
  abstract	= {This paper introduces a novel information retrieval (IR)
		  task of Conversational Entity Retrieval from a Knowledge
		  Graph (CER-KG), which extends non-conversational entity
		  retrieval from a knowledge graph (KG) to the conversational
		  scenario. The user queries in CER-KG dialog turns may rely
		  on the results of the preceding turns, which are KG
		  entities. Similar to the conversational document IR, CER-KG
		  can be viewed as a sequence of interrelated ranking tasks.
		  To enable future research on CER-KG, we created QBLink-KG,
		  a publicly available benchmark that was adapted from
		  QBLink, a benchmark for text-based conversational reading
		  comprehension of Wikipedia. As an initial approach to
		  CER-KG, we experimented with Transformer- and LSTM-based
		  query encoders in combination with the Neural Architecture
		  for Conversational Entity Retrieval (NACER), our proposed
		  feature-based neural architecture for entity ranking in
		  CER-KG. NACER computes the ranking score of a candidate KG
		  entity by taking into account diverse lexical and semantic
		  matching signals between various KG components in its
		  neighborhood, such as entities, categories, and literals,
		  as well as entities in the results of the preceding turns
		  in dialog history. The reported experimental results reveal
		  the key challenges of CER-KG along with the possible
		  directions for new approaches to this task.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {1519–1528},
  numpages	= {10},
  keywords	= {conversational ir, deep learning, entity retrieval, ir
		  benchmarks, knowledge graphs},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3637528.3671745,
  author	= {Komarlu, Tanay and Jiang, Minhao and Wang, Xuan and Han,
		  Jiawei},
  title		= {OntoType: Ontology-Guided and Pre-Trained Language Model
		  Assisted Fine-Grained Entity Typing},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671745},
  doi		= {10.1145/3637528.3671745},
  abstract	= {Fine-grained entity typing (FET), which assigns entities
		  in text with context-sensitive, fine-grained semantic
		  types, is a basic but important task for knowledge
		  extraction from unstructured text. FET has been studied
		  extensively in natural language processing and typically
		  relies on human-annotated corpora for training, which is
		  costly and difficult to scale. Recent studies explore the
		  utilization of pre-trained language models (PLMs) as a
		  knowledge base to generate rich and context-aware weak
		  supervision for FET. However, a PLM still requires
		  direction and guidance to serve as a knowledge base as they
		  often generate a mixture of rough and fine-grained types,
		  or tokens unsuitable for typing. In this study, we vision
		  that an ontology provides a semantics-rich, hierarchical
		  structure, which will help select the best results
		  generated by multiple PLM models and head words.
		  Specifically, we propose a novel annotation-free,
		  ontology-guided FET method, OntoType, which follows a type
		  ontological structure, from coarse to fine, ensembles
		  multiple PLM prompting results to generate a set of type
		  candidates, and refines its type resolution, under the
		  local context with a natural language inference model. Our
		  experiments on the Ontonotes, FIGER, and NYT datasets using
		  their associated ontological structures demonstrate that
		  our method outperforms the state-of-the-art zero-shot
		  fine-grained entity typing methods as well as a typical LLM
		  method, ChatGPT. Our error analysis shows that refinement
		  of the existing ontology structures will further improve
		  fine-grained entity typing.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1407–1417},
  numpages	= {11},
  keywords	= {fine-grained entity typing, masked language model
		  prompting, natural language understanding, zero-shot entity
		  typing},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3652628.3652766,
  author	= {Liu, Yuxin and Yu, Jianhui and Jia, Wenyang and Liu,
		  Yuliang},
  title		= {TCM Automatic Diagnosis System Based on Knowledge Graph
		  and BERT},
  year		= {2024},
  isbn		= {9798400708831},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652628.3652766},
  doi		= {10.1145/3652628.3652766},
  abstract	= {Artificial intelligence technology has provided
		  significant benefits to Traditional Chinese Medicine (TCM)
		  diagnosis. In this paper, we build a TCM automatic system
		  utilizing knowledge graphs and natural language processing.
		  We first train a standard word alignment model by
		  fine-tuning the Bidirectional Encoder Representations from
		  Transformers (BERT) based model to help nonstandard input
		  align to standard text. Then we propose an algorithm for
		  calculating the recommended score for each prescription in
		  the knowledge graph, thereby obtaining the recommended
		  results for a given set of patient symptoms. To evaluate
		  the effectiveness of our system, we conducted experiments
		  using our TCM diagnosis dataset. The results demonstrate
		  that our system has the potential to be a traditional
		  Chinese medicine AI assistant with low computing resource
		  consumption and high accuracy.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Artificial Intelligence and Computer Engineering},
  pages		= {829–834},
  numpages	= {6},
  location	= {Dalian, China},
  series	= {ICAICE '23}
}

@InProceedings{	  10.1145/3639856.3639872,
  author	= {Khatun, Rabina and Sinhababu, Nilanjan},
  title		= {Improved Sequence Predictions using Knowledge Graph
		  Embedding for Large Language Models},
  year		= {2024},
  isbn		= {9798400716492},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639856.3639872},
  doi		= {10.1145/3639856.3639872},
  abstract	= {Large Language Models (LLM) have gained huge popularity
		  recently due to their problem-solving capability in
		  multiple domains. Technically LLMs can be considered a
		  critical mixture of huge amounts of training data, smart
		  and exhaustive prompt engineering, and word prediction
		  models along with Reinforcement and Supervised learning
		  mechanisms. Word prediction models are at the core of any
		  Large Language Model. The latest word prediction techniques
		  are sequential and transformer models. Transformers have
		  overcome most of the drawbacks of sequential models with
		  similar embedding knowledge. The literature survey shows
		  little to no improvement in the embedding techniques. In
		  this paper, we examined the existing word prediction models
		  by replacing embedding models with an auto-engineered
		  Knowledge Graph Embedding. This auto-engineered data
		  representation shows drastic improvements in prediction
		  quality. This mechanism also accelerates the prediction by
		  providing more context information to the models with
		  respect to the general embedding mechanism. Standard
		  evaluation strategies are used to compare the model
		  behavior.},
  booktitle	= {Proceedings of the Third International Conference on AI-ML
		  Systems},
  articleno	= {16},
  numpages	= {5},
  keywords	= {attention mechanisms, generative models, neural networks,
		  text prediction},
  location	= {Bangalore, India},
  series	= {AIMLSystems '23}
}

@InProceedings{	  10.1145/3627673.3680020,
  author	= {Shu, Dong and Zhao, Haoran and Liu, Xukun and Demeter,
		  David and Du, Mengnan and Zhang, Yongfeng},
  title		= {LawLLM: Law Large Language Model for the US Legal System},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680020},
  doi		= {10.1145/3627673.3680020},
  abstract	= {In the rapidly evolving field of legal analytics, finding
		  relevant cases and accurately predicting judicial outcomes
		  are challenging because of the complexity of legal
		  language, which often includes specialized terminology,
		  complex syntax, and historical context. Moreover, the
		  subtle distinctions between similar and precedent cases
		  require a deep understanding of legal knowledge.
		  Researchers often conflate these concepts, making it
		  difficult to develop specialized techniques to effectively
		  address these nuanced tasks. In this paper, we introduce
		  the Law Large Language Model (LawLLM), a multi-task model
		  specifically designed for the US legal domain to address
		  these challenges. LawLLM excels at Similar Case Retrieval
		  (SCR), Precedent Case Recommendation (PCR), and Legal
		  Judgment Prediction (LJP). By clearly distinguishing
		  between precedent and similar cases, we provide essential
		  clarity, guiding future research in developing specialized
		  strategies for these tasks. We propose customized data
		  preprocessing techniques for each task that transform raw
		  legal data into a trainable format. Furthermore, we also
		  use techniques such as in-context learning (ICL) and
		  advanced information retrieval methods in LawLLM. The
		  evaluation results demonstrate that LawLLM consistently
		  outperforms existing baselines in both zero-shot and
		  few-shot scenarios, offering unparalleled multi-task
		  capabilities and filling critical gaps in the legal domain.
		  Code and data are available at
		  https://github.com/Tizzzzy/Law_LLM.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4882–4889},
  numpages	= {8},
  keywords	= {large language models, legal system, multitask learning,
		  natural language processing},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3698587.3701359,
  author	= {Yue, Ling and Xing, Sixue and Chen, Jintai and Fu,
		  Tianfan},
  title		= {ClinicalAgent: Clinical Trial Multi-Agent System with
		  Large Language Model-based Reasoning},
  year		= {2024},
  isbn		= {9798400713026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698587.3701359},
  doi		= {10.1145/3698587.3701359},
  abstract	= {Large Language Models (LLMs) and multi-agent systems have
		  shown impressive capabilities in natural language tasks but
		  face challenges in clinical trial applications, primarily
		  due to limited access to external knowledge. Recognizing
		  the potential of advanced clinical trial tools that
		  aggregate and predict based on the latest medical data, we
		  propose an integrated solution to enhance their
		  accessibility and utility. We introduce Clinical Agent
		  System (ClinicalAgent), a clinical multi-agent system
		  designed for clinical trial tasks, leveraging GPT-4,
		  multi-agent architectures, LEAST-TO-MOST, and ReAct
		  reasoning technology. This integration not only boosts LLM
		  performance in clinical contexts but also introduces novel
		  functionalities. The proposed method achieves competitive
		  predictive performance in clinical trial outcome prediction
		  (0.7908 PR-AUC), obtaining a 0.3326 improvement over the
		  standard prompt Method. Publicly available code can be
		  found at https://github.com/LeoYML/clinical-agent.},
  booktitle	= {Proceedings of the 15th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  articleno	= {11},
  numpages	= {10},
  keywords	= {Clinical Trial, Clinical Trial Outcome Prediction, Drug
		  Development, Healthcare, Large Language Model-based
		  Reasoning, Large Language Models, Multi-Agent Planning},
  location	= {Shenzhen, China},
  series	= {BCB '24}
}

@Article{	  10.1145/3643806,
  author	= {Cao, Jiahang and Fang, Jinyuan and Meng, Zaiqiao and
		  Liang, Shangsong},
  title		= {Knowledge Graph Embedding: A Survey from the Perspective
		  of Representation Spaces},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3643806},
  doi		= {10.1145/3643806},
  abstract	= {Knowledge graph embedding (KGE) is an increasingly popular
		  technique that aims to represent entities and relations of
		  knowledge graphs into low-dimensional semantic spaces for a
		  wide spectrum of applications such as link prediction,
		  knowledge reasoning and knowledge completion. In this
		  article, we provide a systematic review of existing KGE
		  techniques based on representation spaces. Particularly, we
		  build a fine-grained classification to categorise the
		  models based on three mathematical perspectives of the
		  representation spaces: (1) algebraic perspective, (2)
		  geometric perspective and (3) analytical perspective. We
		  introduce the rigorous definitions of fundamental
		  mathematical spaces before diving into KGE models and their
		  mathematical properties. We further discuss different KGE
		  methods over the three categories, as well as summarise how
		  spatial advantages work over different embedding needs. By
		  collating the experimental results from downstream tasks,
		  we also explore the advantages of mathematical space in
		  different scenarios and the reasons behind them. We further
		  state some promising research directions from a
		  representation space perspective, with which we hope to
		  inspire researchers to design their KGE models as well as
		  their related applications with more consideration of their
		  mathematical space properties.},
  journal	= {ACM Comput. Surv.},
  month		= mar,
  articleno	= {159},
  numpages	= {42},
  keywords	= {Knowledge graphs, representation spaces, embedding
		  techniques, mathematical perspectives}
}

@InProceedings{	  10.1145/3631908.3631928,
  author	= {Fan, Jiawei and Ren, Xianghui and Zhang, Hao and Ma,
		  Huisheng and Wei, Xinlei and Yue, Yifeng},
  title		= {Advanced Attention for Causality Classification of Verb
		  Nodes of Knowledge Graph},
  year		= {2024},
  isbn		= {9798400709098},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631908.3631928},
  doi		= {10.1145/3631908.3631928},
  abstract	= {In this paper, the causal relationship between verb nodes
		  in knowledge graph is determined according to the verb
		  nodes and the sentences in which they are located. For the
		  operation of paying attention to the whole sentence, the
		  verb is not studied specifically, which is disadvantageous
		  to the judgment of causality. Therefore, this paper
		  proposes an advanced attention method based on attention
		  mechanism to judge the causality of knowledge graph. It can
		  not only focus on the extraction of verb related
		  information, but also learn the whole sentence features.
		  The experiment verifies the method, and the results show
		  that the method in this paper has advantages.},
  booktitle	= {Proceedings of the 7th International Conference on
		  Algorithms, Computing and Systems},
  pages		= {140–144},
  numpages	= {5},
  keywords	= {attention, classification, knowledge graphs, verbs},
  location	= {Larissa, Greece},
  series	= {ICACS '23}
}

@InProceedings{	  10.1145/3616855.3635772,
  author	= {Deng, Cheng and Zhang, Tianhang and He, Zhongmou and Chen,
		  Qiyuan and Shi, Yuanyuan and Xu, Yi and Fu, Luoyi and
		  Zhang, Weinan and Wang, Xinbing and Zhou, Chenghu and Lin,
		  Zhouhan and He, Junxian},
  title		= {K2: A Foundation Language Model for Geoscience Knowledge
		  Understanding and Utilization},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3635772},
  doi		= {10.1145/3616855.3635772},
  abstract	= {Large language models (LLMs) have achieved great success
		  in general domains of natural language processing. In this
		  paper, we bring LLMs to the realm of geoscience with the
		  objective of advancing research and applications in this
		  field. To this end, we present the first-ever LLM in
		  geoscience, K2, alongside a suite of resources developed to
		  further promote LLM research within geoscience. For
		  instance, we have curated the first geoscience instruction
		  tuning dataset, GeoSignal, which aims to align LLM
		  responses to geoscience-related user queries. Additionally,
		  we have established the first geoscience benchmark,
		  GeoBench, to evaluate LLMs in the context of geoscience. In
		  this work, we experiment with a complete recipe to adapt a
		  pre-trained general-domain LLM to the geoscience domain.
		  Specifically, we further train the LLaMA-7B model on 5.5B
		  tokens of geoscience text corpus, including over 1 million
		  pieces of geoscience literature, and utilize GeoSignal's
		  supervised data to fine-tune the model. Moreover, we share
		  a protocol that can efficiently gather domain-specific data
		  and construct domain-supervised data, even in situations
		  where manpower is scarce. Meanwhile, we equip K2 with the
		  abilities of using tools to be a naive geoscience aide.
		  Experiments conducted on the GeoBench demonstrate the
		  effectiveness of our approach and datasets on geoscience
		  knowledge understanding and utilization.We open-source all
		  the training data and K2 model checkpoints at
		  https://github.com/davendw49/k2},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {161–170},
  numpages	= {10},
  keywords	= {foundation model, geoscience knowledge mining, geoscience
		  large language model},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@Article{	  10.1145/3643745,
  author	= {Zan, Daoguang and Yu, Ailun and Shen, Bo and Chen, Bei and
		  Li, Wei and Gong, Yongshun and Chen, Xiaolin and Yao, Yafen
		  and Luo, Weihua and Guan, Bei and Liu, Yan and Wang, Yongji
		  and Wang, Qianxiang and Cui, Lizhen},
  title		= {DiffCoder: Enhancing Large Language Model on API
		  Invocation via Analogical Code Exercises},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {FSE},
  url		= {https://doi.org/10.1145/3643745},
  doi		= {10.1145/3643745},
  abstract	= {The task of code generation aims to generate code
		  solutions based on given programming problems. Recently,
		  code large language models (code LLMs) have shed new light
		  on this task, owing to their formidable code generation
		  capabilities. While these models are powerful, they seldom
		  focus on further improving the accuracy of library-oriented
		  API invocation. Nonetheless, programmers frequently invoke
		  APIs in routine coding tasks. In this paper, we aim to
		  enhance the proficiency of existing code LLMs regarding API
		  invocation by mimicking analogical learning, which is a
		  critical learning strategy for humans to learn through
		  differences among multiple instances. Motivated by this, we
		  propose a simple yet effective approach, namely DiffCoder,
		  which excels in API invocation by effectively training on
		  the differences (diffs) between analogical code exercises.
		  To assess the API invocation capabilities of code LLMs, we
		  conduct experiments on seven existing benchmarks that focus
		  on mono-library API invocation. Additionally, we construct
		  a new benchmark, namely PanNumEval, to evaluate the
		  performance of multi-library API invocation. Extensive
		  experiments on eight benchmarks demonstrate the impressive
		  performance of DiffCoder. Furthermore, we develop a VSCode
		  plugin for DiffCoder, and the results from twelve invited
		  participants further verify the practicality of
		  DiffCoder.},
  journal	= {Proc. ACM Softw. Eng.},
  month		= jul,
  articleno	= {19},
  numpages	= {21},
  keywords	= {Code Generation, Code Library, Instruction Tuning, Large
		  Language Model}
}

@Article{	  10.1109/taslp.2024.3485500,
  author	= {Li, Pengfei and Zhou, Guangyou and Xie, Zhiwen and Xie,
		  Penghui and Huang, Jimmy Xiangji},
  title		= {Learning Dynamic and Static Representations for
		  Extrapolation-Based Temporal Knowledge Graph Reasoning},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3485500},
  doi		= {10.1109/TASLP.2024.3485500},
  abstract	= {Temporal knowledge graph reasoning aims to predict the
		  missing links (facts) in the future timestamps. However,
		  most existing methods have a common limitation: they focus
		  on learning dynamic representations of temporal knowledge
		  graphs and rarely consider static characteristics that
		  remain unchanged over time. To address the above issues, we
		  propose to learn the dynamic and static representations for
		  temporal knowledge graph reasoning (DSTKG), which
		  introduces two latent variables to capture the dynamic and
		  static characteristics of entities in temporal knowledge
		  graphs. First, we use a Bi-GRU-based inference network to
		  learn the static latent representation of historical facts
		  and a nonlinear discrete-time transition-based inference
		  network to learn the dynamic latent representation. Then,
		  we sample the latent variables multiple times using
		  re-parameterization tricks to obtain high-quality
		  embeddings and make predictions in the future timestamps.
		  The empirical results on four benchmark datasets show that
		  our model is more effective than state-of-the-art
		  approaches. Compared with the strong baseline model DBKGE
		  (RotatE), the proposed model achieves performance
		  improvements of 2.69%, &lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$1.59%$&lt;/tex-math&gt;&lt;/inline-formula&gt;,
		  1.18% and 1.22% on Yago11k, Wikidata12k, ICEWS14 and
		  ICEWS05-15 respectively, regarding the evaluation metric
		  MRR.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= oct,
  pages		= {4741–4754},
  numpages	= {14}
}

@InProceedings{	  10.1145/3689218.3689222,
  author	= {Zhao, Jinxiong and Ma, Zhicheng and Zhao, Hong and Zhang,
		  Xun and Liu, Qichuan and Peng, Xinjie and Zhang, Gefei},
  title		= {Power Large Language Model Exploration: Activation,
		  Measurement and Enhancement for Operations and Maintenance
		  Knowledge: Activation, Measurement and Enhancement for
		  Power O&amp;M Knowledge},
  year		= {2024},
  isbn		= {9798400718250},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689218.3689222},
  doi		= {10.1145/3689218.3689222},
  abstract	= {With the rapid advancement of Large Language Models, their
		  applications are gradually transitioning from general to
		  specific domains. However, the application of LLM in the
		  electric power domain is still in its early stages, and few
		  studies have explored power LLM. Currently, there are two
		  main challenges against power LLMs: (1) determining how to
		  measure the real power knowledge capacity of LLMs to
		  facilitate targeted enhancement of specific knowledge. (2)
		  identifying practical enhancement methods to facilitate
		  efficient and feasible power LLM applications in real-world
		  scenarios. In this paper, we ask three insightful questions
		  that address the power knowledge capacity of LLMs and then
		  draw inspiration from Reflexion and CoT to design an
		  Activation, Measurement and Enhancement framework (AME) for
		  power operations and maintenance (O&amp;M) knowledge.
		  Specifically, we ask three “HOW” questions based on the
		  activation, measurement, and enhancement of power O&amp;M
		  knowledge. We introduce a Reflexion Module to discover the
		  knowledge capacity of LLM and a Knowledge Graph Module to
		  provide external knowledge of LLM in our proposed AME.
		  Experiments on the real-world dataset provide strong
		  evidence when we answer the above three insightful
		  questions.},
  booktitle	= {Proceedings of the 2024 6th International Conference on
		  Pattern Recognition and Intelligent Systems},
  pages		= {1–7},
  numpages	= {7},
  keywords	= {Power Large Language Model, Power Operations and
		  Maintenance, Practical Knowledge Graph, Reflexion},
  location	= {Hong Kong, Hong Kong},
  series	= {PRIS '24}
}

@InProceedings{	  10.1145/3589334.3645720,
  author	= {Jiang, Xuhui and Xu, Chengjin and Shen, Yinghan and Wang,
		  Yuanzhuo and Su, Fenglong and Shi, Zhichao and Sun, Fei and
		  Li, Zixuan and Guo, Jian and Shen, Huawei},
  title		= {Toward Practical Entity Alignment Method Design: Insights
		  from New Highly Heterogeneous Knowledge Graph Datasets},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645720},
  doi		= {10.1145/3589334.3645720},
  abstract	= {The flourishing of knowledge graph (KG) applications has
		  driven the need for entity alignment (EA) across KGs.
		  However, the heterogeneity of practical KGs, characterized
		  by differing scales, structures, and limited overlapping
		  entities, greatly surpasses that of existing EA datasets.
		  This discrepancy highlights an oversimplified heterogeneity
		  in current EA datasets, which obstructs the exploration of
		  the EA application. In this paper, we study the performance
		  of EA methods on the alignment of highly heterogeneous KGs
		  (HHKGs). Firstly, we address the oversimplified
		  heterogeneity settings of current datasets and propose two
		  new HHKG datasets that closely mimic practical EA
		  scenarios. Then, based on these datasets, we conduct
		  extensive experiments to evaluate previous representative
		  EA methods. Our findings reveal that, in aligning HHKGs,
		  valuable structure information can hardly be exploited,
		  which leads to inferior performance of existing EA methods,
		  especially those based on GNNs. These findings shed light
		  on the potential problems associated with the conventional
		  application of GNN-based methods as a panacea for all EA
		  datasets. Consequently, to elucidate what EA methodology is
		  genuinely beneficial in practical scenarios, we undertake
		  an in-depth analysis by implementing a simple but effective
		  approach: Simple-HHEA. Our experiment results conclude that
		  the key to the future EA model design in practice lies in
		  their adaptability and efficiency to varying information
		  quality conditions, as well as their capability to capture
		  patterns across HHKGs. The datasets and source code are
		  available at https://github.com/IDEA-FinAI/Simple-HHEA.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2325–2336},
  numpages	= {12},
  keywords	= {entity alignment, graph neural networks, knowledge
		  graphs},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3672758.3672845,
  author	= {Guo, Yingqi and Zhao, Ying and Wang, Bo},
  title		= {A method integrating enhanced hinge loss function for
		  few-shot knowledge graph completion},
  year		= {2024},
  isbn		= {9798400716942},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672758.3672845},
  doi		= {10.1145/3672758.3672845},
  abstract	= {Few-shot knowledge graph completion (FKGC) is a
		  fundamental task to supply missing triples for knowledge
		  graphs. Many recently proposed few-shot relational learning
		  methods exhibit excellent performance. However, they rely
		  solely on the angle-based scoring function that is valid
		  only for some relations. The distance-based scoring
		  function has difficulty adapting to embedding distribution
		  where positive candidates follow a loose distribution
		  embracing the negatives, which is common in the FKGC
		  dataset. Here, we propose a novel approach integrating
		  enhanced hinge loss function for FKGC. It is based on an
		  adaptive Ensemble Learning FrameworK, namely ELFK, which
		  contains two base modules with different types of scoring
		  functions to improve the model's generalization across
		  different relations. Specially, we utilize a modified
		  margin hinge loss function by incorporating an upper bound
		  loss function for the base module with an
		  Euclidean-distance-based scoring function to improve the
		  embedding distribution and reduce the chance that positive
		  candidates are distributed outside the negatives. In
		  addition, two base module are adaptive integrated by a
		  trainable weight learned in an ensemble module. Experiments
		  on the NELL and wiki datasets demonstrate that our approach
		  achieves state-of-the-art performance.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Computer, Artificial Intelligence and Control Engineering},
  pages		= {535–540},
  numpages	= {6},
  location	= {Xi' an, China},
  series	= {CAICE '24}
}

@InProceedings{	  10.1145/3676288.3676289,
  author	= {Jamil, Hasan M and Oduro-Afriyie, Joel},
  title		= {Knowledge Graph Enhancement for Improved Natural Language
		  Health Question Answering using Large Language Models},
  year		= {2024},
  isbn		= {9798400710209},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3676288.3676289},
  doi		= {10.1145/3676288.3676289},
  abstract	= {In this paper we present a method for enhancing Question
		  Answering (QA) systems by iteratively improving Knowledge
		  Graphs (KGs) with a focus on maintaining monotonicity in
		  the enhancement process. We introduce a mathematical
		  framework employing functions τ and ϕ, where τ
		  transforms text T into a KG K, and ϕ generates an answer
		  from T for a given question. We propose that augmenting K
		  with domain-specific information, denoted as ΔK, leads to
		  a more accurate approximation of the expected answer,
		  adhering to the principle that each enhancement either
		  maintains or improves answer quality. This concept is
		  formalized as ϕ− 1(ϕ(T) ∪ ΔK) yielding better
		  results than ϕ− 1(ϕ(T)). The paper elaborates on this
		  process with practical examples, demonstrating how KG
		  enhancements, under the constraints of monotonicity, lead
		  to successive improvements in the Question Answering (QA)
		  system.},
  booktitle	= {Proceedings of the 36th International Conference on
		  Scientific and Statistical Database Management},
  articleno	= {14},
  numpages	= {4},
  keywords	= {graph augmentation, knowledge graphs, monotonic answer
		  improvement, natural language processing},
  location	= {Rennes, France},
  series	= {SSDBM '24}
}

@InProceedings{	  10.1145/3650400.3650405,
  author	= {Wang, Chen and Hua, Min and Song, Jiale and Tang,
		  Xue-song},
  title		= {Knowledge Graphs Enhanced Large Language Model Prompt for
		  Electric Power Question Answering},
  year		= {2024},
  isbn		= {9798400708305},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3650400.3650405},
  doi		= {10.1145/3650400.3650405},
  abstract	= {With the continuous development and digital transformation
		  in the field of electric power, the application of large
		  language models in the electric power industry has become a
		  remarkable trend. The electric power industry is an
		  information-intensive domain involving extensive data
		  processing, predictive analysis, and decision-making.
		  Therefore, the application of large language models in the
		  electric power sector is of great significance. Current
		  large language models such as GPT3.5 and GLM can perform
		  well in tasks such as question answering dialogues.
		  However, these models still face challenges such as answer
		  hallucination and inaccurate responses. This paper proposes
		  a method to enhance question answering in large language
		  models using knowledge graphs, aiming to improve the
		  accuracy and reliability of these models in question
		  answering tasks in the electric power domain.The proposed
		  method first utilizes local electric power data to extract
		  triplets and generate a question answering dataset specific
		  to the electric power domain using a large language model.
		  Then, the relationships of the knowledge graph triplets are
		  incorporated into the question prompt to enhance the
		  quality of the model's answers. Furthermore, we fine-tune
		  the large language model using the expanded question set
		  derived from the triplets as knowledge enhanced data.
		  Subsequently, we conduct experiments on both an electric
		  power question answering dataset and a knowledge graph
		  question answering dataset. The experimental results
		  demonstrate that our method significantly improves various
		  metrics of the large language model in the electric power
		  question answering task. This research provides new
		  insights and approaches to enhance the effectiveness of
		  question answering systems in the electric power domain.
		  Future studies can further explore and optimize this prompt
		  expansion method for application in broader domains and
		  tasks.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {24–29},
  numpages	= {6},
  location	= {Xiamen, China},
  series	= {EITCE '23}
}

@InProceedings{	  10.1145/3696500.3696523,
  author	= {He, Yudong and Tang, Yinqiu and Chen, Tianhong},
  title		= {A Study on Large Language Model-Based Approach for
		  Construction Contract Risk Detection},
  year		= {2024},
  isbn		= {9798400710278},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696500.3696523},
  doi		= {10.1145/3696500.3696523},
  abstract	= {Construction projects typically involve large-scale
		  operations and are subject to complex external conditions,
		  making it essential to safeguard the interests of
		  contractor enterprises through well-crafted contract
		  clauses. However, the current reliance on expert judgment
		  for identifying contract risks presents several challenges,
		  including lengthy processing times, heavy workloads, and
		  inconsistent results. To address these issues, this study
		  introduces a Large Language Model (LLM)-based approach for
		  automating the identification of risks in construction
		  contracts. The proposed method was rigorously validated on
		  26 actual contracts, achieving an average accuracy of 76.7%
		  across four state-of-the-art LLMs. This research advances
		  the application of LLMs in construction contract
		  management, providing practical solutions to existing
		  challenges and setting the stage for further exploration in
		  LLM-driven contract analysis.},
  booktitle	= {Proceedings of the 2024 International Conference on Big
		  Data and Digital Management},
  pages		= {136–141},
  numpages	= {6},
  location	= {Shanghai, China},
  series	= {ICBDDM '24}
}

@InProceedings{	  10.1145/3644116.3644179,
  author	= {Gao, Mingxia and Li, Hao and Chen, Furong},
  title		= {An Entity Prediction Method for Chinese Medical Knowledge
		  Graph via Bert Sentence Embedding and Classification},
  year		= {2024},
  isbn		= {9798400708138},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644116.3644179},
  doi		= {10.1145/3644116.3644179},
  abstract	= {Automatic Knowledge Graph Completion is becoming the main
		  research direction of Knowledge graph construction. Among,
		  the entity prediction method can complete the complement of
		  entities in RDF triples, and is widely used in the
		  generation process of the Knowledge graph. In order to
		  complete the Chinese medical Knowledge graph, this paper
		  proposes an entity prediction method based on BRRT sentence
		  embedding and classification. This method needs three
		  steps, the first step is to introduce a large-scale medical
		  corpus to fine tune the basic BERT model into a BERT Domain
		  model. The second step is obtaining the sentence embedding
		  through the model for candidate triples. The third step is
		  to obtain the top N candidate entity lists according to the
		  ranking of classifier probabilities of all candidate. In
		  order to verify the effectiveness of this method, a series
		  of experiments are conducted on the BIOS. The experimental
		  results show that the optimal accuracy of the entity
		  prediction method in this paper is 20.5%, which is 7.2%
		  higher than that using Word embedding+distance.},
  booktitle	= {Proceedings of the 2023 4th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {376–381},
  numpages	= {6},
  location	= {Chengdu, China},
  series	= {ISAIMS '23}
}

@Article{	  10.1145/3680469,
  author	= {Huang, Qing and Sun, Yanbang and Xing, Zhenchang and Cao,
		  Yuanlong and Chen, Jieshan and Xu, Xiwei and Jin, Huan and
		  Lu, Jiaxing},
  title		= {Let’s Discover More API Relations: A Large Language
		  Model-Based AI Chain for Unsupervised API Relation
		  Inference},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {8},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3680469},
  doi		= {10.1145/3680469},
  abstract	= {APIs have intricate relations that can be described in
		  text and represented as knowledge graphs to aid software
		  engineering tasks. Existing relation extraction methods
		  have limitations, such as limited API text corpus, and are
		  affected by the characteristics of the input text. To
		  address these limitations, we propose utilizing large
		  language models (LLMs) (e.g., GPT-3.5) as a neural
		  knowledge base for API relation inference. This approach
		  leverages the entire Web used to pre-train LLMs as a
		  knowledge base and is insensitive to the context and
		  complexity of input texts. To ensure accurate inference, we
		  design an AI chain consisting of three AI modules: API
		  Fully Qualified Name (FQN) Parser, API Knowledge Extractor,
		  and API Relation Decider. The accuracy of the API FQN
		  Parser and API Relation Decider is 0.81 and 0.83,
		  respectively. Using the generative capacity of the LLM and
		  our approach’s inference capability, we achieve an
		  average F1 value of 0.76 under the three datasets,
		  significantly higher than the state-of-the-art method’s
		  average F1 value of 0.40. Compared to the original CoT and
		  modularized CoT methods, our AI chain design has improved
		  the performance of API relation inference by 71% and 49%,
		  respectively. Meanwhile, the prompt ensembling strategy
		  enhances the performance of our approach by 32%. The API
		  relations inferred by our method can be further organized
		  into structured forms to provide support for other software
		  engineering tasks.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= dec,
  articleno	= {212},
  numpages	= {34},
  keywords	= {API Relation, AI Chain, Knowledge Inference, Large
		  Language Model}
}

@InProceedings{	  10.1145/3589335.3651941,
  author	= {Zhao, Wenting and Deng, Zhongfen and Yadav, Shweta and Yu,
		  Philip S.},
  title		= {Heterogeneous Knowledge Grounding for Medical Question
		  Answering with Retrieval Augmented Large Language Model},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651941},
  doi		= {10.1145/3589335.3651941},
  abstract	= {The Large Language Model (LLM) is renowned for its ability
		  to encode a vast amount of general domain knowledge,
		  enabling it to excel in question-answering, dialogue
		  systems, and summarization tasks. However, the medical
		  domain presents a unique challenge to LLM due to the
		  distribution of medical knowledge, which follows a
		  long-tail pattern. Existing approaches address this
		  challenge by injecting medical knowledge into LLM through
		  single sources such as medical textbooks or medical
		  knowledge bases. However, medical knowledge is distributed
		  across multiple heterogeneous information sources. A
		  medical question-answering system can enhance answer
		  coverage and confidence by considering these diverse
		  knowledge sources together. To bridge this gap, we propose
		  a novel approach called Heterogeneous Knowledge
		  Retrieval-Augmented LLM for medical domain question
		  answering. Our experiments, conducted on the MedQA-USMLE
		  dataset, demonstrate promising performance improvements.
		  These results underscore the importance of harnessing
		  heterogeneous knowledge sources in the medical domain.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1590–1594},
  numpages	= {5},
  keywords	= {healthcare, medical question answering, retrieval
		  augmented language models},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.14778/3654621.3654640,
  author	= {Huo, Nan and Cheng, Reynold and Kao, Ben and Ning, Wentao
		  and Haldar, Nur Al Hasan and Li, Xiaodong and Li, Jinyang
		  and Najafi, Mohammad Matin and Li, Tian and Qu, Ge},
  title		= {ZeroEA: A Zero-Training Entity Alignment Framework via
		  Pre-Trained Language Model},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {7},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3654621.3654640},
  doi		= {10.14778/3654621.3654640},
  abstract	= {Entity alignment (EA), a crucial task in knowledge graph
		  (KG) research, aims to identify equivalent entities across
		  different KGs to support downstream tasks like KG
		  integration, text-to-SQL, and question-answering systems.
		  Given rich semantic information within KGs, pre-trained
		  language models (PLMs) have shown promise in EA tasks due
		  to their exceptional context-aware encoding capabilities.
		  However, the current solutions based on PLMs encounter
		  obstacles such as the need for extensive training,
		  expensive data annotation, and inadequate incorporation of
		  structural information. In this study, we introduce a novel
		  zero-training EA framework, ZeroEA, which effectively
		  captures both semantic and structural information for PLMs.
		  To be specific, Graph2Prompt module serves as the bridge
		  between graph structure and plain text by converting KG
		  topology into textual context suitable for PLM input.
		  Additionally, in order to provide PLMs with concise and
		  clear input text of reasonable length, we design a
		  motif-based neighborhood filter to eliminate noisy
		  neighbors. The comprehensive experiments and analyses on 5
		  benchmark datasets demonstrate the effectiveness of ZeroEA,
		  outperforming all leading competitors and achieving
		  state-of-the-art performance in entity alignment. Notably,
		  our study highlights the considerable potential of EA
		  technique in improving the performance of downstream tasks,
		  thereby benefitting the broader research field.},
  journal	= {Proc. VLDB Endow.},
  month		= mar,
  pages		= {1765–1774},
  numpages	= {10}
}

@Article{	  10.1109/taslp.2024.3375631,
  author	= {Zhang, Geng and Liu, Jin and Zhou, Guangyou and Zhao,
		  Kunsong and Xie, Zhiwen and Huang, Bo},
  title		= {Question-Directed Reasoning With Relation-Aware Graph
		  Attention Network for Complex Question Answering Over
		  Knowledge Graph},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3375631},
  doi		= {10.1109/TASLP.2024.3375631},
  abstract	= {Complex knowledge graph question answering (KGQA) aims at
		  answering natural language questions by entities retrieving
		  from a knowledge graph (KG). Recently, the relation
		  path-based models have shown the unique advantage for
		  complex KGQA. However, these existing models ignore the
		  dependency between different relation paths, which leads to
		  aimless reasoning over the KG. To resolve this issue, we
		  propose the question-directed reasoning with relation-aware
		  graph attention network (QRGAT) that encodes the reasoning
		  process as a reasoning graph. The relation-aware GAT can
		  recognize neighbor entities along with the corresponding
		  relations for each entity. With the relation-aware GAT
		  stacked in multiple layers, it can collaboratively capture
		  the dependency of different relation paths for each entity.
		  The question-directed reasoning utilizes the information
		  learned by the relation-aware GAT to solve the aimless
		  reasoning on the KG by constructing a reasoning graph.
		  Extensive experiments demonstrate that our QRGAT
		  outperforms the baseline models on both popular datasets
		  WebQuestionsSP and ComplexWebQuestions. Compared with the
		  strong GNN-based baseline
		  NSM&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$_{+h}$&lt;/tex-math&gt;&lt;/inline-formula&gt;,
		  our QRGAT achieves the performance improvements of 2.3% on
		  WebQuestionsSP and 3.6% on ComplexWebQuestions by the
		  metric Hits@1.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {1915–1927},
  numpages	= {13}
}

@InProceedings{	  10.1145/3627673.3679963,
  author	= {Zhang, Feng and Chen, Wei and Ding, Fei and Wang, Tengjiao
		  and Lu, Dawei and Zheng, Jiabin},
  title		= {Meta-Prompt Tuning Vision-Language Model for Multi-Label
		  Few-Shot Image Recognition},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679963},
  doi		= {10.1145/3627673.3679963},
  abstract	= {Multi-label few-shot image recognition aims to identify
		  multiple unseen objects using only a handful of examples.
		  Recent methods typically tune pre-trained vision-language
		  models with shared or class-specific prompts. However, they
		  still have drawbacks. Tuning a shared prompt is
		  insufficient for all samples especially when the tasks are
		  complex and tuning specific prompts for each class is
		  inevitable to lose generalization ability, thus failing to
		  capture diverse visual knowledge. To address these issues,
		  we propose to meta-tune a generalized prompt pool, enabling
		  each prompt to act as an expert for multi-label few-shot
		  image recognition. Specifically, we first construct a
		  diverse prompt pool to handle complex samples and tasks
		  effectively. Then, the meta-tuning strategy is designed to
		  learn meta-knowledge and transfer it from source tasks to
		  target tasks, enhancing the generalization of prompts.
		  Extensive experimental results on two widely used
		  multi-label image recognition datasets demonstrate the
		  effectiveness of our method.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4258–4262},
  numpages	= {5},
  keywords	= {few-shot learning, meta-prompt learning, multi-label image
		  recognition},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3638584.3638635,
  author	= {Zhou, Yifan and Ding, Yizhou and Dong, Yuwu and He, Hao},
  title		= {Ontology-Semantic Alignment On Contrastive Video-Language
		  Model for Multimodel Video Retrieval Task},
  year		= {2024},
  isbn		= {9798400708688},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638584.3638635},
  doi		= {10.1145/3638584.3638635},
  abstract	= {Contrastive Learning-based models have shown impressive
		  performance in text-image retrieval tasks. However, when
		  applied in video retrieval, traditional contrastive
		  learning strategies have faced challenges in achieving
		  satisfactory results due to redundancy of video contents.
		  We discern several potential reasons: (1)Current
		  methodologies sometimes overlook the significant
		  information imbalance between videos and query text,
		  specifically neglecting the in-depth textual representation
		  of the content within the videos. (2) Current video
		  matching methodologies typically focus on cross-model
		  alignment at general entity similarity level, without
		  specific consideration for how entity pair preferences and
		  similarity properties affect the task at hand. (3) Previous
		  vectorized retrieval based on video content features have
		  been somewhat flawed. They primarily focused on aligning
		  overall features without having an video content tags
		  feature for meaningful feature discrimination. Considering
		  the shortcomings identified in the mentioned three aspects,
		  we propose an ontology semantic labels augments retrieval
		  model and introduce a method to integrate video ontology
		  semantic labels into the contrastive learning framework. In
		  particular, we have developed ontology semantic
		  descriptions about entities encompassing both human figures
		  and textual elements within the videos. Subsequently, we
		  conducted training and testing on the CMIVQA dataset to
		  assess the performance of our approach. The experimental
		  results show that employing fine-grained ontology labels as
		  sample pairs for contrastive learning leads to an increased
		  level of precision in video retrieval tasks.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {408–413},
  numpages	= {6},
  keywords	= {Multimodal alignment, Ontology description, Video content
		  understanding},
  location	= {Beijing, China},
  series	= {CSAI '23}
}

@Article{	  10.1145/3654987,
  author	= {Bobed Lisbona, Carlos and Bernad, Jordi and Maillot,
		  Pierre},
  title		= {Language-Model Based Informed Partition of Databases to
		  Speed Up Pattern Mining},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {3},
  url		= {https://doi.org/10.1145/3654987},
  doi		= {10.1145/3654987},
  abstract	= {Extracting interesting patterns from data is the main
		  objective of Data Mining. In this context, Frequent Itemset
		  Mining has shown its usefulness in providing insights from
		  transactional databases, which, in turn, can be used to
		  gain insights about the structure of Knowledge Graphs.
		  While there have been a lot of advances in the field, due
		  to the NP-hard nature of the problem, the main approaches
		  still struggle when they are faced with large databases
		  with large and sparse vocabularies, such as the ones
		  obtained from graph propositionalizations. There have been
		  efforts to propose parallel algorithms, but, so far, the
		  goal has not been to tackle this source of complexity
		  (i.e., vocabulary size), thus, in this paper, we propose to
		  parallelize frequent itemset mining algorithms by
		  partitioning the database horizontally (i.e.,
		  transaction-wise) while not neglecting all the possible
		  vertical information (i.e., item-wise). Instead of relying
		  on pure item co-appearance metrics, we advocate for the
		  adoption of a different approach: modeling databases as
		  documents, where each transaction is a sentence, and each
		  item a word. In this way, we can apply recent language
		  modeling techniques (i.e., word embeddings) to obtain a
		  continuous representation of the database, clusterize it in
		  different partitions, and apply any mining algorithm to
		  them. We show how our proposal leads to informed partitions
		  with a reduced vocabulary size and a reduced entropy (i.e.,
		  disorder). This enhances the scalability, allowing us to
		  speed up mining even in very large databases with sparse
		  vocabularies. We have carried out a thorough experimental
		  evaluation over both synthetic and real datasets showing
		  the benefits of our proposal.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {184},
  numpages	= {27},
  keywords	= {knowledge graphs, language models, pattern mining}
}

@InProceedings{	  10.1145/3650400.3650478,
  author	= {Zhao, Wei and Chen, Qinghui and You, Junling},
  title		= {LlmRe: A zero-shot entity relation extraction method based
		  on the large language model},
  year		= {2024},
  isbn		= {9798400708305},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3650400.3650478},
  doi		= {10.1145/3650400.3650478},
  abstract	= {Entity relation extraction aims to extract knowledge
		  triples from unstructured or semi-structured text data and
		  can be applied to various fields, including medicine,
		  finance knowledge graph construction and intelligent
		  question-answering. Traditional entity relation extraction
		  requires a large amount of labeled data, consumes a lot of
		  labor and time, and the trained model lacks generalization
		  ability, which is difficult to migrate to other fields.
		  Zero-shot entity relation extraction relieves the
		  dependence on labeled data in traditional method. Based on
		  unlabeled text data, zero-shot entity relation extraction
		  has strong domain adaptability, which is a very challenging
		  and practical task. Recent work on large language models
		  shows that large models can effectively complete downstream
		  tasks through natural language instructions and have good
		  generalization ability. Inspired by this, we explore the
		  use of large models for information extraction. Due to the
		  randomness of large language model generation, we introduce
		  in-context learning in entity relation extraction task to
		  guide large language model to output data in a specified
		  format to help obtain structured data. At the same time, we
		  propose a three-stage extraction framework for decomposing
		  entity relation extraction tasks, and each stage is
		  conducted in the form of question and answer to reduce the
		  complexity of extraction. We evaluated the knowledge
		  triples extraction performance of the model on three
		  self-built test datasets in different fields, and the
		  experimental result showed that our proposed method
		  achieved impressive performance in the zero-shot entity
		  relation extraction task, surpassing the comparison model
		  on multiple metrics, proving the effectiveness and domain
		  adaptability of the proposed method.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Electronic Information Technology and Computer
		  Engineering},
  pages		= {475–480},
  numpages	= {6},
  location	= {Xiamen, China},
  series	= {EITCE '23}
}

@InProceedings{	  10.1145/3661304.3661901,
  author	= {Sequeda, Juan and Allemang, Dean and Jacob, Bryon},
  title		= {A Benchmark to Understand the Role of Knowledge Graphs on
		  Large Language Model's Accuracy for Question Answering on
		  Enterprise SQL Databases},
  year		= {2024},
  isbn		= {9798400706530},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3661304.3661901},
  doi		= {10.1145/3661304.3661901},
  abstract	= {Enterprise applications of Large Language Models (LLMs)
		  hold promise for question answering on enterprise SQL
		  databases. However, the extent to which LLMs can accurately
		  respond to enterprise questions in such databases remains
		  unclear, given the absence of suitable Text-to-SQL
		  benchmarks tailored to enterprise settings. Additionally,
		  the potential of Knowledge Graphs (KGs) to enhance
		  LLM-based question answering by providing business context
		  is not well understood. This study aims to evaluate the
		  accuracy of LLM-powered question answering systems in the
		  context of enterprise questions and SQL databases, while
		  also exploring the role of knowledge graphs in improving
		  accuracy. To achieve this, we introduce a benchmark
		  comprising an enterprise SQL schema in the insurance
		  domain, a range of enterprise queries encompassing
		  reporting to metrics, and a contextual layer incorporating
		  an ontology and mappings that define a knowledge graph. Our
		  primary finding reveals that question answering using
		  GPT-4, with zero-shot prompts directly on SQL databases,
		  achieves an accuracy of 16%. Notably, this accuracy
		  increases to 54% when questions are posed over a Knowledge
		  Graph representation of the enterprise SQL database.
		  Therefore, investing in Knowledge Graph provides higher
		  accuracy for LLM powered question answering systems.},
  booktitle	= {Proceedings of the 7th Joint Workshop on Graph Data
		  Management Experiences &amp; Systems (GRADES) and Network
		  Data Analytics (NDA)},
  articleno	= {5},
  numpages	= {12},
  location	= {Santiago, AA, Chile},
  series	= {GRADES-NDA '24}
}

@InProceedings{	  10.1145/3696500.3696588,
  author	= {Yu, Miao and Feng, Chenying and Xu, Xiaodong and Tang,
		  Runheng and Shi, Shengwei},
  title		= {Research on text relation extraction of power
		  administrative duty based on improved pre-trained language
		  model},
  year		= {2024},
  isbn		= {9798400710278},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3696500.3696588},
  doi		= {10.1145/3696500.3696588},
  abstract	= {Strategies for efficiently and accurately extracting
		  relationships from power system administrative shift texts
		  are explored using an optimized pre-trained language model,
		  such as BERT (Bidirectional Encoder Representations from
		  Transformers). Tailored improvements to the model address
		  the nuanced and variable nature of power texts, including
		  the integration of domain-specific pre-training corpora and
		  architectural enhancements. The model, enhanced with the
		  R-BERT (Relation-Bidirectional Encoder Representations from
		  Transformers) algorithm, demonstrates advanced proficiency
		  in identifying entity positions and yields exceptional
		  results in experimental evaluations, particularly achieving
		  a high F1 score for the identification of cooperative
		  relationships. The findings indicate that this innovation
		  can significantly enhance the level of intelligence in
		  power system administrative shift management. Subsequent
		  developments may involve the incorporation of additional
		  expert knowledge to further refine the model.},
  booktitle	= {Proceedings of the 2024 International Conference on Big
		  Data and Digital Management},
  pages		= {528–534},
  numpages	= {7},
  location	= {Shanghai, China},
  series	= {ICBDDM '24}
}

@Article{	  10.1145/3652028,
  author	= {Spinner, Thilo and Kehlbeck, Rebecca and Sevastjanova,
		  Rita and St\"{a}hle, Tobias and Keim, Daniel A. and
		  Deussen, Oliver and El-Assady, Mennatallah},
  title		= {-generAItor: Tree-in-the-loop Text Generation for Language
		  Model Explainability and Adaptation},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {14},
  number	= {2},
  issn		= {2160-6455},
  url		= {https://doi.org/10.1145/3652028},
  doi		= {10.1145/3652028},
  abstract	= {Large language models (LLMs) are widely deployed in
		  various downstream tasks, e.g., auto-completion, aided
		  writing, or chat-based text generation. However, the
		  considered output candidates of the underlying search
		  algorithm are under-explored and under-explained. We tackle
		  this shortcoming by proposing a tree-in-the-loop approach,
		  where a visual representation of the beam search tree is
		  the central component for analyzing, explaining, and
		  adapting the generated outputs. To support these tasks, we
		  present generAItor, a visual analytics technique,
		  augmenting the central beam search tree with various
		  task-specific widgets, providing targeted visualizations
		  and interaction possibilities. Our approach allows
		  interactions on multiple levels and offers an iterative
		  pipeline that encompasses generating, exploring, and
		  comparing output candidates, as well as fine-tuning the
		  model based on adapted data. Our case study shows that our
		  tool generates new insights in gender bias analysis beyond
		  state-of-the-art template-based methods. Additionally, we
		  demonstrate the applicability of our approach in a
		  qualitative user study. Finally, we quantitatively evaluate
		  the adaptability of the model to few samples, as occurring
		  in text-generation use cases.},
  journal	= {ACM Trans. Interact. Intell. Syst.},
  month		= jun,
  articleno	= {14},
  numpages	= {32},
  keywords	= {Large language models, beam search tree, natural language
		  generation, explainability, language transformers, visual
		  analytics}
}

@InProceedings{	  10.1145/3653081.3653131,
  author	= {Na, Qionglan and Li, Xin and Wang, Yifei and Li, Jing and
		  Yang, Yixi and Zhang, Haiming},
  title		= {A Pre-training Method Inspired by Large Language Model for
		  Power Named Entity Recognition},
  year		= {2024},
  isbn		= {9798400716485},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3653081.3653131},
  doi		= {10.1145/3653081.3653131},
  abstract	= {In recent years, the field of natural language processing
		  has witnessed remarkable advancements due to the success of
		  large language models. These models leverage the
		  Transformer architecture and pre-training techniques to
		  achieve impressive results. In this paper, we draw
		  inspiration from large language models and apply these
		  techniques into the task of named entity recognition in the
		  domain of power grids, which is critical for building power
		  grid knowledge graphs and question-answering systems.
		  Specifically, we propose a BERT-CNN-BIGRU-CRF deep learning
		  model for named entity recognition. This model effectively
		  harnesses the semantic modeling capabilities and
		  pre-training knowledge of BERT, which is based on the
		  Transformer architecture. By incorporating CNN and BIGRU,
		  the model captures and models both local and global
		  features, respectively. The CRF layer is employed for label
		  classification. This combination of components ensures a
		  high level of recognition accuracy. To evaluate the
		  performance of the proposed model, we train our model on
		  annotated maintenance plan data. We compare its results
		  with those of other commonly used models. The evaluation
		  metrics include recall, precision, and F1 score, which are
		  widely employed in named entity recognition tasks. Our
		  proposed model achieves optimal performance across all
		  three metrics, demonstrating its superiority over other
		  models.},
  booktitle	= {Proceedings of the 2023 5th International Conference on
		  Internet of Things, Automation and Artificial
		  Intelligence},
  pages		= {308–312},
  numpages	= {5},
  location	= {Nanchang, China},
  series	= {IoTAAI '23}
}

@InProceedings{	  10.1145/3644116.3644294,
  author	= {Zhu, Jinyang and Gong, Qingyue and Zhou, Chunfang and
		  Luan, Huidan},
  title		= {ZhongJing: A Locally Deployed Large Language Model for
		  Traditional Chinese Medicine and Corresponding Evaluation
		  Methodology: A Large Language Model for data fine-tuning in
		  the field of Traditional Chinese Medicine, and a new
		  evaluation method called TCMEval are proposed},
  year		= {2024},
  isbn		= {9798400708138},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644116.3644294},
  doi		= {10.1145/3644116.3644294},
  abstract	= {The success of ChatGPT has showcased the potential
		  applications of Large Language Models (LLMs) in the field
		  of Traditional Chinese Medicine (TCM), encompassing areas
		  such as medical diagnosis, adjunctive therapy, and TCM
		  talent cultivation. However, the current challenges,
		  including hardware constraints, insufficient model domain
		  knowledge, and difficulties in domain-specific evaluation,
		  have constrained the fusion of LLMs with TCM. In an attempt
		  to address these issues, this paper introduces ZhongJing, a
		  domain-specific LLM fine-tuned within the domain of TCM,
		  capable of generating responses at a rate of 8 tokens per
		  second, smoothly operating on local personal computers. To
		  assess the model's domain expertise, this paper introduces
		  the TCMEval evaluation method, designed concerning medical
		  students' exams. Experimental results demonstrate that
		  ZhongJing achieves a 6.49 TCMEval Score improvement over
		  Chinese-LLaMA2 in the field of TCM, indicating the model's
		  ability to generate more specialized responses compared to
		  baseline models.},
  booktitle	= {Proceedings of the 2023 4th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {1036–1042},
  numpages	= {7},
  location	= {Chengdu, China},
  series	= {ISAIMS '23}
}

@InProceedings{	  10.1145/3670474.3685974,
  author	= {Wang, Li-C.},
  title		= {LLM-Assisted Analytics in Semiconductor Test (Invited)},
  year		= {2024},
  isbn		= {9798400706998},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670474.3685974},
  doi		= {10.1145/3670474.3685974},
  abstract	= {The emergence of Large Language Models (LLMs) has impacted
		  our perspective on applying Machine Learning (ML) in
		  semiconductor test. This paper shares our experience in
		  leveraging the power of LLMs to build an AI agent for test
		  data analytics. We advocate for an end-to-end approach
		  where the Knowledge Graph (KG) plays a central role. Using
		  wafermap analytics as an example, we highlight the key
		  ideas behind developing the LLM-assisted AI agent named
		  IEA-Plot, and discuss its practical applications.},
  booktitle	= {Proceedings of the 2024 ACM/IEEE International Symposium
		  on Machine Learning for CAD},
  articleno	= {38},
  numpages	= {7},
  keywords	= {Knowledge Graph, Large Language Model, Machine Learning,
		  Test Data Analytics},
  location	= {Salt Lake City, UT, USA},
  series	= {MLCAD '24}
}

@Article{	  10.1145/3686970,
  author	= {Cheng, Zirui and Xu, Jingfei and Jin, Haojian},
  title		= {TreeQuestion: Assessing Conceptual Learning Outcomes with
		  LLM-Generated Multiple-Choice Questions},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW2},
  url		= {https://doi.org/10.1145/3686970},
  doi		= {10.1145/3686970},
  abstract	= {The advances of generative AI have posed a challenge for
		  using open-ended questions to assess conceptual learning
		  outcomes, as it is increasingly common for students to use
		  tools like ChatGPT to generate long textual answers.
		  However, teachers still have to spend substantial time
		  reading the answers and inferring students' learning
		  outcomes. We present TreeQuestion, a human-in-the-loop
		  system designed to help teachers create a set of
		  multiple-choice questions to assess students' conceptual
		  learning outcomes. When a teacher seeks to assess students'
		  comprehension of specific concepts, TreeQuestion taps into
		  the wealth of knowledge embedded within large language
		  models and generates a set of multiple-choice questions
		  organized in a tree-like structure. We evaluated
		  TreeQuestion with 96 students and 10 teachers. Results
		  indicated that students achieved similar performance in
		  multiple-choice questions generated by TreeQuestion and
		  open-ended questions graded by teachers. Meanwhile,
		  TreeQuestion could reduce teachers' efforts in creating and
		  grading the multiple-choice questions in contrast to
		  manually generated open-ended questions. We estimate that
		  in a hypothetical class with 20 students, using
		  multiple-choice questions from TreeQuestion may require
		  only 4.6% of the time compared to open-ended questions for
		  assessing learning outcomes.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= nov,
  articleno	= {431},
  numpages	= {29},
  keywords	= {education, generative AI, large language models,
		  multiple-choice questions, open-ended questions}
}

@InProceedings{	  10.1145/3678890.3678906,
  author	= {Ma, Hualong and Lv, Peizhuo and Chen, Kai and Zhou,
		  Jiachen},
  title		= {KGDist: A Prompt-Based Distillation Attack against LMs
		  Augmented with Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400709593},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3678890.3678906},
  doi		= {10.1145/3678890.3678906},
  abstract	= {With Knowledge Graph (KG) increasingly applied in various
		  fields, the integration of KG has gained significant
		  attention to augment the knowledge-specific task
		  capabilities of language models (LMs). However,
		  constructing and maintaining large KGs, much like LMs, can
		  be expensive and challenging, often requiring extensive
		  domain knowledge and human resources. This makes KG a
		  valuable resource potentially vulnerable to theft threats
		  from attackers. In this paper, we present KGDist, the first
		  prompt-based KG distillation technique for extracting KG
		  knowledge from KG+LM augmented models. Through iterations
		  of prompt-based queries, we can steal a substitute KG
		  containing task domain knowledge from the original KG.
		  First of all, we initialize entities from a small scale
		  task-specific corpus. Then, we construct specific task
		  prompts for querying the victim LMs. According to the model
		  outputs, we iteratively select entities showing strong
		  correlation and reconstruct the relation edges for
		  subsequent prompt crafting. We also propose a
		  multi-granularity prompt construction method for reducing
		  the querying cost. After acquiring the extracted KG, we
		  launch a relation type-based pruning to cut off redundant
		  edges forming cycles decreasing the performance of
		  distilled KGs. We evaluate the effectiveness of KGDist
		  &nbsp;on five benchmark KG+LM models designed for various
		  tasks. Results demonstrate that our attack successfully
		  extracts the distilled KGs with minimal performance
		  degradation (under 2.4%) applied on LMs and less storage
		  space. And also, the mechanism we apply greatly saves API
		  queries compared to brute force method. In addition,
		  further experiments demonstrate that we can split the KG
		  knowledge from the LM noises effectively, and the distilled
		  KGs have similar properties in knowledge distribution and
		  graph structures to the original ones. Our code is
		  available at https://github.com/Haro-M/KGDist.},
  booktitle	= {Proceedings of the 27th International Symposium on
		  Research in Attacks, Intrusions and Defenses},
  pages		= {480–495},
  numpages	= {16},
  keywords	= {knowledge distillation, knowledge graph, language model},
  location	= {Padua, Italy},
  series	= {RAID '24}
}

@InProceedings{	  10.1145/3703187.3703192,
  author	= {Li, Chengxue and Chen, Xuyang and Ding, Min and Jin, Wei
		  and Gao, Feng},
  title		= {Research on Chinese Knowledge Base and Knowledge Q&amp;A
		  Technology for Power Grid Dispatching},
  year		= {2024},
  isbn		= {9798400707254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703187.3703192},
  doi		= {10.1145/3703187.3703192},
  abstract	= {To support online professional knowledge query for power
		  grid dispatchers, this article proposes a method for
		  constructing a knowledge base based on knowledge graph,
		  which implements systematic organization and management of
		  knowledge resources in the field of power-grid dispatching.
		  Moreover, a questions and answers (Q&amp;A) service is
		  design based on large language model and proposed knowledge
		  base. Based on the constructed knowledge base and Q&amp;A
		  service, auxiliary learning functions can be provided in
		  the domain of power grid operation. This enables accurate
		  acquisition of professional knowledge through Chinese
		  natural language interaction, enhancing the effectiveness
		  and flexibility of online training for power-grid
		  dispatchers.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Computer Information Science and Artificial Intelligence},
  pages		= {18–23},
  numpages	= {6},
  keywords	= {Graph database, Knowledge graph, Large language model,
		  Question answering},
  location	= { },
  series	= {CISAI '24}
}

@Article{	  10.1145/3690391,
  author	= {Liu, Jiangfeng and Ma, Xueliang and Wang, Lanyu and Pei,
		  Lei},
  title		= {How Can Generative Artificial Intelligence Techniques
		  Facilitate Intelligent Research into Ancient Books?},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3690391},
  doi		= {10.1145/3690391},
  abstract	= {Generative AI changes the paradigm of natural language
		  processing research, sets off a new trend of research in
		  computational humanities and computational social sciences,
		  and provides unique perspectives on digital
		  intelligence-enabled ancient book revitalization and
		  intelligent applications. The article explores the role of
		  multimodal large models in image processing and OCR of
		  ancient books. We discuss and exemplify how to use Large
		  Language Models for intelligent information processing of
		  ancient texts and explore combining prompt engineering,
		  retrieval augmented generation (RAG), supervised
		  fine-tuning, LangChain, and other techniques to improve
		  performance in ancient text mining and applications. This
		  article also looks forward to the broad prospect of
		  intelligent agent technology combined with the Large
		  Language Model in the innovative application of ancient
		  book revitalization. The research focuses on digitizing
		  ancient books, intelligent processing of ancient texts, and
		  intelligent application of ancient book revitalization. It
		  demonstrates the feasibility, advancement, and creativity
		  of the application of generative AI and its derivative
		  technologies in the field of computational humanities,
		  especially in the field of ancient book preservation, to
		  provide intelligent solutions for the dissemination of
		  traditional thought and culture, from the perspective of
		  the whole process of the technology of digital humanities
		  and computational humanities research. The article also
		  gives examples of the intelligent application of AI in the
		  restoration of ancient books and the annotation of ancient
		  texts. Although Large Language Models demonstrate
		  transformative potential in advancing the field of ancient
		  text research toward intelligent analysis, there remain
		  certain limitations. This article points out their
		  shortcomings in areas such as knowledge completion for
		  ancient texts, understanding emotions and cultural nuances,
		  as well as ethical and accountability issues. It emphasizes
		  the need for a more balanced perspective on the role that
		  generative AI plays in the exploration and utilization of
		  cultural heritage.},
  journal	= {J. Comput. Cult. Herit.},
  month		= dec,
  articleno	= {57},
  numpages	= {20},
  keywords	= {Computational Humanities, Ancient Book Revitalization,
		  Intelligent Information Processing of Ancient Texts,
		  ChatGPT, Generative AI, AIGC}
}

@InProceedings{	  10.1145/3627673.3679231,
  author	= {Afreen, Neda and Balloccu, Giacomo and Boratto, Ludovico
		  and Fenu, Gianni and Malloci, Francesca Maridina and
		  Marras, Mirko and Martis, Andrea Giovanni},
  title		= {EDGE: A Conversational Interface driven by Large Language
		  Models for Educational Knowledge Graphs Exploration},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679231},
  doi		= {10.1145/3627673.3679231},
  abstract	= {As education adopts digital platforms, the vast amount of
		  information from various sources, such as learning
		  management systems and learning object repositories,
		  presents challenges in navigation and elaboration.
		  Traditional interfaces involve a steep learning curve,
		  limited user accessibility, and lack flexibility. Language
		  models alone cannot address these issues as they do not
		  have access to structured information specific to the
		  educational organization. In this paper, we propose EDGE
		  (EDucational knowledge Graph Explorer), a natural language
		  interface that uses knowledge graphs to organize
		  educational information. EDGE translates natural language
		  requests into queries and converts the results back into
		  natural language responses. We show EDGE's versatility
		  using knowledge graphs built from public datasets,
		  providing example interactions of different stakeholders.
		  Demo video: https://u.garr.it/eYq63.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5159–5163},
  numpages	= {5},
  keywords	= {conversational interface, graph database, information
		  retrieval, knowledge graph, language model, learning
		  management},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3626246.3655999,
  author	= {Dong, Xin Luna},
  title		= {The Journey to a Knowledgeable Assistant with
		  Retrieval-Augmented Generation (RAG)},
  year		= {2024},
  isbn		= {9798400704222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626246.3655999},
  doi		= {10.1145/3626246.3655999},
  abstract	= {For decades, multiple communities (Database, Information
		  Retrieval, Natural Language Processing, Data Mining, AI)
		  have pursued the mission of providing the right information
		  at the right time. Efforts span web search, data
		  integration, knowledge graphs, question answering. Recent
		  advancements in Large Language Models (LLMs) have
		  demonstrated remarkable capabilities in comprehending and
		  generating human language, revolutionizing techniques in
		  every front. However, their inherent limitations such as
		  factual inaccuracies and hallucinations make LLMs less
		  suitable for creating knowledgeable and trustworthy
		  assistants.This talk describes our journey in building a
		  knowledgeable AI assistant by harnessing LLM techniques. We
		  start with our findings from a comprehensive set of
		  experiments to assess LLM reliability in answering factual
		  questions and analyze performance variations across
		  different knowledge types. Next, we describe our federated
		  Retrieval-Augmented Generation (RAG) system that integrates
		  external information from both the web and knowledge graphs
		  for trustworthy text generation on real-time topics like
		  stocks and sports, as well as on torso-to-tail entities
		  like local restaurants. Additionally, we brief our
		  explorations on extending our techniques towards
		  multi-modal, contextualized, and personalized Q&amp;A. We
		  will share our techniques, our findings, and the path
		  forward, high- lighting how we are leveraging and advancing
		  the decades of work in this area.},
  booktitle	= {Companion of the 2024 International Conference on
		  Management of Data},
  pages		= {3},
  numpages	= {1},
  keywords	= {RAG (retrieval augmented generation), data integration,
		  generative AI, knowledge graph, question answering},
  location	= {Santiago AA, Chile},
  series	= {SIGMOD/PODS '24}
}

@InProceedings{	  10.1145/3626246.3653398,
  author	= {Yu, Changlong and Liu, Xin and Maia, Jefferson and Li,
		  Yang and Cao, Tianyu and Gao, Yifan and Song, Yangqiu and
		  Goutam, Rahul and Zhang, Haiyang and Yin, Bing and Li,
		  Zheng},
  title		= {COSMO: A Large-Scale E-commerce Common Sense Knowledge
		  Generation and Serving System at Amazon},
  year		= {2024},
  isbn		= {9798400704222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626246.3653398},
  doi		= {10.1145/3626246.3653398},
  abstract	= {Applications of large-scale knowledge graphs in the
		  e-commerce platforms can improve shopping experience for
		  their customers. While existing e-commerce knowledge graphs
		  (KGs) integrate a large volume of concepts or product
		  attributes, they fail to discover user intentions, leaving
		  the gap with how people think, behave, and interact with
		  the surrounding world. In this work, we present COSMO, a
		  scalable system to mine user-centric commonsense knowledge
		  from massive behaviors and construct industry-scale
		  knowledge graphs to empower diverse online services. In
		  particular, we describe a pipeline for collecting
		  high-quality seed knowledge assertions that are distilled
		  from large language models (LLMs) and further refined by
		  critic classifiers trained over human-in-the-loop annotated
		  data.Since those generations may not always align with
		  human preferences and contain noises, we then describe how
		  we adopt instruction tuning to finetune an efficient
		  language model~(COSMO-LM) for faithful e-commerce
		  commonsense knowledge generation at scale. COSMO-LM
		  effectively expands our knowledge graph to 18 major
		  categories at Amazon, producing millions of high-quality
		  knowledge with only 30k annotated instructions. Finally
		  COSMO has been deployed in Amazon search applications such
		  as search navigation. Both offline and online A/B
		  experiments demonstrate our proposed system achieves
		  significant improvement. Furthermore, these experiments
		  highlight the immense potential of commonsense knowledge
		  extracted from instruction-finetuned large language
		  models.},
  booktitle	= {Companion of the 2024 International Conference on
		  Management of Data},
  pages		= {148–160},
  numpages	= {13},
  keywords	= {commonsense knowledge, knowledge graph, large language
		  model},
  location	= {Santiago AA, Chile},
  series	= {SIGMOD/PODS '24}
}

@InProceedings{	  10.1145/3626772.3657656,
  author	= {Rollings, Nathaniel},
  title		= {Mosaicing Prevention in Declassification},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657656},
  doi		= {10.1145/3626772.3657656},
  abstract	= {Multiple methods can be used to infer as-yet unrecorded
		  information. However, this ability can place
		  confidentiality at risk when some inferences, although
		  correct, could cause harm. We therefore flip the problem,
		  seeking not to enable but to prevent specific inferences.
		  This inference prevention task is motivated by what has
		  been called the "mosaicing'' problem in declassification
		  review for documents that in the past were withheld from
		  public access for national security
		  reasons~citepozen2005mosaic. The goal of such a review is
		  to reveal as much as can now be safely revealed but to also
		  withhold things that could be used to infer facts that
		  require continued protection. This problem is modeled using
		  three primary components: (1) currently public information,
		  (2) a set of secrets (information that is not public and
		  requires continuing protection), and (3) a review set
		  (other information now being reviewed for possible
		  release). The inference prevention task is to determine
		  what in the review set would substantially increase the
		  inference rick for a secret.Our initial work investigated
		  use of knowledge graphs for keeping secrets using Knowledge
		  Graph Completion (KGC) techniques. While declassification
		  is typically text-based, we expect a structured analog to
		  that problem can provide some useful insights. There also
		  are applications where prevention of inference in a
		  knowledge graph is the actual task, such as protecting
		  against specific drug discovery inferences when augmenting
		  the Hetionet knowledge graph. Our mosaicing problem is the
		  inverse of KGC---rather than inferring a link, we need to
		  prevent inference of a link. This challenge is distinct
		  from anonymization for social media graphs because we can't
		  alter most relationships, only those in the review set.
		  Using the FB15K-237 knowledge graph, we analyzed three KGC
		  models to identify the relation in a defined review set
		  most critical to inference of a missing secret relation
		  (thus "nominating" a relation for redaction). We evaluated
		  the impact of redactions nominated by one model on
		  inference by other models by ranking a secret with some
		  selected confounds, finding that our simplest model (RuleN)
		  produced the best nominations, despite being least
		  effective of the three on the KGC task. Future work will
		  use graphs more closely modeling declassification, and
		  other KGC models. It will also explore areas in which
		  differences between the traditional KGC task and the
		  declassification problem may be exploited, most notably in
		  the focus on specific secrets for declassification which
		  may allow more focused training of models and improve
		  scalability.Our ultimate goal is to perform redaction
		  directly on text. We will explore two sets of techniques,
		  one building on traditional Multi-Hop Question Answering
		  (MHQA) and a second using Large Language Models (LLM) which
		  now constitute a major element of text-based inference
		  methods. Both approaches to MHQA typically operate over
		  limited document sets, so a retrieval step is needed for
		  preselection. This retrieval step adds challenges because
		  we must accommodate redundant information spread across the
		  collection. We can evaluate nomination generalizability
		  across model classes and the impact of alternative
		  retrieval approaches using the same confound ranking
		  technique, but ultimately we will also need absolute
		  measures of effectiveness, not just relative comparisons,
		  because we must balance the benefit of releasing
		  information with the cost imposed by the risk of revealing
		  a secret. While our work begins the exploration of the
		  mosaicing problem, it has limitations. We must use analogs
		  for our problem as working with classified information is
		  challenging in access and distribution. While these are
		  selected to serve as reasonable representations of our
		  problem, they will exhibit differences from the actual
		  classified datasets. Furthermore, the performance of the
		  model classes used for inference in both the text and KG
		  scenarios may not generalize against novel approaches
		  developed in the future. The framework established in
		  testing the current models would still be applicable but
		  would have to be rerun with these new classes of models.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3075},
  numpages	= {1},
  keywords	= {information protection, knowledge graph, large language
		  model},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3640457.3691714,
  author	= {Tachioka, Yuuki},
  title		= {User Knowledge Prompt for Sequential Recommendation},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3691714},
  doi		= {10.1145/3640457.3691714},
  abstract	= {The large language model (LLM) based recommendation system
		  is effective for sequential recommendation, because general
		  knowledge of popular items is included in the LLM. To add
		  domain knowledge of items, the conventional method uses a
		  knowledge prompt obtained from the item knowledge graphs
		  and has achieved SOTA performance. However, for
		  personalized recommendation, it is necessary to consider
		  user knowledge, which the conventional method does not
		  fully consider because user knowledge is not included in
		  the item knowledge graphs; thus, we propose a user
		  knowledge prompt, which converts a user knowledge graph
		  into a prompt using the relationship template. The existing
		  prompt denoising framework is extended to prevent
		  hallucination caused by undesirable interactions between
		  knowledge graph prompts. We propose user knowledge prompts
		  of user traits and user preferences and associate relevant
		  items. Experiments on three types of dataset (movie, music,
		  and book) show the significant and consistent improvement
		  of our proposed user knowledge prompt.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {1142–1146},
  numpages	= {5},
  keywords	= {LLM, collaborative filtering, personalization, sequential
		  recommendation, user knowledge graph},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3627673.3679091,
  author	= {Xu, Eric and Zhang, Wenbin and Xu, Weifeng},
  title		= {Transforming Digital Forensics with Large Language Models:
		  Unlocking Automation, Insights, and Justice},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679091},
  doi		= {10.1145/3627673.3679091},
  abstract	= {In the pursuit of justice and accountability in the
		  digital age, the integration of Large Language Models
		  (LLMs) with digital forensics holds immense promise. This
		  half-day tutorial provides a comprehensive exploration of
		  the transformative potential of LLMs in automating digital
		  investigations and uncovering hidden insights. Through a
		  combination of real-world case studies, interactive
		  exercises, and hands-on labs, participants will gain a deep
		  understanding of how to harness LLMs for evidence analysis,
		  entity identification, and knowledge graph reconstruction.
		  By fostering a collaborative learning environment, this
		  tutorial aims to empower professionals, researchers, and
		  students with the skills and knowledge needed to drive
		  innovation in digital forensics. As LLMs continue to
		  revolutionize the field, this tutorial will have
		  far-reaching implications for enhancing justice outcomes,
		  promoting accountability, and shaping the future of digital
		  investigations.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5543–5546},
  numpages	= {4},
  keywords	= {automation, digital forensics, evidence analysis,
		  knowledge graph reconstruction, large language model},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3644815.3644959,
  author	= {Xia, Boming and Lu, Qinghua and Zhu, Liming and Lee, Sung
		  Une and Liu, Yue and Xing, Zhenchang},
  title		= {Towards a Responsible AI Metrics Catalogue: A Collection
		  of Metrics for AI Accountability},
  year		= {2024},
  isbn		= {9798400705915},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644815.3644959},
  doi		= {10.1145/3644815.3644959},
  abstract	= {Artificial Intelligence (AI), particularly through the
		  advent of large-scale generative AI (GenAI) models such as
		  Large Language Models (LLMs), has become a transformative
		  element in contemporary technology. While these models have
		  unlocked new possibilities, they simultaneously present
		  significant challenges, such as concerns over data privacy
		  and the propensity to generate misleading or fabricated
		  content. Current frameworks for Responsible AI (RAI) often
		  fall short in providing the granular guidance necessary for
		  tangible application, especially for Accountability---a
		  principle that is pivotal for ensuring transparent and
		  auditable decision-making, bolstering public trust, and
		  meeting increasing regulatory expectations. This study
		  bridges the Accountability gap by introducing our effort
		  towards a comprehensive metrics catalogue, formulated
		  through a systematic multivocal literature review (MLR)
		  that integrates findings from both academic and grey
		  literature. Our catalogue delineates process metrics that
		  underpin procedural integrity, resource metrics that
		  provide necessary tools and frameworks, and product metrics
		  that reflect the outputs of AI systems. This tripartite
		  framework is designed to operationalize Accountability in
		  AI, with a special emphasis on addressing the intricacies
		  of GenAI.},
  booktitle	= {Proceedings of the IEEE/ACM 3rd International Conference
		  on AI Engineering - Software Engineering for AI},
  pages		= {100–111},
  numpages	= {12},
  keywords	= {responsible AI, accountable AI, risk assessment,
		  generative AI},
  location	= {Lisbon, Portugal},
  series	= {CAIN '24}
}

@InProceedings{	  10.1145/3626772.3661370,
  author	= {Xu, Zhentao and Cruz, Mark Jerome and Guevara, Matthew and
		  Wang, Tie and Deshpande, Manasi and Wang, Xiaofeng and Li,
		  Zheng},
  title		= {Retrieval-Augmented Generation with Knowledge Graphs for
		  Customer Service Question Answering},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3661370},
  doi		= {10.1145/3626772.3661370},
  abstract	= {In customer service technical support, swiftly and
		  accurately retrieving relevant past issues is critical for
		  efficiently resolving customer inquiries. The conventional
		  retrieval methods in retrieval-augmented generation (RAG)
		  for large language models (LLMs) treat a large corpus of
		  past issue tracking tickets as plain text, ignoring the
		  crucial intra-issue structure and inter-issue relations,
		  which limits performance. We introduce a novel customer
		  service question-answering method that amalgamates RAG with
		  a knowledge graph (KG). Our method constructs a KG from
		  historical issues for use in retrieval, retaining the
		  intra-issue structure and inter-issue relations. During the
		  question-answering phase, our method parses consumer
		  queries and retrieves related sub-graphs from the KG to
		  generate answers. This integration of a KG not only
		  improves retrieval accuracy by preserving customer service
		  structure information but also enhances answering quality
		  by mitigating the effects of text segmentation. Empirical
		  assessments on our benchmark datasets, utilizing key
		  retrieval (MRR, Recall@K, NDCG@K) and text generation
		  (BLEU, ROUGE, METEOR) metrics, reveal that our method
		  outperforms the baseline by 77.6% in MRR and by 0.32 in
		  BLEU. Our method has been deployed within LinkedIn's
		  customer service team for approximately six months and has
		  reduced the median per-issue resolution time by 28.6%.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2905–2909},
  numpages	= {5},
  keywords	= {knowledge graph, large language model, question answering,
		  retrieval-augmented generation},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Article{	  10.1145/3640313,
  author	= {Subagdja, Budhitama and Shanthoshigaa, D. and Wang,
		  Zhaoxia and Tan, Ah-Hwee},
  title		= {Machine Learning for Refining Knowledge Graphs: A Survey},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3640313},
  doi		= {10.1145/3640313},
  abstract	= {Knowledge graph (KG) refinement refers to the process of
		  filling in missing information, removing redundancies, and
		  resolving inconsistencies in KGs. With the growing
		  popularity of KG in various domains, many techniques
		  involving machine learning have been applied, but there is
		  no survey dedicated to machine learning-based KG refinement
		  yet. Based on a novel framework following the KG refinement
		  process, this article presents a survey of machine learning
		  approaches to KG refinement according to the kind of
		  operations in KG refinement, the training datasets, mode of
		  learning, and process multiplicity. Furthermore, the survey
		  aims to provide broad practical insights into the
		  development of fully automated KG refinement.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {156},
  numpages	= {38},
  keywords	= {Knowledge graphs, knowledge graph refinement}
}

@InProceedings{	  10.1145/3610978.3640622,
  author	= {Jokinen, Kristiina and Wilcock, Graham},
  title		= {Exploring a Japanese Cooking Database},
  year		= {2024},
  isbn		= {9798400703232},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3610978.3640622},
  doi		= {10.1145/3610978.3640622},
  abstract	= {The paper describes ongoing work applying Generative AI to
		  a real world application. We use Retrieval Augmented
		  Generation and other GenAI tools that combine large
		  language models with Neo4j knowledge graphs. These tools
		  help a robot to chat in English about Japanese cooking
		  using a knowledge base that is in Japanese.},
  booktitle	= {Companion of the 2024 ACM/IEEE International Conference on
		  Human-Robot Interaction},
  pages		= {578–582},
  numpages	= {5},
  keywords	= {Japanese cooking, cypher query language, generative AI,
		  graph databases, knowledge graphs, large language models,
		  retrieval augmented generation, semantic search, social
		  robots},
  location	= {Boulder, CO, USA},
  series	= {HRI '24}
}

@InProceedings{	  10.1145/3613905.3650844,
  author	= {Walker, Johanna and Koutsiana, Elisavet and Nwachukwu,
		  Michelle and Mero\~{n}o Pe\~{n}uela, Albert and Simperl,
		  Elena},
  title		= {The Promise and Challenge of Large Language Models for
		  Knowledge Engineering: Insights from a Hackathon},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3650844},
  doi		= {10.1145/3613905.3650844},
  abstract	= {Knowledge engineering (KE) is the process of building,
		  maintaining and using knowledge-based systems. This
		  recently takes the form of knowledge graphs (KGs). The
		  advent of new technologies like Large Language Models
		  (LLMs) has the potential to improve automation in KE work
		  due to the richness of their training data and their
		  performance at solving natural language processing tasks.
		  We conducted a multiple-methods study exploring user
		  opinions and needs regarding the use of LLMs in KE. We used
		  ethnographic techniques to observe KE workers using LLMs to
		  solve KE tasks during a hackathon, followed by interviews
		  with some of the participants. This interim study found
		  that despite LLMs’ promising capabilities for efficient
		  knowledge acquisition and requirements elicitation, their
		  effective deployment requires an extended set of
		  capabilities and training, particularly in prompting and
		  understanding data. LLMs can be useful for simple quality
		  assessment tasks, but in complex scenarios, the output is
		  hard to control and evaluation may require novel
		  approaches. With this study, we aim to evidence the
		  interaction of KE stakeholders with LLMs, identify areas of
		  potential, and understand the barriers to their effective
		  use. We find copilot approaches may be valuable in
		  developing processes where the human or a team of humans is
		  assisted by generative AI.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {318},
  numpages	= {9},
  keywords	= {Interviews, Knowledge Engineering, Knowledge Graph, Large
		  Language Models},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@InProceedings{	  10.1145/3698587.3701392,
  author	= {ALMutairi, Mariam and AlKulaib, Lulwah and Wang, Shengkun
		  and Chen, Zhiqian and ALMutairi, Youssif and Alenazi,
		  Thamer M. and Luther, Kurt and Lu, Chang-Tien},
  title		= {FHIRViz: Multi-Agent Platform for FHIR Visualization to
		  Advance Healthcare Analytics},
  year		= {2024},
  isbn		= {9798400713026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698587.3701392},
  doi		= {10.1145/3698587.3701392},
  abstract	= {The shift to electronic health records (EHRs) has enhanced
		  patient care and research, but data sharing and complex
		  clinical terminology remain challenges. The Fast Healthcare
		  Interoperability Resource (FHIR) addresses interoperability
		  issues, though extracting insights from FHIR data is still
		  difficult. Traditional analytics often miss critical
		  clinical context, and managing FHIR data requires advanced
		  skills that are in short supply. This study presents
		  FHIRViz, a novel analytics tool that integrates FHIR data
		  with a semantic layer via a knowledge graph. It employs a
		  large language model (LLM) system to extract insights and
		  visualize them effectively. A retrieval vector store
		  improves performance by saving successful generations for
		  fine-tuning. FHIRViz translates clinical queries into
		  actionable insights with high accuracy. Results show
		  FHIRViz with GPT-4 achieving 92.62% accuracy, while Gemini
		  1.5 Pro reaches 89.34%, demonstrating the tool's potential
		  in overcoming healthcare data analytics challenges.},
  booktitle	= {Proceedings of the 15th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  articleno	= {38},
  numpages	= {7},
  keywords	= {Clinical Analytics, FHIR, Health Informatics, Knowledge
		  Graph, LLMs, Multi-Agent, visualization},
  location	= {Shenzhen, China},
  series	= {BCB '24}
}

@Article{	  10.1145/3627994,
  author	= {Lo, Pei-Chi and Lim, Ee-Peng},
  title		= {Non-monotonic Generation of Knowledge Paths for Context
		  Understanding},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {1},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3627994},
  doi		= {10.1145/3627994},
  abstract	= {Knowledge graphs can be used to enhance text search and
		  access by augmenting textual content with relevant
		  background knowledge. While many large knowledge graphs are
		  available, using them to make semantic connections between
		  entities mentioned in the textual content remains to be a
		  difficult task. In this work, we therefore introduce
		  contextual path generation (CPG), which refers to the task
		  of generating knowledge paths, contextual path, to explain
		  the semantic connections between entities mentioned in
		  textual documents with given knowledge graph. To perform
		  the CPG task well, one has to address its three challenges,
		  namely, path relevance, incomplete knowledge graph, and
		  path well-formedness. This article designs a two-stage
		  framework comprised of the following: (1) a
		  knowledge-enabled embedding matching and learning-to-rank
		  with multi-head self-attention context extractor to
		  determine a set of context entities relevant to both the
		  query entities and context document, and (2) a
		  non-monotonic path generation method with pretrained
		  transformer to generate high-quality contextual paths. Our
		  experiment results on two real-world datasets show that our
		  best performing CPG model successfully recovers 84.13% of
		  ground truth contextual paths, outperforming the context
		  window baselines. Finally, we demonstrate that the
		  non-monotonic model generates more well-formed paths
		  compared to the monotonic counterpart.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= mar,
  articleno	= {1},
  numpages	= {28},
  keywords	= {Information retrieval, knowledge graph, contextual path
		  generation, generation model}
}

@InProceedings{	  10.1145/3656156.3665133,
  author	= {Haghighi, Nava},
  title		= {Ontological Breakdown: Toward a World of Many Worlds},
  year		= {2024},
  isbn		= {9798400706325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3656156.3665133},
  doi		= {10.1145/3656156.3665133},
  abstract	= {Examining the taken-for-granted assumptions and views of
		  the world underlying the design of technological artifacts,
		  this work posits that a lack of ontological self-reflection
		  can constrain imagination, impeding movement toward a world
		  of many worlds. I propose ontological breakdown as an
		  analytic lens for interrogating the default assumptions
		  underlying the design of technology, using LLMs as a
		  case-study and drawing parallels to the discourse on values
		  in design. Then, I share three ways in which I have used
		  ontological breakdowns generatively to (1) surface
		  ontological difference and create spaces for experiencing
		  ontological alternatives to our defaults, (2) explore
		  ontological alternatives in the design of artifacts and
		  enable the end-users to notice their own ontological
		  defaults, and (3) expand ontological diversity by
		  empowering the end-users to move beyond the prescribed
		  defaults. I demonstrate the generative potential of
		  ontological breakdowns by providing examples of my work in
		  personal informatics.},
  booktitle	= {Companion Publication of the 2024 ACM Designing
		  Interactive Systems Conference},
  pages		= {70–73},
  numpages	= {4},
  keywords	= {LLM, generative AI, ontological breakdown, ontological
		  design, ontologies, personal informatics},
  location	= {IT University of Copenhagen, Denmark},
  series	= {DIS '24 Companion}
}

@InProceedings{	  10.1145/3670474.3685976,
  author	= {Francisco, Luis and Arikati, Srini},
  title		= {LLM Based Physical Verification Runset Generator},
  year		= {2024},
  isbn		= {9798400706998},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670474.3685976},
  doi		= {10.1145/3670474.3685976},
  abstract	= {The complexity in design rule description and coding is
		  drastically increasing as technology nodes advance. This
		  complexity makes the process of implementing the physical
		  verification (PV) rule checks more time-consuming and
		  susceptible to human error, creating the need to explore
		  alternate methods to improve the runset creation process.
		  The work presented proposes a generative AI solution that
		  uses Large Language Models (LLMs) to interpret rule
		  descriptions and generate design rule check decks (runsets)
		  in a language that a PV tool can interpret. The LLM is
		  fine-tuned with existing design rule manuals and runsets.
		  After post-processing the LLM output, the presented
		  solution can generate rules implementation with up to 97%
		  accuracy. The proposed solution can be used as a runset
		  writer Co-Pilot to help develop the new physical
		  verification runsets.},
  booktitle	= {Proceedings of the 2024 ACM/IEEE International Symposium
		  on Machine Learning for CAD},
  articleno	= {35},
  numpages	= {7},
  keywords	= {AI, Design Rule Checking, GenAI, LLMs, Physical
		  Verification, Runset Creation},
  location	= {Salt Lake City, UT, USA},
  series	= {MLCAD '24}
}

@InProceedings{	  10.1145/3700297.3700331,
  author	= {Yang, Da and Liu, Shutian and Fu, Haoyang and Shen,
		  Jiayi},
  title		= {Research and Practice on the Construction of Course
		  Ideological and Political Education Based on Knowledge
		  Graphs and Large Language Models},
  year		= {2024},
  isbn		= {9798400707100},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3700297.3700331},
  doi		= {10.1145/3700297.3700331},
  abstract	= {Knowledge graphs and large language models (LLMs) have
		  become important tools for educational innovation. This
		  paper explores the application of these two technologies in
		  the construction of ideological and political education in
		  university courses. The paper begins by analyzing the
		  importance of course-based ideological and political
		  education and the challenges currently faced. It then
		  introduces the role of knowledge graphs in integrating
		  educational resources and constructing knowledge systems,
		  as well as the potential and current status of LLMs in
		  natural language processing and providing personalized
		  educational content. This study presents a method that
		  integrates the use of knowledge graphs and LLMs to
		  construct resources and application systems for
		  course-based ideological and political education. The
		  results of practical case studies demonstrate that the
		  proposed method improves the efficiency of constructing
		  ideological and political education content, enhances the
		  effectiveness of moral education within courses, and
		  contributes to the innovative development of ideological
		  and political education.},
  booktitle	= {Proceedings of the 2024 International Symposium on
		  Artificial Intelligence for Education},
  pages		= {193–198},
  numpages	= {6},
  keywords	= {Course Ideological and Political Education, Educational
		  Innovation, Knowledge Graph, Large Language Model (LLM)},
  location	= { },
  series	= {ISAIE '24}
}

@Article{	  10.1145/3658451,
  author	= {Dou, Yutao and Huang, Yuwei and Zhao, Xiongjun and Zou,
		  Haitao and Shang, Jiandong and Lu, Ying and Yang, Xiaolin
		  and Xiao, Jian and Peng, Shaoliang},
  title		= {ShennongMGS: An LLM-based Chinese Medication Guidance
		  System},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3658451},
  doi		= {10.1145/3658451},
  abstract	= {The rapidly evolving field of Large Language Models (LLMs)
		  holds immense promise for healthcare, particularly in
		  medication guidance and adverse drug reaction prediction.
		  Despite their potential, existing LLMs face challenges in
		  dealing with complex polypharmacy scenarios and often
		  grapple with data lag issues. To address these limitations,
		  we introduce an LLM-based Chinese medication guidance
		  system, called ShennongMGS, specifically tailored for
		  robust medication guidance and adverse drug reaction
		  predictions. Our system transforms multi-source
		  heterogeneous medication information into a knowledge graph
		  and employs a two-stage training strategy to construct a
		  specialised LLM (ShennongGPT). This method enables the
		  simulation of professional pharmacists’ decision-making
		  processes and incorporates the capability for knowledge
		  self-updating, thereby significantly enhancing drug safety
		  and the overall quality of medical services. Rigorously
		  evaluated by medical professionals and artificial
		  intelligence experts, our method demonstrates superiority,
		  outperforming existing general and specialised LLMs in
		  performance.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= apr,
  keywords	= {Large Language Model, Model Fine-tuning, Medication
		  Guidance, Chinese Medical System, Natural Language
		  Processing, Software System}
}

@InProceedings{	  10.1145/3589334.3645616,
  author	= {Mou, Xinyi and Li, Zejun and Lyu, Hanjia and Luo, Jiebo
		  and Wei, Zhongyu},
  title		= {Unifying Local and Global Knowledge: Empowering Large
		  Language Models as Political Experts with Knowledge
		  Graphs},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645616},
  doi		= {10.1145/3589334.3645616},
  abstract	= {Large Language Models (LLMs) have revolutionized solutions
		  for general natural language processing (NLP) tasks.
		  However, deploying these models in specific domains still
		  faces challenges like hallucination. While existing
		  knowledge graph retrieval-based approaches offer partial
		  solutions, they cannot be well adapted to the political
		  domain. On one hand, existing generic knowledge graphs lack
		  vital political context, hindering deductions for practical
		  tasks. On the other hand, the nature of political questions
		  often renders the direct facts elusive, necessitating
		  deeper aggregation and comprehension of retrieved evidence.
		  To address these challenges, we propose a Political Experts
		  through Knowledge Graph Integration (PEG) framework. PEG
		  entails the creation and utilization of a multi-view
		  political knowledge graph (MVPKG), which integrates U.S.
		  legislative, election, and diplomatic data, as well as
		  conceptual knowledge from Wikidata. With MVPKG as its
		  foundation, PEG enhances existing methods through knowledge
		  acquisition, aggregation, and injection. This process
		  begins with refining evidence through semantic filtering,
		  followed by its aggregation into global knowledge via
		  implicit or explicit methods. The integrated knowledge is
		  then utilized by LLMs through prompts. Experiments on three
		  real-world datasets across diverse LLMs confirm PEG's
		  superiority in tackling political modeling tasks.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2603–2614},
  numpages	= {12},
  keywords	= {knowledge graph, large language models, political
		  science},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3698587.3701384,
  author	= {Patel, Parth and Chiu, Yu-Chiao and Hunag, Yufei and
		  Zhang, Jianqiu},
  title		= {MetaphorPrompt - An Analogical Reasoning Approach for
		  Extracting Causal Links from Biological Text},
  year		= {2024},
  isbn		= {9798400713026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3698587.3701384},
  doi		= {10.1145/3698587.3701384},
  abstract	= {In recent years, Large Language Models (LLMs) have
		  revolutionized Natural Language Processing (NLP), offering
		  significant improvements for extracting complex information
		  from biomedical literature. Our research introduces a novel
		  metaphor-based approach, MetaphorPrompt, to enhance the
		  accuracy of extracting molecular regulatory pathways (MRPs)
		  from biomedical texts. This method employs LLMs such as
		  GPT4 to develop metaphors that map biological processes
		  onto familiar, real-world scenarios, facilitating a better
		  understanding and extracting causal events in MRPs.
		  MetaphorPrompt is tested using the reguloGPT dataset and
		  compared to a baseline method (without metaphors) and
		  reguloGPT's best prompt. Test results demonstrate improved
		  precision, recall, and F1 scores in node and edge
		  prediction of causal event links through analogical
		  reasoning. The effect of in-context learning (ICL) in
		  MetaphorPrompt is investigated, and it is found that
		  analogical reasoning offers significant improvements over
		  ICL. This supports the claim that LLMs can perform novel
		  problem-solving through analogical reasoning. This work
		  paves the way for more intuitive and user-friendly
		  representations of MRPs in biomedical data, ultimately
		  contributing to advancements in biomedical NLP, knowledge
		  graph construction, and effective applications of LLMs in
		  novel problem-solving through analogical reasoning.},
  booktitle	= {Proceedings of the 15th ACM International Conference on
		  Bioinformatics, Computational Biology and Health
		  Informatics},
  articleno	= {49},
  numpages	= {6},
  keywords	= {Analogical Reasoning, Causal Event Links, GPT4, Knowledge
		  Graph, LLM, Metaphor, Molecular Regulation Pathway,
		  Prompt},
  location	= {Shenzhen, China},
  series	= {BCB '24}
}

@InProceedings{	  10.1145/3613905.3650784,
  author	= {Han, Jiyeon and Park, Jimin and Huh, Jinyoung and Oh, Uran
		  and Do, Jaeyoung and Kim, Daehee},
  title		= {AscleAI: A LLM-based Clinical Note Management System for
		  Enhancing Clinician Productivity},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3650784},
  doi		= {10.1145/3613905.3650784},
  abstract	= {While clinical notes are essential to the field of
		  healthcare, they pose several challenges for clinicians
		  since it is difficult to write down medical information,
		  review prior notes, and extract the desired information at
		  the same time while examining a patient. Thus, we designed
		  a system that can automatically generate clinical notes
		  from dialogues between patients and clinicians and provide
		  specific information upon clinicians’ query using a Large
		  Language Model (LLM) both in real-time. To explore how this
		  system can be used to support clinicians in practice, we
		  conducted an interview with six clinicians followed by a
		  design probe study with the current version of our system
		  for feedback. Findings suggest that our system has the
		  potential to enable clinicians to write and access clinical
		  notes and examine the patients simultaneously with reduced
		  cognitive loads and increased efficiency and accuracy.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {50},
  numpages	= {7},
  keywords	= {Large language model, clinical note, design probe,
		  interview},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@InProceedings{	  10.1145/3637528.3671984,
  author	= {Gong, Jiahui and Ding, Jingtao and Meng, Fanjin and Chen,
		  Guilong and Chen, Hong and Zhao, Shen and Lu, Haisheng and
		  Li, Yong},
  title		= {A Population-to-individual Tuning Framework for Adapting
		  Pretrained LM to On-device User Intent Prediction},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671984},
  doi		= {10.1145/3637528.3671984},
  abstract	= {Mobile devices, especially smartphones, can support rich
		  functions and have developed into indispensable tools in
		  daily life. With the rise of generative AI services,
		  smartphones can potentially transform into personalized
		  assistants, anticipating user needs and scheduling services
		  accordingly. Predicting user intents on smartphones, and
		  reflecting anticipated activities based on past
		  interactions and context, remains a pivotal step towards
		  this vision. Existing research predominantly focuses on
		  specific domains, neglecting the challenge of modeling
		  diverse event sequences across dynamic contexts. Leveraging
		  pre-trained language models (PLMs) offers a promising
		  avenue, yet adapting PLMs to on-device user intent
		  prediction presents significant challenges. To address
		  these challenges, we propose PITuning, a
		  Population-to-Individual Tuning framework. PITuning
		  enhances common pattern extraction through dynamic
		  event-to-intent transition modeling and addresses
		  long-tailed preferences via adaptive unlearning strategies.
		  Experimental results on real-world datasets demonstrate
		  PITuning's superior intent prediction performance,
		  highlighting its ability to capture long-tailed preferences
		  and its practicality for on-device prediction scenarios.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {896–907},
  numpages	= {12},
  keywords	= {device-cloud collaboration, personalization, pretrained
		  language model, user intent},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3689492.3690049,
  author	= {Thiede, Christoph and Taeumel, Marcel and B\"{o}hme, Lukas
		  and Hirschfeld, Robert},
  title		= {Talking to Objects in Natural Language: Toward Semantic
		  Tools for Exploratory Programming},
  year		= {2024},
  isbn		= {9798400712159},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689492.3690049},
  doi		= {10.1145/3689492.3690049},
  abstract	= {In exploratory programming, programmers often face a
		  semantic gap between their high-level understanding and the
		  low-level interfaces available for interacting with objects
		  in a system. That is, technical object structure and
		  behavior need to be interpreted as abstract domain
		  concepts, which then increases cognitive load and thus
		  impedes exploration progress. We propose semantic object
		  interfaces that bridge this gap by enabling contextual,
		  natural-language conversations with objects. Our approach
		  leverages an exploratory programming agent powered by a
		  large language model (LLM) to translate natural-language
		  questions into low-level experiments and provide high-level
		  answers. We describe a framework for integrating semantic
		  object interfaces into existing exploratory programming
		  systems, including a prototype implementation in
		  Squeak/Smalltalk using GPT-4o. We showcase the potential of
		  semantic object interfaces through case studies and discuss
		  their feasibility, limitations, and impact on the
		  programming experience. While challenges remain, our
		  approach promises to reduce mental effort and empower
		  programmers to explore and understand systems at a higher
		  level of abstraction for a better programming experience.},
  booktitle	= {Proceedings of the 2024 ACM SIGPLAN International
		  Symposium on New Ideas, New Paradigms, and Reflections on
		  Programming and Software},
  pages		= {68–84},
  numpages	= {17},
  keywords	= {ChatGPT, LLMs, Smalltalk, conversational agents,
		  exploratory programming, generative AI, natural-language
		  programming, object-oriented programming, semantic tools},
  location	= {Pasadena, CA, USA},
  series	= {Onward! '24}
}

@InProceedings{	  10.1145/3664647.3681705,
  author	= {Wang, Siqi and Liang, Chao and Gao, Yunfan and Liu, Yang
		  and Li, Jing and Wang, Haofen},
  title		= {Decoding Urban Industrial Complexity: Enhancing
		  Knowledge-Driven Insights via IndustryScopeGPT},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681705},
  doi		= {10.1145/3664647.3681705},
  abstract	= {Industrial parks are critical to urban economic growth.
		  Yet, their development often encounters challenges stemming
		  from imbalances between industrial requirements and urban
		  services, underscoring the need for strategic planning and
		  operations. This paper introduces IndustryScopeKG, a
		  pioneering large-scale multi-modal, multi-level industrial
		  park knowledge graph, which integrates diverse urban data
		  including street views, corporate, socio-economic, and
		  geospatial information, capturing the complex relationships
		  and semantics within industrial parks. Alongside this, we
		  present the IndustryScopeGPT framework, which leverages
		  Large Language Models (LLMs) with Monte Carlo Tree Search
		  to enhance tool-augmented reasoning and decision-making in
		  Industrial Park Planning and Operation (IPPO). Our work
		  significantly improves site recommendation and functional
		  planning, demonstrating the potential of combining LLMs
		  with structured datasets to advance industrial park
		  management. This approach sets a new benchmark for
		  intelligent IPPO research and lays a robust foundation for
		  advancing urban industrial development. The dataset and
		  related code are available at
		  https://github.com/Tongji-KGLLM/IndustryScope.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {4757–4765},
  numpages	= {9},
  keywords	= {industrial park planning and operation, large language
		  model agent, urban design and planning, urban knowledge
		  graph},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3703187.3703193,
  author	= {Li, Jiaqi and Li, Guanhua and Liu, Biqi and Zhou, Yuxiao
		  and Li, Shuang and Yu, Haichuan and Wang, Nan},
  title		= {Construction of a Professional Vocabulary Database for
		  Power Transformers Based on Automatic Word Segmentation},
  year		= {2024},
  isbn		= {9798400707254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703187.3703193},
  doi		= {10.1145/3703187.3703193},
  abstract	= {Power transformers are essential equipment in power
		  systems for energy conversion and transmission. The
		  construction of a knowledge graph for power transformers
		  can effectively organize relevant knowledge about them. To
		  achieve this, the first step is to build a specialized
		  vocabulary database for the power industry. This paper
		  employs an automatic word segmentation algorithm capable of
		  processing both specialized power-related corpora and
		  non-specialized general corpora to generate an initial
		  vocabulary list. Given the potential for non-relevant words
		  and non-word elements introduced during unsupervised word
		  segmentation, optimization strategies are further
		  introduced, utilizing the product of left and right
		  entropies as a screening criterion to aggregate and purify
		  the vocabulary. By comparing the word segmentation results
		  of specialized and non-specialized corpora, a professional
		  vocabulary database in the field of power transformers has
		  been successfully constructed. This accomplishment not only
		  automates the recognition and accumulation of power
		  industry-specific vocabulary but also lays a solid
		  foundation for the subsequent construction of a knowledge
		  graph for power transformers.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Computer Information Science and Artificial Intelligence},
  pages		= {24–28},
  numpages	= {5},
  keywords	= {Knowledge graph, Power transformer, Professional term
		  base, Word segmentation algorithm},
  location	= { },
  series	= {CISAI '24}
}

@Article{	  10.1145/3678183,
  author	= {Chi, Te-Yu and Jang, Jyh-Shing Roger},
  title		= {WC-SBERT: Zero-Shot Topic Classification Using SBERT and
		  Light Self-Training on Wikipedia Categories},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {5},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3678183},
  doi		= {10.1145/3678183},
  abstract	= {In natural language processing (NLP), zero-shot topic
		  classification requires machines to understand the
		  contextual meanings of texts in a downstream task without
		  using the corresponding labeled texts for training, which
		  is highly desirable for various applications. In this
		  article, we propose a novel approach to construct a
		  zero-shot task-specific model called WC-SBERT with
		  satisfactory performance. The proposed approach is highly
		  efficient since it uses light self-training requiring
		  target labels (target class names of downstream tasks)
		  only, which is distinct from other research that uses both
		  the target labels and the unlabeled texts for training. In
		  particular, during the pre-training stage, WC-SBERT uses
		  contrastive learning with multiple negative ranking losses
		  to construct the pre-trained model based on the similarity
		  between Wiki categories. For the self-training stage,
		  online contrastive loss is utilized to reduce the distance
		  between a target label and Wiki categories of similar Wiki
		  pages to the label. Experimental results indicate that
		  compared to existing self-training models, WC-SBERT
		  achieves rapid inference on approximately 6.45 million Wiki
		  text entries by utilizing pre-stored Wikipedia text
		  embeddings, significantly reducing inference time per
		  sample by a factor of 2,746 to 16,746. During the
		  fine-tuning step, the time required for each sample is
		  reduced by a factor of 23–67. Overall, the total training
		  time shows a maximum reduction of 27.5 times across
		  different datasets. Most importantly, our model has
		  achieved state-of-the-art (SOTA) accuracy on two of the
		  three commonly used datasets for evaluating zero-shot
		  classification, namely the AG News (0.84) and Yahoo!
		  Answers (0.64) datasets. The code for WC-SBERT is publicly
		  available on GitHub,1 and the dataset can also be accessed
		  on Hugging Face.2},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  articleno	= {111},
  numpages	= {18},
  keywords	= {Zero-shot topic classification, SBERT, Wikipedia,
		  Self-training, Contrastive learning, Knowledge graph, LLM}
}

@Article{	  10.1145/3708326,
  author	= {Mountantonakis, Michalis and Tzitzikas, Yannis},
  title		= {Generating SPARQL Queries over CIDOC-CRM using a Two-Stage
		  Ontology Path Patterns Method in LLM Prompts},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3708326},
  doi		= {10.1145/3708326},
  abstract	= {In this paper, we focus on the task of exploiting the
		  capabilities of Large Language Models (LLMs) to generate
		  SPARQL Queries for answering natural questions over
		  cultural Knowledge Graphs (KGs) expressed according to the
		  ISO standard ontology CIDOC-CRM. Since CIDOC-CRM is an
		  event-based model, usually we have to follow long paths for
		  answering a question, thereby, the challenge is how to
		  construct the prompt for aiding the LLM to produce the
		  right SPARQL query. We propose and comparatively evaluate
		  methods based on the creation of ontology path patterns of
		  a configurable path radius (or length). Then, we construct
		  a new dedicated benchmark that includes 100 natural
		  questions and the corresponding SPARQL queries over two
		  real KGs from the cultural domain describing artworks.
		  Finally, we present comparative results about the
		  effectiveness and efficiency over the benchmark by using
		  ChatGPT-3.5. The most effective method follows a two-stage
		  process that predicts and uses the most appropriate path
		  patterns of (rleq 4) . This method achieves 3.5 (times)
		  higher accuracy than the baseline method (0.66 versus
		  0.19), that includes in the prompt only the list of
		  properties and classes of the KG.Benchmark:},
  note		= {Just Accepted},
  journal	= {J. Comput. Cult. Herit.},
  month		= dec,
  keywords	= {Question Answering, CIDOC-CRM, Prompt Engineering,
		  Cultural Heritage, LLM}
}

@InProceedings{	  10.1145/3613904.3642868,
  author	= {Wang, Sitong and Menon, Samia and Long, Tao and Henderson,
		  Keren and Li, Dingzeyu and Crowston, Kevin and Hansen, Mark
		  and Nickerson, Jeffrey V and Chilton, Lydia B},
  title		= {ReelFramer: Human-AI Co-Creation for News-to-Video
		  Translation},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642868},
  doi		= {10.1145/3613904.3642868},
  abstract	= {Short videos on social media are the dominant way young
		  people consume content. News outlets aim to reach audiences
		  through news reels—short videos conveying news—but
		  struggle to translate traditional journalistic formats into
		  short, entertaining videos. To translate news into social
		  media reels, we support journalists in reframing the
		  narrative. In literature, narrative framing is a high-level
		  structure that shapes the overall presentation of a story.
		  We identified three narrative framings for reels that adapt
		  social media norms but preserve news value, each with a
		  different balance of information and entertainment. We
		  introduce ReelFramer, a human-AI co-creative system that
		  helps journalists translate print articles into scripts and
		  storyboards. ReelFramer supports exploring multiple
		  narrative framings to find one appropriate to the story. AI
		  suggests foundational narrative details, including
		  characters, plot, setting, and key information. ReelFramer
		  also supports visual framing; AI suggests character and
		  visual detail designs before generating a full storyboard.
		  Our studies show that narrative framing introduces the
		  necessary diversity to translate various articles into
		  reels, and establishing foundational details helps generate
		  scripts that are more relevant and coherent. We also
		  discuss the benefits of using narrative framing and
		  foundational details in content retargeting.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {169},
  numpages	= {20},
  keywords	= {creativity support tools, generative AI, narratives,
		  scriptwriting, short videos, storyboarding},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3637528.3671997,
  author	= {Cao, Zongsheng and Li, Jing and Wang, Zigan and Li,
		  Jinliang},
  title		= {DiffusionE: Reasoning on Knowledge Graphs via
		  Diffusion-based Graph Neural Networks},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671997},
  doi		= {10.1145/3637528.3671997},
  abstract	= {Graph Neural Networks (GNNs) have demonstrated powerful
		  capabilities in reasoning within Knowledge Graphs (KGs),
		  gathering increasing attention. Our idea stems from the
		  observation that the prior work typically employs
		  hand-designed or sample-designed paradigms in the process
		  of message propagation, engaging a set of adjacent entities
		  at each step of propagation. As a result, such methods
		  struggle with the increasing number of entities involved as
		  propagation steps extend. Moreover, they neglect the
		  message interactions between adjacent entities and
		  propagation relations in KG reasoning, leading to semantic
		  inconsistency during the message aggregation phase. To
		  address these issues, we introduce a novel knowledge graph
		  embedding method through a diffusion process, termed
		  DiffusionE. Specifically, we reformulate the message
		  propagation in knowledge reasoning as a diffusion process,
		  regarding the message semantics as the diffusion signal. In
		  this sense, guided by semantic information, messages can be
		  transmitted between nodes effectively and adaptively.
		  Furthermore, the theoretical analysis suggests our method
		  can leverage an optimal diffusivity for message propagation
		  in the semantic interactions of KGs. It shows that
		  DiffusionE effectively leverages message interactions
		  between entities and propagation relations, ensuring
		  semantic consistency in KG reasoning. Comprehensive
		  experiments reveal that our method attains state-of-the-art
		  performance compared to prior work on several
		  well-established benchmarks.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {222–230},
  numpages	= {9},
  keywords	= {diffusion process, graph neural networks, knowledge
		  graph},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3627673.3679781,
  author	= {Mannino, Miro and Garcia, Junior and Hazim, Reem and
		  Abouzied, Azza and Papotti, Paolo},
  title		= {Data Void Exploits: Tracking &amp; Mitigation Strategies},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679781},
  doi		= {10.1145/3627673.3679781},
  abstract	= {A data void is a gap in online information, providing an
		  opportunity for the spread of disinformation or a data void
		  exploit. We introduce lightweight measures to track the
		  progress of data void exploits and mitigation efforts in
		  two contexts: Web search and Knowledge Graph (KG) querying.
		  We use case studies to demonstrate the viability of these
		  measures as data void trackers in the Web search context.
		  To tackle data voids, we introduce an adversarial game
		  model involving two agents: a disinformer and a mitigator.
		  Both agents insert content into the information ecosystem
		  to have their narrative rank higher than their counterpart
		  in search results. At every turn, each agent chooses which
		  content to deploy within their resource constraints,
		  mimicking real-world situations where different entities
		  have varying levels of influence and access to resources.
		  Using simulations of this game, we compare and evaluate
		  different mitigation strategies to recommend ones that
		  maximize mitigation impact while minimizing costs.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1627–1637},
  numpages	= {11},
  keywords	= {data void, exploit, knowledge graph, misinformation, web
		  search},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3637528.3671837,
  author	= {Ning, Liang-bo and Wang, Shijie and Fan, Wenqi and Li,
		  Qing and Xu, Xin and Chen, Hao and Huang, Feiran},
  title		= {CheatAgent: Attacking LLM-Empowered Recommender Systems
		  via LLM Agent},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671837},
  doi		= {10.1145/3637528.3671837},
  abstract	= {Recently, Large Language Model (LLM)-empowered recommender
		  systems (RecSys) have brought significant advances in
		  personalized user experience and have attracted
		  considerable attention. Despite the impressive progress,
		  the research question regarding the safety vulnerability of
		  LLM-empowered RecSys still remains largely
		  under-investigated. Given the security and privacy
		  concerns, it is more practical to focus on attacking the
		  black-box RecSys, where attackers can only observe the
		  system's inputs and outputs. However, traditional attack
		  approaches employing reinforcement learning (RL) agents are
		  not effective for attacking LLM-empowered RecSys due to the
		  limited capabilities in processing complex textual inputs,
		  planning, and reasoning. On the other hand, LLMs provide
		  unprecedented opportunities to serve as attack agents to
		  attack RecSys because of their impressive capability in
		  simulating human-like decision-making processes. Therefore,
		  in this paper, we propose a novel attack framework called
		  CheatAgent by harnessing the human-like capabilities of
		  LLMs, where an LLM-based agent is developed to attack
		  LLM-Empowered RecSys. Specifically, our method first
		  identifies the insertion position for maximum impact with
		  minimal input modification. After that, the LLM agent is
		  designed to generate adversarial perturbations to insert at
		  target positions. To further improve the quality of
		  generated perturbations, we utilize the prompt tuning
		  technique to improve attacking strategies via feedback from
		  the victim RecSys iteratively. Extensive experiments across
		  three real-world datasets demonstrate the effectiveness of
		  our proposed attacking method.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2284–2295},
  numpages	= {12},
  keywords	= {adversarial attacks, large language models, llm-empowered
		  recommender systems, llms-based agent, recommender
		  systems},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3627673.3679894,
  author	= {Kasuga, Akira and Yonetani, Ryo},
  title		= {CXSimulator: A User Behavior Simulation using LLM
		  Embeddings for Web-Marketing Campaign Assessment},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679894},
  doi		= {10.1145/3627673.3679894},
  abstract	= {This paper presents the Customer Experience (CX)
		  Simulator, a novel framework designed to assess the effects
		  of untested web-marketing campaigns through user behavior
		  simulations. The proposed framework leverages large
		  language models (LLMs) to represent various events in a
		  user's behavioral history, such as viewing an item,
		  applying a coupon, or purchasing an item, as semantic
		  embedding vectors. We train a model to predict transitions
		  between events from their LLM embeddings, which can even
		  generalize to unseen events by learning from diverse
		  training data. In web-marketing applications, we leverage
		  this transition prediction model to simulate how users
		  might react differently when new campaigns or products are
		  presented to them. This allows us to eliminate the need for
		  costly online testing and enhance the marketers' abilities
		  to reveal insights. Our numerical evaluation and user
		  study, utilizing BigQuery Public Datasets from the Google
		  Merchandise Store, demonstrate the effectiveness of our
		  framework.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3817–3821},
  numpages	= {5},
  keywords	= {embedding, large language model, link prediction,
		  marketing campaign, user simulation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3627673.3680268,
  author	= {Colombo, Andrea},
  title		= {Leveraging Knowledge Graphs and LLMs to Support and
		  Monitor Legislative Systems},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680268},
  doi		= {10.1145/3627673.3680268},
  abstract	= {Knowledge Graphs (KGs) have been used to organize large
		  datasets into structured, interconnected information,
		  enhancing data analytics across various fields. In the
		  legislative context, one potential natural application of
		  KGs is modeling the intricate set of interconnections that
		  link laws and their articles with each other and the
		  broader legislative context.At the same time, the rise of
		  large language models (LLMs) such as GPT has opened new
		  opportunities in legal applications, such as text
		  generation and document drafting. Despite their potential,
		  the use of LLMs in legislative contexts is critical since
		  it requires the absence of hallucinations and reliance on
		  up-to-date information, as new laws are published on a
		  daily basis.This work investigates how Legislative
		  Knowledge Graphs and LLMs can synergize and support
		  legislative processes. We address three key questions: the
		  benefits of using KGs for legislative systems, how LLM can
		  support legislative activities by ensuring an accurate
		  output, and how we can allow non-technical users to use
		  such technologies in their activities. To this aim, we
		  develop Legis AI Platform, an interactive platform focused
		  on Italian legislation that enhances the possibility of
		  conducting legislative analysis and that aims to support
		  lawmaking activities.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5443–5446},
  numpages	= {4},
  keywords	= {graphrag, knowledge graph, large language models, laws,
		  legislative systems},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3652583.3658069,
  author	= {Mu, Hongzhang and Zhang, Shuili and Xu, Hongbo},
  title		= {A Knowledge-Driven Approach to Enhance Topic Modeling with
		  Multi-Modal Representation Learning},
  year		= {2024},
  isbn		= {9798400706196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652583.3658069},
  doi		= {10.1145/3652583.3658069},
  abstract	= {multi-modal topic models strive to integrate semantic
		  information from multi-modal data to generate more precise
		  topics. Topic modeling methods encounter challenges in
		  terms of topic diversity and effectiveness. To address this
		  issue, the majority of current approaches focus on modeling
		  the correlation among numerous multi-modal sources.
		  Nevertheless, little emphasis has been placed on
		  fine-grained feature representation and structured
		  knowledge. In this regard, we propose a fine-grained Prompt
		  representation method. Specifically, we adopt a dual-stream
		  structure where a pre-trained language model and an image
		  model are parallelly combined to construct a multi-modal
		  model. We then enhance the structured representation by
		  integrating fine-grained scene graph knowledge through a
		  Knowledge-Enhanced Encoder, which is constructed based on
		  the scene graph. To validate the effectiveness of the
		  proposed framework, we significantly improve topic quality
		  (such as coherence and diversity) using the aforementioned
		  approach. On publicly available datasets, our approach
		  outperforms state-of-the-art multi-modal topic models
		  respectively.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Multimedia Retrieval},
  pages		= {1347–1355},
  numpages	= {9},
  keywords	= {deep learning, knowledge graph, multi-modal, topic model},
  location	= {Phuket, Thailand},
  series	= {ICMR '24}
}

@InProceedings{	  10.1145/3589335.3651263,
  author	= {Jain, Monika},
  title		= {Knowledge Enabled Relation Extraction},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651263},
  doi		= {10.1145/3589335.3651263},
  abstract	= {Relation extraction is the task of extracting
		  relationships from input text, where input can be a
		  sentence, document, or multiple documents. This task has
		  been popular for decades and is still of keen interest.
		  Various techniques have been proposed to solve the relation
		  extraction problem, among which the most popular are using
		  distant supervision, deep learning-based models,
		  reasoning-based models, and transformer-based models. We
		  propose three approaches (named ReOnto, DocRE-CLip, and
		  KDocRE) for relation extraction from text at three levels
		  of granularity (sentence, document and across documents).
		  These approaches embed knowledge in a deep learning based
		  model to improve performance. ReOnto and DocRE-CLip have
		  been evaluated and the source code is publicly available.
		  We are currently implementing and evaluating KDocRE.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1210–1213},
  numpages	= {4},
  keywords	= {graph neural network, knowledge graph, neurosymbolic ai,
		  ontology, relation extraction},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3639479.3639484,
  author	= {Zhang, Liqi and Yang, Lianhe and Bai, Yinhao and Zhu,
		  Hongfei and Liu, Xingyu},
  title		= {R-TES: Regularized Template Style for Generative Joint
		  Relational Triple Extraction},
  year		= {2024},
  isbn		= {9798400709241},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639479.3639484},
  doi		= {10.1145/3639479.3639484},
  abstract	= {Joint Relational Triple Extraction (RTE) is an important
		  task in the field of information extraction. With the
		  development of the pre-trained language models, the
		  sequence-to-sequence (seq2seq) approaches have become one
		  of the promising methods for this task, utilizing a
		  predefined template to convert the relational triples into
		  a structure target sequence, which can be easily decoded as
		  relational triples. However, most existing seq2seq studies
		  focus on the improvement of methods but ignore that
		  template styles also have impacts on performance. Inspired
		  by this idea, we first explore the effects of different
		  template styles on performance and find that some template
		  styles can help generate models to achieve better
		  performance. Based on the above findings, we argue that
		  different template styles lead to various understandings of
		  the relation triple. Therefore, we propose Regularized
		  template style (R-TES) to improve the performance of a main
		  template by reducing the gap between it and other selected
		  templates. Specifically, R-TES uses the pre-trained
		  language model to select the templates with
		  kullback-leibler (KL) divergence. Then, we further reduce
		  the gap between the main template and these selected
		  templates by minimizing KL divergence. Experimental results
		  show that our method outperforms state-of-the-art methods
		  on the publicly available dataset.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Machine Learning and Natural Language Processing},
  pages		= {21–26},
  numpages	= {6},
  keywords	= {information extraction, language model, neural networks},
  location	= {Sanya, China},
  series	= {MLNLP '23}
}

@InProceedings{	  10.1145/3695080.3695127,
  author	= {Chen, Hao and Hou, Jun},
  title		= {Intelligent data governance: building an enterprise data
		  management system using KG and LLM},
  year		= {2024},
  isbn		= {9798400710223},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3695080.3695127},
  doi		= {10.1145/3695080.3695127},
  abstract	= {In tobacco enterprises, data governance is the key to
		  improving operational efficiency and decision-making
		  quality. This study focuses on how to build an advanced
		  data management system for tobacco enterprises through
		  knowledge graph (KG) and large language model (LLM).
		  Firstly, this paper describes the process of integrating
		  the core resources of tobacco enterprises, such as
		  metadata, data elements, data constraints, arithmetic,
		  storage and network, into a KG. Subsequently, it analyses
		  in depth how to use local LLM combined with the KG to form
		  a ‘think tank’ for tobacco enterprise data governance.
		  This think tank would not only be able to store and process
		  the vast amount of data governance information in the
		  tobacco industry, but also provide intelligent
		  recommendations, predict future trends, and diagnose
		  problems in existing data governance processes. In
		  addition, the paper discusses the potential impact of this
		  integrated approach on enhancing data governance
		  strategies, improving data quality and compliance in
		  tobacco organisations, as well as its role in fostering
		  cross-functional collaboration and improving data
		  governance efficiency. A series of recommendations for
		  implementing and optimising such an integrated data
		  governance system are also presented to address the
		  specificities of the tobacco industry, such as the high
		  requirements for data security and regulatory compliance.
		  These recommendations are designed to help tobacco
		  organisations manage their growing data assets more
		  effectively and ensure data security and compliance to stay
		  ahead of the game in a competitive market. With this
		  advanced data governance system, tobacco companies can
		  better adapt to the trend of digital transformation and
		  maximise the use of their data assets.},
  booktitle	= {Proceedings of the 2024 International Conference on Cloud
		  Computing and Big Data},
  pages		= {266–271},
  numpages	= {6},
  location	= {Dali, China},
  series	= {ICCBD '24}
}

@InProceedings{	  10.1145/3589335.3651955,
  author	= {Zhu, Lixi and Huang, Xiaowen and Sang, Jitao},
  title		= {How Reliable is Your Simulator? Analysis on the
		  Limitations of Current LLM-based User Simulators for
		  Conversational Recommendation},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651955},
  doi		= {10.1145/3589335.3651955},
  abstract	= {Conversational Recommender System (CRS) interacts with
		  users through natural language to understand their
		  preferences and provide personalized recommendations in
		  real-time. CRS has demonstrated significant potential,
		  prompting researchers to address the development of more
		  realistic and reliable user simulators as a key focus.
		  Recently, the capabilities of Large Language Models (LLMs)
		  have attracted a lot of attention in various fields.
		  Simultaneously, efforts are underway to construct user
		  simulators based on LLMs. While these works showcase
		  innovation, they also come with certain limitations that
		  require attention. In this work, we aim to analyze the
		  limitations of using LLMs in constructing user simulators
		  for CRS, to guide future research. To achieve this goal, we
		  conduct analytical validation on the notable work, iEvaLM.
		  Through multiple experiments on two widely-used datasets in
		  the field of conversational recommendation, we highlight
		  several issues with the current evaluation methods for user
		  simulators based on LLMs: (1) Data leakage, which occurs in
		  conversational history and the user simulator's replies,
		  results in inflated evaluation results. (2) The success of
		  CRS recommendations depends more on the availability and
		  quality of conversational history than on the responses
		  from user simulators. (3) Controlling the output of the
		  user simulator through a single prompt template proves
		  challenging. To overcome these limitations, we propose
		  SimpleUserSim, employing a straightforward strategy to
		  guide the topic toward the target items. Our study
		  validates the ability of CRS models to utilize the
		  interaction information, significantly improving the
		  recommendation results.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1726–1732},
  numpages	= {7},
  keywords	= {conversational recommendation system, large language
		  model, user simulator},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3589334.3645397,
  author	= {Sun, Chenchen and Xu, Yang and Shen, Derong and Nie,
		  Tiezheng},
  title		= {Matching Feature Separation Network for Domain Adaptation
		  in Entity Matching},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645397},
  doi		= {10.1145/3589334.3645397},
  abstract	= {Entity matching (EM) determines whether two records from
		  different data sources refer to the same real-world entity.
		  It is a fundamental task in knowledge graph construction
		  and data integration. Currently, deep learning (DL) based
		  EM methods have achieved state-of-the-art (SOTA) results.
		  However, apply-ing DL-based EM methods often costs a lot of
		  human efforts to label the data. To address this challenge,
		  we propose a new do-main adaptation (DA) framework for EM
		  called Matching Fea-ture Separation Network (MFSN). We
		  implement DA by sepa-rating private and common matching
		  features. Briefly, MFSN first uses three encoders to
		  explicitly model the private and common matching features
		  in both the source and target do-mains. Then, it transfers
		  the knowledge learned from the source common matching
		  features to the target domain. We also pro-pose an enhanced
		  variant called Feature Representation and Separation
		  Enhanced MFSN (MFSN-FRSE). Compared with MFSN, it has
		  superior feature representation and separation
		  capabilities. We evaluate the effectiveness of MFSN and
		  MFSN-FRSE on twelve DA in EM tasks. The results show that
		  our framework is approximately 7% higher in F1 score on
		  average than the previous SOTA methods. Then, we verify the
		  effec-tiveness of each module in MFSN and MFSN-FRSE by
		  ablation study. Finally, we explore the optimal strategy of
		  each module in MFSN and MFSN-FRSE through detailed tests.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {1975–1985},
  numpages	= {11},
  keywords	= {data integration, domain adaptation, entity matching,
		  knowledge graph construction, matching feature separation
		  network},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3639233.3639347,
  author	= {Wang, Jingdong and Guo, Yongjia},
  title		= {Named Entities Based on the BERT-BILSTM-ACRF Model
		  Recognition Research},
  year		= {2024},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639233.3639347},
  doi		= {10.1145/3639233.3639347},
  abstract	= {As a crucial first step in the process of constructing
		  knowledge graph, the accuracy of named entity recognition
		  determines the construction effect of the final graph.
		  However, at present, Chinese named entity recognition
		  methods still have many problems, such as long training
		  time and lower accuracy. Hence, we come up with a
		  BERT-BILSTM-ACRF entity recognition method that combines
		  the “self-attention” mechanism. To begin with, Bert
		  model is selected as the embedding layer, the text is
		  vectorized, and the character position in-formation is
		  obtained through the bidirectional Long Short-Term Memory
		  network. Secondly, the internal relationship of the
		  character sequence is further searched through the
		  self-attention mechanism, and finally the final optimal
		  sequence is decoded by the conditional random field model.
		  To check the effectiveness of the BERT-BILSTM-ACRF model,
		  the model is applied to the data set of the university
		  course textbook” Da-ta Structure”, and the result
		  reaches 98.97%F1 value and 98.14%accuracy, which has good
		  experimental results and certain practical value.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {228–233},
  numpages	= {6},
  keywords	= {Knowledge graph, Named entity recognition, Natural
		  language processing, Self-attention mechanism},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '23}
}

@InProceedings{	  10.1109/ase56229.2023.00131,
  author	= {Chakraborty, Sarthak and Agarwal, Shubham and Garg, Shaddy
		  and Sethia, Abhimanyu and Pandey, Udit Narayan and
		  Aggarwal, Videh and Saini, Shiv},
  title		= {ESRO: Experience Assisted Service Reliability against
		  Outages},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00131},
  doi		= {10.1109/ASE56229.2023.00131},
  abstract	= {Modern cloud services are prone to failures due to their
		  complex architecture, making diagnosis a critical process.
		  Site Reliability Engineers (SREs) spend hours leveraging
		  multiple sources of data, including the alerts, error logs,
		  and domain expertise through past experiences to locate the
		  root cause(s). These experiences are documented as natural
		  language text in outage reports for previous outages.
		  However, utilizing the raw yet rich semi-structured
		  information in the reports systematically is
		  time-consuming. Structured information, on the other hand,
		  such as alerts that are often used during fault diagnosis,
		  is voluminous and requires expert knowledge to discern.
		  Several strategies have been proposed to use each source of
		  data separately for root cause analysis. In this work, we
		  build a diagnostic service called ESRO that recommends root
		  causes and remediation for failures by utilizing structured
		  as well as semi-structured sources of data systematically.
		  ESRO constructs a causal graph using alerts and a knowledge
		  graph using outage reports, and merges them in a novel way
		  to form a unified graph during training. A retrieval-based
		  mechanism is then used to search the unified graph and rank
		  the likely root causes and remediation techniques based on
		  the alerts fired during an outage at inference time. Not
		  only the individual alerts, but their respective importance
		  in predicting an outage group is taken into account during
		  recommendation. We evaluated our model on several cloud
		  service outages of a large SaaS enterprise over the course
		  of ~2 years, and obtained an average improvement of 27% in
		  rouge scores after comparing the likely root causes against
		  the ground truth over state-of-the-art baselines. We
		  further establish the effectiveness of ESRO through
		  qualitative analysis on multiple real outage examples.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {255–267},
  numpages	= {13},
  keywords	= {system monitoring, cloud services, causal graph, knowledge
		  graph},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@InProceedings{	  10.1145/3640912.3640993,
  author	= {He, Meng and Bai, Yunli},
  title		= {LAL-JER: Label-Aware Learning for Adaptive Joint Entity
		  and Relation Extraction with LLM data augmentation},
  year		= {2024},
  isbn		= {9798400716683},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640912.3640993},
  doi		= {10.1145/3640912.3640993},
  abstract	= {Joint entity and relation extraction has achieved great
		  improvements in Natural Language Processing (NLP) and has
		  been widely applied, such as constructing knowledge graph,
		  query understanding and question answering. Existing
		  methods usually spend long time on fitting the models on
		  certain datasets with given label type, which greatly lacks
		  the ability of generalization. The model cannot make
		  prediction on label types that have not seen in the
		  training set. To address this issue, we propose to use
		  prompt to incorporate the semantic meaning of the label
		  type description. Furthermore, we use large language model
		  to perform data augmentation to improve the robustness of
		  our model during training. Extensive experiments and
		  ablation study on two joint entity and relation extraction
		  validates the effectiveness of our work on that: 1. Our
		  methods achieved states of art performance on joint entity
		  and relation extraction benchmark based on pretrained
		  language model bert. 2. Our methods can help the model make
		  predictions on label type unseen before given prompts.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Communication Network and Machine Learning},
  pages		= {414–419},
  numpages	= {6},
  location	= {Zhengzhou, China},
  series	= {CNML '23}
}

@InProceedings{	  10.1145/3675417.3675574,
  author	= {Ji, Wei and Sun, Jian and Chen, Biaoxin and Luo,
		  Chuangli},
  title		= {The Current Status and Trends of Research on the Impact of
		  Generative Artificial Intelligence on Employment in China},
  year		= {2024},
  isbn		= {9798400717147},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3675417.3675574},
  doi		= {10.1145/3675417.3675574},
  abstract	= {With the increasing expansion of technology application
		  scenarios, generative artificial intelligence is set to
		  significantly boost global productivity and profoundly
		  impact China's employment market. This article employs 218
		  academic papers from 2013 to 2023, featured in Peking
		  University's core journals and CSSCI, focusing on
		  generative AI's impact on China's job market, for
		  quantitative analysis using CiteSpace software.
		  Multidimensional exploration of research overviews and
		  hotspots reveals that the evolving and rapid application of
		  generative AI on China's employment market becomes more and
		  more in-depth. On this basis, the paper analyzes the
		  challenges and opportunities generative AI presents to
		  China's job market and proposes corresponding strategies.},
  booktitle	= {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater
		  Bay Area International Conference on Digital Economy and
		  Artificial Intelligence},
  pages		= {948–953},
  numpages	= {6},
  location	= {Hongkong, China},
  series	= {DEAI '24}
}

@InProceedings{	  10.1145/3670474.3685952,
  author	= {Qin, Zongyue and Bai, Yunsheng and Sohrabizadeh, Atefeh
		  and Ding, Zijian and Hu, Ziniu and Sun, Yizhou and Cong,
		  Jason},
  title		= {Cross-Modality Program Representation Learning for
		  Electronic Design Automation with High-Level Synthesis},
  year		= {2024},
  isbn		= {9798400706998},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670474.3685952},
  doi		= {10.1145/3670474.3685952},
  abstract	= {In recent years, domain-specific accelerators (DSAs) have
		  gained popularity for applications such as deep learning
		  and autonomous driving. To facilitate DSA designs,
		  programmers use high-level synthesis (HLS) to compile a
		  high-level description written in C/C++ into a design with
		  low-level hardware description languages that eventually
		  synthesize DSAs on circuits. However, creating a
		  high-quality HLS design still demands significant domain
		  knowledge, particularly in microarchitecture decisions
		  expressed as pragmas. Thus, it is desirable to automate
		  such decisions with the help of machine learning for
		  predicting the quality of HLS designs, requiring a deeper
		  understanding of the program that consists of original code
		  and pragmas. Naturally, these programs can be considered as
		  sequence data. In addition, these programs can be compiled
		  and converted into a control data flow graph (CDFG). But
		  existing works either fail to leverage both modalities or
		  combine the two in shallow or coarse ways. We propose
		  ProgSG, a model that allows interaction between the source
		  code sequence modality and the graph modality in a deep and
		  fine-grained way. To alleviate the scarcity of labeled
		  designs, a pre-training method is proposed based on a suite
		  of compiler's data flow analysis tasks. Experimental
		  results show that ProgSG reduces the RMSE of design
		  performance predictions by up to 22%, and identifies
		  designs with an average of 1.10\texttimes{} and
		  1.26\texttimes{} (up to 8.17\texttimes{} and
		  13.31\texttimes{}) performance improvement in design space
		  exploration (DSE) task compared to HARP and AutoDSE,
		  respectively.},
  booktitle	= {Proceedings of the 2024 ACM/IEEE International Symposium
		  on Machine Learning for CAD},
  articleno	= {14},
  numpages	= {12},
  keywords	= {FPGA, GNN, HLS, Language Model},
  location	= {Salt Lake City, UT, USA},
  series	= {MLCAD '24}
}

@InProceedings{	  10.1145/3632410.3633291,
  author	= {Tripathi, Sandhya and King, Christopher Ryan},
  title		= {Contrastive learning: Big Data Foundations and
		  Applications},
  year		= {2024},
  isbn		= {9798400716348},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632410.3633291},
  doi		= {10.1145/3632410.3633291},
  abstract	= {Contrastive learning (CL) has exploded in popularity due
		  to its ability to learn effective representations using
		  vast quantities of unlabelled data across multiple domains.
		  CL underlies some of the most impressive applications of
		  generative AI for the general public. We will review the
		  fundamentals and applied work on contrastive learning
		  representations focusing on three main topics: 1) CL in
		  supervised, unsupervised and self-supervised setup and its
		  revival in AI research as an instance discriminator. In
		  this part, we will focus on learning about the nuts and
		  bolts, such as different augmentation techniques, loss
		  functions, performance evaluation metrics, and some
		  theoretical understanding of contrastive loss. We will also
		  present the methods supporting DALL · E 2, a popular
		  generative AI. 2) Learning contrastive representations
		  across vision, text, time series, tabular data and
		  knowledge graph modalities. Specifically, we will present
		  the literature representative of solution approaches
		  regarding new augmentation techniques, modification in the
		  loss function, and additional information. The first two
		  parts will also have small hands-on session on the
		  application shown and some of the methods learned. 3)
		  Discussing the various theoretical and empirical claims for
		  CL’s success, including the role of negative examples. We
		  will also present some work that challenges the shared
		  information assumption of CL and propose alternative
		  explanations. Finally, we will conclude with some future
		  directions and applications for CL.},
  booktitle	= {Proceedings of the 7th Joint International Conference on
		  Data Science &amp; Management of Data (11th ACM IKDD CODS
		  and 29th COMAD)},
  pages		= {493–497},
  numpages	= {5},
  keywords	= {augmentations, clustering, contrastive learning,
		  distillation, graphs, multi-modal, multi-view, noise
		  estimation loss, tabular datasets, time-series},
  location	= {Bangalore, India},
  series	= {CODS-COMAD '24}
}

@InProceedings{	  10.1145/3627673.3679085,
  author	= {Vakaj, Edlira and Mihindukulasooriya, Nandana and Gaur,
		  Manas and Khan, Arijit},
  title		= {Knowledge Graphs for Responsible AI},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679085},
  doi		= {10.1145/3627673.3679085},
  abstract	= {Responsible AI is built upon a set of principles that
		  prioritize fairness, transparency, accountability, and
		  inclusivity in AI development and deployment. As AI systems
		  become increasingly sophisticated, including the explosion
		  of generative AI, there is a growing need to address
		  ethical considerations and potential societal impacts of
		  their uses. Knowledge graphs (KGs), as structured
		  representations of information, can enhance generative AI
		  performance by providing context, explaining outputs, and
		  reducing biases, thereby offering a powerful framework to
		  address the challenges of responsible AI. By leveraging
		  semantic relationships and contextual understanding, KGs
		  facilitate transparent decision-making, enabling
		  stakeholders to trace and interpret the reasoning behind AI
		  driven outcomes. Moreover, they provide a means to capture
		  and manage diverse knowledge sources, supporting the
		  development of fair and unbiased AI models. The workshop
		  aims to investigate the role of knowledge graphs in
		  promoting responsible AI principles and creating a
		  cooperative space for researchers, practitioners, and
		  policymakers to exchange insights and enhance their
		  comprehension of KGs' impact on achieving responsible AI
		  solutions. It seeks to facilitate collaboration and
		  idea-sharing to advance the understanding of how KGs can
		  contribute to responsible AI.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5596–5598},
  numpages	= {3},
  keywords	= {bias mitigation, ethical AI, fairness, interpretability,
		  knowledge graphs, large language models, privacy,
		  responsible AI},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3627673.3680119,
  author	= {Xu, Jiejun and Tong, Hanghang and Bertozzi, Andrea},
  title		= {The 8th Workshop on Graph Techniques for Adversarial
		  Activity Analytics (GTA3 2024)},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680119},
  doi		= {10.1145/3627673.3680119},
  abstract	= {Graphs are powerful analytic tools for modeling
		  adversarial activities across a wide range of domains and
		  applications. Examples include identifying and responding
		  to cybersecurity systems' threats and vulnerabilities,
		  strengthening critical infrastructure's resilience and
		  robustness, and combating covert illicit activities that
		  span various domains like finance, communication, and
		  transportation. With the rapid development of generative
		  AI, the lifecycle and throughput of adversarial activities,
		  such as generating attacks or synthesizing deceptive
		  signals, have accelerated significantly. For instance, a
		  malicious actor can generate a large number of malware
		  variants to flood defense systems or create agents to
		  disseminate misleading signals, obscuring their activities.
		  Consequently, there is a pressing need for novel and
		  effective technology to autonomously handle these
		  adversarial activities and keep pace with the evolving
		  threats. The purpose of this workshop is to provide a forum
		  to discuss emerging research problems and novel approaches
		  in graph analysis for modeling adversarial activities in
		  the age of generative AI.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5603–5604},
  numpages	= {2},
  keywords	= {adversarial activity analytics, graph machine learning,
		  graph mining, knowledge representation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3626772.3657790,
  author	= {Wang, Duokang and Hu, Linmei and Hao, Rui and Shao,
		  Yingxia and Lv, Xin and Nie, Liqiang and Li, Juanzi},
  title		= {Let Me Show You Step by Step: An Interpretable Graph
		  Routing Network for Knowledge-based Visual Question
		  Answering},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657790},
  doi		= {10.1145/3626772.3657790},
  abstract	= {Visual Question Answering based on external Knowledge
		  Bases (KB-VQA) requires a model to incorporate knowledge
		  beyond the content of given image and question for answer
		  prediction. Most existing works made efforts on using graph
		  neural networks or Multi-modal Large Language Models to
		  incorporate external knowledge for answer generation.
		  Despite the promising results, they have limited
		  interpretability and exhibit a deficiency in handling
		  questions with unseen answers. In this paper, we propose a
		  novel interpretable graph routing network (GRN) which
		  explicitly conducts entity routing over a constructed scene
		  knowledge graph step by step for KB-VQA. At each step, GRN
		  keeps an entity score vector representing how likely of
		  each entity to be activated as the answer, and a transition
		  matrix representing the transition probability from one
		  entity to another. To answer the given question, GRN will
		  focus on certain keywords of the question at each step and
		  correspondingly conduct entity routing by transiting the
		  entity scores according to the transition matrix computed
		  referring to the focused question keywords. In this way, it
		  clearly provides the reasoning process of KB-VQA and can
		  handle the questions with unseen answers without
		  distinction. Experiments on the benchmark dataset KRVQA
		  have demonstrated that GRN improves the performance of
		  KB-VQA by a large margin, surpassing existing state-of-the
		  art KB-VQA methods and Multi-modal Large Language Models,
		  as well as shows competent capability in handling unseen
		  answers and good interpretability in KB-VQA.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1984–1994},
  numpages	= {11},
  keywords	= {graph routing network, knowledge-based visual question
		  answering, scene knowledge graph},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3664647.3680717,
  author	= {yuan, li and Cai, Yi and Huang, Junsheng},
  title		= {Few-Shot Joint Multimodal Entity-Relation Extraction via
		  Knowledge-Enhanced Cross-modal Prompt Model},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3680717},
  doi		= {10.1145/3664647.3680717},
  abstract	= {Joint Multimodal Entity-Relation Extraction (JMERE) is a
		  challenging task that aims to extract entities and their
		  relations from textimage pairs in social media posts.
		  Existing methods for JMERE require large amounts of labeled
		  data. However, gathering and annotating fine-grained
		  multimodal data for JMERE poses significant challenges.
		  Initially, we construct diverse and comprehensive
		  multimodal few-shot datasets fitted to the original data
		  distribution. To address the insufficient information in
		  the few-shot setting, we introduce the Knowledge-Enhanced
		  Cross-modal Prompt Model (KECPM) for JMERE. This method can
		  effectively address the problem of insufficient information
		  in the few-shot setting by guiding a large language model
		  to generate supplementary background knowledge. Our
		  proposed method comprises two stages: (1) a knowledge
		  ingestion stage that dynamically formulates prompts based
		  on semantic similarity guide ChatGPT generating relevant
		  knowledge and employs self-reflection to refine the
		  knowledge; (2) a knowledge-enhanced language model stage
		  that merges the auxiliary knowledge with the original input
		  and utilizes a transformerbased model to align with JMERE's
		  required output format. We extensively evaluate our
		  approach on a few-shot dataset derived from the JMERE
		  dataset, demonstrating its superiority over strong
		  baselines in terms of both micro and macro F1 scores.
		  Additionally, we present qualitative analyses and case
		  studies to elucidate the effectiveness of our model. Code
		  and Data are released at
		  https://github.com/YuanLi95/KECPM.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {8701–8710},
  numpages	= {10},
  keywords	= {few-shot learning, large language model, multimodal
		  information extraction},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Proceedings{	  10.1145/3657604,
  title		= {L@S '24: Proceedings of the Eleventh ACM Conference on
		  Learning @ Scale},
  year		= {2024},
  isbn		= {9798400706332},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to present the Proceedings of the
		  Eleventh Annual ACM Conference on Learning at Scale, L@S
		  2024, held July 18-20, 2024 at Georgia Tech in Atlanta,
		  Georgia, USA.The Learning at Scale conference was created
		  by the Association for Computing Machinery (ACM), inspired
		  by the emergence of Massive Open Online Courses (MOOCs) and
		  the accompanying shift in thinking about education. During
		  the last few years, new opportunities for scaling up
		  learning have emerged, like hybrid learning environments
		  combining online and face-to-face, and informal learning
		  enabled by all sorts of platforms (e.g., gamified language
		  learning, citizen science communities, and collaborative
		  programming communities). In the recent two years, the
		  unprecedented development of generative AI has brought
		  profound opportunities to scale the teaching and learning
		  experiences, with the goal of enhancing learning for the
		  increasingly diverse group of learners in both formal and
		  informal contexts. L@S has evolved along with these
		  emergent massive learning scenarios and opportunities and
		  is today one of the most prominent venues for discussion of
		  the highest quality of research on how learning and
		  teaching can be transformed at scale, in diverse learning
		  environments.The theme of L@S 2024 is Scaling Learning in
		  the Age of AI. Rapid advances in AI have created new
		  opportunities but also challenges for the Learning@Scale
		  community. The advances in generative AI show potential to
		  enhance pedagogical practices and the efficacy of learning
		  at scale. This has led to an unprecedented level of
		  interest in employing generative AI for scaling tutoring
		  and feedback. The prevalence of such tools calls for new
		  practices and understanding on how AI-based methods should
		  be designed and developed to enhance the experiences and
		  outcomes of teachers and learners.Learning@Scale 2024
		  solicits empirical and theoretical papers on, but not
		  limited to, the following topics (in no particular order):
		  1) Instruction at scale: studies that examine how teachers
		  and educators scale their instructions, what aspects of
		  instruction could be scaled effectively, and which of these
		  instructional strategies are the most effective for
		  learning. 2) Interventions at scale: studies that examine
		  the effects of interventions on student learning and
		  performance when implemented at scale. We welcome studies
		  that use both qualitative and quantitative methods. 3) The
		  use of generative AI to scale learning: studies that
		  investigate stakeholders' experiences with generative AI,
		  students' and teachers' interactions with generative AI,
		  and the potentials and limitations of using generative AI
		  in education. 4) Systems and tools to support learning at
		  scale: research that designs and develops systems and tools
		  to support learning at scale. For example, this involves
		  scaling learning through web-based systems, MOOCs,
		  visualization, intelligent tutoring systems, gamification,
		  immersive techniques (AR/VR/MR), mobile technologies,
		  tangible interfaces, and various other technologies. 5) The
		  evaluation of existing learning at scale systems and online
		  learning environments using but not limited to the
		  above-mentioned technologies. 6) Methods and algorithms
		  that model learner behavior: research that contributes
		  methods, algorithms, and pipelines that process large
		  student data to enhance learning at scale. 7) Scaling
		  learning in informal contexts: studies that explore how
		  people take advantage of online environments to pursue
		  their interests informally. 8) Review and synthesis of
		  existing literature related to learning at scale. 9)
		  Empirical studies and interventions that address equity,
		  trust, algorithmic transparency and explainability,
		  fairness and bias when using AI in education. 10) Research
		  that addresses accessibility in learning at scale contexts.
		  11) Design and deployment of learning at scale systems for
		  learners from underrepresented groups.},
  location	= {Atlanta, GA, USA}
}

@InProceedings{	  10.1145/3691422.3691471,
  author	= {Ma, Xiaoqian},
  title		= {The potential legal risks of artificial intelligence},
  year		= {2025},
  isbn		= {9798400717260},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691422.3691471},
  doi		= {10.1145/3691422.3691471},
  abstract	= {ChatGPT, as a typical application of generative artificial
		  intelligence, means that human society is moving towards
		  the "era of high knowledge revolution". From the
		  perspective of functionalism, generative artificial
		  intelligence will certainly have an important impact on the
		  reshaping of legal society at the level of "instrument" and
		  "Tao". However, while generative AI is deeply embedded in
		  Chinese society, it also impacts personal information
		  security on a large scale, challenges national security,
		  causes intellectual property rights disputes, and academic
		  ethics irregularities. Therefore, it is necessary for the
		  national regulatory authorities to exercise careful
		  governance, formulate accurate and fair market access
		  guidelines and mechanism accountability systems, improve
		  the public's awareness of risk prevention, and gradually
		  improve the generative AI governance system.},
  booktitle	= {Proceedings of the 2024 15th International Conference on
		  E-Business, Management and Economics},
  pages		= {366–370},
  numpages	= {5},
  keywords	= {ChatGPT, Generative artificial intelligence, Legal
		  regulation, Risk management},
  location	= { },
  series	= {ICEME '24}
}

@InProceedings{	  10.1145/3589334.3645584,
  author	= {Shi, Jingchuan and Dong, Hang and Chen, Jiaoyan and Wu,
		  Zhe and Horrocks, Ian},
  title		= {Taxonomy Completion via Implicit Concept Insertion},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645584},
  doi		= {10.1145/3589334.3645584},
  abstract	= {beginabstract High quality taxonomies play a critical role
		  in various domains such as e-commerce, web search and
		  ontology engineering. While there has been extensive work
		  on expanding taxonomies from externally mined data, there
		  has been less attention paid to enriching taxonomies by
		  exploiting existing concepts and structure within the
		  taxonomy. In this work, we show the usefulness of this kind
		  of enrichment, and explore its viability with a new
		  taxonomy completion system ICON (I mplicit CON cept
		  Insertion). ICON generates new concepts by identifying
		  implicit concepts based on the existing concept structure,
		  generating names for such concepts and inserting them in
		  appropriate positions within the taxonomy. ICON integrates
		  techniques from entity retrieval, text summary, and
		  subsumption prediction; this modular architecture offers
		  high flexibility while achieving state-of-the-art
		  performance. We have evaluated ICON on two e-commerce
		  taxonomies, and the results show that it offers significant
		  advantages over strong baselines including recent taxonomy
		  completion models and the large language model, ChatGPT.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2159–2169},
  numpages	= {11},
  keywords	= {ontology engineering, pre-trained language model, taxonomy
		  completion, taxonomy enrichment, text summarisation},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1145/3639363,
  author	= {Fan, Wenfei and Lu, Ping and Pang, Kehan and Jin, Ruochun
		  and Yu, Wenyuan},
  title		= {Linking Entities across Relations and Graphs},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {49},
  number	= {1},
  issn		= {0362-5915},
  url		= {https://doi.org/10.1145/3639363},
  doi		= {10.1145/3639363},
  abstract	= {This article proposes a notion of parametric simulation to
		  link entities across a relational database 𝒟 and a graph
		  G. Taking functions and thresholds for measuring vertex
		  closeness, path associations, and important properties as
		  parameters, parametric simulation identifies tuples t in
		  𝒟 and vertices v in G that refer to the same real-world
		  entity, based on both topological and semantic matching. We
		  develop machine learning methods to learn the parameter
		  functions and thresholds. We show that parametric
		  simulation is in quadratic-time by providing such an
		  algorithm. Moreover, we develop an incremental algorithm
		  for parametric simulation; we show that the incremental
		  algorithm is bounded relative to its batch counterpart,
		  i.e., it incurs the minimum cost for incrementalizing the
		  batch algorithm. Putting these together, we develop HER, a
		  parallel system to check whether (t, v) makes a match, find
		  all vertex matches of t in G, and compute all matches
		  across 𝒟 and G, all in quadratic-time; moreover, HER
		  supports incremental computation of these in response to
		  updates to 𝒟 and G. Using real-life and synthetic data,
		  we empirically verify that HER is accurate with F-measure
		  of 0.94 on average, and is able to scale with database 𝒟
		  and graph G for both batch and incremental computations.},
  journal	= {ACM Trans. Database Syst.},
  month		= feb,
  articleno	= {2},
  numpages	= {50},
  keywords	= {Entity resolution, Knowledge graph, Relational database,
		  Parallelization, Incremental algorithm, Relative
		  boundedness}
}

@InProceedings{	  10.1145/3648188.3675120,
  author	= {Wehnert, Sabine and Fiorelli, Manuel and Picca, Davide and
		  De Luca, Ernesto William and Stellato, Armando},
  title		= {LIRAI'24: 2nd Workshop on Legal Information Retrieval
		  meets Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400705953},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3648188.3675120},
  doi		= {10.1145/3648188.3675120},
  abstract	= {LIRAI is a workshop series on Legal Information Retrieval
		  and Legal Artificial Intelligence. It provides a forum for
		  discussing current trends and challenges in legal
		  artificial intelligence, specifically related to the
		  hypertext nature of legal documents and retrieval tasks.
		  The second edition of LIRAI focuses on three main
		  directions: explainable / justifiable artificial
		  intelligence, hybrid systems that combine formal approaches
		  and machine learning-based methods, including deep
		  learning-based methods, and finally generative artificial
		  intelligence. We call for contributions on these topics in
		  the form of short and long papers, and we aim to publish
		  them as open-access proceedings on CEUR-WS.org once
		  again.},
  booktitle	= {Proceedings of the 35th ACM Conference on Hypertext and
		  Social Media},
  pages		= {390–392},
  numpages	= {3},
  keywords	= {Explainable AI, FAIRness, Generative AI, High-Recall
		  Retrieval, Hybrid Approaches, Legal Compliance, Legal
		  Informatics, Legal Information Retrieval, Legal Knowledge
		  Representation, Legal Text Mining, Linguistic Legal Linked
		  Open Data, Semantic Web},
  location	= {Poznan, Poland},
  series	= {HT '24}
}

@InProceedings{	  10.1109/jcdl52503.2021.00014,
  author	= {Kroll, Hermann and Pirklbauer, Jan and Balke, Wolf-Tilo},
  title		= {A Toolbox for the Nearly-Unsupervised Construction of
		  Digital Library Knowledge Graphs},
  year		= {2024},
  isbn		= {9781665417709},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL52503.2021.00014},
  doi		= {10.1109/JCDL52503.2021.00014},
  abstract	= {Knowledge graphs are essential for digital libraries to
		  store entity-centric knowledge. The applications of
		  knowledge graphs range from summarizing entity information
		  over answering complex queries to inferring new knowledge.
		  Yet, building knowledge graphs means either relying on
		  manual curation or designing supervised extraction
		  processes to harvest knowledge from unstructured text.
		  Obviously, both approaches are cost-intensive. Yet, the
		  question is whether we can minimize the efforts to build a
		  knowledge graph. And indeed, we propose a toolbox that
		  provides methods to extract knowledge from arbitrary text.
		  Our toolkit bypasses the need for supervision nearly
		  completely and includes a novel algorithm to close the
		  missing gaps. As a practical demonstration, we analyze our
		  toolbox on established biomedical benchmarks. As far as we
		  know, we are the first who propose, analyze and share a
		  nearly unsupervised and complete toolbox for building
		  knowledge graphs from text.},
  booktitle	= {Proceedings of the 2021 ACM/IEEE Joint Conference on
		  Digital Libraries},
  pages		= {21–30},
  numpages	= {10},
  keywords	= {knowledge graph, information extraction, digital library},
  location	= {Virtual Event},
  series	= {JCDL '21}
}

@InProceedings{	  10.1145/3652037.3652072,
  author	= {Tsampos, Ioannis and Marakakis, Emmanouil},
  title		= {Querying Knowledge Graphs in Greek Language},
  year		= {2024},
  isbn		= {9798400717604},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652037.3652072},
  doi		= {10.1145/3652037.3652072},
  abstract	= {We present a method for querying knowledge graphs through
		  natural language, emphasizing its application to the Greek
		  language. It integrates NLP techniques with the
		  capabilities of graph databases to enable seamless
		  interaction with knowledge graphs through natural language
		  queries. Upon receiving a user's question in Greek
		  language, we use linguistic analysis tools to convert it
		  into a graph structure by employing predefined rules and
		  adhering to a graph database schema. This methodology
		  enables the handling of different question types and the
		  efficient extraction of relations, ensuring accurate
		  mapping of linguistic structures to database queries. The
		  representation of a question as a knowledge graph enables
		  its direct translation to a Cypher query, facilitating the
		  extraction of related answers. The approach can handle
		  complex questions and is language independent.},
  booktitle	= {Proceedings of the 17th International Conference on
		  PErvasive Technologies Related to Assistive Environments},
  pages		= {27–33},
  numpages	= {7},
  keywords	= {Graph Databases, Greek Language, Knowledge Graph
		  Construction, Knowledge Representation, Natural Language
		  Processing, Query languages, Question Answering},
  location	= {Crete, Greece},
  series	= {PETRA '24}
}

@InProceedings{	  10.1145/3627673.3679845,
  author	= {Guan, Saiping and Wei, Jiyao and Jin, Xiaolong and Guo,
		  Jiafeng and Cheng, Xueqi},
  title		= {Look Globally and Reason: Two-stage Path Reasoning over
		  Sparse Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679845},
  doi		= {10.1145/3627673.3679845},
  abstract	= {Sparse Knowledge Graphs (KGs), frequently encountered in
		  real-world applications, contain fewer facts in the form of
		  (head entity, relation, tail entity) compared to more
		  populated KGs. The sparse KG completion task, which reasons
		  answers for given queries in the form of (head entity,
		  relation, ?) for sparse KGs, is particularly challenging
		  due to the necessity of reasoning missing facts based on
		  limited facts. Path-based models, known for excellent
		  explainability, are often employed for this task. However,
		  existing path-based models typically rely on external
		  models to fill in missing facts and subsequently perform
		  path reasoning. This approach introduces unexplainable
		  factors or necessitates meticulous rule design. In light of
		  this, this paper proposes an alternative approach by
		  looking inward instead of seeking external assistance. We
		  introduce a two-stage path reasoning model called LoGRe
		  (Look Globally and Reason) over sparse KGs. LoGRe
		  constructs a relation-path reasoning schema by globally
		  analyzing the training data to alleviate the sparseness
		  problem. Based on this schema, LoGRe then aggregates paths
		  to reason out answers. Experimental results on five
		  benchmark sparse KG datasets demonstrate the effectiveness
		  of the proposed LoGRe model.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {695–705},
  numpages	= {11},
  keywords	= {path reasoning, reasoning schema, sparse knowledge graph,
		  sparse knowledge graph completion},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3589334.3648152,
  author	= {Shang, Lanyu and Zhang, Yang and Chen, Bozhang and Zong,
		  Ruohan and Yue, Zhenrui and Zeng, Huimin and Wei, Na and
		  Wang, Dong},
  title		= {MMAdapt: A Knowledge-guided Multi-source Multi-class
		  Domain Adaptive Framework for Early Health Misinformation
		  Detection},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3648152},
  doi		= {10.1145/3589334.3648152},
  abstract	= {This paper studies a critical problem of emergent health
		  misinformation detection, aiming to mitigate the spread of
		  misinformation in emergent health domains to support
		  well-informed healthcare decisions towards a Web for good
		  health. Our work is motivated by the lack of timely
		  resources (e.g., medical knowledge, annotated data) during
		  the initial phases of an emergent health event or topic. In
		  this paper, we develop a multi-source domain adaptive
		  framework that jointly exploits medical knowledge and
		  annotated data from different high-resource source domains
		  (e.g., cancer, COVID-19) to detect misleading posts in an
		  emergent target domain (e.g., mpox, polio). Two important
		  challenges exist in developing our solution: 1) how to
		  accurately detect the partially misleading and unverifiable
		  content in an emergent target domain? 2) How to identify
		  the conflicting knowledge facts from different source
		  domains to accurately detect emergent misinformation in the
		  target domain? To address these challenges, we develop
		  MMAdapt, a multi-source multi-class domain adaptive
		  misinformation detection framework that effectively
		  explores diverse knowledge facts from different source
		  domains to accurately detect not only the outright
		  misleading but also the partially misleading or
		  unverifiable posts on the Web. Extensive experimental
		  results on four real-world misinformation datasets
		  demonstrate that MMAdapt substantially outperforms
		  state-of-the-art baselines in accurately detecting
		  misinformation in an emergent health domain.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {4653–4663},
  numpages	= {11},
  keywords	= {domain adaptation, healthcare misinformation, knowledge
		  graph, multiclass classification},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3649158.3657036,
  author	= {Ahmed, Mohiuddin and Wei, Jinpeng and Al-Shaer, Ehab},
  title		= {Prompting LLM to Enforce and Validate CIS Critical
		  Security Control},
  year		= {2024},
  isbn		= {9798400704918},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3649158.3657036},
  doi		= {10.1145/3649158.3657036},
  abstract	= {Proper security control enforcement reduces the attack
		  surface and protects the organizations against attacks.
		  Organizations like NIST and CIS (Center for Internet
		  Security) provide critical security controls (CSCs) as a
		  guideline to enforce cyber security. Automated enforcement
		  and measurability mechanisms for these CSCs still need to
		  be developed. Analyzing the implementations of security
		  products to validate security control enforcement is
		  non-trivial. Moreover, manually analyzing and developing
		  measures and metrics to monitor, and implementing those
		  monitoring mechanisms are resource-intensive tasks and
		  massively dependent on the security analyst's expertise and
		  knowledge. To tackle those problems, we use large language
		  models (LLMs) as a knowledge base and reasoner to extract
		  measures, metrics, and monitoring mechanism implementation
		  steps from security control descriptions to reduce the
		  dependency on security analysts. Our approach used few-shot
		  learning with chain-of-thought (CoT) prompting to generate
		  measures and metrics and generated knowledge prompting for
		  metrics implementation. Our evaluation shows that prompt
		  engineering to extract measures, metrics, and monitoring
		  implementation mechanisms can reduce dependency on humans
		  and semi-automate the extraction process. We also
		  demonstrate metric implementation steps using generated
		  knowledge prompting with LLMs.},
  booktitle	= {Proceedings of the 29th ACM Symposium on Access Control
		  Models and Technologies},
  pages		= {93–104},
  numpages	= {12},
  keywords	= {account management., critical security control, llm,
		  prompt engineering},
  location	= {San Antonio, TX, USA},
  series	= {SACMAT 2024}
}

@InProceedings{	  10.1145/3653081.3653117,
  author	= {Xing, Xueyang and Jia, Bo and Huang, Zhicheng and Chen,
		  Yongzhi and Wang, Junjie and Fan, Anfei and Chen, Xin and
		  Cao, Lei},
  title		= {A fusion inference method for large language models and
		  knowledge graphs based on structured injection and causal
		  inference},
  year		= {2024},
  isbn		= {9798400716485},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3653081.3653117},
  doi		= {10.1145/3653081.3653117},
  abstract	= {In this paper, we propose a large language model and
		  knowledge graph fusion reasoning method based on structured
		  injection and causal reasoning (LKFSC) to address the
		  limitations of existing large language models and knowledge
		  graphs in practical applications. The approach effectively
		  mitigates the problems of long-distance dependency and
		  limited contextual information, and improves the reasoning
		  capability of the large language model. Meanwhile, by
		  fusing the generative ability of the large language model
		  and the inference ability of the knowledge graph, the
		  method realizes intelligent reasoning for complex problems.
		  The main contributions of this paper include proposing a
		  structured injection method that introduces causality for
		  reasoning, and constructing a fusion reasoning framework
		  that effectively mitigates the illusory problem of large
		  language models and provides powerful and intelligent
		  decision support for practical applications.},
  booktitle	= {Proceedings of the 2023 5th International Conference on
		  Internet of Things, Automation and Artificial
		  Intelligence},
  pages		= {208–213},
  numpages	= {6},
  location	= {Nanchang, China},
  series	= {IoTAAI '23}
}

@InProceedings{	  10.1145/3641825.3687716,
  author	= {Christiansen, Frederik Roland and Hollensberg, Linus
		  N\o{}rgaard and Jensen, Niko Bach and Julsgaard, Kristian
		  and Jespersen, Kristian Nyborg and Nikolov, Ivan},
  title		= {Exploring Presence in Interactions with LLM-Driven NPCs: A
		  Comparative Study of Speech Recognition and Dialogue
		  Options},
  year		= {2024},
  isbn		= {9798400705359},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3641825.3687716},
  doi		= {10.1145/3641825.3687716},
  abstract	= {Combining modern technologies like large-language models
		  (LLMs), speech-to-text, and text-to-speech can enhance
		  immersion in virtual reality (VR) environments. However,
		  challenges exist in effectively implementing LLMs and
		  educating users. This paper explores implementing
		  LLM-powered virtual social actors and facilitating user
		  communication. We developed a murder mystery game where
		  users interact with LLM-based non-playable characters
		  (NPCs) through interrogation, clue-gathering, and
		  exploration. Two versions were tested: one using speech
		  recognition and another with traditional dialog boxes.
		  While both provided similar social presence, users felt
		  more immersed with speech recognition but found it
		  overwhelming, while the dialog version was more
		  challenging. Slow NPC response times were a source of
		  frustration, highlighting the need for faster generation or
		  better masking for a seamless experience.},
  booktitle	= {Proceedings of the 30th ACM Symposium on Virtual Reality
		  Software and Technology},
  articleno	= {6},
  numpages	= {11},
  keywords	= {Immersive systems, Large Language Models (LLM), NPC,
		  Presence, Social Actors, Speech Recognition, VR},
  location	= {Trier, Germany},
  series	= {VRST '24}
}

@InProceedings{	  10.1145/3694811.3697819,
  author	= {Alfasi, Daniel and Shapira, Tal and Bremler-Barr, Anat},
  title		= {VulnScopper: Unveiling Hidden Links Between Unseen
		  Security Entities},
  year		= {2024},
  isbn		= {9798400712548},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3694811.3697819},
  doi		= {10.1145/3694811.3697819},
  abstract	= {The Common Vulnerabilities and Exposures (CVE) system is
		  crucial for cybersecurity, providing standardized
		  identification of vulnerabilities. In February 2024, the
		  National Vulnerability Database (NVD) announced it could no
		  longer enrich new CVEs due to increasing volumes,
		  significantly impacting global security efforts. This paper
		  introduces VulnScopper, an innovative approach to automate
		  and enhance vulnerability enrichment using Graph Neural
		  Networks (GNNs). VulnScopper combines Knowledge Graphs (KG)
		  with Natural Language Processing (NLP) by leveraging ULTRA,
		  a GNN-based knowledge graph foundation model, alongside a
		  Large Language Model (LLM). VulnScopper's inductive
		  approach enables it to handle unseen entities, overcoming a
		  crucial limitation of previous CVE enrichment methods. We
		  evaluate VulnScopper on the NVD dataset in inductive and
		  transductive setups for CVE to Common Platform Enumerations
		  (CPE) linking. Our results show that VulnScopper
		  outperforms state-of-the-art techniques, achieving up to
		  60% Hits@10 accuracy in linking CVEs to CPE on unseen CVE
		  records. We demonstrate VulnScopper's effectiveness on
		  unseen 2023 CVEs, showcasing its ability to uncover new
		  vulnerable products and potentially reduce vulnerability
		  remediation time.},
  booktitle	= {Proceedings of the 3rd GNNet Workshop on Graph Neural
		  Networking Workshop},
  pages		= {33–40},
  numpages	= {8},
  keywords	= {cpe, cve, cwe, cybersecurity, graph neural networks (gnn),
		  knowledge graphs, large language models (llm), link
		  prediction, vulnerabilities},
  location	= {Los Angeles, CA, USA},
  series	= {GNNet '24}
}

@InProceedings{	  10.1145/3589334.3645594,
  author	= {Pan, Yudai and Liu, Jun and Zhao, Tianzhe and Zhang,
		  Lingling and Lin, Yun and Dong, Jin Song},
  title		= {A Symbolic Rule Integration Framework with Logic
		  Transformer for Inductive Relation Prediction},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645594},
  doi		= {10.1145/3589334.3645594},
  abstract	= {Relation prediction in knowledge graphs (KGs) aims at
		  predicting missing relations in incomplete triples, whereas
		  the dominant paradigm by KG embeddings has a limitation to
		  predict the relation between unseen entities. This
		  situation is called an inductive setting, which is more
		  common in the real-world scenario. To handle this issue,
		  implicit symbolic rules have shown great potential in
		  capturing the inductive capability. However, it is still
		  challenging to obtain precise representations of logic
		  rules from KGs. The argument variability and predicate
		  non-commutativity in symbolic rule integration make the
		  modeling of component symbols difficult. To this end, we
		  propose a novel inductive relation prediction model named
		  SymRITa with a logic transformer integrating rules. SymRITa
		  firstly extracts the subgraph, whose embeddings are
		  captured by a graph network. Meanwhile, symbolic rule
		  graphs in the subgraph can be generated. Then, the symbolic
		  rules are modeled by a proposed logic transformer.
		  Specifically, the input format based on the subgraph-based
		  embeddings is to focus on the argument variability in
		  symbolic rules. In addition, a conjunction attention
		  mechanism in the logic transformer can resolve predicate
		  non-commutativity in the symbolic rule integration process.
		  Finally, the subgraph-based and symbol-based embeddings
		  obtained from the previous steps are combined for the
		  training regime, and prediction results as well as rules
		  explaining the reasoning process are explicitly output.
		  Extensive experiments on twelve inductive datasets show
		  that SymRITa achieves outstanding effectiveness compared to
		  state-of-the-art inductive baselines. Moreover, the logic
		  rules with corresponding confidences provide an
		  interpretable paradigm.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2181–2192},
  numpages	= {12},
  keywords	= {first-order logic, inductive relation prediction,
		  knowledge graph, logic transformer},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3616855.3638207,
  author	= {Dong, Xin Luna},
  title		= {The Journey to A Knowledgeable Assistant with
		  Retrieval-Augmented Generation (RAG)},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3638207},
  doi		= {10.1145/3616855.3638207},
  abstract	= {Large Language Models (LLMs) have demonstrated strong
		  capabilities in comprehending and generating human
		  language, as well as emerging abilities like reasoning and
		  using tools. These advancements have been revolutionizing
		  techniques in every front, including the development of
		  personal assistants. However, their inherent limitations
		  such as lack of factuality and hallucinations make LLMs
		  less suitable for creating knowledgeable and trustworthy
		  assistants. In this talk, we describe our journey in
		  building a knowledgeable AI assistant by harnessing LLM
		  techniques. We start with a comprehensive set of
		  experiments designed to answer the questions of em how
		  reliable are LLMs on answering factual questions and em how
		  the performance differs across different types of factual
		  knowledge. Subsequently, we constructed a em federated
		  Retrieval-Augmented Generation (RAG) system that integrates
		  external information from both the web and knowledge graphs
		  in text generation. This system supports conversation
		  functionality for the Ray-ban Meta smart glasses, providing
		  trustworthy information on real-time topics like stocks and
		  sports, and information on torso-to-tail entities such as
		  local restaurants. Additionally, we are exploring the
		  potential of external knowledge to facilitate multi-modal
		  Q&amp;A. We will share our techniques, our findings, and
		  the path forward in this talk.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {4},
  numpages	= {1},
  keywords	= {ai assistant, large language model, retrieval-augmented
		  generation (rag)},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@Article{	  10.1145/3650041,
  author	= {Sekuli\'{c}, Ivan and Alinannejadi, Mohammad and Crestani,
		  Fabio},
  title		= {Analysing Utterances in LLM-Based User Simulation for
		  Conversational Search},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {3},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3650041},
  doi		= {10.1145/3650041},
  abstract	= {Clarifying underlying user information needs by asking
		  clarifying questions is an important feature of modern
		  conversational search systems. However, evaluation of such
		  systems through answering prompted clarifying questions
		  requires significant human effort, which can be
		  time-consuming and expensive. In our recent work, we
		  proposed an approach to tackle these issues with a user
		  simulator, USi. Given a description of an information need,
		  USi is capable of automatically answering clarifying
		  questions about the topic throughout the search session.
		  However, while the answers generated by USi are both in
		  line with the underlying information need and in natural
		  language, a deeper understanding of such utterances is
		  lacking. Thus, in this work, we explore utterance
		  formulation of large language model (LLM)–based user
		  simulators. To this end, we first analyze the differences
		  between USi, based on GPT-2, and the next generation of
		  generative LLMs, such as GPT-3. Then, to gain a deeper
		  understanding of LLM-based utterance generation, we compare
		  the generated answers to the recently proposed set of
		  patterns of human-based query reformulations. Finally, we
		  discuss potential applications as well as limitations of
		  LLM-based user simulators and outline promising directions
		  for future work on the topic.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= may,
  articleno	= {62},
  numpages	= {22},
  keywords	= {User simulation, conversational search, mixed-initiative}
}

@Article{	  10.1145/3659610,
  author	= {Garcia, Kimberly and Vontobel, Jonathan and Mayer, Simon},
  title		= {A Digital Companion Architecture for Ambient
		  Intelligence},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {2},
  url		= {https://doi.org/10.1145/3659610},
  doi		= {10.1145/3659610},
  abstract	= {Ambient Intelligence (AmI) focuses on creating
		  environments capable of proactively and transparently
		  adapting to users and their activities. Traditionally, AmI
		  focused on the availability of computational devices, the
		  pervasiveness of networked environments, and means to
		  interact with users. In this paper, we propose a renewed
		  AmI architecture that takes into account current
		  technological advancements while focusing on proactive
		  adaptation for assisting and protecting users. This
		  architecture consist of four phases: Perceive, Interpret,
		  Decide, and Interact. The AmI systems we propose, called
		  Digital Companions (DC), can be embodied in a variety of
		  ways (e.g., through physical robots or virtual agents) and
		  are structured according to these phases to assist and
		  protect their users. We further categorize DCs into Expert
		  DCs and Personal DCs, and show that this induces a
		  favorable separation of concerns in AmI systems, where user
		  concerns (including personal user data and preferences) are
		  handled by Personal DCs and environment concerns (including
		  interfacing with environmental artifacts) are assigned to
		  Expert DCs; this separation has favorable privacy
		  implications as well. Herein, we introduce this
		  architecture and validate it through a prototype in an
		  industrial scenario where robots and humans collaborate to
		  perform a task.},
  journal	= {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month		= may,
  articleno	= {66},
  numpages	= {26},
  keywords	= {ambient intelligence, architecture, connected devices,
		  digital companion systems, industrial environments,
		  knowledge graph, mixed reality, scene graph generation
		  algorithm}
}

@Article{	  10.1145/3641286,
  author	= {Lu, Kezhi and Zhang, Qian and Hughes, Danny and Zhang,
		  Guangquan and Lu, Jie},
  title		= {AMT-CDR: A Deep Adversarial Multi-Channel Transfer Network
		  for Cross-Domain Recommendation},
  year		= {2024},
  issue_date	= {August 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {4},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3641286},
  doi		= {10.1145/3641286},
  abstract	= {Recommender systems are one of the most successful
		  applications of using AI for providing personalized
		  e-services to customers. However, data sparsity is
		  presenting enormous challenges that are hindering the
		  further development of advanced recommender systems.
		  Although cross-domain recommendation partly overcomes data
		  sparsity by transferring knowledge from a source domain
		  with relatively dense data to augment data in the target
		  domain, the current methods do not handle heterogeneous
		  data very well. For example, using today’s cross-domain
		  transfer learning schemes with data comprising clicks,
		  ratings, user reviews, item metadata, and knowledge graphs
		  will likely result in a poorly performing model. User
		  preferences will not be comprehensively profiled, and
		  accurate recommendations will not be generated. To solve
		  these three challenges—handling heterogeneous data,
		  avoiding negative transfer, and dealing with data
		  sparsity—we designed a new end-to-end deep Adversarial
		  Multi-channel Transfer network for Cross-Domain
		  Recommendation named AMT-CDR. Heterogeneous data is handled
		  by constructing a cross-domain graph based on real-world
		  knowledge graphs—we used Freebase and YAGO. Negative
		  transfer is prevented through an adversarial learning
		  strategy that maintains consistency across the different
		  data channels. Data sparsity is addressed with an
		  end-to-end neural network that considers data across
		  multiple channels and generates accurate recommendations by
		  leveraging knowledge from both the source and target
		  domains. Extensive experiments on three dual-target
		  cross-domain recommendation tasks demonstrate the
		  superiority of AMT-CDR compared to eight state-of-the-art
		  methods. All source code is available at .},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= jul,
  articleno	= {87},
  numpages	= {26},
  keywords	= {Recommender systems, cross-domain recommender systems,
		  semantic representations, knowledge graph, knowledge
		  transfer}
}

@InProceedings{	  10.1145/3627673.3679077,
  author	= {Wallace, Joseph and Dogra, Tushar and Qiao, Wei and Wang,
		  Yuan},
  title		= {Advertiser Content Understanding via LLMs for Google Ads
		  Safety},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679077},
  doi		= {10.1145/3627673.3679077},
  abstract	= {Ads Content Safety at Google requires classifying billions
		  of ads for Google Ads content policies. Consistent and
		  accurate policy enforcement is important for advertiser
		  experience and user safety and it is a challenging problem,
		  so there is a lot of value for improving it for advertisers
		  and users. Inconsistent policy enforcement causes increased
		  policy friction and poor experience with good advertisers,
		  and bad advertisers exploit the inconsistency by creating
		  multiple similar ads in the hope that some will get through
		  our defenses. This study proposes a method to understand
		  advertiser's intent for content policy violations, using
		  Large Language Models (LLMs). We focus on identifying good
		  advertisers to reduce content over-flagging and improve
		  advertiser experience, though the approach can easily be
		  extended to classify bad advertisers too. We generate
		  advertiser's content profile based on multiple signals from
		  their ads, domains, targeting info, etc. We then use LLMs
		  to classify the advertiser content profile, along with
		  relying on any knowledge the LLM has of the advertiser,
		  their products or brand, to understand whether they are
		  likely to violate a certain policy or not. After minimal
		  prompt tuning our method was able to reach 95% accuracy on
		  a small test set.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5566–5567},
  numpages	= {2},
  keywords	= {content moderation, content understanding, large language
		  model},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3642979.3642995,
  author	= {B\'{e}n\'{e}dict, Gabriel and Zhang, Ruqing and Metzler,
		  Donald and Yates, Andrew and Deffayet, Romain and Hager,
		  Philipp and Jullien, Sami},
  title		= {Report on the 1st Workshop on Generative Information
		  Retrieval (Gen-IR 2023) at SIGIR 2023},
  year		= {2024},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {2},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3642979.3642995},
  doi		= {10.1145/3642979.3642995},
  abstract	= {The first edition of the workshop on Generative
		  Information Retrieval (Gen-IR 2023) took place in July 2023
		  in a hybrid fashion, co-located with the ACM SIGIR
		  Conference 2023 in Taipei (SIGIR 2023). The aim was to
		  bring information retrieval researchers together around the
		  topic of generative AI that gathered attention in 2022 and
		  2023 with large language models and diffusion models. Given
		  the novelty of the topic, the workshop was focused around
		  multi-sided discussions, namely panels and poster sessions
		  of the accepted proceedings papers. Two main research
		  outcomes are the proceedings of the workshop1 and the
		  potential research directions discussed in this
		  report.Date: 27 July 2023.Website:
		  https://coda.io/@sigir/gen-ir.},
  journal	= {SIGIR Forum},
  month		= jan,
  articleno	= {13},
  numpages	= {23}
}

@InProceedings{	  10.1145/3637528.3671515,
  author	= {Cao, Lele and von Ehrenheim, Vilhelm and Granroth-Wilding,
		  Mark and Anselmo Stahl, Richard and McCornack, Andrew and
		  Catovic, Armin and Cavalcanti Rocha, Dhiana Deva},
  title		= {CompanyKG: A Large-Scale Heterogeneous Graph for Company
		  Similarity Quantification},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671515},
  doi		= {10.1145/3637528.3671515},
  abstract	= {This paper presents CompanyKG (version 2), a large-scale
		  heterogeneous graph developed for fine-grained company
		  similarity quantification and relationship prediction,
		  crucial for applications in the investment industry such as
		  market mapping, competitor analysis, and mergers and
		  acquisitions. CompanyKG comprises 1.17 million companies
		  represented as graph nodes, enriched with company
		  description embeddings, and 51.06 million weighted edges
		  denoting 15 distinct inter-company relations. To facilitate
		  a thorough evaluation of methods for company similarity
		  quantification and relationship prediction, we have created
		  four annotated evaluation tasks: similarity prediction,
		  competitor retrieval, similarity ranking, and edge
		  prediction. We offer extensive benchmarking results for 11
		  reproducible predictive methods, categorized into three
		  groups: node-only, edge-only, and node+edge. To our
		  knowledge, CompanyKG is the first large-scale heterogeneous
		  graph dataset derived from a real-world investment
		  platform, specifically tailored for quantifying
		  inter-company similarity and relationships.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4816–4827},
  numpages	= {12},
  keywords	= {benchmark, company similarity quantification, edge
		  prediction, graph neural network, investment, knowledge
		  graph, private equity},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3687273.3687288,
  author	= {Azzopardi, Leif and Clarke, Charles L. A. and Kantor, Paul
		  and Mitra, Bhaskar and Trippas, Johanne R. and Ren,
		  Zhaochun and Aliannejadi, Mohammad and Arabzadeh, Negar and
		  Chandrasekar, Raman and de Rijke, Maarten and Eustratiadis,
		  Panagiotis and Hersh, William and Huang, Jin and Kanoulas,
		  Evangelos and Kareem, Jasmin and Li, Yongkang and Lupart,
		  Simon and Mekonnen, Kidist Amde and Roegiest, Adam and
		  Soboroff, Ian and Silvestri, Fabrizio and Verberne, Suzan
		  and Vos, David and Yang, Eugene and Zhao, Yuyue},
  title		= {Report on the Search Futures Workshop at ECIR 2024},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {58},
  number	= {1},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3687273.3687288},
  doi		= {10.1145/3687273.3687288},
  abstract	= {The First Search Futures Workshop, in conjunction with the
		  Fourty-sixth European Conference on Information Retrieval
		  (ECIR) 2024, looked into the future of search to ask
		  questions such as:• How can we harness the power of
		  generative AI to enhance, improve and re-imagine
		  Information Retrieval (IR)?• What are the principles and
		  fundamental rights that the field of Information Retrieval
		  should strive to uphold?• How can we build trustworthy IR
		  systems in light of Large Language Models and their ability
		  to generate content at super human speeds?• What new
		  applications and affordances does generative AI offer and
		  enable, and can we go back to the future, and do what we
		  only dreamed of previously?The workshop started with
		  seventeen lightning talks from a diverse set speakers.
		  Instead of conventional paper presentations, the lightning
		  talks provided a rapid and concise overview of ideas,
		  allowing speakers to share critical points or novel
		  concepts quickly. This format was designed to encourage
		  discussion and introduce a wide range of topics within a
		  short period, thereby maximising the exchange of ideas and
		  ensuring that participants could gain insights into various
		  future search areas without the deep dive typically
		  required in longer presentations. This report, co-authored
		  by the workshop's organisers and its participants,
		  summarises the talks and discussions. This report aims to
		  provide the broader IR community with the insights and
		  ideas discussed and debated during the workshop - and to
		  provide a platform for future discussion.Date: 24 March
		  2024.Website: https://searchfutures.github.io/.},
  journal	= {SIGIR Forum},
  month		= aug,
  pages		= {1–41},
  numpages	= {41}
}

@InProceedings{	  10.1145/3637528.3672354,
  author	= {Zhang, Fanjin and Shi, Shijie and Zhu, Yifan and Chen, Bo
		  and Cen, Yukuo and Yu, Jifan and Chen, Yelin and Wang, Lulu
		  and Zhao, Qingfei and Cheng, Yuqing and Han, Tianyi and An,
		  Yuwei and Zhang, Dan and Tam, Weng Lam and Cao, Kun and
		  Pang, Yunhe and Guan, Xinyu and Yuan, Huihui and Song, Jian
		  and Li, Xiaoyan and Dong, Yuxiao and Tang, Jie},
  title		= {OAG-Bench: A Human-Curated Benchmark for Academic Graph
		  Mining},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3672354},
  doi		= {10.1145/3637528.3672354},
  abstract	= {With the rapid proliferation of scientific literature,
		  versatile academic knowledge services increasingly rely on
		  comprehensive academic graph mining. Despite the
		  availability of public academic graphs, benchmarks, and
		  datasets, these resources often fall short in multi-aspect
		  and fine-grained annotations, are constrained to specific
		  task types and domains, or lack underlying real academic
		  graphs. In this paper, we present OAG-Bench, a
		  comprehensive, multi-aspect, and fine-grained human-curated
		  benchmark based on the Open Academic Graph (OAG). OAG-Bench
		  covers 10 tasks, 20 datasets, 70+ baselines, and 120+
		  experimental results to date. We propose new data
		  annotation strategies for certain tasks and offer a suite
		  of data pre-processing codes, algorithm implementations,
		  and standardized evaluation protocols to facilitate
		  academic graph mining. Extensive experiments reveal that
		  even advanced algorithms like large language models (LLMs)
		  encounter difficulties in addressing key challenges in
		  certain tasks, such as paper source tracing and scholar
		  profiling. We also introduce the Open Academic Graph
		  Challenge (OAG-Challenge) to encourage community input and
		  sharing. We envisage that OAG-Bench can serve as a common
		  ground for the community to evaluate and compare algorithms
		  in academic graph mining, thereby accelerating algorithm
		  development and advancement in this field. OAG-Bench is
		  accessible at https://www.aminer.cn/data/.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6214–6225},
  numpages	= {12},
  keywords	= {academic graph mining, academic knowledge graph,
		  benchmark},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3640457.3688104,
  author	= {Xi, Yunjia and Liu, Weiwen and Lin, Jianghao and Cai,
		  Xiaoling and Zhu, Hong and Zhu, Jieming and Chen, Bo and
		  Tang, Ruiming and Zhang, Weinan and Yu, Yong},
  title		= {Towards Open-World Recommendation with Knowledge
		  Augmentation from Large Language Models},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688104},
  doi		= {10.1145/3640457.3688104},
  abstract	= {Recommender system plays a vital role in various online
		  services. However, its insulated nature of training and
		  deploying separately within a specific closed domain limits
		  its access to open-world knowledge. Recently, the emergence
		  of large language models (LLMs) has shown promise in
		  bridging this gap by encoding extensive world knowledge and
		  demonstrating reasoning capabilities. Nevertheless,
		  previous attempts to directly use LLMs as recommenders
		  cannot meet the inference latency demand of industrial
		  recommender systems. In this work, we propose an Open-World
		  Knowledge Augmented Recommendation Framework with Large
		  Language Models, dubbed KAR, to acquire two types of
		  external knowledge from LLMs — the reasoning knowledge on
		  user preferences and the factual knowledge on items. We
		  introduce factorization prompting to elicit accurate
		  reasoning on user preferences. The generated reasoning and
		  factual knowledge are effectively transformed and condensed
		  into augmented vectors by a hybrid-expert adaptor in order
		  to be compatible with the recommendation task. The obtained
		  vectors can then be directly used to enhance the
		  performance of any recommendation model. We also ensure
		  efficient inference by preprocessing and prestoring the
		  knowledge from the LLM. Extensive experiments show that KAR
		  significantly outperforms the state-of-the-art baselines
		  and is compatible with a wide range of recommendation
		  algorithms. We deploy KAR to Huawei’s news and music
		  recommendation platforms and gain a 7% and 1.7% improvement
		  in the online A/B test, respectively.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {12–22},
  numpages	= {11},
  keywords	= {Knowledge Augmentation, Large Language Model, Recommender
		  System},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@Article{	  10.1145/3705314,
  author	= {Mai, Cheng-Cheng and Chen, Yu and Gong, Ziyu and Wang,
		  Hanxiang and Qiu, Mengchuan and Yuan, Chunfeng and Huang,
		  Yihua},
  title		= {PromptCNER: A Segmentation-based Method for Few-shot
		  Chinese NER with Prompt-tuning},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3705314},
  doi		= {10.1145/3705314},
  abstract	= {Recognizing Chinese entities in low-resource settings is a
		  challenging but promising task, which extracts structured
		  pre-defined entities and corresponding types from
		  unstructured text. Compared with the prosperous Named
		  Entity Recognition (NER) methods for Indo-European
		  languages, such as English, the research on Chinese NER is
		  still in its infancy. The main obstacles to the development
		  of Chinese NER methods include the ambiguity of Chinese
		  entity boundary recognition and limited data resources. To
		  address these issues, in this paper, a
		  word-segmentation-based model is present for few-shot
		  Chinese NER. First, we enumerate all possible candidate
		  entity spans on the character level for accurate entity
		  boundary identification with the proposed word segmentation
		  and combination strategy. Then, one kind of
		  question-answer-based prompt template loaded with the
		  candidate entity spans is proposed to cast entity
		  extraction into the masked token prediction task, for
		  dealing with the low-data problem by taking full advantage
		  of the generality and transferability of the pre-trained
		  language model. The extensive experimental results show
		  that our method outperforms the state-of-the-art baselines
		  in low-data settings and also achieves comparable
		  performance in full-data settings.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  keywords	= {Chinese Named Entity Recognition, Word Segmentation,
		  Prompt-tuning, Pre-training Language Model, Few-shot
		  Learning}
}

@InProceedings{	  10.1145/3647444.3647910,
  author	= {Rawat, Swati and Mittal, Sumit and Nehra, Deepa and
		  Sharma, Chandani and Kamboj, Dalip},
  title		= {Exploring the Potential of ChatGPT to improve experiential
		  learning in Education},
  year		= {2024},
  isbn		= {9798400709418},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3647444.3647910},
  doi		= {10.1145/3647444.3647910},
  abstract	= {Artificial Intelligence (AI) is revolutionizing the field
		  of education by offering new possibilities for personalized
		  and experiential learning along with data-driven insights.
		  The advancement in AI to Generative Artificial Intelligence
		  (GAI) has made the tables turn notably in the field of
		  education. Generative AI application tool ChatGPT is
		  emerging as a game changer offering personalized learning
		  experiences by analyzing huge amounts of student data,
		  generating study materials and pacing to individual needs.
		  Intelligent educational tools powered by AI provide
		  personalized guidance and feedback, adapting to curriculum
		  to address knowledge gaps. GAI also automates the grading
		  process, providing instant feedback and relieving teachers
		  for qualitative assessments. This research paper offers a
		  thorough examination of the potential uses, advantages,
		  difficulties, and moral issues related to implementing
		  ChatGPT in educational contexts. The authors closely
		  analyze how ChatGPT can improve educational experiences,
		  assist personalized learning, and encourage student-
		  teacher interaction, while exploring the drawbacks of using
		  generative AI models in education, such as concerns about
		  bias, data privacy, and over-reliance on technology. This
		  research article intends to offer educators &amp;
		  academicians useful insights into the usage of ChatGPT in
		  the educational field through a critical analysis of the
		  existing literature and real- world experiences.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Information Management &amp; Machine Intelligence},
  articleno	= {83},
  numpages	= {8},
  keywords	= {ChatGPT, Generative Artificial Intelligence (GAI), Natural
		  Language Processing, OpenAI, Teaching &amp; Learning,
		  Education},
  location	= {Jaipur, India},
  series	= {ICIMMI '23}
}

@Proceedings{	  10.1145/3644032,
  title		= {AST '24: Proceedings of the 5th ACM/IEEE International
		  Conference on Automation of Software Test (AST 2024)},
  year		= {2024},
  isbn		= {9798400705885},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {AST continues to be a venue for researchers and
		  practitioners where they can discuss high quality research
		  contributions on methods for software test automation, and
		  various case studies reporting practices in this field.
		  Indeed, software test automation is a discipline that has
		  produced noteworthy research in the last decade.The special
		  theme of AST 2024 is "Test automation for and with
		  Generative AI". This innovative and promising research
		  direction deals with the application of test automation
		  technologies to the testing of Generative AI applications,
		  as well as the adoption of generative AI to facilitate test
		  automation.},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3616855.3636450,
  author	= {Jin, Bowen and Zhang, Yu and Li, Sha and Han, Jiawei},
  title		= {Bridging Text Data and Graph Data: Towards Semantics and
		  Structure-aware Knowledge Discovery},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3636450},
  doi		= {10.1145/3616855.3636450},
  abstract	= {Graphs and texts are two key modalities in data mining. In
		  many cases, the data presents a mixture of the two
		  modalities and the information is often complementary: in
		  e-commerce data, the product-user graph and product
		  descriptions capture different aspects of product features;
		  in scientific literature, the citation graph, author
		  metadata, and the paper content all contribute to modeling
		  the paper impact.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {1122–1125},
  numpages	= {4},
  keywords	= {graph mining, pretrained language model},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@InProceedings{	  10.1145/3687311.3687340,
  author	= {Liu, Xianghong and Li, Haoyu},
  title		= {A comprehensive survey and visual analysis of AIGC
		  education and teaching},
  year		= {2024},
  isbn		= {9798400709920},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3687311.3687340},
  doi		= {10.1145/3687311.3687340},
  abstract	= {To comprehensively understand the research landscape of
		  Artificial Intelligence-Generated Content applications in
		  educational scenarios. This study employs the CiteSpace
		  visual bibliometric tool to conduct a quantitative analysis
		  of 366 publications related to AIGC in education and
		  teaching. The findings indicate that research on AIGC in
		  education and teaching focuses primarily on three aspects.
		  The connotation and characteristics of AIGC in education,
		  the educational threats posed by AIGC, and the application
		  scenarios of AIGC in education and teaching. It is evident
		  that education researchers need to address the potential
		  crises introduced by AIGC, actively explore strategies and
		  models suited for various educational scenarios based on
		  its functional characteristics and adapt to contemporary
		  needs.Dominant typeAdvantages and
		  characteristicsPersonalized learning supportAIGC can
		  conduct one-on-one open-ended question-and-answer sessions
		  with usersteacher instructional assistanceGenerative AI can
		  assist teachers in lesson preparation and teaching[4]timely
		  feedback evaluationProvide immediate feedback to learners
		  to help them identify and correct mistakes in their
		  learning.Multi-dimensional educational interactionExtending
		  into the cyberspace that facilitates human-machine-human
		  interaction.educational resource sharingEfficiently create
		  personalized and cross-modal educational resources based on
		  different resources[5].},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Intelligent Education and Computer Technology},
  pages		= {158–162},
  numpages	= {5},
  location	= {Guilin, China},
  series	= {IECT '24}
}

@Article{	  10.1145/3677376,
  author	= {Zou, Jie and Sun, Aixin and Long, Cheng and Kanoulas,
		  Evangelos},
  title		= {Knowledge-Enhanced Conversational Recommendation via
		  Transformer-Based Sequential Modeling},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3677376},
  doi		= {10.1145/3677376},
  abstract	= {In conversational recommender systems (CRSs),
		  conversations usually involve a set of items and
		  item-related entities or attributes, e.g., director is a
		  related entity of a movie. These items and item-related
		  entities are often mentioned along the development of a
		  dialog, leading to potential sequential dependencies among
		  them. However, most of existing CRSs neglect these
		  potential sequential dependencies. In this article, we
		  first propose a Transformer-based sequential conversational
		  recommendation method, named TSCR, to model the sequential
		  dependencies in the conversations to improve CRS. In TSCR,
		  we represent conversations by items and the item-related
		  entities, and construct user sequences to discover user
		  preferences by considering both the mentioned items and
		  item-related entities. Based on the constructed sequences,
		  we deploy a Cloze task to predict the recommended items
		  along a sequence. Meanwhile, in certain domains, knowledge
		  graphs formed by the items and their related entities are
		  readily available, which provide various different kinds of
		  associations among them. Given that TSCR does not benefit
		  from such knowledge graphs, we then propose a knowledge
		  graph enhanced version of TSCR, called TSCRKG. In specific,
		  we leverage the knowledge graph to offline initialize our
		  model TSCRKG, and augment the user sequence of
		  conversations (i.e., sequence of the mentioned items and
		  item-related entities in the conversation) with multi-hop
		  paths in the knowledge graph. Experimental results
		  demonstrate that our TSCR model significantly outperforms
		  state-of-the-art baselines, and the enhanced version TSCRKG
		  further improves recommendation performance on top of
		  TSCR.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= oct,
  articleno	= {162},
  numpages	= {27},
  keywords	= {Conversational recommendation, sequential recommendation,
		  recommender system, transformer}
}

@InProceedings{	  10.1145/3687311.3687317,
  author	= {Ji, Zibo and Li, Yanjun and Yang, Ruiting and Wu,
		  Haoning},
  title		= {Research on the application and practice of curriculum
		  with AI assistance based on students' adaptive learning
		  needs},
  year		= {2024},
  isbn		= {9798400709920},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3687311.3687317},
  doi		= {10.1145/3687311.3687317},
  abstract	= {In the era of continuous development of education from
		  informationization to digital transformation, in order to
		  improve students' learning autonomy in mechanics courses
		  and to solve the problem of resource richness and
		  personalized demand of mechanics course teaching under the
		  demand of students' self-adaptive learning, through the
		  fusion technology of big language model and knowledge
		  graph, the interaction technology of Solidworks 3D modeling
		  and Realibox rendering, and cloud computing, cloud
		  supervision and other technological tools to assist
		  teaching by providing knowledge systematic model and
		  visualization model to help students effectively complete
		  the learning tasks and cultivate and improve their
		  independent learning ability. Supervision and other
		  technical tools through the provision of knowledge
		  systematic model and visualization model to assist
		  teaching, help students effectively complete the learning
		  task and cultivate and improve independent learning
		  ability.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Intelligent Education and Computer Technology},
  pages		= {30–34},
  numpages	= {5},
  location	= {Guilin, China},
  series	= {IECT '24}
}

@InProceedings{	  10.1145/3627673.3679673,
  author	= {Xu, Derong and Zhang, Ziheng and Zhu, Zhihong and Lin,
		  Zhenxi and Liu, Qidong and Wu, Xian and Xu, Tong and Wang,
		  Wanyu and Ye, Yuyang and Zhao, Xiangyu and Chen, Enhong and
		  Zheng, Yefeng},
  title		= {Editing Factual Knowledge and Explanatory Ability of
		  Medical Large Language Models},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679673},
  doi		= {10.1145/3627673.3679673},
  abstract	= {Model editing aims to precisely alter the behaviors of
		  large language models (LLMs) in relation to specific
		  knowledge, while leaving unrelated knowledge intact. This
		  approach has proven effective in addressing issues of
		  hallucination and outdated information in LLMs. However,
		  the potential of using model editing to modify knowledge in
		  the medical field remains largely unexplored, even though
		  resolving hallucination is a pressing need in this area.
		  Our observations indicate that current methods face
		  significant challenges in dealing with specialized and
		  complex knowledge in medical domain. Therefore, we propose
		  MedLaSA, a novel Layer-wise Scalable Adapter strategy for
		  medical model editing. MedLaSA harnesses the strengths of
		  both adding extra parameters and locate-then-edit methods
		  for medical model editing. We utilize causal tracing to
		  identify the association of knowledge in neurons across
		  different layers, and generate a corresponding scale set
		  from the association value for each piece of knowledge.
		  Subsequently, we incorporate scalable adapters into the
		  dense layers of LLMs. These adapters are assigned scaling
		  values based on the corresponding specific knowledge, which
		  allows for the adjustment of the adapter's weight and rank.
		  The more similar the content, the more consistent the scale
		  between them. This ensures precise editing of semantically
		  identical knowledge while avoiding impact on unrelated
		  knowledge. To evaluate the editing impact on the behaviours
		  of LLMs, we propose two model editing studies for medical
		  domain: (1) editing factual knowledge for medical
		  specialization and (2) editing the explanatory ability for
		  complex knowledge. We build two novel medical benchmarking
		  datasets and introduce a series of challenging and
		  comprehensive metrics. Extensive experiments on medical
		  LLMs demonstrate the editing efficiency of MedLaSA, without
		  affecting unrelated knowledge.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2660–2670},
  numpages	= {11},
  keywords	= {large language model, medical sciences, model editing},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3626772.3657676,
  author	= {Zhang, Wenling and Li, Yixiao and Li, Zhaotian and Sun,
		  Hailong and Gao, Xiang and Liu, Xudong},
  title		= {ModelGalaxy: A Versatile Model Retrieval Platform},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657676},
  doi		= {10.1145/3626772.3657676},
  abstract	= {With the growing number of available machine learning
		  models and the emergence of model-sharing platforms, model
		  reuse has become a significant approach to harnessing the
		  power of artificial intelligence. One of the key issues to
		  realizing model reuse resides in efficiently and accurately
		  finding the target models that meet user needs from a model
		  repository. However, the existing popular model-sharing
		  platforms (e.g., Hugging Face) mainly support model
		  retrieval based on model name matching and task filtering.
		  If not familiar with the platform or specific models, users
		  may suffer from low retrieval efficiency and a less
		  user-friendly interaction experience. To address these
		  issues, we have developed ModelGalaxy, a versatile model
		  retrieval platform supporting multiple model retrieval
		  methods, including keyword-based search, dataset-based
		  search, and user-task-centric search. Moreover, ModelGalaxy
		  leverages the power of large language models to provide
		  users with easily retrieving and using models. Our source
		  code is available at
		  https://github.com/zwl906711886/ModelGalaxy.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2771–2775},
  numpages	= {5},
  keywords	= {large language model, meta-learning, model retrieval},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3627673.3679156,
  author	= {Peng, Yiwen and Bonald, Thomas and Alam, Mehwish},
  title		= {Refining Wikidata Taxonomy using Large Language Models},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679156},
  doi		= {10.1145/3627673.3679156},
  abstract	= {Due to its collaborative nature, Wikidata is known to have
		  a complex taxonomy, with recurrent issues like the
		  ambiguity between instances and classes, the inaccuracy of
		  some taxonomic paths, the presence of cycles, and the high
		  level of redundancy across classes. Manual efforts to clean
		  up this taxonomy are time-consuming and prone to errors or
		  subjective decisions. We present WiKC, a new version of
		  Wikidata taxonomy cleaned automatically using a combination
		  of Large Language Models (LLMs) and graph mining
		  techniques. Operations on the taxonomy, such as cutting
		  links or merging classes, are performed with the help of
		  zero-shot prompting on an open-source LLM. The quality of
		  the refined taxonomy is evaluated from both intrinsic and
		  extrinsic perspectives, on a task of entity typing for the
		  latter, showing the practical interest of WiKC.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5395–5399},
  numpages	= {5},
  keywords	= {graph mining, knowledge graphs, large language model},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3627673.3679791,
  author	= {Shi, Yuchen and Jiang, Guochao and Qiu, Tian and Yang,
		  Deqing},
  title		= {AgentRE: An Agent-Based Framework for Navigating Complex
		  Information Landscapes in Relation Extraction},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679791},
  doi		= {10.1145/3627673.3679791},
  abstract	= {The relation extraction (RE) in complex scenarios faces
		  challenges such as diverse relation types and ambiguous
		  relations between entities within a single sentence,
		  leading to the poor performance of pure "text-in, text-out"
		  language models (LMs). To address these challenges, in this
		  paper, we propose an agent-based RE framework, namely
		  "AgentRE", which fully leverages the potential of large
		  language models (LLMs) including memory, retrieval and
		  reflection, to achieve RE in complex scenarios.
		  Specifically, three major modules are built in AgentRE
		  serving as the tools to help the agent acquire and process
		  various useful information, thereby obtaining improved RE
		  performance. Our extensive experimental results upon two
		  datasets in English and Chinese demonstrate our AgentRE's
		  superior performance, especially in low-resource scenarios.
		  Additionally, the trajectories generated by AgentRE can be
		  refined to construct a high-quality training dataset
		  incorporating different reasoning methods, which can be
		  used to fine-tune smaller models.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2045–2055},
  numpages	= {11},
  keywords	= {agent, large language model, memory, relation extraction,
		  retrieval},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3627673.3679229,
  author	= {Lee, Zhicheng and Huang, Zhidian and Yao, Zijun and Liu,
		  Jinxin and Xin, Amy and Hou, Lei and Li, Juanzi},
  title		= {DiaKoP: Dialogue-based Knowledge-oriented Programming for
		  Neural-symbolic Knowledge Base Question Answering},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679229},
  doi		= {10.1145/3627673.3679229},
  abstract	= {We present Dialogue-based Knowledge-oriented Programming
		  system (DiaKoP), a system with a chat interface designed
		  for multi-turn knowledge base question answering (KBQA).
		  DiaKoP enables users to decompose complex questions into
		  multiple simpler follow-up questions and interact with the
		  system to obtain answers. Multi-turn KBQA presents unique
		  challenges because users may switch topics or ask
		  incomplete questions that rely on previous interactions. To
		  address this, we develop a Dialogue History Tracker and
		  Dialogue Policy to manage user conversations effectively.
		  Additionally, we enhance the knowledge from the knowledge
		  graph by integrating parametric knowledge from a large
		  language model (LLM) to provide more comprehensive answers.
		  To mitigate the issue of wrongly parsed questions by
		  semantic parser, we implement a human-in-the-loop
		  mechanism, allowing users to correct errors. We evaluate
		  DiaKoP both qualitatively and quantitatively, with user
		  study indicating that our system better meets users' needs.
		  DiaKoP is open-sourced on https://github.com/THU-KEG/DiaKoP
		  with a guiding demo on https://youtu.be/Tq17k0OxPVg.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5234–5238},
  numpages	= {5},
  keywords	= {explainability, human-in-the-loop, knowledge based
		  question answering system, multi-turn dialogue},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3664647.3681661,
  author	= {Luo, Pengfei and Xu, Tong and Liu, Che and Zhang, Suojuan
		  and Xu, Linli and Li, Minglei and Chen, Enhong},
  title		= {Bridging Gaps in Content and Knowledge for Multimodal
		  Entity Linking},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681661},
  doi		= {10.1145/3664647.3681661},
  abstract	= {Multimodal Entity Linking (MEL) aims to address the
		  ambiguity in multimodal mentions and associate them with
		  Multimodal Knowledge Graphs (MMKGs). Existing works
		  primarily focus on designing multimodal interaction and
		  fusion mechanisms to enhance the performance of MEL.
		  However, these methods still overlook two crucial gaps
		  within the MEL task. One is the content discrepancy between
		  mentions and entities, manifested as uneven information
		  density. The other is the knowledge gap, indicating
		  insufficient knowledge extraction and reasoning during the
		  linking process. To bridge these gaps, we propose a novel
		  framework FissFuse, as well as a plug-and-play
		  knowledge-aware re-ranking method KAR. Specifically,
		  FissFuse collaborates with the Fission and Fusion branches,
		  establishing dynamic features for each mention-entity pair
		  and adaptively learning multimodal interactions to
		  alleviate content discrepancy. Meanwhile, KAR is endowed
		  with carefully crafted instruction for intricate knowledge
		  reasoning, serving as re-ranking agents empowered by Large
		  Language Models (LLMs). Extensive experiments on two
		  well-constructed MEL datasets demonstrate outstanding
		  performance of FissFuse compared with various baselines.
		  Comprehensive evaluations and ablation experiments validate
		  the effectiveness and generality of KAR.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {9311–9320},
  numpages	= {10},
  keywords	= {content discrepancy, multimodal entity linking, multimodal
		  fusion, multimodal knowledge graph},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3664647.3681593,
  author	= {Li, Haoxuan and Yang, Zhengmao and Ma, Yunshan and Bin, Yi
		  and Yang, Yang and Chua, Tat-Seng},
  title		= {MM-Forecast: A Multimodal Approach to Temporal Event
		  Forecasting with Large Language Models},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681593},
  doi		= {10.1145/3664647.3681593},
  abstract	= {We study an emerging and intriguing problem of multimodal
		  temporal event forecasting with large language models.
		  Compared to using text or graph modalities, the
		  investigation of utilizing images for temporal event
		  forecasting has not been fully explored, especially in the
		  era of large language models (LLMs). To bridge this gap, we
		  are particularly interested in two key questions of: 1) why
		  images will help in temporal event forecasting, and 2) how
		  to integrate images into the LLM-based forecasting
		  framework. To answer these research questions, we propose
		  to identify two essential functions that images play in the
		  scenario of temporal event forecasting, i.e., highlighting
		  and complementary. Then, we develop a novel framework,
		  named MM-Forecast. It employs an Image Function
		  Identification module to recognize these functions as
		  verbal descriptions using multimodal large language models
		  (MLLMs), and subsequently incorporates these function
		  descriptions into LLM-based forecasting models. To evaluate
		  our approach, we construct a new multimodal dataset,
		  MidEast-TE-mm, by extending an existing event dataset
		  MidEast-TE-mini with images. Empirical studies demonstrate
		  that our MM-Forecast can correctly identify the image
		  functions, and further more, incorporating these verbal
		  function descriptions significantly improves the
		  forecasting performance. The dataset, code, and prompts are
		  available at https://github.com/LuminosityX/MM-Forecast.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {2776–2785},
  numpages	= {10},
  keywords	= {multimodal event forecasting, multimodal large language
		  model, temporal event forecasting},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3627673.3679582,
  author	= {Zhu, Yinghao and Ren, Changyu and Wang, Zixiang and Zheng,
		  Xiaochen and Xie, Shiyun and Feng, Junlan and Zhu, Xi and
		  Li, Zhoujun and Ma, Liantao and Pan, Chengwei},
  title		= {EMERGE: Enhancing Multimodal Electronic Health Records
		  Predictive Modeling with Retrieval-Augmented Generation},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679582},
  doi		= {10.1145/3627673.3679582},
  abstract	= {The integration of multimodal Electronic Health Records
		  (EHR) data has significantly advanced clinical predictive
		  capabilities. Existing models, which utilize clinical notes
		  and multivariate time-series EHR data, often fall short of
		  incorporating the necessary medical context for accurate
		  clinical tasks, while previous approaches with knowledge
		  graphs (KGs) primarily focus on structured knowledge
		  extraction. In response, we propose EMERGE, a
		  Retrieval-Augmented Generation (RAG) driven framework to
		  enhance multimodal EHR predictive modeling. We extract
		  entities from both time-series data and clinical notes by
		  prompting Large Language Models (LLMs) and align them with
		  professional PrimeKG, ensuring consistency. In addition to
		  triplet relationships, we incorporate entities' definitions
		  and descriptions for richer semantics. The extracted
		  knowledge is then used to generate task-relevant summaries
		  of patients' health statuses. Finally, we fuse the summary
		  with other modalities using an adaptive multimodal fusion
		  network with cross-attention. Extensive experiments on the
		  MIMIC-III and MIMIC-IV datasets' in-hospital mortality and
		  30-day readmission tasks demonstrate the superior
		  performance of the EMERGE framework over baseline models.
		  Comprehensive ablation studies and analysis highlight the
		  efficacy of each designed module and robustness to data
		  sparsity. EMERGE contributes to refining the utilization of
		  multimodal EHR data in healthcare, bridging the gap with
		  nuanced medical contexts essential for informed clinical
		  predictions. We have publicly released the code at
		  https://github.com/yhzhu99/EMERGE.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3549–3559},
  numpages	= {11},
  keywords	= {electronic health record, large language model, multimodal
		  learning, retrieval-augmented generation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3589335.3651572,
  author	= {Hsu, Chi-Yang and Cox, Kyle and Xu, Jiawei and Tan, Zhen
		  and Zhai, Tianhua and Hu, Mengzhou and Pratt, Dexter and
		  Chen, Tianlong and Hu, Ziniu and Ding, Ying},
  title		= {Thought Graph: Generating Thought Process for Biological
		  Reasoning},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651572},
  doi		= {10.1145/3589335.3651572},
  abstract	= {We present the Thought Graph as a novel framework to
		  support complex reasoning and use gene set analysis as an
		  example to uncover semantic relationships between
		  biological processes. Our framework stands out for its
		  ability to provide a deeper understanding of gene sets,
		  significantly surpassing GSEA by 40.28% and LLM baselines
		  by 5.38% based on cosine similarity to human annotations.
		  Our analysis further provides insights into future
		  directions of biological processes naming, and implications
		  for bioinformatics and precision medicine.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {537–540},
  numpages	= {4},
  keywords	= {bioinformatics, biological process., gene ontology, large
		  language model, natural language processing, semantic web},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3605098.3636053,
  author	= {Alharbi, Reham and Tamma, Valentina and Grasso, Floriana
		  and Payne, Terry},
  title		= {An Experiment in Retrofitting Competency Questions for
		  Existing Ontologies},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3636053},
  doi		= {10.1145/3605098.3636053},
  abstract	= {Competency Questions (CQs) are a form of ontology
		  functional requirements expressed as natural language
		  questions. Inspecting CQs together with the axioms in an
		  ontology provides critical insights into the intended scope
		  and applicability of the ontology. CQs also underpin a
		  number of tasks in the development of ontologies e.g.
		  ontology reuse, ontology testing, requirement
		  specification, and the definition of patterns that
		  implement such requirements. Although CQs are integral to
		  the majority of ontology engineering methodologies, the
		  practice of publishing CQs alongside the ontological
		  artefacts is not widely observed by the community.In this
		  context, we present an experiment in retrofitting CQs from
		  existing ontologies. We propose RETROFIT-CQs, a method to
		  extract candidate CQs directly from ontologies using
		  Generative AI. In the paper we present the pipeline that
		  facilitates the extraction of CQs by leveraging Large
		  Language Models (LLMs) and we discuss its application to a
		  number of existing ontologies.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1650–1658},
  numpages	= {9},
  keywords	= {ontology engineering, competency questions, large language
		  models},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@InProceedings{	  10.1145/3677779.3677794,
  author	= {Wang, Cangqing and Yang, Yutian and Li, Ruisi and Sun, Dan
		  and Cai, Ruicong and Zhang, Yuzhu and Fu, Chengqian},
  title		= {Adapting LLMs for Efficient Context Processing through
		  Soft Prompt Compression},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677779.3677794},
  doi		= {10.1145/3677779.3677794},
  abstract	= {The rapid advancement of Large Language Models (LLMs) has
		  inaugurated a transformative epoch in natural language
		  processing, fostering unprecedented proficiency in text
		  generation, comprehension, and contextual scrutiny.
		  Nevertheless, effectively handling extensive contexts,
		  crucial for myriad applications, poses a formidable
		  obstacle owing to the intrinsic constraints of the
		  models’ context window sizes and the computational
		  burdens entailed by their operations. This investigation
		  presents an innovative framework that strategically tailors
		  LLMs for streamlined context processing by harnessing the
		  synergies among natural language summarization, soft prompt
		  com- pression, and augmented utility preservation
		  mechanisms. Our methodology, dubbed SoftPromptComp,
		  amalgamates natural language prompts extracted from
		  summarization methodologies with dynamically generated soft
		  prompts to forge a concise yet semantically robust
		  depiction of protracted contexts. This depiction undergoes
		  further refinement via a weighting mechanism optimizing
		  information retention and utility for subsequent tasks. We
		  substantiate that our framework markedly diminishes
		  computational overhead and enhances LLMs’ efficacy across
		  various benchmarks, while upholding or even augmenting the
		  caliber of the produced content. By amalgamating soft
		  prompt compression with sophisticated summarization,
		  SoftPromptComp confronts the dual challenges of managing
		  lengthy contexts and ensuring model scalability. Our
		  findings point towards a propitious trajectory for
		  augmenting LLMs’ applicability and efficiency, rendering
		  them more versatile and pragmatic for real- world
		  applications. This research enriches the ongoing discourse
		  on optimizing language models, providing insights into the
		  potency of soft prompts and summarization techniques as
		  pivotal instruments for the forthcoming generation of NLP
		  solutions.},
  booktitle	= {Proceedings of the International Conference on Modeling,
		  Natural Language Processing and Machine Learning},
  pages		= {91–97},
  numpages	= {7},
  keywords	= {Knowledge Graph Reasoning, Reinforcement Learning, Reward
		  Shaping, Transfer Learning},
  location	= {Xi'an, China},
  series	= {CMNM '24}
}

@InProceedings{	  10.1145/3635059.3635062,
  author	= {Giarelis, Nikolaos and Mastrokostas, Charalampos and
		  Siachos, Ilias and Karacapilidis, Nikos},
  title		= {A Review of Greek NLP Technologies for Chatbot
		  Development},
  year		= {2024},
  isbn		= {9798400716263},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3635059.3635062},
  doi		= {10.1145/3635059.3635062},
  abstract	= {The advent of Generative AI has certainly boosted the
		  interest in developing innovative chatbot applications.
		  Despite a vast amount of machine learning (ML) and natural
		  language processing (NLP) research and English language
		  resources that greatly improve chatbot technology, the
		  corresponding research and resources for the Greek language
		  are limited. The contribution of this paper is twofold: (i)
		  it reports on the state-of-the-art research in Greek NLP,
		  as far as language resources, embeddings-based techniques,
		  deep learning models, and existing chatbot applications are
		  concerned; (ii) it offers a set of insights on current NLP
		  models and chatbot implementation methodologies, and
		  outlines a set of pending issues and future research
		  directions.},
  booktitle	= {Proceedings of the 27th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {15–20},
  numpages	= {6},
  keywords	= {Deep Learning, Greek Language, Large Language Models,
		  Review, Text Classification, Text Summarization, Word
		  Embeddings},
  location	= {Lamia, Greece},
  series	= {PCI '23}
}

@InProceedings{	  10.1145/3652620.3687809,
  author	= {G\"{o}bel, Susanne and L\"{a}mmel, Ralf},
  title		= {Model-Based Trust Analysis of LLM Conversations},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687809},
  doi		= {10.1145/3652620.3687809},
  abstract	= {LLM-based chatbots are routinely advertised as supporting
		  the collaboration of humans and AI. We study LLM
		  conversations from a knowledge elicitation perspective with
		  the objective of being able to understand and assess the
		  human's trust in knowledge elicited from the LLM and
		  complementary sources. Our approach is supported by the
		  DSML KEML, the Knowledge Elicitation Modeling Language,
		  subject to abstract and visual syntax as well as a model
		  transformation-based model semantics for trust analysis.
		  Conversations are modeled by a combination of sequence
		  diagrams and enhanced argumentation graphs --- the latter
		  for the purpose of relating information pieces (facts and
		  instructions) that are extracted from messages. The
		  analysis of the corresponding models entails trust scores
		  for gathered information (i.e., elicited knowledge).},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {602–610},
  numpages	= {9},
  keywords	= {MDE for AI, knowledge representation models, model-based
		  analysis of LLMS, dsmls for AI usage},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.1145/3709138,
  author	= {Yuan, Wei and Yang, Chaoqun and Ye, Guanhua and Chen, Tong
		  and Hung, Nguyen Quoc Viet and Yin, Hongzhi},
  title		= {FELLAS: Enhancing Federated Sequential Recommendation with
		  LLM as External Services},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3709138},
  doi		= {10.1145/3709138},
  abstract	= {Sequential recommendation has been widely studied in the
		  recommendation domain since it can capture users’
		  temporal preferences and provide more accurate and timely
		  recommendations. To address user privacy concerns, the
		  combination of federated learning and sequential
		  recommender systems (FedSeqRec) has gained growing
		  attention. Unfortunately, the performance of FedSeqRec is
		  still unsatisfactory because the models used in FedSeqRec
		  have to be lightweight to accommodate communication
		  bandwidth and clients’ on-device computational resource
		  constraints. Recently, large language models (LLMs) have
		  exhibited strong transferable and generalized language
		  understanding abilities and therefore, in the NLP area,
		  many downstream tasks now utilize LLMs as a service to
		  achieve superior performance without constructing complex
		  models. Inspired by this successful practice, we propose a
		  generic FedSeqRec framework, FELLAS, which aims to enhance
		  FedSeqRec by utilizing LLMs as an external
		  service.Specifically, FELLAS employs an LLM server to
		  provide both item-level and sequence-level representation
		  assistance. The item-level representation service is
		  queried by the central server to enrich the original
		  ID-based item embedding with textual information, while the
		  sequence-level representation service is accessed by each
		  client. However, invoking the sequence-level representation
		  service requires clients to send sequences to the external
		  LLM server. To safeguard privacy, we implement
		  (d_{mathcal{X}}) -privacy satisfied sequence perturbation,
		  which protects clients’ sensitive data with guarantees.
		  Additionally, a contrastive learning-based method is
		  designed to transfer knowledge from the noisy sequence
		  representation to clients’ sequential recommendation
		  models. Furthermore, to empirically validate the privacy
		  protection capability of FELLAS, we propose two interacted
		  item inference attacks, considering the threats posed by
		  the LLM server and the central server acting as
		  curious-but-honest adversaries in cooperation. Extensive
		  experiments conducted on three datasets with two widely
		  used sequential recommendation models demonstrate the
		  effectiveness and privacy-preserving capability of
		  FELLAS.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= dec,
  keywords	= {Recommender System, Federated Learning, Privacy
		  Protection}
}

@Article{	  10.1145/3704729,
  author	= {Asprino, Luigi and Damiano, Rossana and Daquino, Marilena
		  and De Giorgis, Stefano and Gangemi, Aldo and Lieto,
		  Antonio and Sartini, Bruno and Striani, Manuel},
  title		= {An Ontology Network for Citizen Curation},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3704729},
  doi		= {10.1145/3704729},
  abstract	= {Citizen curation is gaining momentum as a new form of
		  engagement with cultural heritage. Citizen curatorial
		  activities require and produce a wealth of information,
		  ranging from descriptions of the artefacts to visitor
		  experience feedback. Although formalising and integrating
		  such various data is of paramount importance, the domain
		  lacks comprehensive ontologies to enable querying,
		  interpreting and reasoning over the collected data. Social
		  Participation, Cohesion and Inclusion through Cultural
		  Engagement (SPICE) is an EU project dedicated to
		  experimenting with citizen curation activities to foster
		  cultural engagement. SPICE develops technologies that help
		  communities to create and share their own interpretation of
		  cultural artefacts, hence developing a better understanding
		  of, and empathy for, themselves and other communities. Part
		  of the SPICE ecosystem of technologies is the SPICE
		  Ontology Network (SON), which empowers applications with
		  knowledge-level reasoning abilities and supports both
		  applications and users interacting with data involved in
		  citizen curation activities. This article provides an
		  overview of the SON and outlines its main use cases.},
  journal	= {J. Comput. Cult. Herit.},
  month		= dec,
  articleno	= {72},
  numpages	= {30},
  keywords	= {citizen curation, ontologies, cultural heritage, semantic
		  web}
}

@InProceedings{	  10.1145/3626772.3657966,
  author	= {Zhang, Wenjia and Gui, Lin and Procter, Rob and He,
		  Yulan},
  title		= {Multi-Layer Ranking with Large Language Models for News
		  Source Recommendation},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657966},
  doi		= {10.1145/3626772.3657966},
  abstract	= {To seek reliable information sources for news events, we
		  introduce a novel task of expert recommendation, which aims
		  to identify trustworthy sources based on their previously
		  quoted statements. To achieve this, we built a novel
		  dataset, called NewsQuote, consisting of 23,571
		  quote-speaker pairs sourced from a collection of news
		  articles. We formulate the recommendation task as the
		  retrieval of experts based on their likelihood of being
		  associated with a given query. We also propose a
		  multi-layer ranking framework employing Large Language
		  Models to improve the recommendation performance. Our
		  results show that employing an in-context learning based
		  LLM ranker and a multi-layer ranking-based filter
		  significantly improve both the predictive quality and
		  behavioural quality of the recommender system.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2537–2542},
  numpages	= {6},
  keywords	= {in-context learning, large language model, recommender
		  system},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3639233.3639242,
  author	= {Miskell, Cameron and Diaz, Richard and Ganeriwala, Parth
		  and Slhoub, Khaled and Nembhard, Fitzroy},
  title		= {Automated Framework to Extract Software Requirements from
		  Source Code},
  year		= {2024},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639233.3639242},
  doi		= {10.1145/3639233.3639242},
  abstract	= {Software maintenance and innovation are constant
		  challenges across industries, especially as programming
		  languages evolve with technology. Similarly, poor lexicon
		  quality degrades program comprehension, increasing the
		  effort required by developers to improve existing software
		  products. To address these challenges, we propose a novel
		  automated framework that extracts software requirements
		  directly from source code using a baseline AI language
		  model applied to a Java code base. Leveraging natural
		  language processing techniques, the framework validates
		  programs and generates easily readable requirements by
		  analyzing file contents. The framework enhances agility and
		  flexibility by providing comprehensive documentation for
		  existing software systems. It caters to both experienced
		  and less-experienced developers, offering an intuitive
		  graphical user interface and enabling efficient
		  identification and resolution of errors. The resulting
		  output facilitates natural interaction through language
		  processing. By automating the extraction process, the
		  framework allows developers to better understand software
		  systems, make informed decisions, and adapt to evolving
		  needs.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Natural Language Processing and Information Retrieval},
  pages		= {130–134},
  numpages	= {5},
  keywords	= {AI language model, Extracting functional requirements,
		  Legacy code, Natural Language Processing, Software
		  evolution, Software verification},
  location	= {Seoul, Republic of Korea},
  series	= {NLPIR '23}
}

@InProceedings{	  10.1145/3589334.3645686,
  author	= {Yu, Weijian and Yang, Jie and Yang, Dingqi},
  title		= {Robust Link Prediction over Noisy Hyper-Relational
		  Knowledge Graphs via Active Learning},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645686},
  doi		= {10.1145/3589334.3645686},
  abstract	= {Modern Knowledge Graphs (KGs) are inevitably noisy due to
		  the nature of their construction process. Existing robust
		  learning techniques for noisy KGs mostly focus on triple
		  facts, where the fact-wise confidence is straightforward to
		  evaluate. However, hyper-relational facts, where an
		  arbitrary number of key-value pairs are associated with a
		  base triplet, have become increasingly popular in modern
		  KGs, but significantly complicate the confidence assessment
		  of the fact. Against this background, we study the problem
		  of robust link prediction over noisy hyper-relational KGs,
		  and propose NYLON, a underlineN oise-resistant hunderlineY
		  per-reunderlineL atiunderlineON al link prediction
		  technique via active crowd learning. Specifically, beyond
		  the traditional fact-wise confidence, we first introduce
		  element-wise confidence measuring the fine-grained
		  confidence of each entity or relation of a hyper-relational
		  fact. We connect the element- and fact-wise confidences via
		  a "least confidence'' principle to allow efficient crowd
		  labeling. NYLON is then designed to systematically
		  integrate three key components, where a hyper-relational
		  link predictor uses the fact-wise confidence for robust
		  prediction, a cross-grained confidence evaluator predicts
		  both element- and fact-wise confidences, and an
		  effort-efficient active labeler selects informative facts
		  for crowd annotators to label using an efficient labeling
		  mechanism guided by the element-wise confidence under the
		  "least confidence'' principle and further followed by data
		  augmentation. We evaluate NYLON on three real-world KG
		  datasets against a sizeable collection of baselines.
		  Results show that NYLON achieves superior and robust
		  performance in both link prediction and error detection
		  tasks on noisy KGs, and outperforms best baselines by
		  2.42-10.93% and 3.46-10.65% in the two tasks,
		  respectively.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2282–2293},
  numpages	= {12},
  keywords	= {hyper-relation, link prediction, noisy knowledge graph},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3664647.3680790,
  author	= {Hu, Linmei and Wang, Duokang and Pan, Yiming and Yu, Jifan
		  and Shao, Yingxia and Feng, Chong and Nie, Liqiang},
  title		= {NovaChart: A Large-scale Dataset towards Chart
		  Understanding and Generation of Multimodal Large Language
		  Models},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3680790},
  doi		= {10.1145/3664647.3680790},
  abstract	= {Multimodal Large Language Models (MLLMs) have shown
		  significant potential for chart understanding and
		  generation. However, they are still far from achieving the
		  desired effectiveness in practical applications. This could
		  be due to the limitations of the used training chart data.
		  Existing chart datasets suffer from scarcity of chart
		  types, limited coverage of tasks, and insufficient
		  scalability, making them incapable of effectively enhancing
		  the chart-related capabilities of MLLMs. To tackle these
		  obstacles, we construct NovaChart, a large-scale dataset
		  for chart understanding and generation of MLLMs. NovaChart
		  contains 47K high-resolution chart images and 856K
		  chart-related instructions, covering 18 different chart
		  types and 15 unique tasks of chart understanding and
		  generation. To build NovaChart, we propose a data
		  generation engine for metadata curation, chart
		  visualization and instruction formulation. Chart metadata
		  in NovaChart contains detailed annotations, i.e., data
		  points, visual elements, source data and the visualization
		  code of every chart. This additional information endows
		  NovaChart with considerable scalability, as it can
		  facilitate the extension of chart instruction data to a
		  larger scale and greater diversity. We utilize NovaChart to
		  train several open-source MLLMs. Experimental results
		  demonstrate NovaChart empowers MLLMs with stronger
		  capabilities in 15 chart understanding and generation tasks
		  by a large-margin (35.47%-619.47%), bringing them a step
		  closer to smart chart assistants. Our dataset is now
		  available at https://github.com/Elucidator-V/NovaChart.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {3917–3925},
  numpages	= {9},
  keywords	= {chart generation, chart understanding, multimodal large
		  language model},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3589334.3645627,
  author	= {Huang, Xuanwen and Han, Kaiqiao and Yang, Yang and Bao,
		  Dezheng and Tao, Quanjin and Chai, Ziwei and Zhu, Qi},
  title		= {Can GNN be Good Adapter for LLMs?},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645627},
  doi		= {10.1145/3589334.3645627},
  abstract	= {Recently, large language models (LLMs) have demonstrated
		  superior capabilities in understanding and zero-shot
		  learning on textual data, promising significant advances
		  for many text-related domains. In the graph domain, various
		  real-world scenarios also involve textual data, where tasks
		  and node features can be described by text. These
		  text-attributed graphs (TAGs) have broad applications in
		  social media, recommendation systems, etc. Thus, this paper
		  explores how to utilize LLMs to model TAGs. Previous
		  methods for TAG modeling are based on million-scale LMs.
		  When scaled up to billion-scale LLMs, they face huge
		  challenges in computational costs. Additionally, they also
		  ignore the zero-shot inference capabilities of LLMs.
		  Therefore, we propose GraphAdapter, which uses a graph
		  neural network (GNN) as an efficient adapter in
		  collaboration with LLMs to tackle TAGs. In terms of
		  efficiency, the GNN adapter introduces only a few trainable
		  parameters and can be trained with low computation costs.
		  The entire framework is trained using auto-regression on
		  node text (next token prediction). Once trained,
		  GraphAdapter can be seamlessly fine-tuned with
		  task-specific prompts for various downstream tasks. Through
		  extensive experiments across multiple real-world TAGs,
		  GraphAdapter based on Llama 2 gains an average improvement
		  of approximately 5% in terms of node classification.
		  Furthermore, GraphAdapter can also adapt to other language
		  models, including RoBERTa, GPT-2. The promising results
		  demonstrate that GNNs can serve as effective adapters for
		  LLMs in TAG modeling.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {893–904},
  numpages	= {12},
  keywords	= {graph neural networks, large language model,
		  text-attributed graph},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3643795.3648384,
  author	= {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and
		  Ashiwal, Virendra and Linsbauer, Sofia and Eskandani,
		  Nafise},
  title		= {LLM-based and Retrieval-Augmented Control Code
		  Generation},
  year		= {2024},
  isbn		= {9798400705793},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643795.3648384},
  doi		= {10.1145/3643795.3648384},
  abstract	= {Control code is designed and implemented for industrial
		  automation applications that manage power plants,
		  petrochemical processes, or steel production. Popular large
		  language models (LLM) can synthesize low-level control code
		  in the Structured Text programming notation according to
		  the standard IEC 61131-3, but are not aware of proprietary
		  control code function block libraries, which are often used
		  in practice. To automate control logic implementation
		  tasks, we proposed a retrieval-augmented control code
		  generation method that can integrate such function blocks
		  into the generated code. With this method control engineers
		  can benefit from the code generation capabilities of LLMs,
		  re-use proprietary and well-tested function blocks, and
		  speed up typical programming tasks significantly. We have
		  evaluated the method using a prototypical implementation
		  based on GPT-4, LangChain, Open-PLC, and the open-source
		  OSCAT function block library. In several spot sample tests,
		  we successfully generated IEC 61131-3 ST code that
		  integrated the desired function blocks, could be compiled,
		  and validated through simulations.},
  booktitle	= {Proceedings of the 1st International Workshop on Large
		  Language Models for Code},
  pages		= {22–29},
  numpages	= {8},
  keywords	= {large language models, code generation, IEC 61131-3,
		  industrial automation, PLC, DCS, ChatGPT, GPT-4},
  location	= {Lisbon, Portugal},
  series	= {LLM4Code '24}
}

@Article{	  10.1145/3685679,
  author	= {Hou, Jingrui and Zhang, Shitou},
  title		= {Exploring Thematic Diversity in Classical Chinese Poetry:
		  A Novel Dataset and a BERT-enhanced Ensemble Learning
		  Approach},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {17},
  number	= {4},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3685679},
  doi		= {10.1145/3685679},
  abstract	= {Classical Chinese poetry, as an essential aspect of
		  cultural heritage, exhibits rich theme diversity often
		  overlooked in natural language processing research. To
		  address this gap, we aim to explore the classification of
		  thematic categories within this literary domain. We curate
		  a dataset of 2,918 annotated poems spanning 7 common themes
		  and propose a BERT-based ensemble learning approach for
		  effective classification. Although this method integrates
		  existing models, it achieves an accuracy and F1 score of
		  over 72% in the 7-class task, surpassing established
		  baselines, and providing a baseline for future research.
		  The experimental findings reveal the effectiveness of
		  ensemble strategies in improving individual base model
		  performance and highlight the potential of the MLP-based
		  ensemble technique. The study contributes to a deeper
		  understanding of thematic categories and textual features
		  in classical Chinese poetry and offers an automated
		  classification system for classical Chinese poems.},
  journal	= {J. Comput. Cult. Herit.},
  month		= dec,
  articleno	= {60},
  numpages	= {19},
  keywords	= {Thematic Classification, Classical Chinese Poetry,
		  Ensemble Learning, Pre-trained Language Model}
}

@InProceedings{	  10.1145/3651671.3651721,
  author	= {Zhang, Haiying and Shi, Yunmei and Yang, Teng},
  title		= {BP-TEG: Topic-to-Essay Generation for Official Documents},
  year		= {2024},
  isbn		= {9798400709234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3651671.3651721},
  doi		= {10.1145/3651671.3651721},
  abstract	= {In natural language generation tasks, topic-to-essay
		  generation (TEG) is a challenging task. The main
		  diﬀiculty is that the amount of source information is
		  much smaller than that of generated information. To solve
		  the problem, the paper proposes a topic-to-essay generation
		  model combined BART-based pointer generator networks
		  (BP-TEG for short), which can generate high quality text by
		  expanding the given topic words. The topic word expansion
		  method is based on the LDA method, which is used to select
		  appropriate expansion words that semantically consist with
		  the topic words. In the proposed model, BART is introduced
		  to get semantic and syntactic knowledge from a large-scale
		  corpus. In order to improve the generated text quality and
		  topic relevance, BP-TEG model introduces the pointer
		  generator networks to pay more attention to unexpressed
		  topic words. Experiments on both the oﬀicial document
		  dataset and the public dataset Zhihu show that BP-TEG is
		  superior to other advanced baseline models; manual
		  evaluation results show that the model can generate more
		  complete sentences, topic-relevant contents, in addition,
		  the generated text has coherent language, and consistent
		  with semantic logic and oﬀicial document standards.},
  booktitle	= {Proceedings of the 2024 16th International Conference on
		  Machine Learning and Computing},
  pages		= {545–556},
  numpages	= {12},
  keywords	= {Natural Language Generation, Pointer Generation Network,
		  Pre-trained Language Model, Topic-to-Essay Generation},
  location	= {Shenzhen, China},
  series	= {ICMLC '24}
}

@Article{	  10.1145/3702647,
  author	= {Wang, Yuqi and Chen, Qiuyi and Zhang, Haiyang and Wang,
		  Wei and Wang, Qiufeng and Pan, Yushan and Xie, Liangru and
		  Huang, Kaizhu and Nguyen, Anh},
  title		= {Biomedical Information Retrieval with Positive-Unlabeled
		  Learning and Knowledge Graphs},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3702647},
  doi		= {10.1145/3702647},
  abstract	= {The rapid growth of biomedical publications has presented
		  significant challenges in the field of information
		  retrieval. Most existing work focuses on document retrieval
		  given explicit queries. However, in real applications such
		  as curated biomedical database maintenance, explicit
		  queries are missing. In this paper, we propose a two-step
		  model for biomedical information retrieval in the case that
		  only a small set of example documents is available without
		  explicit queries. Initially, we extract keywords from the
		  observed documents using large pre-trained language models
		  and biomedical knowledge graphs. These keywords are then
		  enriched with domain-specific entities. Information
		  retrieval techniques can subsequently use the collected
		  entities to rank the documents. Following this, we
		  introduce an iterative Positive-Unlabeled learning method
		  to classify all unlabeled documents. Experiments conducted
		  on the PubMed dataset demonstrate that the proposed
		  technique outperforms the state-of-the-art
		  positive-unlabeled learning methods. The results underscore
		  the effectiveness of integrating large language models and
		  biomedical knowledge graphs in improving zero-shot
		  information retrieval performance in the biomedical
		  domain.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= nov,
  keywords	= {Knowledge graph embedding, pre-trained large language
		  models, positive-unlabeled learning, text classification,
		  natural language processing, information retrieval}
}

@InProceedings{	  10.1145/3627673.3679228,
  author	= {Dew, Rebecca and Li, Mingzhao and Baratha Raj, Sandya},
  title		= {A Skill Proficiency Framework for Workforce Learning and
		  Development},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679228},
  doi		= {10.1145/3627673.3679228},
  abstract	= {Understanding the skills and proficiency levels required
		  for various roles is crucial for effective workforce
		  planning, learning and development. In this paper, we
		  propose a robust skill proficiency modeling framework that
		  offers a structured method to help describe, assess and
		  develop proficiency in key skills, facilitating
		  individuals' career pathways and aiding organizations in
		  talent management and adaptability. We first design a skill
		  proficiency description pipeline, which generates
		  statements describing the requirements at each proficiency
		  level of a skill. Following this, we build a skill
		  proficiency by occupation model using large-scale job ad
		  data to help organizations and individuals understand the
		  skill proficiency requirements for different roles.
		  Finally,we design a visual analytics system, based on a
		  real-world career pathway scenario, to demonstrate the
		  practical usefulness and effectiveness of our framework. A
		  demo video is available at
		  www.dropbox.com/scl/fi/nd0f3vi03n12g4y0sluaw/cikm24_demo.mp4?rlkey=55vya144q5ftai1uqqaubr5u5.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5210–5214},
  numpages	= {5},
  keywords	= {GPT, large language model, skill proficiency, visual
		  analytics},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3677052.3698597,
  author	= {Cho, Nicole and Srishankar, Nishan and Cecchi, Lucas and
		  Watson, William},
  title		= {FISHNET: Financial Intelligence from Sub-querying,
		  Harmonizing, Neural-Conditioning, Expert Swarms, and Task
		  Planning},
  year		= {2024},
  isbn		= {9798400710810},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677052.3698597},
  doi		= {10.1145/3677052.3698597},
  abstract	= {Financial intelligence generation from vast data sources
		  has typically relied on traditional methods of
		  knowledge-graph construction or database engineering.
		  Recently, fine-tuned financial domain-specific Large
		  Language Models (LLMs), have emerged. While these
		  advancements are promising, limitations such as high
		  inference costs, hallucinations, and the complexity of
		  concurrently analyzing high-dimensional financial data,
		  emerge. This motivates our invention FISHNET (Financial
		  Intelligence from Sub-querying, Harmonizing,
		  Neural-Conditioning, Expert swarming, and Task planning),
		  an agentic architecture that accomplishes highly complex
		  analytical tasks for more than 98,000 regulatory filings
		  that vary immensely in terms of semantics, data hierarchy,
		  or format. FISHNET shows remarkable performance for
		  financial insight generation (61.8% success rate over 5.0%
		  Routing, 45.6% RAG R-Precision). We conduct rigorous
		  ablations to empirically prove the success of FISHNET, each
		  agent’s importance, and the optimized performance of
		  assembling all agents. Our modular architecture can be
		  leveraged for a myriad of use-cases, enabling scalability,
		  flexibility, and data integrity that are critical for
		  financial tasks.},
  booktitle	= {Proceedings of the 5th ACM International Conference on AI
		  in Finance},
  pages		= {591–599},
  numpages	= {9},
  keywords	= {Harmonizing, LLM Agents, Planning, Sub-querying,
		  Swarming},
  location	= {Brooklyn, NY, USA},
  series	= {ICAIF '24}
}

@InProceedings{	  10.1145/3664647.3681349,
  author	= {Ma, Yunshan and He, Yingzhi and Zhong, Wenjun and Wang,
		  Xiang and Zimmermann, Roger and Chua, Tat-Seng},
  title		= {CIRP: Cross-Item Relational Pre-training for Multimodal
		  Product Bundling},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681349},
  doi		= {10.1145/3664647.3681349},
  abstract	= {Product bundling has been a prevailing marketing strategy
		  that is beneficial in the online shopping scenario.
		  Effective product bundling methods depend on high-quality
		  item representations capturing both the individual items'
		  semantics and cross-item relations. However, previous item
		  representation learning methods, either feature fusion or
		  graph learning, suffer from inadequate cross-modal
		  alignment and struggle to capture the cross-item relations
		  for cold-start items. Multimodal pre-train models could be
		  the potential solutions given their promising performance
		  on various multimodal downstream tasks. However, the
		  cross-item relations have been under-explored in the
		  current multimodal pre-train models.To bridge this gap, we
		  propose a novel and simple framework Cross-Item Relational
		  Pre-training (CIRP) for item representation learning in
		  product bundling. Specifically, we employ a multimodal
		  encoder to generate image and text representations. Then we
		  leverage both the cross-item contrastive loss (CIC) and
		  individual item's image-text contrastive loss (ITC) as the
		  pre-train objectives. Our method seeks to integrate
		  cross-item relation modeling capability into the multimodal
		  encoder. Therefore, even for cold-start items without
		  explicit relations, their representations are still
		  relation-aware. Furthermore, to eliminate the potential
		  noise and reduce the computational cost, we harness a
		  relation pruning module to remove the noisy and redundant
		  relations. We apply the item representations extracted by
		  CIRP to the product bundling model ItemKNN, and experiments
		  on three e-commerce datasets demonstrate that CIRP
		  outperforms various leading representation learning
		  methods. The code and dataset are available at
		  https://github.com/HappyPointer/CIRP.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {9641–9649},
  numpages	= {9},
  keywords	= {bundle recommendation, multimodal bundle construction,
		  multimodal pre-train, vision language model},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3677052.3698603,
  author	= {Li, Xiaohui Victor and Sanna Passino, Francesco},
  title		= {FinDKG: Dynamic Knowledge Graphs with Large Language
		  Models for Detecting Global Trends in Financial Markets},
  year		= {2024},
  isbn		= {9798400710810},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677052.3698603},
  doi		= {10.1145/3677052.3698603},
  abstract	= {Dynamic knowledge graphs (DKGs) are popular structures to
		  express different types of connections between objects over
		  time. They can also serve as an efficient mathematical tool
		  to represent information extracted from complex
		  unstructured data sources, such as text or images. Within
		  financial applications, DKGs could be used to detect trends
		  for strategic thematic investing, based on information
		  obtained from financial news articles. In this work, we
		  explore the properties of large language models (LLMs) as
		  dynamic knowledge graph generators, proposing a novel
		  open-source fine-tuned LLM for this purpose, called the
		  Integrated Contextual Knowledge Graph Generator (ICKG). We
		  use ICKG to produce a novel open-source DKG from a corpus
		  of financial news articles, called FinDKG, and we propose
		  an attention-based GNN architecture for analysing it,
		  called KGTransformer. We test the performance of the
		  proposed model on benchmark datasets and FinDKG,
		  demonstrating superior performance on link prediction
		  tasks. Additionally, we evaluate the performance of the
		  KGTransformer on FinDKG for thematic investing, showing it
		  can outperform existing thematic ETFs.},
  booktitle	= {Proceedings of the 5th ACM International Conference on AI
		  in Finance},
  pages		= {573–581},
  numpages	= {9},
  keywords	= {Dynamic knowledge graphs, graph attention networks, graph
		  neural networks, graph transformers, large language
		  models.},
  location	= {Brooklyn, NY, USA},
  series	= {ICAIF '24}
}

@InProceedings{	  10.1145/3613904.3642698,
  author	= {Liu, Yiren and Chen, Si and Cheng, Haocong and Yu, Mengxia
		  and Ran, Xiao and Mo, Andrew and Tang, Yiliu and Huang,
		  Yun},
  title		= {How AI Processing Delays Foster Creativity: Exploring
		  Research Question Co-Creation with an LLM-based Agent},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642698},
  doi		= {10.1145/3613904.3642698},
  abstract	= {Developing novel research questions (RQs) often requires
		  extensive literature reviews, especially in
		  interdisciplinary fields. To support RQ development through
		  human-AI co-creation, we leveraged Large Language Models
		  (LLMs) to build an LLM-based agent system named CoQuest. We
		  conducted an experiment with 20 HCI researchers to examine
		  the impact of two interaction designs: breadth-first and
		  depth-first RQ generation. The findings revealed that
		  participants perceived the breadth-first approach as more
		  creative and trustworthy upon task completion. Conversely,
		  during the task, participants considered the depth-first
		  generated RQs as more creative. Additionally, we discovered
		  that AI processing delays allowed users to reflect on
		  multiple RQs simultaneously, leading to a higher quantity
		  of generated RQs and an enhanced sense of control. Our work
		  makes both theoretical and practical contributions by
		  proposing and evaluating a mental model for human-AI
		  co-creation of RQs. We also address potential ethical
		  issues, such as biases and over-reliance on AI, advocating
		  for using the system to improve human research creativity
		  rather than automating scientific inquiry. The system’s
		  source is available at:
		  https://github.com/yiren-liu/coquest.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {17},
  numpages	= {25},
  keywords	= {Co-creation Systems, Large Language Models,
		  Mixed-initiative Design, Scientifc Discovery},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3674805.3690751,
  author	= {Deng, Yang and Wang, Bangchao and Zou, Zhiyuan and Ye,
		  Luyao},
  title		= {PromptLink: Multi-template prompt learning with
		  adversarial training for issue-commit link recovery},
  year		= {2024},
  isbn		= {9798400710476},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3674805.3690751},
  doi		= {10.1145/3674805.3690751},
  abstract	= {In recent years, Prompt Learning, based on pre-training,
		  prompting, and prediction, has achieved significant success
		  in natural language processing (NLP). The current
		  issue-commit link recovery (ILR) method converts the ILR
		  into a classification task using pre-trained language
		  models (PLMs) and dedicated neural networks. However, due
		  to inconsistencies between the ILR task and PLMs, these
		  methods not fully leverage the semantic information in
		  PLMs. To imitate the above problem, we make the first trial
		  of the new paradigm to propose a Multi-template prompt
		  learning method with adversarial training for issue-commit
		  link recovery (PromptLink), which transforms the ILR task
		  into a cloze task through the template. Specifically, a
		  Multi-template PromptLink is designed to enhance the
		  generalisation capability by integrating various templates
		  and adopting adversarial training to mitigate the model
		  overfitting. Experiments are conducted on six open-source
		  projects and comprehensively evaluated across six commonly
		  measures. The results show that PromptLink achieves an
		  average F1 of 96.10%, Precision of 96.49%, Recall of
		  95.92%, MCC of 94.04%, AUC of 96.05%, and ACC of 98.15%,
		  significantly outperforming existing state-of-the-art
		  methods on all measures. Overall, PromptLink not only
		  enhances performance and generalisation but also emerges
		  new ideas and methods for future research. The source code
		  of PromptLink is available at
		  https://figshare.com/s/6130d42ff464c579cdec.},
  booktitle	= {Proceedings of the 18th ACM/IEEE International Symposium
		  on Empirical Software Engineering and Measurement},
  pages		= {461–467},
  numpages	= {7},
  keywords	= {Issue-commit link recovery, Natural language processing,
		  Pre-trained language model, Prompt learning},
  location	= {Barcelona, Spain},
  series	= {ESEM '24}
}

@InProceedings{	  10.1145/3680529.3688968,
  author	= {Kraft, Georgy \`{E}gor},
  title		= {One and Infinite Chairs: 1 &amp; ∞ ⑁},
  year		= {2024},
  isbn		= {9798400711329},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3680529.3688968},
  doi		= {10.1145/3680529.3688968},
  abstract	= {Joseph Kossuth's 'One &amp; Three Chairs' is the most
		  textbook -introductory - example to conceptual art, as it
		  touches upon a number of characteristics definitive of
		  conceptual art. Emphasising the concept above all other
		  perceptual content, it dematerialises art itself as a
		  practice.In 1 &amp; ∞ Initial images of chairs were
		  generated using the text-to-image Ai model, based on the
		  prompt: 'a single chair on a plain background'. This
		  dataset, of photo-realistic images of a wide variety of
		  chairs, was then used to train the Stable Diffusion model
		  again, extending its knowledge capacity of what 'a chair on
		  a plain background' can look like. This process of
		  re-training the model on its own generated imagery was
		  repeated again and again. Until, at the 6th iteration,
		  instead of photo-realistic images of chairs, as seen in the
		  initial step, the model produced colourful digital noise in
		  which any resemblance to the represented subject -a chair,
		  would fade completely.In another iconic conceptual sound
		  artwork, 'I am sitting in a room,' the author Alvin Lucier
		  is recording himself narrating a text, and then playing the
		  tape recording back into the room, re-recording it.
		  Eventually the words become unintelligible, replaced by the
		  characteristic resonant frequencies of the room itself.In
		  data science, the phenomena of AI feeding into AI is often
		  referred to as data-cannibalism. Through the necessity to
		  augment datasets and due to AI image and data generation's
		  increasing and insidious prevalence, more and more new Ai
		  systems will be trained on synthetic datasets, produced by
		  generative Ai models, thus posing ontological challenges
		  and poisoning future datasets and epistemic accuracy of
		  those models. Via such feedback loops within echo chambers
		  of auto-generated and consumed data, the domain ontology of
		  a subject and its visual representation decay into
		  non-figurative abstraction... at least so for a human
		  eye.},
  booktitle	= {SIGGRAPH Asia 2024 Art Gallery},
  articleno	= {1},
  numpages	= {1},
  location	= {Tokyo, Japan},
  series	= {SA Art Gallery '24}
}

@InProceedings{	  10.1145/3663649.3664371,
  author	= {Prakash, Kishore and Rao, Shashwat and Hamza, Rayan and
		  Lukich, Jack and Chaudhari, Vatsal and Nandi, Arnab},
  title		= {Integrating LLMs into Database Systems Education},
  year		= {2024},
  isbn		= {9798400706783},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3663649.3664371},
  doi		= {10.1145/3663649.3664371},
  abstract	= {Large Language Models (LLMs) have sparked a drastic
		  improvement in the ways computers can understand, process,
		  and generate language. As LLM-based offerings become
		  mainstream, we explore the incorporation of such LLMs into
		  introductory or undergraduate database systems education.
		  Students and instructors are both faced with the calculator
		  dilemma: while the use of LLM-based tools may “solve”
		  tasks such as assignments and exams, do they impede or
		  accelerate the learning itself? We review deficiencies of
		  using existing off-the-shelf tools for learning, and
		  further articulate the differentiated needs of database
		  systems students as opposed to trained data practitioners.
		  Building on our exploration, we outline a vision that
		  integrates LLMs into database education in a principled
		  manner, keeping pedagogical best practices in mind. If
		  implemented correctly, we posit that LLMs can drastically
		  amplify the impact of existing instruction, minimizing
		  costs and barriers towards learning database systems
		  fundamentals.},
  booktitle	= {Proceedings of the 3rd International Workshop on Data
		  Systems Education: Bridging Education Practice with
		  Education Research},
  pages		= {33–39},
  numpages	= {7},
  keywords	= {ChatGPT, database systems education, foundation models,
		  intro to db, large language models, llm, undergrad
		  databases},
  location	= {Santiago, AA, Chile},
  series	= {DataEd '24}
}

@Article{	  10.1145/3689040,
  author	= {Bomba, Federico and Men\'{e}ndez-Blanco, Mar\'{\i}a and
		  Grigis, Paolo and Cremaschi, Michele and De Angeli,
		  Antonella},
  title		= {The Choreographer-Performer Continuum: A Diffraction Tool
		  to Illuminate Authorship in More Than Human
		  Co-Performances},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {31},
  number	= {6},
  issn		= {1073-0516},
  url		= {https://doi.org/10.1145/3689040},
  doi		= {10.1145/3689040},
  abstract	= {The design of robust and trustworthy Generative AI (GenAI)
		  requires a deep understanding of the agencies emerging from
		  human interactions with them. To contribute to this goal,
		  we retrospectively studied an art project involving a
		  visual artist, a computer scientist, an artistic director,
		  and a generative model (GPT-2). The model was fine-tuned
		  with trip reports describing the experience of eating
		  psychedelic mushrooms. Building on agential realism, we
		  analysed the co-performance between the artist and the
		  model as their agency moved along the
		  choreographer-performer continuum. Results reveal
		  ontological surprises, leading to the proposal of entangled
		  authorship to de-individualise the production of knowledge
		  from a More Than Human perspective. The paper illustrates
		  how art can expose different forms of relationships,
		  challenging the idea of GenAI as just a tool that
		  simplifies or replaces human labour. We conclude by
		  emphasising the transformational potential of GenAI for
		  novel modes of engagement between humans and machines.},
  journal	= {ACM Trans. Comput.-Hum. Interact.},
  month		= dec,
  articleno	= {75},
  numpages	= {23},
  keywords	= {Agency, Agential Realism, Large Language Models, AI and
		  Art, Creative AI, Hallucination}
}

@InProceedings{	  10.1145/3625007.3627505,
  author	= {Ranade, Priyanka and Joshi, Anupam},
  title		= {FABULA: Intelligence Report Generation Using
		  Retrieval-Augmented Narrative Construction},
  year		= {2024},
  isbn		= {9798400704093},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625007.3627505},
  doi		= {10.1145/3625007.3627505},
  abstract	= {Narrative construction is the process of representing
		  disparate event information into a logical plot structure
		  that models an end to end story. Intelligence analysis is
		  an example of a domain that can benefit tremendously from
		  narrative construction techniques, particularly in aiding
		  analysts during the largely manual and costly process of
		  synthesizing event information into comprehensive
		  intelligence reports. Manual intelligence report generation
		  is often prone to challenges such as integrating dynamic
		  event information, writing fine-grained queries, and
		  closing information gaps. This motivates the development of
		  a system that retrieves and represents critical aspects of
		  events in a form that aids in automatic generation of
		  intelligence reports.We introduce a Retrieval Augmented
		  Generation (RAG) approach to augment prompting of an
		  autoregressive decoder by retrieving structured information
		  asserted in a knowledge graph to generate targeted
		  information based on a narrative plot model. We apply our
		  approach to the problem of neural intelligence report
		  generation and introduce FABULA, framework to augment
		  intelligence analysis workflows using RAG. An analyst can
		  use FABULA to query an Event Plot Graph (EPG) to retrieve
		  relevant event plot points, which can be used to augment
		  prompting of a Large Language Model (LLM) during
		  intelligence report generation. Our evaluation studies show
		  that the plot points included in the generated intelligence
		  reports have high semantic relevance, high coherency, and
		  low data redundancy.},
  booktitle	= {Proceedings of the 2023 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {603–610},
  numpages	= {8},
  keywords	= {retrieval augmented generation, large language models,
		  knowledge graphs, narratives},
  location	= {Kusadasi, Turkiye},
  series	= {ASONAM '23}
}

@InProceedings{	  10.1109/ase56229.2023.00019,
  author	= {Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi,
		  Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and
		  Kaulgud, Vikrant},
  title		= {Smart Prompt Advisor: Multi-Objective Prompt Framework for
		  Consistency and Best Practices},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00019},
  doi		= {10.1109/ASE56229.2023.00019},
  abstract	= {Recent breakthroughs in Large Language Models (LLM),
		  comprised of billions of parameters, have achieved the
		  ability to unveil exceptional insight into a wide range of
		  Natural Language Processing (NLP) tasks. The onus of the
		  performance of these models lies in the sophistication and
		  completeness of the input prompt. Minimizing the
		  enhancement cycles of prompt with improvised keywords
		  becomes critically important as it directly affects the
		  time to market and cost of the developing solution.
		  However, this process inevitably has a trade-off between
		  the learning curve/proficiency of the user and completeness
		  of the prompt, as generating such a solutions is an
		  incremental process. In this paper, we have designed a
		  novel solution and implemented it in the form of a plugin
		  for Visual Studio Code IDE, which can optimize this
		  trade-off, by learning the underlying prompt intent to
		  enhance with keywords. This will tend to align with
		  developers' collection of semantics while developing a
		  secure code, ensuring parameter and local variable names,
		  return expressions, simple pre and post-conditions, and
		  basic control and data flow are met.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {1846–1848},
  numpages	= {3},
  keywords	= {prompt engineering, artificial intelligence, deep
		  learning, LLM, ontology},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@InProceedings{	  10.1145/3637528.3671857,
  author	= {Ouyang, Siru and Huang, Jiaxin and Pillai, Pranav and
		  Zhang, Yunyi and Zhang, Yu and Han, Jiawei},
  title		= {Ontology Enrichment for Effective Fine-grained Entity
		  Typing},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671857},
  doi		= {10.1145/3637528.3671857},
  abstract	= {Fine-grained entity typing (FET) is the task of
		  identifying specific entity types at a fine-grained level
		  for entity mentions based on their contextual information.
		  Conventional methods for FET require extensive human
		  annotation, which is time-consuming and costly given the
		  massive scale of data. Recent studies have been developing
		  weakly supervised or zero-shot approaches. We study the
		  setting of zero-shot FET where only an ontology is
		  provided. However, most existing ontology structures lack
		  rich supporting information and even contain ambiguous
		  relations, making them ineffective in guiding FET. Recently
		  developed language models, though promising in various
		  few-shot and zero-shot NLP tasks, may face challenges in
		  zero-shot FET due to their lack of interaction with
		  task-specific ontology. In this study, we propose \o{}urs,
		  where we (1) enrich each node in the ontology structure
		  with two categories of extra information:instance
		  information for training sample augmentation andtopic
		  information to relate types with contexts, and (2) develop
		  a coarse-to-fine typing algorithm that exploits the
		  enriched information by training an entailment model with
		  contrasting topics and instance-based augmented training
		  samples. Our experiments show that \o{}urs achieves
		  high-quality fine-grained entity typing without human
		  annotation, outperforming existing zero-shot methods by a
		  large margin and rivaling supervised methods. \o{}urs also
		  enjoys strong transferability to unseen and finer-grained
		  types. We will open source this work upon acceptance.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2318–2327},
  numpages	= {10},
  keywords	= {fine-grained entity typing, language models, natural
		  language inference, zero-shot learning},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3670105.3670144,
  author	= {Lin, Yunxiao and Tang, Jiahao and Huang, Wenjun and Ding,
		  Yanyu and Hu, Jianguo},
  title		= {Chinese Named Entity Recognition for IC Patent Domain
		  Based on RoBERTa-wwm-ext, GCN and Efficient Global
		  Pointer},
  year		= {2024},
  isbn		= {9798400716751},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670105.3670144},
  doi		= {10.1145/3670105.3670144},
  abstract	= {In the domain of Natural Language Processing (NLP), the
		  task of Named Entity Recognition (NER)for Integrated
		  Circuits (IC) Intellectual Property (IP) presents a
		  substantial challenge.This challenge stems from the
		  intricacies of the Chinese language, characterized by its
		  complexity and character-based nature, the unique
		  structural attributes of IP data, and the difficulties in
		  identifying lengthy and nested entities. To tackle these
		  challenges, our study introduces a novel Chinese Entity
		  Recognition approach named RGEGP, meticulously crafted for
		  the IC IP domain. This approach integrates RoBERTa-wwm-ext,
		  Graph Convolutional Networks (GCN), and Efficient Global
		  Pointer (EGP) techniques into a cohesive framework.
		  Specifically, RoBERTa-wwm-ext, which is designed for
		  Chinese character attributes through a whole word masking
		  pretraining approach, considerably improves the capability
		  of model in recognizing Chinese entities. Additionally, the
		  incorporation of GCN enhances the ability of model to
		  exploit entity relationships and structured Chinese textual
		  information, markedly boosting its performance in entity
		  recognition within complex Chinese texts. Addressing the
		  occurrence of elongated and nested entities, the EGP method
		  utilizes global pointers coupled with an efficient encoding
		  strategy to directly predict entity boundaries and
		  categories. Experimental results on two datasets,
		  particularly concerning IC domain, show that our
		  methodology outperforms existing leading NER models in this
		  field, achieving significant advancements in precision,
		  recall, and F1 scores.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computing, Networks and Internet of Things},
  pages		= {234–240},
  numpages	= {7},
  keywords	= {Large Language Model, Machine Learning, Named Entity
		  Recognition, Natural Language Processing},
  location	= {Tokyo, Japan},
  series	= {CNIOT '24}
}

@InProceedings{	  10.1145/3671151.3671313,
  author	= {Shao, Yujie and Zou, Mengyuan and Kong, Ruiyuan and Dong,
		  Shuhan},
  title		= {Research on Entity Linking Techniques for
		  Architecture-Oriented Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400718106},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3671151.3671313},
  doi		= {10.1145/3671151.3671313},
  abstract	= {In the face of the current increasing demand for the use
		  of architecture knowledge graph, in order to improve the
		  use of architecture knowledge graph efficiency and
		  functional effect, the end of entity linking suitable for
		  architecture knowledge graph is utilized to realize the
		  linking of entities with multiple different entity names
		  referring to the same entity object. Firstly, the
		  BERT-based organizational relationship classification model
		  is used to classify the entities, and the combination of
		  BiLSTM neural network and CRF is used to construct the
		  BERT-BiLSTM-CRF entity recognition framework for entity
		  category identification, and the similarity matching
		  algorithm is used to select the entities with the top three
		  similarity rankings as the entity candidate list after
		  determining the classification of the user input content,
		  and the weights of the entities are increased based on the
		  similarity score and user selection, and the weights are
		  increased based on the similarity score and user selection.
		  degree score and user selection to increase the weight to
		  determine the linking entity pairs, thus improving the
		  quality and efficiency of the past entity categorization
		  identification and linking in the system knowledge graph,
		  and realizing the effect of entity fusion and
		  localization.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Computer Information and Big Data Applications},
  pages		= {924–930},
  numpages	= {7},
  location	= {Wuhan, China},
  series	= {CIBDA '24}
}

@Article{	  10.1145/3657631,
  author	= {Biancofiore, Giovanni Maria and Deldjoo, Yashar and Noia,
		  Tommaso Di and Di Sciascio, Eugenio and Narducci,
		  Fedelucio},
  title		= {Interactive Question Answering Systems: Literature
		  Review},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3657631},
  doi		= {10.1145/3657631},
  abstract	= {Question-answering systems are recognized as popular and
		  frequently effective means of information seeking on the
		  web. In such systems, information seekers can receive a
		  concise response to their queries by presenting their
		  questions in natural language. Interactive question
		  answering is a recently proposed and increasingly popular
		  solution that resides at the intersection of question
		  answering and dialogue systems. On the one hand, the user
		  can ask questions in normal language and locate the actual
		  response to her inquiry; on the other hand, the system can
		  prolong the question-answering session into a dialogue if
		  there are multiple probable replies, very few, or
		  ambiguities in the initial request. By permitting the user
		  to ask more questions, interactive question answering
		  enables users to interact with the system and receive more
		  precise results dynamically.This survey offers a detailed
		  overview of the interactive question-answering methods that
		  are prevalent in current literature. It begins by
		  explaining the foundational principles of
		  question-answering systems, hence defining new notations
		  and taxonomies to combine all identified works inside a
		  unified framework. The reviewed published work on
		  interactive question-answering systems is then presented
		  and examined in terms of its proposed methodology,
		  evaluation approaches, and dataset/application domain. We
		  also describe trends surrounding specific tasks and issues
		  raised by the community, so shedding light on the future
		  interests of scholars. Our work is further supported by a
		  GitHub page synthesizing all the major topics covered in
		  this literature study.},
  journal	= {ACM Comput. Surv.},
  month		= may,
  articleno	= {239},
  numpages	= {38},
  keywords	= {Question answering, natural language processing,
		  interactive systems, human-computer interaction, artificial
		  intelligence, large language model}
}

@InProceedings{	  10.1145/3605098.3635889,
  author	= {Arrieta, Kutz and Fillottrani, Pablo R and Keet, C.
		  Maria},
  title		= {CoSMo: A multilingual modular language for Content
		  Selection Modelling},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3635889},
  doi		= {10.1145/3605098.3635889},
  abstract	= {Representing snippets of information abstractly is a task
		  that needs to be performed for various purposes, such as
		  database view specification and the first stage in the
		  natural language generation pipeline for generative AI from
		  structured input, i.e., the content selection stage to
		  determine what needs to be verbalised. For the Abstract
		  Wikipedia project, requirements analysis revealed that such
		  an abstract representation requires multilingual modelling,
		  content selection covering declarative content and
		  functions, and both classes and instances. There is no
		  modelling language that meets either of the three features,
		  let alone a combination. Following a rigorous language
		  design process inclusive of broad stakeholder consultation,
		  we created CoSMo, a novel Content Selection Modeling
		  language that meets these and other requirements so that it
		  may be useful both in Abstract Wikipedia as well as other
		  contexts. We describe the design process, rationale and
		  choices, the specification, and preliminary evaluation of
		  the language.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {706–713},
  numpages	= {8},
  keywords	= {modeling language, query language, wikidata,
		  multilingualism},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@InProceedings{	  10.1145/3626772.3657815,
  author	= {Joko, Hideaki and Chatterjee, Shubham and Ramsay, Andrew
		  and de Vries, Arjen P. and Dalton, Jeff and Hasibi,
		  Faegheh},
  title		= {Doing Personal LAPS: LLM-Augmented Dialogue Construction
		  for Personalized Multi-Session Conversational Search},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657815},
  doi		= {10.1145/3626772.3657815},
  abstract	= {The future of conversational agents will provide users
		  with personalized information responses. However, a
		  significant challenge in developing models is the lack of
		  large-scale dialogue datasets that span multiple sessions
		  and reflect real-world user preferences. Previous
		  approaches rely on experts in a wizard-of-oz setup that is
		  difficult to scale, particularly for personalized tasks.
		  Our method, LAPS, addresses this by using large language
		  models (LLMs) to guide a single human worker in generating
		  personalized dialogues. This method has proven to speed up
		  the creation process and improve quality. LAPS can collect
		  large-scale, human-written, multi-session, and multi-domain
		  conversations, including extracting user preferences. When
		  compared to existing datasets, LAPS-produced conversations
		  are as natural and diverse as expert-created ones, which
		  stays in contrast with fully synthetic methods. The
		  collected dataset is suited to train preference extraction
		  and personalized response generation. Our results show that
		  responses generated explicitly using extracted preferences
		  better match user's actual preferences, highlighting the
		  value of using extracted preferences over simple dialogue
		  history. Overall, LAPS introduces a new method to leverage
		  LLMs to create realistic personalized conversational data
		  more efficiently and effectively than previous methods.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {796–806},
  numpages	= {11},
  keywords	= {conversational search, dialogue collection,
		  personalization},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3654522.3654568,
  author	= {Nguyen, Anh Quynh and Tran, My Tu and Nguyen, Quang Nhat
		  and Huynh, Huy Khai and Le, Lan Thi Thu and Quach,
		  Luyl-Da},
  title		= {Classification of Rice Plant Disease Based on Descriptive
		  Information with DistilBERT's Architecture},
  year		= {2024},
  isbn		= {9798400716713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3654522.3654568},
  doi		= {10.1145/3654522.3654568},
  abstract	= {Rice has a significant role in human life, and currently,
		  the issue of rice plant diseases is receiving attention in
		  image-related data processing. However, the possibility of
		  applying text classification to address this issue has yet
		  to be explored. Nonetheless, it deserves attention due to
		  the complexity of image-related data. The study gathered
		  descriptive passages on four prevalent rice diseases in
		  Vietnam, with 365 descriptions. The collected data
		  underwent data preprocessing through stopword removal, then
		  visualization to identify crucial words and phrases for
		  disease identification in rice plants. The study used
		  feature extraction and fine-tuning based on DistilBERT's
		  architecture to build models. The research findings showed
		  an impressive peak accuracy rate of 87%, highlighting the
		  potential of using text classification algorithms to
		  classify diseases in crops using descriptions.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Intelligent Information Technology},
  pages		= {155–163},
  numpages	= {9},
  keywords	= {DistilBERT, Large Language model,, Rice Disease, Text
		  Classification},
  location	= {Ho Chi Minh City, Vietnam},
  series	= {ICIIT '24}
}

@InProceedings{	  10.1145/3626772.3661349,
  author	= {Kejriwal, Mayank and Haidarian, Hamid and Chiu, Min-Hsueh
		  and Xiang, Andy and Shrestha, Deep and Javed, Faizan},
  title		= {A Semantic Search Engine for Helping Patients Find Doctors
		  and Locations in a Large Healthcare Organization},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3661349},
  doi		= {10.1145/3626772.3661349},
  abstract	= {Efficiently finding doctors and locations (FDL) is an
		  important search problem for patients in the healthcare
		  domain, for which traditional information retrieval (IR)
		  methods tend to be sub-optimal. This paper introduces and
		  defines FDL as an important healthcare industry-specific
		  problem in IR. We then propose a semantic search engine as
		  a robust solution to FDL in Kaiser Permanente (KP), a large
		  healthcare organization with 12 million members. Our
		  solution meets practical needs of data security and
		  privacy, scalability, cost-effectiveness, backward
		  compatibility with existing indexes and search
		  infrastructure, and interpretability of outputs for
		  patients. It uses a concept-rich ontology to model raw data
		  from multiple sources as entities, relations, and
		  attributes in a knowledge graph that is stored and indexed
		  in an industry-scale graph database. We evaluate the
		  solution on a real patient-query log and demonstrate its
		  practical utility. The system has been implemented and
		  deployed live to KP customers.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2945–2949},
  numpages	= {5},
  keywords	= {finding doctors and locations, healthcare, knowledge
		  graphs, neo4j},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3677779.3677821,
  author	= {Zhang, Shuai and Guan, Yanzhi and Gu, Zhongyu},
  title		= {Research on named entity recognition in the field of CNC
		  machine tool design based on deep learningKnowledge map of
		  mechanical field},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677779.3677821},
  doi		= {10.1145/3677779.3677821},
  abstract	= {Our goal is to extract entities from the text data of
		  unstructured CNC machine tool design for the construction
		  of knowledge graph. The key entity extraction problem in
		  the construction of CNC machine tool design knowledge graph
		  is studied. In order to realize the recognition of named
		  entities, we have formulated the standard and labeling
		  method of knowledge classification for the field of CNC
		  machine tools, and constructed the corresponding domain
		  data set. In addition, we also propose an entity
		  recognition technology based on RoBertTa-BiLSTM-LCRF for
		  CNC machine tool design text. Firstly, we fine-tune the
		  RoBertTa-BiLSTM-LCRF model using data sets in the field of
		  CNC machine tools, and then use RoBERTa to encode the text
		  to generate a vector representation ; next, we use
		  bidirectional long short-term memory ( BiLSTM ) to extract
		  the features of vectors. Finally, we introduce LCRF as the
		  overall optimization layer of the label, so as to derive
		  the best answer and label the entity.The experimental
		  results show that the F1 value of the model in the data set
		  reaches 71.16 % ; for most of the key entities, the value
		  of F1 exceeds 65 % ; this method shows significant
		  advantages in the entity recognition of CNC machine tool
		  design knowledge. It can accurately identify the core
		  entities in the machine tool design knowledge document, and
		  provides a solid data support for the construction of CNC
		  machine tool design knowledge graph.},
  booktitle	= {Proceedings of the International Conference on Modeling,
		  Natural Language Processing and Machine Learning},
  pages		= {257–262},
  numpages	= {6},
  location	= {Xi'an, China},
  series	= {CMNM '24}
}

@InProceedings{	  10.1145/3589335.3641292,
  author	= {Graux, Damien and Montella, S\'{e}bastien and Jabeen,
		  Hajira and Gardent, Claire and Pan, Jeff Z.},
  title		= {[PromptEng] First International Workshop on Prompt
		  Engineering for Pre-Trained Language Models},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3641292},
  doi		= {10.1145/3589335.3641292},
  abstract	= {The recent achievements and availability of Large Language
		  Models have paved the road to a new range of applications
		  and use-cases. Pre-trained language models are now being
		  involved at-scale in many fields where they were until now
		  absent from. More specifically, the progress made by causal
		  generative models has open the door to using them through
		  textual instructions aka. prompts. Unfortunately, the
		  performances of these prompts are highly dependent on the
		  exact phrasing used and therefore practitioners need to
		  adopt fail-retry strategies. This first international
		  workshop on prompt engineering aims at gathering
		  practitioners (both from Academia and Industry) to exchange
		  about good practices, optimizations, results and novel
		  paradigms about the design of efficient prompts to make use
		  of LLMs.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1311–1312},
  numpages	= {2},
  keywords	= {best practices, collective task, llm, prompt engineering},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3652628.3652747,
  author	= {Wang, Nan and Yilahun, Hankiz and Hamdulla, Askar},
  title		= {Research on the Construction and Knowledge Representation
		  Learning Based on Multi-modal Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400708831},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652628.3652747},
  doi		= {10.1145/3652628.3652747},
  abstract	= {Knowledge is the crystallization of human wisdom. As a
		  form of structured human knowledge, knowledge graphs have
		  received widespread attention from the academic and
		  industrial communities since their inception, and have been
		  widely applied in fields such as information retrieval,
		  intelligent question answering, and recommendation systems.
		  Traditionally, these graphs focused solely on textual data,
		  overlooking inter-modality influences. However, with the
		  internet's information explosion, the demand for
		  multi-modal knowledge graphs has surged. Knowledge
		  representation learning is an important application for
		  knowledge graphs completing, and traditional knowledge
		  graph representation learning has often only considered
		  single modality information, which has certain limitations.
		  To address this, our study leverages the FreeBase15K
		  dataset to create a sample multi-modal knowledge graph
		  incorporating text, images, and speech. We also develop an
		  information retrieval system tailored to this knowledge
		  graph and conduct knowledge representation learning across
		  these multi-modal data. Our experiments demonstrate the
		  positive impact of integrating multi-modal data into
		  knowledge representation models. Moreover, compared to the
		  original model, our proposed method, with the same
		  parameters, achieved a 24% improvement on the Hit@10 (raw)
		  metric. To our knowledge, this is the first study to
		  construct a multi-modal knowledge graph and apply it to
		  knowledge representation learning tasks using text, images,
		  and speech as a combined approach.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Artificial Intelligence and Computer Engineering},
  pages		= {715–720},
  numpages	= {6},
  location	= {Dalian, China},
  series	= {ICAICE '23}
}

@InProceedings{	  10.1145/3638884.3638961,
  author	= {Liu, Wenjing and Zhang, Suxiang and Sun, Yang and Sheng,
		  Xing and Wu, Zhidong},
  title		= {New Energy Power Domain Question-Method Extraction And
		  Soft Clustering},
  year		= {2024},
  isbn		= {9798400708909},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638884.3638961},
  doi		= {10.1145/3638884.3638961},
  abstract	= {In recent years, as the field of new energy power has
		  gradually become a research hotspot, there are more and
		  more research results related to new energy power. This
		  paper first proposes to Fine-tune the Chinese LLaMA large
		  language model to realize the extraction of research
		  questions and methods in new energy power results. The
		  fine-tuning dataset is constructed by the combination of
		  rule template and gpt-3.5 enhancement, which avoids the
		  costly and time-consuming problem caused by manual
		  construction. The fine-tuning method adopts LoRA
		  high-efficiency fine-tuning to save computing resources;
		  Then, F1 value is used as the evaluation index to compare
		  the extraction effect of the model under different
		  fine-tuning datasets. The results show that the model has a
		  good extraction effect on the research questions and method
		  terms when training the dataset constructed by the
		  combination of rule template and gpt-3.5 enhancement.
		  Finally, according to the extracted research question
		  phrases, BTM(Biterm Topic Model) is used to study the
		  distribution of topic words, and soft clustering of
		  research question phrases is carried out according to the
		  obtained topic words, so as to realize the correlation
		  between the research results and professional terms, which
		  provides the foundation for the future establishment of the
		  knowledge graph and knowledge base of new energy power.CCS
		  CONCEPTS • Theory of computation • Theory and
		  algorithms for application domains • Unsupervised
		  learning and clustering},
  booktitle	= {Proceedings of the 2023 9th International Conference on
		  Communication and Information Processing},
  pages		= {484–491},
  numpages	= {8},
  keywords	= {Biterm Topic Model, Chinese LLaMA Fine-Tuning, Soft
		  clustering, Terminology extraction},
  location	= {Lingshui, China},
  series	= {ICCIP '23}
}

@InProceedings{	  10.1145/3589334.3645376,
  author	= {Yuan, Chenhan and Xie, Qianqian and Huang, Jimin and
		  Ananiadou, Sophia},
  title		= {Back to the Future: Towards Explainable Temporal Reasoning
		  with Large Language Models},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645376},
  doi		= {10.1145/3589334.3645376},
  abstract	= {Temporal reasoning is a crucial natural language
		  processing (NLP) task, providing a nuanced understanding of
		  time-sensitive contexts within textual data. Although
		  recent advancements in Large Language Models (LLMs) have
		  demonstrated their potential in temporal reasoning, the
		  predominant focus has been on tasks such as temporal
		  expression detection, normalization, and temporal relation
		  extraction. These tasks are primarily designed for the
		  extraction of direct and past temporal cues from given
		  contexts and to engage in simple reasoning processes. A
		  significant gap remains when considering complex reasoning
		  tasks such as event forecasting, which requires multi-step
		  temporal reasoning on events and prediction on the future
		  timestamp. Another notable limitation of existing methods
		  is their incapability to illustrate their reasoning process
		  for explaining their prediction, hindering explainability.
		  In this paper, we introduce the first task of explainable
		  temporal reasoning, to predict an event's occurrence at a
		  future timestamp based on context which requires multiple
		  reasoning over multiple events, and subsequently provide a
		  clear explanation for their prediction. Our task offers a
		  comprehensive evaluation of both the LLMs' complex temporal
		  reasoning ability, the future event prediction ability, and
		  explainability-a critical attribute for AI applications. To
		  support this task, we present the first instruction-tuning
		  dataset of explainable temporal reasoning (ExpTime) with
		  26k derived from the temporal knowledge graph datasets,
		  using a novel knowledge-graph-instructed-generation
		  strategy. Based on the dataset, we propose the first
		  open-source LLM series TimeLlaMA based on the foundation
		  LLM LlaMA2, with the ability of instruction following for
		  explainable temporal reasoning. We compare the performance
		  of our method and a variety of LLMs, where our method
		  achieves the state-of-the-art performance of temporal
		  prediction and explanation generation. We also explore the
		  impact of instruction tuning and different training sizes
		  of instruction-tuning data, highlighting LLM's capabilities
		  and limitations in complex temporal prediction and
		  explanation generation.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {1963–1974},
  numpages	= {12},
  keywords	= {event forecasting, explainable AI, large language models,
		  temporal reasoning},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.14778/3648160.3648174,
  author	= {Zhu, Junhao and Mao, Yuren and Chen, Lu and Ge, Congcong
		  and Wei, Ziheng and Gao, Yunjun},
  title		= {FusionQuery: On-demand Fusion Queries over Multi-source
		  Heterogeneous Data},
  year		= {2024},
  issue_date	= {February 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {6},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3648160.3648174},
  doi		= {10.14778/3648160.3648174},
  abstract	= {Centralised data management systems (e.g., data lakes)
		  support queries over multi-source heterogeneous data.
		  However, the query results from multiple sources commonly
		  involve between-source conflicts, which makes query results
		  unreliable and confusing and degrades the usability of
		  centralised data management systems. Therefore, resolving
		  the between-sourced conflicts is one of the most important
		  problems for centralised data management systems. To solve
		  it, many batch data fusion-based methods have been
		  proposed, which require traversing all the data in the
		  centralised data management systems and cause scalability
		  and flexibility issues.To address these issues, this paper
		  explores the problem of on-demand fusion queries, where the
		  between-sourced conflicts are solved with only the
		  query-related data; moreover, we propose an efficient
		  on-demand fusion query framework, FusionQuery, which
		  consists of a query stage and a fusion stage. In the query
		  stage, we frame the heterogeneous data query problem as a
		  knowledge graph matching problem and present a line
		  graph-based method to accelerate it. In the fusion stage,
		  we develop an Expectation Maximization-style algorithm to
		  iteratively updates data veracity and source
		  trustworthiness. Furthermore, we design an incremental
		  estimation method of source trustworthiness to address the
		  lack of sufficient observations. Extensive experiments on
		  two real-world datasets demonstrate that FusionQuery
		  outperforms state-of-the-art data fusion methods in terms
		  of both effectiveness and efficiency.},
  journal	= {Proc. VLDB Endow.},
  month		= feb,
  pages		= {1337–1349},
  numpages	= {13}
}

@InProceedings{	  10.5555/3643142.3643333,
  author	= {Tu, Ming-Yu and Ehm, Hans and Ismail, Abdelgafar and
		  Ulrich, Philipp},
  title		= {Reusable Ontology Generation and Matching from Simulation
		  Models},
  year		= {2024},
  isbn		= {9798350369663},
  publisher	= {IEEE Press},
  abstract	= {As simulating semiconductor manufacturing grows complex,
		  model reuse becomes appealing since it can reduce the time
		  incurred in developing future models. Also, considering a
		  large network of the semiconductor supply chain, knowledge
		  sharing can enable the efficient development of simulation
		  models in a collaborative organization. Such necessity of
		  reusability and interoperability of simulation models
		  motivates this paper. We will address these challenges
		  through ontological modeling and linking of the simulation
		  components. The first application is generating reusable
		  ontologies from simulation models. Another discussed
		  application is ontology matching for knowledge sharing
		  between simulation components and a meta-model of the
		  semiconductor supply chain. The proposed approach succeeds
		  in automatically transforming simulation into reusable
		  knowledge and identifying interconnection in a
		  semiconductor manufacturing system.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {2298–2309},
  numpages	= {12},
  location	= {San Antonio, Texas, USA},
  series	= {WSC '23}
}

@InProceedings{	  10.1145/3660395.3660474,
  author	= {Liang, Xiangdong and Ham, Hyun-Jin},
  title		= {Target sequences model based on the results of the
		  corelation of demographic variables with other variables:
		  application of the sequential patternmining algorithm},
  year		= {2024},
  isbn		= {9798400716362},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3660395.3660474},
  doi		= {10.1145/3660395.3660474},
  abstract	= {Sequential pattern mining (SPM) is an important technique
		  of pattern mining, which has many applications in reality.
		  Although many efficient sequential pattern mining
		  algorithms have been proposed, there are few studies can
		  focus on target sequences.. This study describes an AI
		  task-specific model application solution method with the
		  results of the influence of demographic variables on other
		  variables for a certain group to construct an intelligent
		  system with improved functions. After investigating the
		  results of the influence of demographic variables on
		  Chinese language learning anxiety and learning engagement,
		  the results are improved from four aspects, namely,
		  sequence pattern mining, knowledge graph, virtual digital
		  human and intelligent emotion monitoring system.},
  booktitle	= {Proceedings of the 2023 3rd Guangdong-Hong Kong-Macao
		  Greater Bay Area Artificial Intelligence and Big Data
		  Forum},
  pages		= {456–460},
  numpages	= {5},
  location	= {Guangzhou, China},
  series	= {AIBDF '23}
}

@InProceedings{	  10.1145/3589335.3651914,
  author	= {Hanikov\'{a}, Kate\v{r}ina and Chud\'{a}n, David and
		  Sv\'{a}tek, Vojt\v{e}ch and Vajde\v{c}ka, Peter and Troncy,
		  Rapha\"{e}l and Vencovsk\'{y}, Filip and Syrov\'{a}tkov\'{a}, Jana},
  title		= {Towards Fact-check Summarization Leveraging on
		  Argumentation Elements Tied to Entity Graphs},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651914},
  doi		= {10.1145/3589335.3651914},
  abstract	= {Fact-check consumers can have different preferences
		  regarding the amount of text being used for explaining the
		  claim veracity verdict. Dynamically adapting the size of a
		  fact-check report is thus an important functionality for
		  systems designed to convey claim verification
		  explainability. Recent works have experimented with
		  applying transformers-based or LLM-based text summarization
		  methods in a zero-shot or few-shot manner, making use of
		  some existing texts available in the summary parts of
		  fact-check reports (e.g., called "justification'' in
		  PolitiFact). However, for complex fact-checks, the purely
		  sub-symbolic summarizers tend to either omit some elements
		  of the fact-checker's argumentation chains or include
		  contextual statements that may not be essential at the
		  given level of granularity. In this paper, we propose a new
		  method for enhancing fact-check summarization with the aim
		  of injecting elements of structured fact-checker
		  argumentation. This argumentation is, in turn, not only
		  captured at the discourse level but tied to an entity graph
		  representing the fact-check, for which we employ the PURO
		  diagrammatic language. We have empirically performed a
		  manual analysis of fact-check reports from two fact-checker
		  websites, yielding (1) textual snippets containing the
		  argumentation essence of the fact-check report and (2)
		  categorized argumentation elements tied to entity graphs.
		  These snippets are then fed to a state-of-the-art hybrid
		  summarizer which has previously produced accurate
		  fact-check summaries, as an additional input. We observe
		  mild improvements on various ROUGE metrics, even if the
		  validity of the results is limited given the small size of
		  the dataset. We also compare the human-provided
		  argumentation element categories with those returned, for
		  the given fact-check ground truth summary, using a
		  pre-trained language model upon both basic and augmented
		  prompting. This yields a moderate accuracy as the model
		  often fails to comply with the explicit given
		  instructions.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1473–1481},
  numpages	= {9},
  keywords	= {argumentation, entity graph, fact-checking, text
		  summarization},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3677525.3678686,
  author	= {Bazouzi, Aymen and Le Capitaine, Ho\"{e}l and Miklos,
		  Zoltan and Foursov, Micka\"{e}l},
  title		= {Precedability Prediction Between Open Educational
		  Resources},
  year		= {2024},
  isbn		= {9798400710940},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677525.3678686},
  doi		= {10.1145/3677525.3678686},
  abstract	= {The abundance of Educational Resources (ERs) has allowed
		  people to have access to a vast amount of knowledge.
		  However, it can be difficult, for both educators and
		  learners, to navigate through these resources. One way to
		  facilitate navigation is to identify useful relations
		  between these resources. This can improve the teaching and
		  learning experiences by allowing the users to go from one
		  resource to another based on the identified relations, such
		  as precedence. In this work, we introduce the notion of
		  precedability between educational resources; whether a
		  resource A can precede another resource B. Then, we propose
		  a two-step method to identify precedability relations
		  between educational resources. Our method structures the
		  educational resources in an enriched Knowledge Graph (KG).
		  Then, it uses a Graph Neural Network (GNN) model to predict
		  precedability relations. Our method performed better than
		  multiple baselines on different benchmarks.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Information Technology for Social Good},
  pages		= {386–393},
  numpages	= {8},
  keywords	= {Educational Resources, Graph Machine Learning, Knowledge
		  graphs},
  location	= {Bremen, Germany},
  series	= {GoodIT '24}
}

@Article{	  10.1145/3660639,
  author	= {Sharma, Mandar and Gogineni, Ajay Kumar and Ramakrishnan,
		  Naren},
  title		= {Neural Methods for Data-to-text Generation},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {5},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3660639},
  doi		= {10.1145/3660639},
  abstract	= {The neural boom that has sparked natural language
		  processing (NLP) research throughout the last decade has
		  similarly led to significant innovations in data-to-text
		  (D2T) generation. This survey offers a consolidated view
		  into the neural D2T paradigm with a structured examination
		  of the approaches, benchmark datasets, and evaluation
		  protocols. This survey draws boundaries separating D2T from
		  the rest of the natural language generation (NLG)
		  landscape, encompassing an up-to-date synthesis of the
		  literature, and highlighting the stages of technological
		  adoption from within and outside the greater NLG umbrella.
		  With this holistic view, we highlight promising avenues for
		  D2T research that focus not only on the design of
		  linguistically capable systems but also on systems that
		  exhibit fairness and accountability.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  articleno	= {89},
  numpages	= {46},
  keywords	= {Narration, data-to-text, data-to-text generation, natural
		  language generation}
}

@InProceedings{	  10.1109/ase56229.2023.00122,
  author	= {Shao, Shuai and Yu, Tingting},
  title		= {Information Retrieval-Based Fault Localization for
		  Concurrent Programs},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00122},
  doi		= {10.1109/ASE56229.2023.00122},
  abstract	= {Information retrieval-based fault localization (IRFL)
		  techniques have been proposed as a solution to identify the
		  files that are likely to contain faults that are root
		  causes of failures reported by users. These techniques have
		  been extensively studied to accurately rank source files,
		  however, none of the existing approaches have focused on
		  the specific case of concurrent programs. This is a
		  critical issue since concurrency bugs are notoriously
		  difficult to identify. To address this problem, this paper
		  presents a novel approach called BLCoiR, which aims to
		  reformulate bug report queries to more accurately localize
		  source files related to concurrency bugs. The key idea of
		  BLCoiR is based on a novel knowledge graph (KG), which
		  represents the domain entities extracted from the
		  concurrency bug reports and their semantic relations. The
		  KG is then transformed into the IR query to perform fault
		  localization. BLCoiR leverages natural language processing
		  (NLP) and concept modeling techniques to construct the
		  knowledge graph. Specifically, NLP techniques are used to
		  extract relevant entities from the bug reports, such as the
		  word entities related to concurrency constructs. These
		  entities are then linked together based on their semantic
		  relationships, forming the KG. We have conducted an
		  empirical study on 692 concurrency bug reports from 44
		  real-world applications. The results show that BLCoiR
		  outperforms existing IRFL techniques in terms of accuracy
		  and efficiency in localizing concurrency bugs. BLCoiR
		  demonstrates effectiveness of using a knowledge graph to
		  model the domain entities and their relationships,
		  providing a promising direction for future research in this
		  area.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {1467–1479},
  numpages	= {13},
  keywords	= {concurrent program, fault localization, information
		  retrieval},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@InProceedings{	  10.1145/3589335.3651970,
  author	= {Joshi, Saurav and Ilievski, Filip and Luceri, Luca},
  title		= {Contextualizing Internet Memes Across Social Media
		  Platforms},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651970},
  doi		= {10.1145/3589335.3651970},
  abstract	= {Internet memes have emerged as a novel format for
		  communication and expressing ideas on the web. Their
		  fluidity and creative nature are reflected in their
		  widespread use, often across platforms and occasionally for
		  unethical or harmful purposes. While computational work has
		  already analyzed their high-level virality over time and
		  developed specialized classifiers for hate speech
		  detection, there have been no efforts to date that aim to
		  holistically track, identify, and map internet memes posted
		  on social media. To bridge this gap, we investigate whether
		  internet memes across social media platforms can be
		  contextualized by using a semantic repository of knowledge,
		  namely, a knowledge graph. We collect thousands of
		  potential internet meme posts from two social media
		  platforms, namely Reddit and Discord, and develop an
		  extract-transform-load procedure to create a data lake with
		  candidate meme posts. By using vision transformer-based
		  similarity, we match these candidates against the memes
		  cataloged in IMKG --- a recently released knowledge graph
		  of internet memes. We leverage this grounding to highlight
		  the potential of our proposed framework to study the
		  prevalence of memes on different platforms, map them to
		  IMKG, and provide context about memes on social media.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1831–1840},
  numpages	= {10},
  keywords	= {internet memes, knowledge graphs, social media},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3643489.3661127,
  author	= {Rossetto, Luca and Kyriakou, Athina and Lange, Svenja and
		  Ruosch, Florian and Wang, Ruijie and Wardatzky, Kathrin and
		  Bernstein, Abraham},
  title		= {LifeGraph 4 - Lifelog Retrieval using Multimodal Knowledge
		  Graphs and Vision-Language Models},
  year		= {2024},
  isbn		= {9798400705502},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643489.3661127},
  doi		= {10.1145/3643489.3661127},
  abstract	= {In the scope of the 7th Lifelog Search Challenge (LSC'24),
		  we present the 4th iteration of LifeGraph, a multimodal
		  knowledge-graph approach with data augmentations using
		  Vision-Language Models (VLM). We extend the LifeGraph model
		  presented in former LSC challenges by event-based
		  clustering using temporal and spatial relations as well as
		  information extracted from descriptions of Lifelog image
		  captions produced by VLMs.},
  booktitle	= {Proceedings of the 7th Annual ACM Workshop on the Lifelog
		  Search Challenge},
  pages		= {88–92},
  numpages	= {5},
  keywords	= {lifelogging, lifelog search challenge, multimodal
		  knowledge graphs, graph-based retrieval, multi-modal
		  retrieval, vision-language models},
  location	= {Phuket, Thailand},
  series	= {LSC '24}
}

@InProceedings{	  10.1145/3640457.3688146,
  author	= {Yang, Ting and Chen, Li},
  title		= {Unleashing the Retrieval Potential of Large Language
		  Models in Conversational Recommender Systems},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688146},
  doi		= {10.1145/3640457.3688146},
  abstract	= {Conversational recommender systems (CRSs) aim to capture
		  user preferences and provide personalized recommendations
		  through interactive natural language interaction. The
		  recent advent of large language models (LLMs) has
		  revolutionized human engagement in natural conversation,
		  driven by their extensive world knowledge and remarkable
		  natural language understanding and generation capabilities.
		  However, introducing LLMs into CRSs presents new technical
		  challenges. Directly prompting LLMs for recommendation
		  generation requires understanding a large and evolving item
		  corpus, as well as grounding the generated recommendations
		  in the real item space. On the other hand, generating
		  recommendations based on external recommendation engines or
		  directly integrating their suggestions into responses may
		  constrain the overall performance of LLMs, since these
		  engines generally have inferior representation abilities
		  compared to LLMs. To address these challenges, we propose
		  an end-to-end large-scale CRS model, named as ReFICR, a
		  novel LLM-enhanced conversational recommender that empowers
		  a retrievable large language model to perform
		  conversational recommendation by following retrieval and
		  generation instructions through lightweight tuning. By
		  decomposing the complex CRS task into multiple subtasks, we
		  formulate these subtasks into two types of instruction
		  formats: retrieval and generation. The hidden states of
		  ReFICR are utilized for generating text embeddings for
		  retrieval, and simultaneously ReFICR is fine-tuned to
		  handle generation subtasks. We optimize the contrastive
		  objective to enhance text embeddings for retrieval and
		  jointly fine-tune the large language model objective for
		  generation. Our experimental results on public datasets
		  demonstrate that ReFICR significantly outperforms baselines
		  in terms of recommendation accuracy and response quality.
		  Our code is publicly available at the link:
		  https://github.com/yt556677/ReFICR.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {43–52},
  numpages	= {10},
  keywords	= {Conversational Recommender Systems, Instruction Tuning,
		  Retrievable Large Language Models},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3626772.3657899,
  author	= {Sojitra, Daivik and Jain, Raghav and Saha, Sriparna and
		  Jatowt, Adam and Gupta, Manish},
  title		= {Timeline Summarization in the Era of LLMs},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657899},
  doi		= {10.1145/3626772.3657899},
  abstract	= {Timeline summarization is the task of automatically
		  generating concise overviews of documents that capture the
		  key events and their progression on timelines. While this
		  capability is useful for quickly comprehending event
		  sequences without reading lengthy descriptions, timeline
		  summarization remains a relatively underexplored area in
		  recent years when compared to traditional document
		  summarization task and their evolution. The advent of large
		  language models (LLMs) has led some to presume
		  summarization as a solved problem. However, timeline
		  summarization poses unique challenges for LLMs. Our
		  investigation is centered on evaluating the performance of
		  LLMs, against state-of-the-art models in this field. We
		  employed three different approaches: chunking, knowledge
		  graph-based summarization, and TimeRanker. Each of these
		  methods was systematically tested on three benchmark
		  datasets for timeline summarization to assess their
		  effectiveness in capturing and condensing key events and
		  their evolution within timelines. Our findings reveal that
		  while LLMs show promise, timeline summarization remains a
		  complex task that is not yet fully resolved.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2657–2661},
  numpages	= {5},
  keywords	= {benchmarking, knowledge graphs, llms, timeline
		  summarization},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3660043.3660172,
  author	= {Zhu, Guibin and Zhao, Bo and Tang, Jianbo},
  title		= {Research on the Application of AI in Personalized
		  Education},
  year		= {2024},
  isbn		= {9798400716157},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3660043.3660172},
  doi		= {10.1145/3660043.3660172},
  abstract	= {Smart education uses advanced information technology,
		  combined with educational theories and teaching methods, to
		  achieve automatic, intelligent, and efficient teaching
		  process. Personalized education represents the core content
		  and goal of smart education because traditional classroom
		  or remote teaching can't accurately grasp every student's
		  individual knowledge and understanding of the content being
		  taught. The development of artificial intelligence
		  technology has provided technical support for smart
		  education, particularly for personalized education. Natural
		  language processing models such as chatGPT and knowledge
		  graph technology have made personalized education
		  increasingly practicable. This article describes the
		  technological framework of smart education, its potential
		  applications, and emphasizes the use of AI technology in
		  personalized education. The article covers topic areas,
		  including learning situation analysis, implementing
		  personalized instruction, personalized teaching
		  management.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Information Education and Artificial Intelligence},
  pages		= {723–727},
  numpages	= {5},
  location	= {Xiamen, China},
  series	= {ICIEAI '23}
}

@InProceedings{	  10.1145/3665939.3665969,
  author	= {Li, Yilin and Jobson, Deddy},
  title		= {LLMs as an Interactive Database Interface for Designing
		  Large Queries},
  year		= {2024},
  isbn		= {9798400706936},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665939.3665969},
  doi		= {10.1145/3665939.3665969},
  abstract	= {Text2SQL is typically considered a one-shot process where
		  the user gives a natural language query and receives an SQL
		  query in return. This approach is fraught with potential
		  concerns, such as syntactical errors, logical mismatches,
		  and schema hallucination, which often require
		  time-consuming validations by end users. These challenges
		  are exacerbated by the complexity of large queries typical
		  in industry settings and the inherent ambiguity of natural
		  language. To address these limitations, we propose a system
		  that employs an iterative process for both query creation
		  and validation, ensuring that the resulting data set meets
		  the user's expectations. We tested this system against
		  existing text-to-SQL LLM approaches using a standard
		  industry use case, showcasing our system's ability to
		  deliver coherent and accurate outcomes. Opportunities for
		  future research to further refine this approach are also
		  discussed1.},
  booktitle	= {Proceedings of the 2024 Workshop on Human-In-the-Loop Data
		  Analytics},
  pages		= {1–7},
  numpages	= {7},
  keywords	= {Text2SQL, LLM, large language models, human-in-the-loop},
  location	= {Santiago, AA, Chile},
  series	= {HILDA 24}
}

@InProceedings{	  10.1145/3627050.3630732,
  author	= {Gui, Zhou and Freund, Michael and Harth, Andreas},
  title		= {A Natural Language Interface for IoT Systems Using the Web
		  of Things Abstraction},
  year		= {2024},
  isbn		= {9798400708541},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627050.3630732},
  doi		= {10.1145/3627050.3630732},
  abstract	= {We present a demo of a Natural Language Interface (NLI)
		  for controlling Internet of Things (IoT) devices using the
		  Web of Things (WoT) specification as an intermediate
		  abstraction layer. All interaction information of a device
		  is stored in a Knowledge Graph using the thing description
		  ontology. The central component of the NLI is a
		  sequence-to-sequence neural network model for text to code
		  translation. We build a data corpus based on the
		  functionalities of a Philips Hue smart lamp and use the
		  corpus to train the text to code model. Our demonstration
		  illustrates how to control the power state, the light
		  colour, and the brightness of a Philips Hue smart lamp
		  using natural language commands. The implementation of an
		  NLI system based on the WoT specification represents an
		  approach towards the development of easy-to-use and
		  interoperable IoT systems.},
  booktitle	= {Proceedings of the 13th International Conference on the
		  Internet of Things},
  pages		= {186–188},
  numpages	= {3},
  keywords	= {Knowledge Graphs, Natural Language Understanding, Text to
		  Code, Web of Things},
  location	= {Nagoya, Japan},
  series	= {IoT '23}
}

@InProceedings{	  10.1145/3627673.3679083,
  author	= {Cui, Xiquan and Dave, Vachik and Su, Yi and Al Jadda,
		  Khalifeh and Kumar, Srijan and McAuley, Julian and Ye, Tao
		  and Guo, Stephen and Huyen, Chip},
  title		= {International Workshop on Online and Adaptive Recommender
		  Systems (OARS 2024)},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679083},
  doi		= {10.1145/3627673.3679083},
  abstract	= {Recommender system (RecSys) plays important roles in
		  helping users navigate, discover, and consume massive and
		  highly-dynamic information. Today, many RecSys solutions
		  deployed in the real world rely on categorical
		  user-profiles and/or pre-calculated recommendation actions
		  that stay static during a user session. However, recent
		  trends suggest that RecSys need to model user intent in
		  real time and constantly adapt to meet user needs at the
		  moment or change user behavior in situ. There are three
		  primary drivers for this emerging need of online
		  adaptation. First, in order to meet the increasing demand
		  for a better personalized experience, the personalization
		  dimensions and space will grow larger and larger. It would
		  not be feasible to pre-compute recommended actions for all
		  personalization scenarios beyond a certain scale. Second,
		  in many settings the system does not have user prior
		  history to leverage. Estimating user intent in real time is
		  the only feasible way to personalize. As various consumer
		  privacy laws tighten, it is foreseeable that many
		  businesses will reduce their reliance on static user
		  profiles. Therefore, it makes the modeling of user intent
		  in real time an important research topic. Third, a user's
		  intent often changes within a session and between sessions,
		  and user behavior could shift significantly during dramatic
		  events. Therefore, it is important to investigate more on
		  online and adaptive recommender system (OARS) that can
		  adapt in real time to meet user needs and be robust against
		  distribution shifts. Every year, the organizers survey the
		  most important topics for OARS and propose a new workshop
		  program. In light of the recent advancement of LLMs and
		  foundation models in RecSys, in this new edition, we decide
		  to formally add the new topic of foundation and LLM models
		  in OARS. We will invite experts and papers in the field to
		  facilitate its further advancement. Our workshop offers a
		  focused discussion of the new study and application of
		  OARS, and will bring together an interdisciplinary
		  community of researchers and practitioners from both
		  industry and academia to discuss on new topics in the area,
		  grow a community, and push the direction forward.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5580–5583},
  numpages	= {4},
  keywords	= {artificial intelligence, foundation model, gen ai, llm,
		  recommender system},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3639476.3639770,
  author	= {Maninger, Daniel and Narasimhan, Krishna and Mezini,
		  Mira},
  title		= {Towards Trustworthy AI Software Development Assistance},
  year		= {2024},
  isbn		= {9798400705007},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639476.3639770},
  doi		= {10.1145/3639476.3639770},
  abstract	= {It is expected that in the near future, AI software
		  development assistants will play an important role in the
		  software industry. However, current software development
		  assistants tend to be unreliable, often producing
		  incorrect, unsafe, or low-quality code. We seek to resolve
		  these issues by introducing a holistic architecture for
		  constructing, training, and using trustworthy AI software
		  development assistants. In the center of the architecture,
		  there is a foundational LLM trained on datasets
		  representative of real-world coding scenarios and complex
		  software architectures, and fine-tuned on code quality
		  criteria beyond correctness. The LLM will make use of
		  graph-based code representations for advanced semantic
		  comprehension. We envision a knowledge graph integrated
		  into the system to provide up-to-date background knowledge
		  and to enable the assistant to provide appropriate
		  explanations. Finally, a modular framework for constrained
		  decoding will ensure that certain guarantees (e.g., for
		  correctness and security) hold for the generated code.},
  booktitle	= {Proceedings of the 2024 ACM/IEEE 44th International
		  Conference on Software Engineering: New Ideas and Emerging
		  Results},
  pages		= {112–116},
  numpages	= {5},
  location	= {Lisbon, Portugal},
  series	= {ICSE-NIER'24}
}

@InProceedings{	  10.1145/3627673.3679722,
  author	= {Shi, Yucheng and Tan, Qiaoyu and Wu, Xuansheng and Zhong,
		  Shaochen and Zhou, Kaixiong and Liu, Ninghao},
  title		= {Retrieval-enhanced Knowledge Editing in Language Models
		  for Multi-Hop Question Answering},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679722},
  doi		= {10.1145/3627673.3679722},
  abstract	= {Large Language Models (LLMs) have shown proficiency in
		  question-answering tasks but often struggle to integrate
		  real-time knowledge, leading to potentially outdated or
		  inaccurate responses. This problem becomes even more
		  challenging when dealing with multi-hop questions, since
		  they require LLMs to update and integrate multiple
		  knowledge pieces relevant to the questions. To tackle the
		  problem, we propose the Retrieval-Augmented model Editing
		  (RAE) framework for multi-hop question answering. RAE first
		  retrieves edited facts and then refines the language model
		  through in-context learning. Specifically, our retrieval
		  approach, based on mutual information maximization,
		  leverages the reasoning abilities of LLMs to identify chain
		  facts that traditional similarity-based searches might
		  miss. In addition, our framework includes a pruning
		  strategy to eliminate redundant information from the
		  retrieved facts, which enhances the editing accuracy and
		  mitigates the hallucination problem. Our framework is
		  supported by theoretical justification for its fact
		  retrieval efficacy. Finally, comprehensive evaluation
		  across various LLMs validates RAE's ability in providing
		  accurate answers with updated knowledge. Our code is
		  available at: https://github.com/sycny/RAE.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2056–2066},
  numpages	= {11},
  keywords	= {model editing, question answering, retrieval-augmented
		  generation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3685650.3685667,
  author	= {Wanna, Selma and Solovyev, Nicholas and Barron, Ryan and
		  Eren, Maksim E. and Bhattarai, Manish and Rasmussen, Kim
		  \O{}. and Alexandrov, Boian S.},
  title		= {TopicTag: Automatic Annotation of NMF Topic Models Using
		  Chain of Thought and Prompt Tuning with LLMs},
  year		= {2024},
  isbn		= {9798400711695},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3685650.3685667},
  doi		= {10.1145/3685650.3685667},
  abstract	= {Topic modeling is a technique for organizing and
		  extracting themes from large collections of unstructured
		  text. Non-negative matrix factorization (NMF) is a common
		  unsupervised approach that decomposes a term
		  frequency-inverse document frequency (TF-IDF) matrix to
		  uncover latent topics and segment the dataset accordingly.
		  While useful for highlighting patterns and clustering
		  documents, NMF does not provide explicit topic labels,
		  necessitating subject matter experts (SMEs) to assign
		  labels manually. We present a methodology for automating
		  topic labeling in documents clustered via NMF with
		  automatic model determination (NMFk). By leveraging the
		  output of NMFk and employing prompt engineering, we utilize
		  large language models (LLMs) to generate accurate topic
		  labels. Our case study on over 34,000 scientific abstracts
		  on Knowledge Graphs demonstrates the effectiveness of our
		  method in enhancing knowledge management and document
		  organization.},
  booktitle	= {Proceedings of the ACM Symposium on Document Engineering
		  2024},
  articleno	= {8},
  numpages	= {4},
  keywords	= {chain of thought, llm, nmf, prompt tuning, topic
		  labeling},
  location	= {San Jose, CA, USA},
  series	= {DocEng '24}
}

@InProceedings{	  10.1145/3664647.3680995,
  author	= {He, Liang and Wang, Hongke and Wu, Zhen and Zhang,
		  Jianbing and Dai, Xinyu and Chen, Jiajun},
  title		= {Focus &amp; Gating: A Multimodal Approach for Unveiling
		  Relations in Noisy Social Media},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3680995},
  doi		= {10.1145/3664647.3680995},
  abstract	= {Multimedia content's surge on the internet has made
		  multimodal relation extraction vital for applications like
		  intelligent search and knowledge graph construction. As a
		  rich source of image-text data, social media plays a
		  crucial role in populating knowledge bases. However, the
		  noisy information present in social media poses a challenge
		  in multimodal relation extraction. Current methods focus on
		  extracting relevant information from images to improve
		  model performance but often overlook the importance of
		  global image information. In this paper, we propose a novel
		  multimodal relation extraction method FocalMRE, which
		  leverages image focal augmentation, focal attention, and
		  gating mechanisms. FocalMRE enables the model to
		  concentrate on the image's focal regions while effectively
		  utilizing the global information in the image. Through
		  gating mechanisms, FocalMRE optimizes the multimodal fusion
		  strategy, allowing the model to select the most relevant
		  augmented regions for overcoming noise interference in
		  relation extraction. The experimental results on the public
		  MNRE dataset reveal that FocalMRE exhibits robust and
		  significant performance advantages in the multimodal
		  relation extraction task, especially in scenarios with high
		  noise, long-tail distributions, and limited resources. The
		  code is available at https://github.com/NJUNLP/FocalMRE.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {1379–1388},
  numpages	= {10},
  keywords	= {focal attention, focal augmentation, gating mechanism,
		  multimodal relation extraction, noisy social media},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3589335.3651234,
  author	= {Jiang, Xinxi and Li, Xiang and Zhou, Qifeng and Wang,
		  Qing},
  title		= {GRACE: Generating Cause and Effect of Disaster Sub-Events
		  from Social Media Text},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651234},
  doi		= {10.1145/3589335.3651234},
  abstract	= {In recent years, social media has emerged as a pivotal
		  source of emergency response for natural disasters. Causal
		  analysis of disaster sub-events is one of crucial concerns.
		  However, the design and implementation of its application
		  scenario present significant challenges, due to the
		  intricate nature of events and information overload. In
		  this work, we introduce GRACE, a system designed for
		  generating the cause and effect of disaster sub-events from
		  social media text. GRACE aims to provide a rapid,
		  comprehensive, and real-time analysis of disaster
		  intelligence. Different from conventional information
		  digestion systems, GRACE employs event evolution reasoning
		  by constructing a causal knowledge graph for disaster
		  sub-events (referred to as DSECG) and fine-tuning GPT-2 on
		  DSECG. This system offers users a comprehensive
		  understanding of disaster events and supports human
		  organizations in enhancing response efforts during disaster
		  situations. Moreover, an online demo is accessible,
		  allowing user interaction with GRACE and providing a visual
		  representation of the cause and effect of disaster
		  sub-events.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {999–1002},
  numpages	= {4},
  keywords	= {cause and effect, disaster information system, social
		  media text},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3627673.3679837,
  author	= {Huang, Shulin and Ma, Shirong and Li, Yangning and Li,
		  Yinghui and Zheng, Hai-Tao},
  title		= {From Retrieval to Generation: Efficient and Effective
		  Entity Set Expansion},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679837},
  doi		= {10.1145/3627673.3679837},
  abstract	= {Entity Set Expansion (ESE) is a critical task aiming at
		  expanding entities of the target semantic class described
		  by seed entities. Most existing ESE methods are
		  retrieval-based frameworks that need to extract contextual
		  features of entities and calculate the similarity between
		  seed entities and candidate entities. To achieve the two
		  purposes, they iteratively traverse the corpus and the
		  entity vocabulary, resulting in poor efficiency and
		  scalability. Experimental results indicate that the time
		  consumed by the retrieval-based ESE methods increases
		  linearly with entity vocabulary and corpus size. In this
		  paper, we firstly propose Generative Entity Set Expansion
		  (GenExpan) framework, which utilizes a generative
		  pre-trained auto-regressive language model to accomplish
		  ESE task. Specifically, a prefix tree is employed to
		  guarantee the validity of entity generation, and
		  automatically generated class names are adopted to guide
		  the model to generate target entities. Moreover, we propose
		  Knowledge Calibration and Generative Ranking to further
		  bridge the gap between generic knowledge of the language
		  model and the goal of ESE task. For efficiency, expansion
		  time consumed by GenExpan is independent of entity
		  vocabulary and corpus size, and GenExpan achieves an
		  average 600% speedup compared to strong baselines. For
		  expansion effectiveness, our framework outperforms previous
		  state-of-the-art ESE methods.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {921–931},
  numpages	= {11},
  keywords	= {entity set expansion, generative framework, knowledge
		  discovery},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3709155,
  author	= {Villena, Fabi\'{a}n and Quiroga, Tamara and Dunstan,
		  Jocelyn},
  title		= {Clinical analogy resolution performance for foundation
		  language models},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3709155},
  doi		= {10.1145/3709155},
  abstract	= {Using extensive data sources to create foundation language
		  models has revolutionized the performance of deep
		  learning-based architectures. This remarkable improvement
		  has led to state-of-the-art results for various downstream
		  NLP tasks, including clinical tasks. However, more research
		  is needed to measure model performance intrinsically,
		  especially in the clinical domain. We revisit the use of
		  analogy questions as an effective method to measure the
		  intrinsic performance of language models for the clinical
		  domain in English. We tested multiple Transformers-based
		  language models over analogy questions constructed from the
		  Unified Medical Language System (UMLS), a massive knowledge
		  graph of clinical concepts. Our results show that large
		  language models are significantly more performant for
		  analogy resolution than small language models. Similarly,
		  domain-specific language models perform better than general
		  domain language models. We also found a correlation between
		  intrinsic and extrinsic performance, validated through
		  PubMedQA extrinsic task. Creating clinical-specific and
		  language-specific language models is essential for
		  advancing biomedical and clinical NLP and will ensure a
		  valid application in clinical practice. Finally, given that
		  our proposed intrinsic test is based on a term graph
		  available in multiple languages, the dataset can be built
		  to measure the performance of models in languages other
		  than English.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= dec,
  keywords	= {Foundation Models, Clinical NLP, Intrinsic Tests}
}

@InProceedings{	  10.1145/3673791.3698435,
  author	= {Wang, Zihan and Ge, Xuri and Jose, Joemon M. and Yu,
		  Haitao and Ma, Weizhi and Ren, Zhaochun and Xin, Xin},
  title		= {R3AG: First Workshop on Refined and Reliable Retrieval
		  Augmented Generation},
  year		= {2024},
  isbn		= {9798400707247},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3673791.3698435},
  doi		= {10.1145/3673791.3698435},
  abstract	= {Retrieval-augmented generation (RAG) has gained wide
		  attention as the key component to improve generative models
		  with external knowledge augmentation from information
		  retrieval. It has shown great prominence in enhancing the
		  functionality and performance of large language model
		  (LLM)-based applications. However, with the comprehensive
		  application of RAG, more and more problems and limitations
		  have been identified, thus urgently requiring further
		  fundamental exploration to improve current RAG frameworks.
		  This workshop aims to explore in depth how to conduct
		  refined and reliable RAG for downstream AI tasks.To this
		  end, we propose to organize the first R3AG workshop at
		  SIGIR-AP 2024 to call for participants to re-examine and
		  formulate the basic principles and practical implementation
		  of refined and reliable RAG. The workshop serves as a
		  platform for both academia and industry researchers to
		  conduct discussions, share insights, and foster research to
		  build the next generation of RAG systems. Participants will
		  engage in discussions and presentations focusing on
		  fundamental challenges, cutting-edge research, and
		  potential pathways to improve RAG. At the end of the
		  workshop, we aim to have a clearer understanding of how to
		  improve the reliability and applicability of RAG with more
		  robust information retrieval and language generation.},
  booktitle	= {Proceedings of the 2024 Annual International ACM SIGIR
		  Conference on Research and Development in Information
		  Retrieval in the Asia Pacific Region},
  pages		= {307–310},
  numpages	= {4},
  keywords	= {information retrieval, large lan- guage models,
		  reliability, retrieval-augmented generation},
  location	= {Tokyo, Japan},
  series	= {SIGIR-AP 2024}
}

@InProceedings{	  10.1145/3627673.3679659,
  author	= {Huang, Yubo and Zeng, Guosun},
  title		= {RD-P: A Trustworthy Retrieval-Augmented Prompter with
		  Knowledge Graphs for LLMs},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679659},
  doi		= {10.1145/3627673.3679659},
  abstract	= {Large Language Models (LLMs) face challenges due to
		  hallucination issues. Current solutions use
		  retrieval-augmented generation (RAG), integrating LLMs with
		  external knowledge to enhance answer accuracy. However, the
		  misuse of irrelevant external knowledge can be misleading.
		  In this paper, we propose a novel method called
		  Retrieve-and-Discriminate Prompter (RD-P), which leverages
		  knowledge graphs (KGs) for trustworthy RAG by synchronizing
		  knowledge retrieval and discrimination in a unified model.
		  Specifically, we train a prompter based on a pre-trained
		  language model with shared parameters. It has two key
		  modules: the retriever and the discriminator. The retriever
		  identifies relevant reasoning paths in the KG, while the
		  discriminator evaluates their credibility through "logical
		  coverage calculation" and in turn instructs the retrieval
		  process. Prompts are then constructed to guide LLMs in
		  reasoning and answering questions using both retrieved and
		  implicit knowledge. Experiments on knowledge-intensive
		  question answering (QA) tasks demonstrate that our method
		  significantly improves answer coverage rate while reducing
		  the retrieval scale, achieving superior performance in
		  complex KGQA tasks compared with state-of-the-art RAG
		  methods at a low cost.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {942–952},
  numpages	= {11},
  keywords	= {kgqa, large language models, prompter, retrieval-augmented
		  generation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.14778/3681954.3681987,
  author	= {Yan, Mengyi and Fan, Wenfei and Wang, Yaoshu and Xie,
		  Min},
  title		= {Enriching Relations with Additional Attributes for ER},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {11},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3681954.3681987},
  doi		= {10.14778/3681954.3681987},
  abstract	= {This paper studies a new problem of relation enrichment.
		  Given a relation D of schema R and a knowledge graph G with
		  overlapping information, it is to identify a small number
		  of relevant features from G, and extend schema R with the
		  additional attributes, to maximally improve the accuracy of
		  resolving entities represented by the tuples of D. We
		  formulate the enrichment problem and show its
		  intractability. Nonetheless, we propose a method to extract
		  features from G that are diverse from the existing
		  attributes of R, minimize null values, and moreover, reduce
		  false positives and false negatives of entity resolution
		  (ER) models. The method links tuples and vertices that
		  refer to the same entity, learns a robust policy to extract
		  attributes via reinforcement learning, and jointly trains
		  the policy and ER models. Moreover, we develop algorithms
		  for (incrementally) enriching D. Using real-life data, we
		  experimentally verify that relation enrichment improves the
		  accuracy of ER above 15.4% (percentage points) by adding 5
		  attributes, up to 33%.},
  journal	= {Proc. VLDB Endow.},
  month		= jul,
  pages		= {3109–3123},
  numpages	= {15}
}

@InProceedings{	  10.1145/3589334.3645610,
  author	= {Bl\"{u}baum, Lukas and Heindorf, Stefan},
  title		= {Causal Question Answering with Reinforcement Learning},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645610},
  doi		= {10.1145/3589334.3645610},
  abstract	= {Causal questions inquire about causal relationships
		  between different events or phenomena. They are important
		  for a variety of use cases, including virtual assistants
		  and search engines. However, many current approaches to
		  causal question answering cannot provide explanations or
		  evidence for their answers. Hence, in this paper, we aim to
		  answer causal questions with a causality graph, a
		  large-scale dataset of causal relations between noun
		  phrases along with the relations' provenance data. Inspired
		  by recent, successful applications of reinforcement
		  learning to knowledge graph tasks, such as link prediction
		  and fact-checking, we explore the application of
		  reinforcement learning on a causality graph for causal
		  question answering. We introduce an Actor-Critic-based
		  agent which learns to search through the graph to answer
		  causal questions. We bootstrap the agent with a supervised
		  learning procedure to deal with large action spaces and
		  sparse rewards. Our evaluation shows that the agent
		  successfully prunes the search space to answer binary
		  causal questions by visiting less than 30 nodes per
		  question compared to over 3,000 nodes by a naive
		  breadth-first search. Our ablation study indicates that our
		  supervised learning strategy provides a strong foundation
		  upon which our reinforcement learning agent improves. The
		  paths returned by our agent explain the mechanisms by which
		  a cause produces an effect. Moreover, for each edge on a
		  path, our causality graph provides its original source
		  allowing for easy verification of paths.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2204–2215},
  numpages	= {12},
  keywords	= {causality graphs, question answering, reinforcement
		  learning},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3605098.3635949,
  author	= {Layegh, Amirhossein and Payberah, Amir H. and Soylu, Ahmet
		  and Roman, Dumitru and Matskin, Mihhail},
  title		= {Wiki-based Prompts for Enhancing Relation Extraction using
		  Language Models},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3635949},
  doi		= {10.1145/3605098.3635949},
  abstract	= {Prompt-tuning and instruction-tuning of language models
		  have exhibited significant results in few-shot Natural
		  Language Processing (NLP) tasks, such as Relation
		  Extraction (RE), which involves identifying relationships
		  between entities within a sentence. However, the
		  effectiveness of these methods relies heavily on the design
		  of the prompts. A compelling question is whether
		  incorporating external knowledge can enhance the language
		  model's understanding of NLP tasks. In this paper, we
		  introduce wiki-based prompt construction that leverages
		  Wikidata as a source of information to craft more
		  informative prompts for both prompt-tuning and
		  instruction-tuning of language models in RE. Our
		  experiments show that using wiki-based prompts enhances
		  cutting-edge language models in RE, emphasizing their
		  potential for improving RE tasks. Our code and datasets are
		  available at GitHub 1.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {731–740},
  numpages	= {10},
  keywords	= {relation extraction, language models, prompt construction,
		  knowledge integration},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@Article{	  10.1145/3665252.3665263,
  author	= {Fan, Ju and Tu, Jianhong and Li, Guoliang and Wang, Peng
		  and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song and Tang,
		  Nan},
  title		= {Unicorn: A Unified Multi-Tasking Matching Model},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {1},
  issn		= {0163-5808},
  url		= {https://doi.org/10.1145/3665252.3665263},
  doi		= {10.1145/3665252.3665263},
  abstract	= {Data matching, which decides whether two data elements
		  (e.g., string, tuple, column, or knowledge graph entity)
		  are the "same" (a.k.a. a match), is a key concept in data
		  integration. The widely used practice is to build
		  task-specific or even dataset-specific solutions, which are
		  hard to generalize and disable the opportunities of
		  knowledge sharing that can be learned from different
		  datasets and multiple tasks. In this paper, we propose
		  Unicorn, a unified model for generally supporting common
		  data matching tasks. Building such a unified model is
		  challenging due to heterogeneous formats of input data
		  elements and various matching semantics of multiple tasks.
		  To address the challenges, Unicorn employs one generic
		  Encoder that converts any pair of data elements (a, b) into
		  a learned representation, and uses a Matcher, which is a
		  binary classifier, to decide whether a matches b. To align
		  matching semantics of multiple tasks, Unicorn adopts a
		  mixture-of-experts model that enhances the learned
		  representation into a better representation. We conduct
		  extensive experiments using 20 datasets on 7 well-studied
		  data matching tasks, and find that our unified model can
		  achieve better performance on most tasks and on average,
		  compared with the state-of-the-art specific models trained
		  for ad-hoc tasks and datasets separately. Moreover, Unicorn
		  can also well serve new matching tasks with zero-shot
		  learning.},
  journal	= {SIGMOD Rec.},
  month		= may,
  pages		= {44–53},
  numpages	= {10}
}

@InProceedings{	  10.1145/3637528.3671576,
  author	= {Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang,
		  Lun and Zhang, Shuyang and Chen, Ting},
  title		= {RareBench: Can LLMs Serve as Rare Diseases Specialists?},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671576},
  doi		= {10.1145/3637528.3671576},
  abstract	= {Generalist Large Language Models (LLMs), such as GPT-4,
		  have shown considerable promise in various domains,
		  including medical diagnosis. Rare diseases, affecting
		  approximately 300 million people worldwide, often have
		  unsatisfactory clinical diagnosis rates primarily due to a
		  lack of experienced physicians and the complexity of
		  differentiating among many rare diseases. In this context,
		  recent news such as "ChatGPT correctly diagnosed a
		  4-year-old's rare disease after 17 doctors failed"
		  underscore LLMs' potential, yet underexplored, role in
		  clinically diagnosing rare diseases. To bridge this
		  research gap, we introduce RareBench, a pioneering
		  benchmark designed to systematically evaluate the
		  capabilities of LLMs on 4 critical dimensions within the
		  realm of rare diseases. Meanwhile, we have compiled the
		  largest open-source dataset on rare disease patients,
		  establishing a benchmark for future studies in this domain.
		  To facilitate differential diagnosis of rare diseases, we
		  develop a dynamic few-shot prompt methodology, leveraging a
		  comprehensive rare disease knowledge graph synthesized from
		  multiple knowledge bases, significantly enhancing LLMs'
		  diagnostic performance. Moreover, we present an exhaustive
		  comparative study of GPT-4's diagnostic capabilities
		  against those of specialist physicians. Our experimental
		  findings underscore the promising potential of integrating
		  LLMs into the clinical diagnostic process for rare
		  diseases. This paves the way for exciting possibilities in
		  future advancements in this field.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4850–4861},
  numpages	= {12},
  keywords	= {benchmark for llms, evaluation, rare disease diagnosis},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3641032.3641044,
  author	= {Mengjun, Du and Jin, Qian and Ang, Li and Xue, Feng and
		  Yi, Yang},
  title		= {A Dual Information Flow Model For Entity Relation
		  Extraction},
  year		= {2024},
  isbn		= {9798400709173},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3641032.3641044},
  doi		= {10.1145/3641032.3641044},
  abstract	= {To leverage the vast amount of enterprise-related data on
		  the Internet and construct an enterprise relation graph,
		  entity relation extraction has received extensive
		  attention. Currently, the research focus of joint entity
		  relation extraction (JERE) models has shifted from the
		  issue of overlapping to the problem of dependency between
		  subtasks. Building upon previous work, this paper further
		  explores the dependence between entity recognition and
		  relation extraction tasks. Firstly, through experiments,
		  this paper discovers the mutual dependency between entity
		  recognition and relation extraction tasks. Then, this paper
		  designs and implements the Dual Information Branch for JERE
		  (DIB) model for joint entity relation extraction. The DIB
		  model employs a dual-branch fusion structure on top of the
		  encoding layer, learning the dependency between entity
		  recognition and relation extraction tasks in both forward
		  and backward propagation. Additionally, to extract
		  enterprise relations and assist in constructing an
		  enterprise relation knowledge graph, we manually annotate
		  the Company dataset, which consists of 1000 prospectuses
		  and 40,000 positive samples, with low overall noise.
		  Finally, experimental results demonstrate that the DIB
		  model achieves superior performance on both Company and NYT
		  dataset.},
  booktitle	= {Proceedings of the 2023 8th International Conference on
		  Information Systems Engineering},
  pages		= {95–100},
  numpages	= {6},
  keywords	= {Keywords natural language processing, dependence between
		  subtasks, entity relation extraction},
  location	= {Bangkok, Thailand},
  series	= {ICISE '23}
}

@InProceedings{	  10.1145/3644815.3644945,
  author	= {Barnett, Scott and Kurniawan, Stefanus and Thudumu,
		  Srikanth and Brannelly, Zach and Abdelrazek, Mohamed},
  title		= {Seven Failure Points When Engineering a Retrieval
		  Augmented Generation System},
  year		= {2024},
  isbn		= {9798400705915},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644815.3644945},
  doi		= {10.1145/3644815.3644945},
  abstract	= {Software engineers are increasingly adding semantic search
		  capabilities to applications using a strategy known as
		  Retrieval Augmented Generation (RAG). A RAG system involves
		  finding documents that semantically match a query and then
		  passing the documents to a large language model (LLM) such
		  as ChatGPT to extract the right answer using an LLM. RAG
		  systems aim to: a) reduce the problem of hallucinated
		  responses from LLMs, b) link sources/references to
		  generated responses, and c) remove the need for annotating
		  documents with meta-data. However, RAG systems suffer from
		  limitations inherent to information retrieval systems and
		  from reliance on LLMs. In this paper, we present an
		  experience report on the failure points of RAG systems from
		  three case studies from separate domains: research,
		  education, and biomedical. We share the lessons learned and
		  present 7 failure points to consider when designing a RAG
		  system. The two key takeaways arising from our work are: 1)
		  validation of a RAG system is only feasible during
		  operation, and 2) the robustness of a RAG system evolves
		  rather than designed in at the start. We conclude with a
		  list of potential research directions on RAG systems for
		  the software engineering community.},
  booktitle	= {Proceedings of the IEEE/ACM 3rd International Conference
		  on AI Engineering - Software Engineering for AI},
  pages		= {194–199},
  numpages	= {6},
  keywords	= {retrieval augmented generation, RAG, SE4AI, case study},
  location	= {Lisbon, Portugal},
  series	= {CAIN '24}
}

@InProceedings{	  10.1145/3665065.3665081,
  author	= {Han, Yanbo and Zhan, Buchao and Zhang, Bin and Zhao, Chao
		  and Yan, Shankai},
  title		= {BiCalBERT: An Efficient Transformer-based Model for
		  Chinese Question Answering},
  year		= {2024},
  isbn		= {9798400717291},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665065.3665081},
  doi		= {10.1145/3665065.3665081},
  abstract	= {The exponentially growing content on the Internet includes
		  online publications, scientific news, and other expert
		  websites. It brings a formidable challenge to extracting
		  pertinent answers from such vast information. We proposed a
		  deep neural network model based on BiLSTM(Bi-directional
		  Long Short-Term Memory) and ALBERT(A Lite BERT for
		  Self-Supervised Learning of Language
		  Representations)&nbsp;for Chinese question-answering in the
		  scientific context. Our model significantly enhances the
		  generalization capabilities of the transformer-based model
		  for question answering. The character extraction and word
		  embedding modules are designed&nbsp;for tackling intricate
		  science-related queries, swiftly assimilating knowledge
		  from cutting-edge scientific literature, and contributing
		  to constructing&nbsp;a comprehensive scientific knowledge
		  graph. Our model is characterized by compactness, swift
		  execution, and satisfactory accuracy. It has been
		  meticulously fine-tuned, emphasizing multi-sentence
		  coherence augmented by attention mechanisms, thereby
		  ensuring robust scalability. Empirical evaluations on the
		  LCQMC and XNLI datasets demonstrate that our approach
		  surpasses the performance results of BERT and ALBERT,
		  showing&nbsp;the potential for large-scale Chinese
		  question-matching problems.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Intelligent Systems, Metaheuristics &amp; Swarm
		  Intelligence},
  pages		= {100–104},
  numpages	= {5},
  keywords	= {NLP, Question Answering, Transformer},
  location	= {Singapore, Singapore},
  series	= {ISMSI '24}
}

@InProceedings{	  10.1145/3674399.3674423,
  author	= {Xu, Ke and Yi, Hanxiao and Xu, Zichen and Wu, Dan},
  title		= {Data-driven Contribution-based Disciplinary Assessment
		  System},
  year		= {2024},
  isbn		= {9798400710117},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3674399.3674423},
  doi		= {10.1145/3674399.3674423},
  abstract	= {A scientific disciplinary assessment system is crucial for
		  nurturing high-quality disciplines within Computer Science.
		  Computer Science Education (CSE) emphasizes the need for a
		  scientific and comprehensive assessment method that guides
		  the development of the discipline, with a particular focus
		  on practical contributions. However, traditional assessment
		  systems tend to prioritize the theoretical outcomes.
		  Moreover, data expansion demands significant effort and
		  time from educational professionals, making it challenging
		  to conduct a thorough evaluation of the disciplines. To
		  tackle these issues, we introduce a data-driven,
		  contribution-based disciplinary assessment system. This
		  system takes into account both theoretical and practical
		  contributions to provide a holistic evaluation. Our
		  proposed system employs a contribution-based assessment
		  approach to establish a correct evaluative direction,
		  steering discipline construction to align with societal
		  needs. It also incorporates intelligent algorithms and a
		  Large Language Model (LLM), leveraging their substantial
		  computational power in the evaluation process. This
		  integration alleviates the workload of educational
		  professionals by automating the collection and analysis of
		  information. The paper outlines a detailed implementation
		  plan that integrates contribution evaluation theory with
		  intelligent technologies, aiming to foster the ongoing
		  advancement of CSE education.},
  booktitle	= {Proceedings of the ACM Turing Award Celebration Conference
		  - China 2024},
  pages		= {42–47},
  numpages	= {6},
  keywords	= {Big Data-driven, Contribution-Based Evaluation Method,
		  Disciplinary assessment},
  location	= {Changsha, China},
  series	= {ACM-TURC '24}
}

@InProceedings{	  10.1145/3626772.3657989,
  author	= {Cai, Qingpeng and Zhao, Xiangyu and Pan, Ling and Xin, Xin
		  and Huang, Jin and Zhang, Weinan and Zhao, Li and Yin,
		  Dawei and Yang, Grace Hui},
  title		= {AgentIR: 1st Workshop on Agent-based Information
		  Retrieval},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657989},
  doi		= {10.1145/3626772.3657989},
  abstract	= {Information retrieval (IR) systems have become an
		  essential component in modern society to help users find
		  useful information, which consists of a series of processes
		  including query expansion, item recall, item ranking and
		  re-ranking, etc. Based on the ranked information list,
		  users can provide their feedbacks. Such an interaction
		  process between users and IR systems can be naturally
		  formulated as a decision-making problem, which can be
		  either one-step or sequential. In the last ten years, deep
		  reinforcement learning (DRL) has become a promising
		  direction for decision-making, since DRL utilizes the high
		  model capacity of deep learning for complex decision-making
		  tasks. On the one hand, there have been emerging research
		  works focusing on leveraging DRL for IR tasks. However, the
		  fundamental information theory under DRL settings, the
		  challenge of RL methods for Industrial IR tasks, or the
		  simulations of DRL-based IR systems, has not been deeply
		  investigated. On the other hand, the emerging LLM provides
		  new opportunities for optimizing and simulating IR systems.
		  To this end, we propose the first Agent-based IR workshop
		  at SIGIR 2024, as a continuation from one of the most
		  successful IR workshops, DRL4IR. It provides a venue for
		  both academia researchers and industry practitioners to
		  present the recent advances of both DRL-based IR systems
		  and LLM-based IR systems from the agent-based IR's
		  perspective, to foster novel research, interesting
		  findings, and new applications.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3025–3028},
  numpages	= {4},
  keywords	= {agent-based information retrieval, drl, llm},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3589334.3645378,
  author	= {Yan, Yibo and Wen, Haomin and Zhong, Siru and Chen, Wei
		  and Chen, Haodong and Wen, Qingsong and Zimmermann, Roger
		  and Liang, Yuxuan},
  title		= {UrbanCLIP: Learning Text-enhanced Urban Region Profiling
		  with Contrastive Language-Image Pretraining from the Web},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645378},
  doi		= {10.1145/3589334.3645378},
  abstract	= {Urban region profiling from web-sourced data is of utmost
		  importance for urban computing. We are witnessing a blossom
		  of LLMs for various fields, especially in multi-modal data
		  research such as vision-language learning, where text
		  modality serves as a supplement for images. As textual
		  modality has rarely been introduced into modality
		  combinations in urban region profiling, we aim to answer
		  two fundamental questions: i) Can text modality enhance
		  urban region profiling? ii) and if so, in what ways and
		  which aspects? To answer the questions, we leverage the
		  power of Large Language Models (LLMs) and introduce the
		  first-ever LLM-enhanced framework that integrates the
		  knowledge of text modality into urban imagery, named
		  LLM-enhanced Urban Region Profiling with Contrastive
		  Language-Image Pretraining (UrbanCLIP ). Specifically, it
		  first generates a detailed textual description for each
		  satellite image by Image-to-Text LLMs. Then, the model is
		  trained on image-text pairs, seamlessly unifying language
		  supervision for urban visual representation learning,
		  jointly with contrastive loss and language modeling loss.
		  Results on urban indicator prediction in four major
		  metropolises show its superior performance, with an average
		  improvement of 6.1% on R2 compared to the state-of-the-art
		  methods. Our code and dataset are available at
		  https://github.com/StupidBuluchacha/UrbanCLIP.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {4006–4017},
  numpages	= {12},
  keywords	= {language-image pretraining, spatio-temporal data, urban
		  computing},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3641142.3641165,
  author	= {Ananta, Ila and Khetarpaul, Sonia and Sharma, Dolly},
  title		= {Symptoms-Disease Detecting Conversation Agent using
		  Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400717307},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3641142.3641165},
  doi		= {10.1145/3641142.3641165},
  abstract	= {Conversational agents have become extraordinarily popular
		  over the last few years, with accelerated adoption due to
		  COVID-19. Even though a lot of work has been done to devise
		  a real-time agent very few of them focus on dynamic
		  responses. The challenges for automatic medical diagnosis
		  not only include issues for topic transition coherency and
		  question understanding but also issues regarding the
		  context of medical knowledge and symptoms of disease
		  relations. In this paper, we propose a conversational agent
		  that not only generates answers to specific medical
		  questions but also makes more natural and human-like
		  conversations and can adapt to the context and evolve over
		  time. We propose an End-to-End knowledge-routed Relational
		  Dialogue System that would incorporate a rich medical
		  knowledge graph into the topic transition in dialogue
		  management, and make it accommodative with NLU (Natural
		  Language Understanding) and NLG (Natural Language
		  Generation). A knowledge-routed graph for topic
		  decision-making is used, which helps to identify
		  relationships between symptoms and symptom-disease pairs.
		  However, there are constraints on the extent of questions
		  that knowledge graphs can address independently. To
		  overcome these, we have used a fine-tuned GPT-3 model.
		  While knowledge graphs organize data as interconnected
		  entities, GPT-3 generates human-like text using learned
		  patterns from large datasets. This approach enhances
		  responses to intricate queries.},
  booktitle	= {Proceedings of the 2024 Australasian Computer Science
		  Week},
  pages		= {98–107},
  numpages	= {10},
  location	= {Sydney, NSW, Australia},
  series	= {ACSW '24}
}

@InProceedings{	  10.1145/3703187.3703290,
  author	= {Ma, Xiangfei and Li, Lin},
  title		= {Geological Disaster Named Entity Recognition with Small
		  Samples Based on Data Augmentation and Prompt Engineering},
  year		= {2024},
  isbn		= {9798400707254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703187.3703290},
  doi		= {10.1145/3703187.3703290},
  abstract	= {This paper uses a large language model to perform
		  generative data enhancement on the original small sample
		  data by performing random synonym replacement and random
		  mask filling operations. In accordance with the reasoning
		  logic of the large language model, three prompt templates
		  are designed and the reasons are explored. Experiments show
		  that when the parameters remain unchanged, the data
		  enhanced by this method has been greatly improved under the
		  three prompt templates, alleviating the difficulty of low
		  resources of geological disaster data. And by comparing the
		  performance of different instructions under different
		  learning rates, the fine-tuning learning rate range
		  suitable for the field of geological disasters is
		  summarized. The limitation is that it is constrained by
		  local computing resources, which reduces the parameter
		  scale of LLM, and the recognition performance is low for
		  extremely long or complex texts.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Computer Information Science and Artificial Intelligence},
  pages		= {613–617},
  numpages	= {5},
  keywords	= {Data Augmentation, Geological Disasters, LLMs, Named
		  Entity Recognition, Prompt Engineering},
  location	= { },
  series	= {CISAI '24}
}

@InProceedings{	  10.1145/3626772.3657982,
  author	= {B\'{e}n\'{e}dict, Gabriel and Zhang, Ruqing and Metzler,
		  Donald and Yates, Andrew and Jiang, Ziyan},
  title		= {Gen-IR @ SIGIR 2024: The Second Workshop on Generative
		  Information Retrieval},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657982},
  doi		= {10.1145/3626772.3657982},
  abstract	= {Generative information retrieval (Gen-IR) is a
		  fast-growing interdisciplinary research area that
		  investigates how to leverage advances in generative
		  Artificial Intelligence (AI) to improve information
		  retrieval systems. Gen-IR has attracted interest from the
		  information retrieval, natural language processing, and
		  machine learning communities, among others. Since the dawn
		  of Gen-IR last year, there has been an explosion of Gen-IR
		  systems that have launched and are now widely used.
		  Interest in this area across academia and industry is only
		  expected to continue to grow as new research challenges and
		  application opportunities arise. The goal of this proposed
		  workshop, The Second Workshop on Generative Information
		  Retrieval (Gen-IR @ SIGIR 2024) is to provide an
		  interactive venue for exploring a broad range of
		  foundational and applied Gen-IR research. The workshop will
		  focus on tasks such as generative document retrieval,
		  grounded answer generation, generative recommendation, and
		  generative knowledge graphs, all through the lens of model
		  training, model behavior, and broader issues. The workshop
		  will be highly interactive, favoring panel discussions,
		  poster sessions, and roundtable discussions over one-sided
		  keynotes and paper talks.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3029–3032},
  numpages	= {4},
  keywords	= {generative models, information retrieval, large language
		  models},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3664647.3681598,
  author	= {Li, Ziyan and Yu, Jianfei and Yang, Jia and Wang, Wenya
		  and Yang, Li and Xia, Rui},
  title		= {Generative Multimodal Data Augmentation for Low-Resource
		  Multimodal Named Entity Recognition},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681598},
  doi		= {10.1145/3664647.3681598},
  abstract	= {As an important task in multimodal information extraction,
		  Multimodal Named Entity Recognition (MNER) has recently
		  attracted considerable attention. One key challenge of MNER
		  lies in the lack of sufficient fine-grained annotated data,
		  especially in low-resource scenarios. Although data
		  augmentation is a widely used technique to tackle the above
		  issue, it is challenging to simultaneously generate
		  synthetic text-image pairs and their corresponding
		  high-quality entity annotations. In this work, we propose a
		  novel Generative Multimodal Data Augmentation (GMDA)
		  framework for MNER, which contains two stages: Multimodal
		  Text Generation and Multimodal Image Generation.
		  Specifically, we first transform each annotated sentence
		  into a linearized labeled sequence, and then train a
		  Label-aware Multimodal Large Language Model (LMLLM) to
		  generate the labeled sequence based on a label-aware prompt
		  and its associated image. We further employ a Stable
		  Diffusion model to generate the synthetic images that are
		  semantically related to these sentences. Experimental
		  results on three benchmark datasets demonstrate the
		  effectiveness of the proposed GMDA framework, which
		  consistently boosts the performance of several competitive
		  methods for two subtasks of MNER in both full-supervision
		  and low-resource settings. The low-resource dataset and
		  source code are released at
		  https://github.com/NUSTM/GMDA.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {7336–7345},
  numpages	= {10},
  keywords	= {data augmentation, generative framework, grounded
		  multimodal named entity recognition, multimodal named
		  entity recognition},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3663529.3663833,
  author	= {Zhang, Shenglin and Zhu, Jun and Hao, Bowen and Sun,
		  Yongqian and Nie, Xiaohui and Zhu, Jingwen and Liu, Xilin
		  and Li, Xiaoqian and Ma, Yuchi and Pei, Dan},
  title		= {Fault Diagnosis for Test Alarms in Microservices through
		  Multi-source Data},
  year		= {2024},
  isbn		= {9798400706585},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3663529.3663833},
  doi		= {10.1145/3663529.3663833},
  abstract	= {Nowadays, the testing of large-scale microservices could
		  produce an enormous number of test alarms daily. Manually
		  diagnosing these alarms is time-consuming and laborious for
		  the testers. Automatic fault diagnosis with fault
		  classification and localization can help testers
		  efficiently handle the increasing volume of failed test
		  cases. However, the current methods for diagnosing test
		  alarms struggle to deal with the complex and frequently
		  updated microservices. In this paper, we introduce
		  SynthoDiag, a novel fault diagnosis framework for test
		  alarms in microservices through multi-source logs
		  (execution logs, trace logs, and test case information)
		  organized with a knowledge graph. An Entity Fault
		  Association and Position Value (EFA-PV) algorithm is
		  proposed to localize the fault-indicative log entries.
		  Additionally, an efficient block-based differentiation
		  approach is used to filter out fault-irrelevant entries in
		  the test cases, significantly improving the overall
		  performance of fault diagnosis. At last, SynthoDiag is
		  systematically evaluated with a large-scale real-world
		  dataset from a top-tier global cloud service provider,
		  Huawei Cloud, which provides services for more than three
		  million users. The results show the Micro-F1 and Macro-F1
		  scores improvement of SynthoDiag over baseline methods in
		  fault classification are 21% and 30%, respectively, and its
		  top-5 accuracy of fault localization is 81.9%,
		  significantly surpassing the previous methods.},
  booktitle	= {Companion Proceedings of the 32nd ACM International
		  Conference on the Foundations of Software Engineering},
  pages		= {115–125},
  numpages	= {11},
  keywords	= {Execution Logs, Fault Diagnosis, Microservice, Test Case,
		  Trace Logs},
  location	= {Porto de Galinhas, Brazil},
  series	= {FSE 2024}
}

@InProceedings{	  10.1145/3657604.3662030,
  author	= {Moore, Steven and Schmucker, Robin and Mitchell, Tom and
		  Stamper, John},
  title		= {Automated Generation and Tagging of Knowledge Components
		  from Multiple-Choice Questions},
  year		= {2024},
  isbn		= {9798400706332},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3657604.3662030},
  doi		= {10.1145/3657604.3662030},
  abstract	= {Knowledge Components (KCs) linked to assessments enhance
		  the measurement of student learning, enrich analytics, and
		  facilitate adaptivity. However, generating and linking KCs
		  to assessment items requires significant effort and
		  domain-specific knowledge. To streamline this process for
		  higher-education courses, we employed GPT-4 to generate KCs
		  for multiple-choice questions (MCQs) in Chemistry and
		  E-Learning. We analyzed discrepancies between the KCs
		  generated by the Large Language Model (LLM) and those made
		  by humans through evaluation from three domain experts in
		  each subject area. This evaluation aimed to determine
		  whether, in instances of non-matching KCs, evaluators
		  showed a preference for the LLM-generated KCs over their
		  human-created counterparts. We also developed an ontology
		  induction algorithm to cluster questions that assess
		  similar KCs based on their content. Our most effective LLM
		  strategy accurately matched KCs for 56% of Chemistry and
		  35% of E-Learning MCQs, with even higher success when
		  considering the top five KC suggestions. Human evaluators
		  favored LLM-generated KCs, choosing them over
		  human-assigned ones approximately two-thirds of the time, a
		  preference that was statistically significant across both
		  domains. Our clustering algorithm successfully grouped
		  questions by their underlying KCs without needing explicit
		  labels or contextual information. This research advances
		  the automation of KC generation and classification for
		  assessment items, alleviating the need for student data or
		  predefined KC labels.},
  booktitle	= {Proceedings of the Eleventh ACM Conference on Learning @
		  Scale},
  pages		= {122–133},
  numpages	= {12},
  keywords	= {concept labeling, knowledge component, knowledge labeling,
		  learning engineering, multiple-choice question},
  location	= {Atlanta, GA, USA},
  series	= {L@S '24}
}

@InProceedings{	  10.1145/3638550.3641130,
  author	= {Xu, Huatao and Han, Liying and Yang, Qirui and Li, Mo and
		  Srivastava, Mani},
  title		= {Penetrative AI: Making LLMs Comprehend the Physical
		  World},
  year		= {2024},
  isbn		= {9798400704970},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638550.3641130},
  doi		= {10.1145/3638550.3641130},
  abstract	= {Recent developments in Large Language Models (LLMs) have
		  demonstrated their remarkable capabilities across a range
		  of tasks. Questions, however, persist about the nature of
		  LLMs and their potential to integrate common-sense human
		  knowledge when performing tasks involving information about
		  the real physical world. This paper delves into these
		  questions by exploring how LLMs can be extended to interact
		  with and reason about the physical world through IoT
		  sensors and actuators, a concept that we term "Penetrative
		  AI". The paper explores such an extension at two levels of
		  LLMs' ability to penetrate into the physical world via the
		  processing of sensory signals. Our preliminary findings
		  indicate that LLMs, with ChatGPT being the representative
		  example in our exploration, have considerable and unique
		  proficiency in employing the embedded world knowledge for
		  interpreting IoT sensor data and reasoning over them about
		  tasks in the physical realm. Not only this opens up new
		  applications for LLMs beyond traditional text-based tasks,
		  but also enables new ways of incorporating human knowledge
		  in cyber-physical systems.},
  booktitle	= {Proceedings of the 25th International Workshop on Mobile
		  Computing Systems and Applications},
  pages		= {1–7},
  numpages	= {7},
  keywords	= {LLM, CPS, IoT, penetrative AI},
  location	= {San Diego, CA, USA},
  series	= {HotMobile '24}
}

@Article{	  10.14778/3681954.3681973,
  author	= {Sun, Yushi and Xin, Hao and Sun, Kai and Xu, Yifan Ethan
		  and Yang, Xiao and Dong, Xin Luna and Tang, Nan and Chen,
		  Lei},
  title		= {Are Large Language Models a Good Replacement of
		  Taxonomies?},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {11},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3681954.3681973},
  doi		= {10.14778/3681954.3681973},
  abstract	= {Large language models (LLMs) demonstrate an impressive
		  ability to internalize knowledge and answer natural
		  language questions. Although previous studies validate that
		  LLMs perform well on general knowledge while presenting
		  poor performance on long-tail nuanced knowledge, the
		  community is still doubtful about whether the traditional
		  knowledge graphs should be replaced by LLMs. In this paper,
		  we ask if the schema of knowledge graph (i.e., taxonomy) is
		  made obsolete by LLMs. Intuitively, LLMs should perform
		  well on common taxonomies and at taxonomy levels that are
		  common to people. Unfortunately, there lacks a
		  comprehensive benchmark that evaluates the LLMs over a wide
		  range of taxonomies from common to specialized domains and
		  at levels from root to leaf so that we can draw a confident
		  conclusion. To narrow the research gap, we constructed a
		  novel taxonomy hierarchical structure discovery benchmark
		  named TaxoGlimpse to evaluate the performance of LLMs over
		  taxonomies. TaxoGlimpse covers ten representative
		  taxonomies from common to specialized domains with in-depth
		  experiments of different levels of entities in this
		  taxonomy from root to leaf. Our comprehensive experiments
		  of eighteen LLMs under three prompting settings validate
		  that LLMs perform miserably poorly in handling specialized
		  taxonomies and leaf-level entities. Specifically, the QA
		  accuracy of the best LLM drops by up to 30% as we go from
		  common to specialized domains and from root to leaf levels
		  of taxonomies.},
  journal	= {Proc. VLDB Endow.},
  month		= jul,
  pages		= {2919–2932},
  numpages	= {14}
}

@InProceedings{	  10.1145/3613904.3642450,
  author	= {Zulfikar, Wazeer Deen and Chan, Samantha and Maes,
		  Pattie},
  title		= {Memoro: Using Large Language Models to Realize a Concise
		  Interface for Real-Time Memory Augmentation},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642450},
  doi		= {10.1145/3613904.3642450},
  abstract	= {People have to remember an ever-expanding volume of
		  information. Wearables that use information capture and
		  retrieval for memory augmentation can help but can be
		  disruptive and cumbersome in real-world tasks, such as in
		  social settings. To address this, we developed Memoro, a
		  wearable audio-based memory assistant with a concise user
		  interface. Memoro uses a large language model (LLM) to
		  infer the user’s memory needs in a conversational
		  context, semantically search memories, and present minimal
		  suggestions. The assistant has two interaction modes: Query
		  Mode for voicing queries and Queryless Mode for on-demand
		  predictive assistance, without explicit query. Our study of
		  (N=20) participants engaged in a real-time conversation,
		  demonstrated that using Memoro reduced device interaction
		  time and increased recall confidence while preserving
		  conversational quality. We report quantitative results and
		  discuss the preferences and experiences of users. This work
		  contributes towards utilizing LLMs to design wearable
		  memory augmentation systems that are minimally
		  disruptive.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {450},
  numpages	= {18},
  keywords	= {context-aware agent, large language models, memory
		  assistant, minimal interfaces, voice interfaces},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@Article{	  10.1145/3674501,
  author	= {Zhao, Xiaoyan and Deng, Yang and Yang, Min and Wang,
		  Lingzhi and Zhang, Rui and Cheng, Hong and Lam, Wai and
		  Shen, Ying and Xu, Ruifeng},
  title		= {A Comprehensive Survey on Relation Extraction: Recent
		  Advances and New Frontiers},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {11},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3674501},
  doi		= {10.1145/3674501},
  abstract	= {Relation extraction (RE) involves identifying the
		  relations between entities from underlying content. RE
		  serves as the foundation for many natural language
		  processing (NLP) and information retrieval applications,
		  such as knowledge graph completion and question answering.
		  In recent years, deep neural networks have dominated the
		  field of RE and made noticeable progress. Subsequently, the
		  large pre-trained language models (PLMs) have taken the
		  state-of-the-art RE to a new level. This survey provides a
		  comprehensive review of existing deep learning techniques
		  for RE. First, we introduce RE resources, including
		  datasets and evaluation metrics. Second, we propose a new
		  taxonomy to categorize existing works from three
		  perspectives, i.e., text representation, context encoding,
		  and triplet prediction. Third, we discuss several important
		  challenges faced by RE and summarize potential techniques
		  to tackle these challenges. Finally, we outline some
		  promising future directions and prospects in this field.
		  This survey is expected to facilitate researchers’
		  collaborative efforts to address the challenges of
		  real-world RE systems.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {293},
  numpages	= {39},
  keywords	= {Relation extraction, deep learning, pre-trained language
		  models, low-resource relation extraction}
}

@InProceedings{	  10.1145/3589335.3651980,
  author	= {Damianou, Andreas and Fabbri, Francesco and Gigioli, Paul
		  and De Nadai, Marco and Wang, Alice and Palumbo, Enrico and
		  Lalmas, Mounia},
  title		= {Towards Graph Foundation Models for Personalization},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651980},
  doi		= {10.1145/3589335.3651980},
  abstract	= {In the realm of personalization, integrating diverse
		  information sources such as consumption signals and
		  content-based representations is becoming increasingly
		  critical to build state-of-the-art solutions. In this
		  regard, two of the biggest trends in research around this
		  subject are Graph Neural Networks (GNNs) and Foundation
		  Models (FM). While GNNs emerged as a popular solution in
		  industry for powering personalization at scale, FMs have
		  only recently caught attention for their promising
		  performance in personalization tasks like ranking and
		  retrieval. In this paper, we present a graph-based
		  foundation modeling approach tailored to personalization.
		  Central to this approach is a Heterogeneous GNN (HGNN)
		  designed to capture multi-hop content and consumption
		  relationships across a range of recommendable item types.
		  To ensure the generality required from a Foundation Model,
		  we employ a Large Language Model (LLM) text-based
		  featurization of nodes that accommodates all item types,
		  and construct the graph using co-interaction signals, which
		  inherently transcend content specificity. To facilitate
		  practical generalization, we further couple the HGNN with
		  an adaptation mechanism based on a two-tower (2T)
		  architecture, which also operates agnostically to content
		  type. This multi-stage approach ensures high scalability;
		  while the HGNN produces general purpose embeddings, the 2T
		  component models in a continuous space the sheer size of
		  user-item interaction data. Our comprehensive approach has
		  been rigorously tested and proven effective in delivering
		  recommendations across a diverse array of products within a
		  real-world, industrial audio streaming platform.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1798–1802},
  numpages	= {5},
  keywords	= {foundation models, graph neural networks, personalization,
		  recommender systems},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3640457.3688133,
  author	= {Zhang, Xiaoyu and Xie, Ruobing and Lyu, Yougang and Xin,
		  Xin and Ren, Pengjie and Liang, Mingfei and Zhang, Bo and
		  Kang, Zhanhui and de Rijke, Maarten and Ren, Zhaochun},
  title		= {Towards Empathetic Conversational Recommender Systems},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688133},
  doi		= {10.1145/3640457.3688133},
  abstract	= {Conversational recommender systems (CRSs) are able to
		  elicit user preferences through multi-turn dialogues. They
		  typically incorporate external knowledge and pre-trained
		  language models to capture the dialogue context. Most CRS
		  approaches, trained on benchmark datasets, assume that the
		  standard items and responses in these benchmarks are
		  optimal. However, they overlook that users may express
		  negative emotions with the standard items and may not feel
		  emotionally engaged by the standard responses. This issue
		  leads to a tendency to replicate the logic of recommenders
		  in the dataset instead of aligning with user needs. To
		  remedy this misalignment, we introduce empathy within a
		  CRS. With empathy we refer to a system’s ability to
		  capture and express emotions. We propose an empathetic
		  conversational recommender (ECR) framework. ECR contains
		  two main modules: emotion-aware item recommendation and
		  emotion-aligned response generation. Specifically, we
		  employ user emotions to refine user preference modeling for
		  accurate recommendations. To generate human-like emotional
		  responses, ECR applies retrieval-augmented prompts to
		  fine-tune a pre-trained language model aligning with
		  emotions and mitigating hallucination. To address the
		  challenge of insufficient supervision labels, we enlarge
		  our empathetic data using emotion labels annotated by large
		  language models and emotional reviews collected from
		  external resources. We propose novel evaluation metrics to
		  capture user satisfaction in real-world CRS scenarios. Our
		  experiments on the ReDial dataset validate the efficacy of
		  our framework in enhancing recommendation accuracy and
		  improving user satisfaction.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {84–93},
  numpages	= {10},
  keywords	= {Conversational recommender system, Empathetic response
		  generation, Prompt engineering, User preference modeling},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3663741.3664785,
  author	= {Barbon Junior, Sylvio and Ceravolo, Paolo and Groppe, Sven
		  and Jarrar, Mustafa and Maghool, Samira and S\`{e}des,
		  Florence and Sahri, Soror and Van Keulen, Maurice},
  title		= {Are Large Language Models the New Interface for Data
		  Pipelines?},
  year		= {2024},
  isbn		= {9798400706790},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3663741.3664785},
  doi		= {10.1145/3663741.3664785},
  abstract	= {A Language Model is a term that encompasses various types
		  of models designed to understand and generate human
		  communication. Large Language Models (LLMs) have gained
		  significant attention due to their ability to process text
		  with human-like fluency and coherence, making them valuable
		  for a wide range of data-related tasks fashioned as
		  pipelines. The capabilities of LLMs in natural language
		  understanding and generation, combined with their
		  scalability, versatility, and state-of-the-art performance,
		  enable innovative applications across various AI-related
		  fields, including eXplainable Artificial Intelligence
		  (XAI), Automated Machine Learning (AutoML), and Knowledge
		  Graphs (KG). Furthermore, we believe these models can
		  extract valuable insights and make data-driven decisions at
		  scale, a practice commonly referred to as Big Data
		  Analytics (BDA). In this position paper, we provide some
		  discussions in the direction of unlocking synergies among
		  these technologies, which can lead to more powerful and
		  intelligent AI solutions, driving improvements in data
		  pipelines across a wide range of applications and domains
		  integrating humans, computers, and knowledge.},
  booktitle	= {Proceedings of the International Workshop on Big Data in
		  Emergent Distributed Environments},
  articleno	= {6},
  numpages	= {6},
  keywords	= {Automated Machine Learning, Big Data Analytic,
		  Human-Computer Interaction, Knowledge Graphs, Natural
		  Language Understanding, eXplainable Artificial
		  Intelligence},
  location	= {Santiago, AA, Chile},
  series	= {BiDEDE '24}
}

@InProceedings{	  10.1145/3632410.3632494,
  author	= {Shiri, Aidin and Roy, Kaushik and Sheth, Amit and Gaur,
		  Manas},
  title		= {L3 Ensembles: Lifelong Learning Approach for Ensemble of
		  Foundational Language Models✱},
  year		= {2024},
  isbn		= {9798400716348},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632410.3632494},
  doi		= {10.1145/3632410.3632494},
  abstract	= {Fine-tuning pre-trained foundational language models (FLM)
		  for specific tasks is often impractical, especially for
		  resource-constrained devices. This necessitates the
		  development of a Lifelong Learning (L3) framework that
		  continuously adapts to a stream of Natural Language
		  Processing (NLP) tasks efficiently. We propose an approach
		  that focuses on extracting meaningful representations from
		  unseen data, constructing a structured knowledge base, and
		  improving task performance incrementally. We conducted
		  experiments on various NLP tasks to validate its
		  effectiveness, including benchmarks like GLUE and
		  SuperGLUE. We measured good performance across the
		  accuracy, training efficiency, and knowledge transfer
		  metrics. Initial experimental results show that the
		  proposed L3 ensemble method increases the model accuracy 4%
		  ∼ 36% compared to the fine-tuned FLM. Furthermore, L3
		  model outperforms naive fine-tuning approaches while
		  maintaining competitive or superior performance (up to
		  15.4% increase in accuracy) compared to the
		  state-of-the-art language model (T5) for the given task,
		  STS benchmark.},
  booktitle	= {Proceedings of the 7th Joint International Conference on
		  Data Science &amp; Management of Data (11th ACM IKDD CODS
		  and 29th COMAD)},
  pages		= {592–594},
  numpages	= {3},
  location	= {Bangalore, India},
  series	= {CODS-COMAD '24}
}

@InProceedings{	  10.1145/3637528.3672500,
  author	= {Dong, Xin Luna},
  title		= {Next-generation Intelligent Assistants for Wearable
		  Devices},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3672500},
  doi		= {10.1145/3637528.3672500},
  abstract	= {An intelligent assistant shall be an agent that knows you
		  and the world, can receive your requests or predict your
		  needs, and provide you the right services at the right time
		  with your permission. As smart devices such as Amazon
		  Alexa, Google Home, Ray-ban Meta get popular, Intelligent
		  Assistants are gradually playing an important role in
		  people's lives. The Emergence of wearable devices brings
		  more opportunities and calls for the next generation of
		  Intelligent Assistants. In this talk, we discuss the many
		  challenges and opportunities we face to grow intelligent
		  assistants from voice-only to multi-modal, from
		  context-agnostic to context-aware, from listening to the
		  users' requests to predicting the user's needs, and from
		  server-side to on-device. We describe our LLM-based
		  solutions toward multi-modality, contextualization,
		  personalization, and retrieval-augmentation, and discuss
		  how they are enabled on devices. We expect these new
		  challenges to open doors to new research areas and start a
		  new chapter for providing personal assistance services},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {4735},
  numpages	= {1},
  keywords	= {multi-modal personal information management, smart
		  assistant},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Proceedings{	  10.1145/3616855,
  title		= {WSDM '24: Proceedings of the 17th ACM International
		  Conference on Web Search and Data Mining},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the 17th ACM
		  International Conference on Web Search and Data Mining -
		  WSDM 2024. WSDM is one of the premier conferences in the
		  fields of web search and data mining, with a dynamic and
		  growing community from academia and industry. After two
		  years of virtual conferences and in-person conferences in
		  Singapore, the 2024 edition is an in-person conference with
		  virtual elements. We hope you enjoy the conference at the
		  "Centro Internacional de Congresos de Yucatan (CIC)" in
		  Merida from March 4 to March 8, 2024.We are excited to kick
		  off the program with a dynamic mix of Tutorials and
		  Industry Day. Our seven tutorials will cover a broad range
		  of search and data mining topics. Industry Day will provide
		  valuable insights from leaders at major technology
		  companies. The core technical program continues WSDM's
		  tradition of a single-track format, featuring 109
		  thought-provoking papers from both academic and industry
		  experts. We're honored to have inspiring keynote speakers
		  each day: Nicolas Christin (CMU), Elizabeth Reid (Google),
		  and Saiph Savage (Civic A.I. Lab). Additionally, 17
		  interactive demonstrations will showcase the latest
		  prototypes and systems. The final day offers a stimulating
		  Doctoral Consortium and six engaging workshops on topics
		  including integrity in social networks, large language
		  model for society, psychology-informed information access
		  system, interactive and scalable information retrieval
		  system and machine learning on graphs. WSDM 2024 proudly
		  presents WSDM day on information retrieval and Web in the
		  region. WSDM Cup Day highlights finalists' presentations
		  addressing challenges in Conversational Multi-Doc QA. This
		  diverse and stimulating program promises to be an enriching
		  experience for all!.},
  location	= {Merida, Mexico}
}

@InProceedings{	  10.1145/3678717.3691318,
  author	= {Silva, Jo\~{a}o Daniel and Magalh\~{a}es, Jo\~{a}o and
		  Tuia, Devis and Martins, Bruno},
  title		= {Multilingual Vision-Language Pre-training for the Remote
		  Sensing Domain},
  year		= {2024},
  isbn		= {9798400711077},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3678717.3691318},
  doi		= {10.1145/3678717.3691318},
  abstract	= {Methods based on Contrastive Language-Image Pre-training
		  (CLIP) are nowadays extensively used in support of
		  vision-and-language tasks involving remote sensing data,
		  such as cross-modal retrieval. The adaptation of CLIP to
		  this specific domain has relied on model fine-tuning with
		  the standard contrastive objective, using existing
		  human-labeled image-caption datasets, or using synthetic
		  data corresponding to image-caption pairs derived from
		  other annotations over remote sensing images (e.g., object
		  classes). The use of different pre-training mechanisms has
		  received less attention, and only a few exceptions have
		  considered multilingual inputs. This work proposes a novel
		  vision-and-language model for the remote sensing domain,
		  exploring the fine-tuning of a multilingual CLIP model and
		  testing the use of a self-supervised method based on
		  aligning local and global representations from individual
		  input images, together with the standard CLIP objective.
		  Model training relied on assembling pre-existing datasets
		  of remote sensing images paired with English captions,
		  followed by the use of automated machine translation into
		  nine additional languages. We show that translated data is
		  indeed helpful, e.g. improving performance also on English.
		  Our resulting model, which we named Remote Sensing
		  Multilingual CLIP (RS-M-CLIP), obtains state-of-the-art
		  results in a variety of vision-and-language tasks,
		  including cross-modal and multilingual image-text
		  retrieval, or zero-shot image classification.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Advances in Geographic Information Systems},
  pages		= {220–232},
  numpages	= {13},
  keywords	= {Contrastive Language-Image Pre-training, Cross-Modal
		  Retrieval, Remote Sensing, Self-Supervised Pre-training,
		  Vision and Language},
  location	= {Atlanta, GA, USA},
  series	= {SIGSPATIAL '24}
}

@Article{	  10.1145/3660826,
  author	= {Yan, Chuan and Meng, Mark Huasong and Xie, Fuman and Bai,
		  Guangdong},
  title		= {Investigating Documented Privacy Changes in Android OS},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  number	= {FSE},
  url		= {https://doi.org/10.1145/3660826},
  doi		= {10.1145/3660826},
  abstract	= {Android has empowered third-party apps to access data and
		  services on mobile devices since its genesis.This involves
		  a wide spectrum of user privacy-sensitive data, such as the
		  device ID and location. In recent years, Android has taken
		  proactive measures to adapt its access control policies for
		  such data, in response to the increasingly strict privacy
		  protection regulations around the world. When each new
		  Android version is released, its privacy changes induced by
		  the version evolution are transparently disclosed, and we
		  refer to them as documented privacy changes (DPCs).
		  Implementing DPCs in Android OS is a non-trivial task, due
		  to not only the dispersed nature of those access control
		  points within the OS, but also the challenges posed by
		  backward compatibility. As a result, whether the actual
		  access control enforcement in the OS implementations aligns
		  with the disclosed DPCs becomes a critical concern.
		  
		  In this work, we conduct the first systematic study on the
		  consistency between the operational behaviors of the OS at
		  runtime and the officially disclosed DPCs. We propose
		  DopCheck, an automatic DPC-driven testing framework
		  equipped with a large language model (LLM) pipeline. It
		  features a serial of analysis to extract the ontology from
		  the privacy change documents written in natural language,
		  and then harnesses the few-shot capability of LLMs to
		  construct test cases for the detection of DPC-compliance
		  issues in OS implementations. We apply DopCheck with the
		  latest versions (10 to 13) of Android Open Source Project
		  (AOSP). Our evaluation involving 79 privacy-sensitive APIs
		  demonstrates that DopCheck can effectively recognize DPCs
		  from Android documentation and generate rigorous test
		  cases. Our study reveals that the status quo of the
		  DPC-compliance issues is concerning, evidenced by 19 bugs
		  identified by DopCheck. Notably, 12 of them are discovered
		  in Android 13 and 6 in Android 10 for the first time,
		  posing more than 35% Android users to the risk of privacy
		  leakage. Our findings should raise an alert to Android
		  users and app developers on the DPC compliance issues when
		  using or developing an app, and would also underscore the
		  necessity for Google to comprehensively validate the actual
		  implementation against its privacy documentation prior to
		  the OS release.},
  journal	= {Proc. ACM Softw. Eng.},
  month		= jul,
  articleno	= {119},
  numpages	= {24},
  keywords	= {Android, documentation, privacy, testing}
}

@Article{	  10.1145/3676956,
  author	= {Pei, Jiahuan and Yan, Guojun and De Rijke, Maarten and
		  Ren, Pengjie},
  title		= {Mixture-of-Languages Routing for Multilingual Dialogues},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3676956},
  doi		= {10.1145/3676956},
  abstract	= {We consider multilingual dialogue systems and ask how the
		  performance of a dialogue system can be improved by using
		  information that is available in other languages than the
		  language in which a conversation is being conducted. We
		  adopt a collaborative chair-experts framework, where each
		  expert agent can be either monolingual or cross-lingual,
		  and a chair agent follows a mixture-of-experts procedure
		  for globally optimizing multilingual task-oriented dialogue
		  systems. We propose a mixture-of-languages routing
		  framework that includes four functional components, i.e.,
		  input embeddings of multilingual dialogues, language model,
		  pairwise alignment between the representation of every two
		  languages, and mixture-of-languages. We quantify language
		  characteristics of unity and diversity using a number of
		  similarity metrics, i.e., genetic similarity and word and
		  sentence similarity based on embeddings. Our main finding
		  is that the performance of multilingual task-oriented
		  dialogue systems can be greatly impacted by three key
		  aspects, i.e., data sufficiency, language characteristics,
		  and model design in a mixture-of-languages routing
		  framework.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= oct,
  articleno	= {165},
  numpages	= {33},
  keywords	= {multilingual systems, task-oriented dialogue systems,
		  collaborative agents, mixture-of-experts}
}

@InProceedings{	  10.1145/3652583.3658040,
  author	= {Dai, Ruiting and Tan, Yuqiao and Mo, Lisi and Liang,
		  Shuang and Huo, Guohao and Luo, Jiayi and Cheng, Yao},
  title		= {G-SAP: Graph-based Structure-Aware Prompt Learning over
		  Heterogeneous Knowledge for Commonsense Reasoning},
  year		= {2024},
  isbn		= {9798400706196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652583.3658040},
  doi		= {10.1145/3652583.3658040},
  abstract	= {Commonsense question answering has demonstrated
		  considerable potential across various applications like
		  assistants and social robots. Although fully fine-tuned
		  Pre-trained Language Model(PLM) has achieved remarkable
		  performance in commonsense reasoning, their tendency to
		  excessively prioritize textual information hampers the
		  precise transfer of structural knowledge and undermines
		  interpretability. Some studies have explored combining
		  Language Models (LM) with Knowledge Graphs (KGs) by
		  coarsely fusing the two modalities to perform Graph Neural
		  Network (GNN)-based reasoning that lacks a profound
		  interaction between heterogeneous modalities. In this
		  paper, we propose a novel underlineG raph-based underlineS
		  tructure-underlineA ware underlineP rompt Learning Model
		  for commonsense reasoning, named G-SAP, aiming to maintain
		  a balance between heterogeneous knowledge and enhance the
		  cross-modal interaction within the LM+GNNs model. In
		  particular, an evidence graph is constructed by integrating
		  multiple knowledge sources, i.e. ConceptNet, Wikipedia, and
		  Cambridge Dictionary to boost the performance. Afterward, a
		  structure-aware frozen PLM is employed to fully incorporate
		  the structured and textual information from the evidence
		  graph, where the generation of prompts is driven by graph
		  entities and relations. Finally, a heterogeneous
		  message-passing reasoning module is used to facilitate deep
		  interaction of knowledge between the LM and graph-based
		  networks. Empirical validation, conducted through extensive
		  experiments on three benchmark datasets, demonstrates the
		  notable performance of the proposed model. The results
		  reveal a significant advancement over the existing models,
		  especially, with 6.12% improvement over the SoTA LM+GNNs
		  model ~citehuang2023mvp on the OpenbookQA dataset.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Multimedia Retrieval},
  pages		= {1051–1060},
  numpages	= {10},
  keywords	= {commonsense question answering, graph-based networks,
		  heterogeneous modalities, prompt learning},
  location	= {Phuket, Thailand},
  series	= {ICMR '24}
}

@InProceedings{	  10.1145/3637528.3672043,
  author	= {Luo, Yizhen and Yang, Kai and Hong, Massimo and Liu, Xing
		  Yi and Nie, Zikun and Zhou, Hao and Nie, Zaiqing},
  title		= {Learning Multi-view Molecular Representations with
		  Structured and Unstructured Knowledge},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3672043},
  doi		= {10.1145/3637528.3672043},
  abstract	= {Capturing molecular knowledge with representation learning
		  approaches holds significant potential in vast scientific
		  fields such as chemistry and life science. An effective and
		  generalizable molecular representation is expected to
		  capture the consensus and complementary molecular expertise
		  from diverse views and perspectives. However, existing
		  works fall short in learning multi-view molecular
		  representations, due to challenges in explicitly
		  incorporating view information and handling molecular
		  knowledge from heterogeneous sources. To address these
		  issues, we present MV-Mol, a molecular representation
		  learning model that harvests multi-view molecular expertise
		  from chemical structures, unstructured knowledge from
		  biomedical texts, and structured knowledge from knowledge
		  graphs. We utilize text prompts to model view information
		  and design a fusion architecture to extract view-based
		  molecular representations. We develop a two-stage
		  pre-training procedure, exploiting heterogeneous data of
		  varying quality and quantity. Through extensive
		  experiments, we show that MV-Mol provides improved
		  representations that substantially benefit molecular
		  property prediction. Additionally, MV-Mol exhibits
		  state-of-the-art performance in multi-modal comprehension
		  of molecular structures and texts. Code and data are
		  available at https://github.com/PharMolix/OpenBioMed.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2082–2093},
  numpages	= {12},
  keywords	= {knowledge graphs, multi-view molecular representation
		  learning, text mining},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3637528.3671460,
  author	= {Ren, Xubin and Tang, Jiabin and Yin, Dawei and Chawla,
		  Nitesh and Huang, Chao},
  title		= {A Survey of Large Language Models for Graphs},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671460},
  doi		= {10.1145/3637528.3671460},
  abstract	= {Graphs are an essential data structure utilized to
		  represent relationships in real-world scenarios. Prior
		  research has established that Graph Neural Networks (GNNs)
		  deliver impressive outcomes in graph-centric tasks, such as
		  link prediction and node classification. Despite these
		  advancements, challenges like data sparsity and limited
		  generalization capabilities continue to persist. Recently,
		  Large Language Models (LLMs) have gained attention in
		  natural language processing. They excel in language
		  comprehension and summarization. Integrating LLMs with
		  graph learning techniques has attracted interest as a way
		  to enhance performance in graph learning tasks. In this
		  survey, we conduct an in-depth review of the latest
		  state-of-the-art LLMs applied in graph learning and
		  introduce a novel taxonomy to categorize existing methods
		  based on their framework design. We detail four unique
		  designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii)
		  LLMs-Graphs Integration, and iv) LLMs-Only, highlighting
		  key methodologies within each category. We explore the
		  strengths and limitations of each framework, and emphasize
		  potential avenues for future research, including overcoming
		  current integration challenges between LLMs and graph
		  learning techniques, and venturing into new application
		  areas. This survey aims to serve as a valuable resource for
		  researchers and practitioners eager to leverage large
		  language models in graph learning, and to inspire continued
		  progress in this dynamic field. We consistently maintain
		  the related open-source materials at
		  https://github.com/HKUDS/Awesome-LLM4Graph-Papers.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6616–6626},
  numpages	= {11},
  keywords	= {graph learning, large language models},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3688868.3689189,
  author	= {Zhou, Luping},
  title		= {Automated Medical Report Generation and Visual Question
		  Answering},
  year		= {2024},
  isbn		= {9798400711954},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3688868.3689189},
  doi		= {10.1145/3688868.3689189},
  abstract	= {The rapid growth of medical imaging data has far outpaced
		  the availability of trained radiologists, significantly
		  increasing their workload. To alleviate this burden, reduce
		  diagnostic errors, and streamline clinical workflows, the
		  need for automated medical diagnostic report generation has
		  become more urgent than ever. However, this task is
		  particularly challenging, as it requires the ability to
		  capture and describe clinically significant fine-grained
		  visual differences in highly similar medical images.
		  Additionally, critical disease-related keywords can easily
		  be overshadowed by the prevalence of similar phrases
		  describing common image content. Moreover, generating
		  comprehensive reports that detail both normal and
		  pathological findings within images adds to the
		  complexity.In this presentation, I will showcase our latest
		  research on automated medical diagnostic report generation
		  and medical visual question answering, highlighting how we
		  have tackled these challenges. Our work has transitioned
		  from traditional encoder-decoder models to cutting-edge
		  approaches utilizing large language models (LLMs). I will
		  also discuss the current limitations of these methods and
		  propose potential future directions.Specifically, I will
		  present two methods we developed before the advent of
		  pretrained LLMs, which enhance fine-grained recognition for
		  medical report generation from different angles. The first
		  is a self-boosting framework designed to learn highly
		  correlated image and text features, enabling the model to
		  narrate even finer visual changes in the generated reports.
		  The second method is inspired by the 'multi-expert joint
		  diagnosis' scenario and introduces multiple learnable
		  'expert' tokens into the transformer architecture, with
		  each expert focusing on distinct image regions. These
		  complementary perspectives are then aggregated to produce a
		  final, more accurate report. In addition to report
		  generation, I will also present our efforts in improving
		  medical visual question answering (VQA).Following this, I
		  will introduce our recent work on integrating LLMs for
		  medical report generation. I will outline two frameworks we
		  developed: the first employs a frozen LLM for report
		  generation, training only a lightweight visual alignment
		  module to achieve state-of-the-art performance. The second
		  framework goes a step further by integrating a knowledge
		  graph to unlock disease-related knowledge within the LLM,
		  thereby enhancing the clinical relevance of the generated
		  reports. Additionally, I will share our latest
		  investigation into GPT-4V's multimodal capabilities in
		  chest X-ray analysis and discuss the limitations of current
		  evaluation metrics for radiology report generation. To
		  address these limitations, I will introduce our recently
		  developed MRScore framework, which guides LLMs in radiology
		  report evaluation to ensure alignment with human expert
		  analysis.},
  booktitle	= {Proceedings of the 1st International Workshop on
		  Multimedia Computing for Health and Medicine},
  pages		= {3–4},
  numpages	= {2},
  keywords	= {large language models, medical report generation, medical
		  visual question answering},
  location	= {Melbourne VIC, Australia},
  series	= {MCHM'24}
}

@InProceedings{	  10.1145/3664647.3681377,
  author	= {Mei, Xin and Mao, Rui and Cai, Xiaoyan and Yang, Libin and
		  Cambria, Erik},
  title		= {Medical Report Generation via Multimodal Spatio-Temporal
		  Fusion},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681377},
  doi		= {10.1145/3664647.3681377},
  abstract	= {Medical report generation aims at automating the synthesis
		  of accurate and comprehensive diagnostic reports from
		  radiological images. The task can significantly enhance
		  clinical decision-making and alleviate the workload on
		  radiologists. Existing works normally generate reports from
		  single chest radiographs, although historical examination
		  data also serve as crucial references for radiologists in
		  real-world clinical settings. To address this constraint,
		  we introduce a novel framework that mimics the workflow of
		  radiologists. This framework compares past and present
		  patient images to monitor disease progression and
		  incorporates prior diagnostic reports as references for
		  generating current personalized reports. We tackle the
		  textual diversity challenge in cross-modal tasks by
		  promoting style-agnostic discrete report representation
		  learning and token generation. Furthermore, we propose a
		  novel spatio-temporal fusion method with
		  multi-granularities to fuse textual and visual features by
		  disentangling the differences between current and
		  historical data. We also tackle token generation biases,
		  which arise from long-tail frequency distributions,
		  proposing a novel feature normalization technique. This
		  technique ensures unbiased generation for tokens, whether
		  they are frequent or infrequent, enabling the robustness of
		  report generation for rare diseases. Experimental results
		  on the two public datasets demonstrate that our proposed
		  model outperforms state-of-the-art baselines.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {4699–4708},
  numpages	= {10},
  keywords	= {cross-modal generation, medical report generation,
		  multimodal fusion},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3691720.3691749,
  author	= {Yi, Chonghua and Li, Sihao and Ge, Bin},
  title		= {The Tremendous Influence of Large Language Models on
		  Academia and Strategies for Coping with It},
  year		= {2024},
  isbn		= {9798400710230},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691720.3691749},
  doi		= {10.1145/3691720.3691749},
  abstract	= {Large language models have a profound influence on
		  academic work, with both advantages and disadvantages. The
		  beneficial effects are concentrated on promoting the
		  formation of the fifth research paradigm, unprecedentedly
		  enhancing researchers' efficiency in grasping academic
		  frontiers, generating new knowledge through emergent
		  effects, and facilitating the enhancement and expansion of
		  researchers' innovative capabilities. The adverse effects
		  are mainly embodied in the threat posed by large language
		  model writing to the three fundamental principles of
		  academia: objectivity, innovation, and openness, posing a
		  serious threat to the long-term healthy development of
		  academia. In response to the public product nature of
		  academia and the characteristics of large language models
		  as major international engineering projects, the
		  countermeasures to address the significant influence of
		  large language models require systematic construction and
		  comprehensive implementation.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Educational Knowledge and Informatization},
  pages		= {170–177},
  numpages	= {8},
  location	= {Shanghai, China},
  series	= {EKI '24}
}

@InProceedings{	  10.1145/3691620.3695037,
  author	= {Guo, An and Zhou, Yuan and Tian, Haoxiang and Fang,
		  Chunrong and Sun, Yunjian and Sun, Weisong and Gao, Xinyu
		  and Luu, Anh Tuan and Liu, Yang and Chen, Zhenyu},
  title		= {SoVAR: Build Generalizable Scenarios from Accident Reports
		  for Autonomous Driving Testing},
  year		= {2024},
  isbn		= {9798400712487},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691620.3695037},
  doi		= {10.1145/3691620.3695037},
  abstract	= {Autonomous driving systems (ADSs) have undergone
		  remarkable development and are increasingly employed in
		  safety-critical applications. However, recently reported
		  data on fatal accidents involving ADSs suggests that the
		  desired level of safety has not yet been fully achieved.
		  Consequently, there is a growing need for more
		  comprehensive and targeted testing approaches to ensure
		  safe driving. Scenarios from real-world accident reports
		  provide valuable resources for ADS testing, including
		  critical scenarios and high-quality seeds. However,
		  existing scenario reconstruction methods from accident
		  reports often exhibit limited accuracy in information
		  extraction. Moreover, due to the diversity and complexity
		  of road environments, matching current accident information
		  with the simulation map data for reconstruction poses
		  significant challenges.In this paper, we design and
		  implement SoVAR, a tool for automatically generating
		  road-generalizable scenarios from accident reports. SoVAR
		  utilizes well-designed prompts with linguistic patterns to
		  guide the large language model (LLM) in extracting accident
		  information from textual data. Subsequently, it formulates
		  and solves accident-related constraints in conjunction with
		  the extracted accident information to generate accident
		  trajectories. Finally, SoVAR reconstructs accident
		  scenarios on various map structures and converts them into
		  test scenarios to evaluate its capability to detect defects
		  in industrial ADSs. We experiment with SoVAR, using the
		  accident reports from the National Highway Traffic Safety
		  Administration's (NHTSA) database to generate test
		  scenarios for the industrial-grade ADS Apollo. The
		  experimental findings demonstrate that SoVAR can
		  effectively generate generalized accident scenarios across
		  different road structures. Furthermore, the results confirm
		  that SoVAR identified 5 distinct safety violation types
		  that contributed to the crash of Baidu Apollo.},
  booktitle	= {Proceedings of the 39th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {268–280},
  numpages	= {13},
  keywords	= {software testing, automatic test generation, constraint
		  solving, autonomous driving system},
  location	= {Sacramento, CA, USA},
  series	= {ASE '24}
}

@Article{	  10.1145/3696413,
  author	= {Li, Zihao and Yang, Chao and Chen, Yakun and Wang, Xianzhi
		  and Chen, Hongxu and Xu, Guandong and Yao, Lina and Sheng,
		  Michael},
  title		= {Graph and Sequential Neural Networks in Session-based
		  Recommendation: A Survey},
  year		= {2024},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3696413},
  doi		= {10.1145/3696413},
  abstract	= {Recent years have witnessed the remarkable success of
		  recommendation systems (RSs) in alleviating the information
		  overload problem. As a new paradigm of RSs, session-based
		  recommendation (SR) specializes in users’ short-term
		  preferences and aims at providing a more dynamic and timely
		  recommendation based on ongoing interactions. This survey
		  presents a comprehensive overview of the recent works on
		  SR. First, we clarify the key definitions within SR and
		  compare the characteristics of SR against other
		  recommendation tasks. Then, we summarize the existing
		  methods in two categories: sequential neural network based
		  methods and graph neural network (GNN) based methods. The
		  relevant frameworks and technical details are further
		  introduced. Finally, we discuss the challenges of SR and
		  new research directions in this area.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {40},
  numpages	= {37},
  keywords	= {Recommendation survey, session-based recommendation, graph
		  neural networks, sequential neural networks}
}

@Article{	  10.1109/taslp.2024.3419415,
  author	= {Song, Ran and Huang, Xiang and Peng, Hao and Gao,
		  Shengxiang and Yu, Zhengtao and Yu, Philip},
  title		= {WDEA: The Structure and Semantic Fusion With Wasserstein
		  Distance for Low-Resource Language Entity Alignment},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3419415},
  doi		= {10.1109/TASLP.2024.3419415},
  abstract	= {Entity Alignment (EA) aims to identify pairs of entities
		  from two distinct language knowledge graphs (KGs) that
		  represent the same real-world objects. Current EA methods
		  have exhibited impressive performance by leveraging both
		  structural and semantic information. However, these
		  approaches often falter when confronted with EA in
		  low-resource languages. The primary challenge is that
		  low-resource language KGs have sparse graph structures,
		  resulting in difficulty in obtaining accurate entity
		  representations. High-quality entity representation is the
		  key to improving EA performance. Therefore, we propose
		  augmenting entity representations with additional features
		  derived from within the graph. In this paper, we introduce
		  a novel approach: Structure and Semantic Fusion with WD for
		  Low-Resource Language Entity Alignment (WDEA). Our method
		  integrates structural and semantic information using the
		  Wasserstein Distance. Specifically, we design a Wasserstein
		  Graph Convolutional Network (WGCN), a GNN-based model that
		  integrates multi-hop information using a message passing
		  mechanism with WD. Additionally, our method adapts the
		  semantic information from the pre-trained language model in
		  the Wasserstein space to facilitate smooth integration. We
		  also propose the Wasserstein Fusion Encoder (WFE), which
		  effectively combines structural and semantic information in
		  the Wasserstein space. To validate the efficacy of our
		  proposed method, we construct low-resource language EA
		  datasets, encompassing uncommon linguistic varieties with
		  sparser structures compared to mainstream datasets.
		  Experimental results show the superiority of our approach,
		  demonstrating significant performance enhancements in
		  low-resource language EA compared to prevailing baseline
		  models across various information configurations.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jun,
  pages		= {4511–4525},
  numpages	= {15}
}

@Article{	  10.1145/3676280,
  author	= {O'Leary Jr, Daniel E.},
  title		= {USING LARGE LANGUAGE MODELS FOR ARMCHAIR AUDITORS},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3676280},
  doi		= {10.1145/3676280},
  abstract	= {Armchair auditors are citizens who use open data to
		  investigate and monitor government activities, typically
		  using analytics and other approaches. Armchair auditors
		  provide a valuable role in holding governments and
		  organizations accountable. This paper investigates the
		  potential use of large language models (LLM) to support
		  armchair auditor analyzes of different governmental
		  entities. Unfortunately, the literature, prior to the
		  development of LLM suggested several challenges for
		  armchair auditors. However, the analysis in this paper
		  suggests that LLM can provide substantial data and analytic
		  process support for armchair auditors mitigating issues
		  such as, providing guidelines for analyses, guiding users
		  to appropriate communities, suggesting potential data
		  availability opportunities, doing analysis and other
		  issues. As part of an approach to unifying armchair auditor
		  searches, this paper also suggests a prompt library
		  designed to support, standardize and promote best practice
		  analyzes among armchair auditors. In addition to these
		  issues, this paper also analyzes emerging ethical issues
		  associated with armchair auditors and their use of open
		  data and LLMs. Finally, this paper extends the activity
		  theory model to account for LLMs.},
  note		= {Just Accepted},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= jul,
  keywords	= {Activity Theory, Large Language Models, Armchair Auditor,
		  Open Data, ChatGPT, BARD}
}

@InProceedings{	  10.1145/3652628.3652731,
  author	= {Yu, Tingjie and Liao, Liefa and Yang, Yiguo and Xia,
		  Weihuan and Zou, Zhenyuan},
  title		= {DBMAT: Research on Chinese Named Entity Recognition Using
		  the Dilated Bidirectional Multi-layer Attentive Transformer
		  Fusion Model},
  year		= {2024},
  isbn		= {9798400708831},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652628.3652731},
  doi		= {10.1145/3652628.3652731},
  abstract	= {In the realm of Chinese Named Entity Recognition tasks,
		  conventional models have frequently fallen short in
		  adequately addressing linguistic features and recognizing
		  the essential role of context. To address this challenge,
		  our research presents a unique Chinese Named Entity
		  Recognition model, referred to as the LERT-DBMAT-CRF model.
		  Initially, the LERT pre-trained language model incorporates
		  language-informed pretraining strategies to enrich the
		  semantic attributes in textual data. Subsequently, we apply
		  the DBMAT module, unifying bidirectional Long Short-Term
		  Memory networks with residual dilated convolutional
		  networks, coordinated through a multi-head additive
		  attention mechanism. This approach enhances feature
		  extraction by employing the Exponential Linear Unit
		  function, thus enhancing the model's capacity to capture
		  temporal and spatial information relevant to semantic
		  features. Lastly, a Conditional Random Field layer is
		  introduced to exploit contextual information for label
		  prediction. Experimental results demonstrate the
		  exceptional model performance, achieving impressive F1
		  scores of 97.38% and 96.55% on the Resume dataset and the
		  MSRA dataset, respectively, surpassing the performance of
		  current mainstream models.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Artificial Intelligence and Computer Engineering},
  pages		= {616–621},
  numpages	= {6},
  location	= {Dalian, China},
  series	= {ICAICE '23}
}

@InProceedings{	  10.1145/3691720.3691807,
  author	= {Chen, Hongzhi and Gu, Shijia and Wang, Xiaoyan and Pang,
		  Feng and Lin, Xiufeng},
  title		= {A human motivation driven based user-activity recommending
		  service using AIGC and self-supervised agent cluster with
		  debating scheme},
  year		= {2024},
  isbn		= {9798400710230},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691720.3691807},
  doi		= {10.1145/3691720.3691807},
  abstract	= {The manuscript introduces a self-supervised agent
		  framework grounded in AIGC and a multi-debate mechanism.
		  This framework leverages the multi-debate mechanism to
		  facilitate the reflection and continuous iterative
		  enhancement of agents, thereby enabling precise invocation
		  of specialized models within the AIGC large-scale model. It
		  enhances data insights through knowledge graph-augmented
		  retrieval, ultimately replacing traditional keyboard-mouse
		  interactions with natural language interfaces for executing
		  critical user data prediction and insights,
		  multi-model/application scheduling, integration with
		  existing systems, and user decision support. Experiments
		  conducted on a dataset comprising five major categories and
		  over 10,000 SKUs from a medical consumables supplier,
		  focusing on user interaction logic recommendation, model
		  selection and prediction, as well as material demand
		  forecasting and interpretation, have demonstrated the
		  efficacy of our approach. The results confirm that the
		  proposed multi-agent framework and multi-round debate
		  mechanism achieve levels of human-machine interaction logic
		  determination, predictive model selection, and business
		  interpretability that are comparable to or on par with
		  those of domain experts.},
  booktitle	= {Proceedings of the 2nd International Conference on
		  Educational Knowledge and Informatization},
  pages		= {510–514},
  numpages	= {5},
  location	= {Shanghai, China},
  series	= {EKI '24}
}

@InProceedings{	  10.1145/3589334.3645424,
  author	= {Liu, Wenhan and Zhao, Ziliang and Zhu, Yutao and Dou,
		  Zhicheng},
  title		= {Mining Exploratory Queries for Conversational Search},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645424},
  doi		= {10.1145/3589334.3645424},
  abstract	= {Users' queries are usually vague, and their search intents
		  tend to be ambiguous, thereby needing search clarification
		  to clarify users' current intent by asking a clarifying
		  question and providing several clickable sub-intent items
		  as clarification options. However, in addition to drilling
		  down the current query, users may also have exploratory
		  needs that diverge from their current intent. For example,
		  a user searching for the query "Cartier women watches'' may
		  also potentially want to explore some parallel information
		  by issuing queries such as "Rolex women watches'' or
		  "Cartier women bracelets'', named exploratory queries in
		  this paper. These exploratory needs are common during the
		  search process yet cannot be satisfied by current search
		  clarification approaches which typically stick to the
		  sub-intents of the query. This paper focuses on mining
		  exploratory queries as additional options to meet users'
		  exploratory needs in conversational search systems.
		  Specifically, we first design a rule-based model that
		  generates exploratory queries based on the current query's
		  top retrieved documents. Then, we propose using the data
		  generated by the rule-based model to train a neural
		  generation model through multi-task learning for further
		  generalization. Finally, we borrow the in-context learning
		  ability of the large language model to generate exploratory
		  queries based on prompt engineering. We constructed an
		  evaluation dataset based on human annotations and conduct
		  an extensive set of experiments. The results show that our
		  proposed methods generate higher-quality exploratory
		  queries compared with several baselines.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {1386–1394},
  numpages	= {9},
  keywords	= {conversational search, exploratory search, search
		  clarification},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3616855.3635744,
  author	= {I, Muneeswaran and Shankar, Advaith and V, Varun and
		  Gopalakrishnan, Saisubramaniam and Vaddina, Vishal},
  title		= {Mitigating Factual Inconsistency and Hallucination in
		  Large Language Models},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3635744},
  doi		= {10.1145/3616855.3635744},
  abstract	= {Large Language Models (LLMs) have demonstrated remarkable
		  capabilities in various language-related tasks enabling
		  applications in various fields such as healthcare,
		  education, financial services etc. However, they are prone
		  to producing factually incorrect responses or
		  ''hallucinations'' which can have detrimental consequences
		  such as loss of credibility, diminished customer trust etc.
		  In this presentation, we showcase a solution that addresses
		  the challenge of minimizing hallucinations. Our solution
		  provides accurate responses and generates detailed
		  explanations, thereby enabling the users to know how the
		  model arrived at the final response. Additionally, it
		  verifies if the explanations are factually correct and
		  offers insights into whether the generated explanations are
		  directly derived from the provided context or if they are
		  inferred from it. We also systematically assess the quality
		  of generated responses using an LLM-based evaluation
		  technique. We present empirical results on benchmark
		  datasets to demonstrate the effectiveness of our approach.
		  Our presentation also examines the impact of individual
		  components in the solution, enhancing the factual
		  correctness of the final response. This research is vital
		  for industries utilizing LLMs, as it provides a means to
		  enhance the reliability of responses and mitigate the risks
		  associated with factual hallucinations. Researchers and
		  practitioners seeking to enhance the reliability of LLM
		  responses will find valuable insights in this
		  presentation.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {1169–1170},
  numpages	= {2},
  keywords	= {hallucinations, information retrieval, large language
		  models},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@Article{	  10.1109/tcbb.2024.3451348,
  author	= {Li, Zhijing and Tian, Liwei and Jiang, Yiping and Huang,
		  Yucheng},
  title		= {Relation Extraction in Biomedical Texts: A Cross-Sentence
		  Approach},
  year		= {2024},
  issue_date	= {Nov.-Dec. 2024},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {21},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2024.3451348},
  doi		= {10.1109/TCBB.2024.3451348},
  abstract	= {Relation extraction, a crucial task in understanding the
		  intricate relationships between entities in biomedical
		  domains, has predominantly focused on binary relations
		  within single sentences. However, in practical biomedical
		  scenarios, relationships often extend across multiple
		  sentences, leading to extraction errors with potential
		  impacts on clinical decision-making and medical diagnosis.
		  To overcome this limitation, we present a novel
		  cross-sentence relation extraction framework that
		  integrates and enhances coreference resolution and relation
		  extraction models. Coreference resolution serves as the
		  foundation, breaking sentence boundaries and linking
		  entities across sentences. Our framework incorporates
		  pre-trained deep language representations and leverages
		  graph LSTMs to effectively model cross-sentence entity
		  mentions. The use of a self-attentive Transformer
		  architecture and external semantic information further
		  enhances the modeling of intricate relationships.
		  Comprehensive experiments conducted on two standard
		  datasets, namely the BioNLP dataset and THYME dataset,
		  demonstrate the state-of-the-art performance of our
		  proposed approach.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= sep,
  pages		= {2156–2166},
  numpages	= {11}
}

@InProceedings{	  10.1145/3626772.3657732,
  author	= {Sun, Zhongxiang and Si, Zihua and Zhang, Xiao and Zang,
		  Xiaoxue and Song, Yang and Xu, Hongteng and Xu, Jun},
  title		= {To Search or to Recommend: Predicting Open-App Motivation
		  with Neural Hawkes Process},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657732},
  doi		= {10.1145/3626772.3657732},
  abstract	= {Incorporating Search and Recommendation (S&amp;R) services
		  within a singular application is prevalent in online
		  platforms, leading to a new task termed open-app motivation
		  prediction, which aims to predict whether users initiate
		  the application with the specific intent of information
		  searching, or to explore recommended content for
		  entertainment. Studies have shown that predicting users'
		  motivation to open an app can help to improve user
		  engagement and enhance performance in various downstream
		  tasks. However, accurately predicting open-app motivation
		  is not trivial, as it is influenced by user-specific
		  factors, search queries, clicked items, as well as their
		  temporal occurrences. Furthermore, these activities occur
		  sequentially and exhibit intricate temporal dependencies.
		  Inspired by the success of the Neural Hawkes Process (NHP)
		  in modeling temporal dependencies in sequences, this paper
		  proposes a novel neural Hawkes process model to capture the
		  temporal dependencies between historical user browsing and
		  querying actions. The model, referred to as Neural Hawkes
		  Process-based Open-App Motivation prediction model
		  (NHP-OAM), employs a hierarchical transformer and a novel
		  intensity function to encode multiple factors, and open-app
		  motivation prediction layer to integrate time and
		  user-specific information for predicting users' open-app
		  motivations. To demonstrate the superiority of our NHP-OAM
		  model and construct a benchmark for the Open-App Motivation
		  Prediction task, we not only extend the public S&amp;R
		  dataset ZhihuRec but also construct a new real-world
		  Open-App Motivation Dataset (OAMD). Experiments on these
		  two datasets validate NHP-OAM's superiority over baseline
		  models. Further downstream application experiments
		  demonstrate NHP-OAM's effectiveness in predicting users'
		  Open-App Motivation, highlighting the immense application
		  value of NHP-OAM.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1018–1028},
  numpages	= {11},
  keywords	= {behavior modeling, neural hawkes process, open-app
		  motivation},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Article{	  10.1145/3687035,
  author	= {Sun, Yuling and Chen, Jiaju and Yao, Bingsheng and Liu,
		  Jiali and Wang, Dakuo and Ma, Xiaojuan and Lu, Yuxuan and
		  Xu, Ying and He, Liang},
  title		= {Exploring Parent's Needs for Children-Centered AI to
		  Support Preschoolers' Interactive Storytelling and Reading
		  Activities},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW2},
  url		= {https://doi.org/10.1145/3687035},
  doi		= {10.1145/3687035},
  abstract	= {Interactive storytelling is vital for preschooler
		  development. While children's interactive partners have
		  traditionally been their parents and teachers, recent
		  advances in artificial intelligence (AI) have sparked a
		  surge of AI-based storytelling and reading technologies. As
		  these technologies become increasingly ubiquitous in
		  preschoolers' lives, questions arise regarding how they
		  function in practical storytelling and reading scenarios
		  and, how parents, the most critical stakeholders,
		  experience and perceive these technologies. This paper
		  investigates these questions through a qualitative study
		  with 17 parents of children aged 3-6. Our findings suggest
		  that even though AI-based storytelling and reading
		  technologies provide more immersive and engaging
		  interaction, they still cannot meet parents' expectations
		  due to a series of interactive and algorithmic challenges.
		  We elaborate on these challenges and discuss the possible
		  implications of future AI-based interactive storytelling
		  technologies for preschoolers.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= nov,
  articleno	= {496},
  numpages	= {25},
  keywords	= {ai, artificial intelligence, interactive, parents,
		  preschoolers, storybook reading, storytelling}
}

@Proceedings{	  10.1145/3690712,
  title		= {In2Writing '24: Proceedings of the Third Workshop on
		  Intelligent and Interactive Writing Assistants},
  year		= {2024},
  isbn		= {9798400710315},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Honolulu, HI, USA}
}

@InProceedings{	  10.1145/3626772.3657985,
  author	= {Bendersky, Michael and Li, Cheng and Mei, Qiaozhu and
		  Murdock, Vanessa and Tang, Jie and Wang, Hongning and
		  Zamani, Hamed and Zhang, Mingyang and Zhang, Xingjian},
  title		= {The Second Workshop on Large Language Models for
		  Individuals, Groups, and Society},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657985},
  doi		= {10.1145/3626772.3657985},
  abstract	= {This is the second workshop in the series which discusses
		  the cutting-edge developments in research and applications
		  of personalizing large language models (LLMs) and adapting
		  them to the demands of diverse user populations and
		  societal needs. The full-day workshop plan includes several
		  keynotes and invited talks, a poster session and a panel
		  discussion.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {3062–3064},
  numpages	= {3},
  keywords	= {large language models, personalization},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Article{	  10.1145/3696427,
  author	= {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla,
		  Sandeep Kumar},
  title		= {TTPXHunter: Actionable Threat Intelligence Extraction as
		  TTPs from Finished Cyber Threat Reports},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {4},
  url		= {https://doi.org/10.1145/3696427},
  doi		= {10.1145/3696427},
  abstract	= {Understanding the modus operandi of adversaries aids
		  organizations to employ efficient defensive strategies and
		  share intelligence in the community. This knowledge is
		  often present in unstructured natural language text within
		  threat analysis reports. A translation tool is needed to
		  interpret the modus operandi explained in the sentences of
		  the threat report and convert it into a structured format.
		  This research introduces a methodology named TTPXHunter for
		  automated extraction of threat intelligence in terms of
		  Tactics, Techniques, and Procedures (TTPs) from finished
		  cyber threat reports. It leverages cyber domain-specific
		  state-of-the-art natural language model to augment
		  sentences for minority class TTPs and refine pinpointing
		  the TTPs in threat analysis reports significantly. We
		  create two datasets: an augmented sentence-TTP dataset of
		  (39,296) sentence samples and a (149) real-world cyber
		  threat intelligence report-to-TTP dataset. Further, we
		  evaluate TTPXHunter on the augmented sentence and report
		  datasets. The TTPXHunter achieves the highest performance
		  of (92.42%) f1-score on the augmented dataset, and it also
		  outperforms existing state-of-the-art TTP extraction method
		  by achieving an f1-score of (97.09%) when evaluated over
		  the report dataset. TTPXHunter significantly improves
		  cybersecurity threat intelligence by offering quick,
		  actionable insights into attacker behaviors. This
		  advancement automates threat intelligence analysis and
		  provides a crucial tool for cybersecurity professionals to
		  combat cyber threats.},
  journal	= {Digital Threats},
  month		= dec,
  articleno	= {37},
  numpages	= {19},
  keywords	= {Threat Intelligence, TTP Extraction, MITRE ATT&amp;CK,
		  Natural Language Processing, Threat Intelligence
		  Extraction, TTP Classification, Cyber Security and AI,
		  Cyber Security Threats, NLP, Cybersecurity}
}

@InBook{	  10.1145/3674127.3702962,
  title		= {Authors’ Biography/Index},
  year		= {2024},
  isbn		= {9798400710506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  url		= {https://doi.org/10.1145/3674127.3702962},
  booktitle	= {Information Retrieval: Advanced Topics and Techniques},
  pages		= {773–815},
  numpages	= {43}
}

@InProceedings{	  10.1109/jcdl57899.2023.00038,
  author	= {Sierra-M\'{u}nera, Alejandro and Westphal, Jan and
		  Krestel, Ralf},
  title		= {Efficient Ultrafine Typing of Named Entities},
  year		= {2024},
  isbn		= {9798350399318},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL57899.2023.00038},
  doi		= {10.1109/JCDL57899.2023.00038},
  abstract	= {Ultrafine named entity typing (UFET) refers to the
		  assignment of predefined labels to entity mentions in a
		  given context. In contrast to traditional named entity
		  typing, the number of potential labels is in the thousands
		  and one mention can have more than one assigned type.
		  Previous approaches either depend on large training
		  datasets, or require inefficient encoding of all input-type
		  combinations. Therefore, there is a need for investigating
		  the efficiency during training and prediction of entity
		  typing models in the ultrafine-grained setting, considering
		  its distinctively bigger search space, compared to the
		  coarse- and fine-grained tasks. To efficiently solve UFET,
		  we propose Decent, a lightweight model that encodes, using
		  a pretrained language model, the input sentences separately
		  from the type labels. Additionally, we make use of negative
		  oversampling to speed up the training while improving the
		  generalization of unseen types. Using an openly available
		  UFET dataset, we evaluated the classification and runtime
		  performance of Decent and observed that training and
		  prediction runtime is orders of magnitude faster than the
		  current state-of-the-art approaches, while maintaining a
		  competitive classification performance.},
  booktitle	= {Proceedings of the 2023 ACM/IEEE Joint Conference on
		  Digital Libraries},
  pages		= {205–214},
  numpages	= {10},
  keywords	= {ultrafine enity typing, named entity recognition},
  location	= {Santa Fe, New Mexico, USA},
  series	= {JCDL '23}
}

@InProceedings{	  10.1145/3589334.3645406,
  author	= {Lin, Xin and Su, Tianhuang and Huang, Zhenya and Xue,
		  Shangzi and Liu, Haifeng and Chen, Enhong},
  title		= {A Knowledge-Injected Curriculum Pretraining Framework for
		  Question Answering},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645406},
  doi		= {10.1145/3589334.3645406},
  abstract	= {Knowledge-based question answering (KBQA) is a key task in
		  natural language processing research, and also an approach
		  to access the web data and knowledge, which requires
		  exploiting knowledge graphs (KGs) for reasoning. In the
		  literature, one promising solution for KBQA is to
		  incorporate the pretrained language model (LM) with KGs by
		  generating KG-centered pretraining corpus, which has shown
		  its superiority. However, these methods often depend on
		  specific techniques and resources to work, which may not
		  always be available and restrict its application. Moreover,
		  existing methods focus more on improving language
		  understanding with KGs, while neglect the more important
		  human-like complex reasoning. To this end, in this paper,
		  we propose a general K nowledge-I njected C urriculum P
		  retraining framework (KICP) to achieve comprehensive KG
		  learning and exploitation for KBQA tasks, which is composed
		  of knowledge injection (KI), knowledge adaptation (KA) and
		  curriculum reasoning (CR). Specifically, the KI module
		  first injects knowledge into the LM by generating
		  KG-centered pretraining corpus, and generalizes the process
		  into three key steps that could work with different
		  implementations for flexible application. Next, the KA
		  module learns knowledge from the generated corpus with LM
		  equipped with an adapter as well as keeps its original
		  natural language understanding ability to reduce the
		  negative impacts of the difference between the generated
		  and natural corpus. Last, to enable the LM with complex
		  reasoning, the CR module follows human reasoning patterns
		  to construct three corpora with increasing difficulties of
		  reasoning, and further trains the LM from easy to hard in a
		  curriculum manner to promote model learning. We provide an
		  implementation of the general framework, and evaluate the
		  proposed KICP on four real-word datasets. The results
		  demonstrate that our framework can achieve higher
		  performances, and have good generalization ability to other
		  QA tasks.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {1986–1997},
  numpages	= {12},
  keywords	= {curriculum learning, knowledge-injected pretraining,
		  question answering},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1109/tcbb.2024.3447037,
  author	= {Yu, Xindi and Zhou, Shusen and Zang, Mujun and Wang,
		  Qingjun and Liu, Chanjuan and Liu, Tong},
  title		= {Parallel Convolutional Contrastive Learning Method for
		  Enzyme Function Prediction},
  year		= {2024},
  issue_date	= {Nov.-Dec. 2024},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {21},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2024.3447037},
  doi		= {10.1109/TCBB.2024.3447037},
  abstract	= {The function labeling of enzymes has a wide range of
		  application value in the medical field, industrial biology
		  and other fields. Scientists define enzyme categories by
		  enzyme commission (EC) numbers. At present, although there
		  are some tools for enzyme function prediction, their
		  effects have not reached the application level. To improve
		  the precision of enzyme function prediction, we propose a
		  parallel convolutional contrastive learning (PCCL) method
		  to predict enzyme functions. First, we use the advanced
		  protein language model ESM-2 to preprocess the protein
		  sequences. Second, PCCL combines convolutional neural
		  networks (CNNs) and contrastive learning to improve the
		  prediction precision of multifunctional enzymes.
		  Contrastive learning can make the model better deal with
		  the problem of class imbalance. Finally, the deep learning
		  framework is mainly composed of three parallel CNNs for
		  fully extracting sample features. we compare PCCL with
		  state-of-art enzyme function prediction methods based on
		  three evaluation metrics. The performance of our model
		  improves on both two test sets. Especially on the smaller
		  test set, PCCL improves the AUC by 2.57%.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= aug,
  pages		= {2604–2609},
  numpages	= {6}
}

@Proceedings{	  10.1145/3643479,
  title		= {AIQAM '24: Proceedings of the 1st ACM Workshop on
		  AI-Powered Q&amp;A Systems for Multimedia},
  year		= {2024},
  isbn		= {9798400705472},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Phuket, Thailand}
}

@Article{	  10.1145/3631392,
  author	= {Yang, Jian and Hu, Xinyu and Xiao, Gang and Shen, Yulong},
  title		= {A Survey of Knowledge Enhanced Pre-trained Language
		  Models},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3631392},
  doi		= {10.1145/3631392},
  abstract	= {Pre-trained language models learn informative word
		  representations on a large-scale text corpus through
		  self-supervised learning, which has achieved promising
		  performance in fields of natural language processing (NLP)
		  after fine-tuning. These models, however, suffer from poor
		  robustness and lack of interpretability. We refer to
		  pre-trained language models with knowledge injection as
		  knowledge-enhanced pre-trained language models (KEPLMs).
		  These models demonstrate deep understanding and logical
		  reasoning and introduce interpretability. In this survey,
		  we provide a comprehensive overview of KEPLMs in NLP. We
		  first discuss the advancements in pre-trained language
		  models and knowledge representation learning. Then we
		  systematically categorize existing KEPLMs from three
		  different perspectives. Finally, we outline some potential
		  directions of KEPLMs for future research.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= mar,
  keywords	= {natural language processing, pre-trained language models,
		  symbolic knowledge, knowledge enhanced pre-trained language
		  models}
}

@InProceedings{	  10.1145/3613905.3650798,
  author	= {Reif, Emily and Qian, Crystal and Wexler, James and Kahng,
		  Minsuk},
  title		= {Automatic Histograms: Leveraging Language Models for Text
		  Dataset Exploration},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3650798},
  doi		= {10.1145/3613905.3650798},
  abstract	= {Making sense of unstructured text datasets is perennially
		  difficult, yet increasingly relevant with Large Language
		  Models. Data practitioners often rely on dataset summaries,
		  especially distributions of various derived features. Some
		  features, like toxicity or topics, are relevant to many
		  datasets, but many interesting features are domain
		  specific: instruments and genres for a music dataset, or
		  diseases and symptoms for a medical dataset. Accordingly,
		  data practitioners often run custom analyses for each
		  dataset, which is cumbersome and difficult, or use
		  unsupervised methods. We present AutoHistograms, a
		  visualization tool leveraging LLMs. AutoHistograms
		  automatically identifies relevant entity-based features,
		  visualizes them, and allows the user to interactively query
		  the dataset for new categories of entities. In a user study
		  with (n=10) data practitioners, we observe that
		  participants were able to quickly onboard to
		  AutoHistograms, use the tool to identify actionable
		  insights, and conceptualize a broad range of applicable use
		  cases. Together, this tool and user study contribute to the
		  growing field of LLM-assisted sensemaking tools.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {53},
  numpages	= {9},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@InProceedings{	  10.1145/3616855.3635726,
  author	= {Bendersky, Michael and Li, Cheng and Mei, Qiaozhu and
		  Murdock, Vanessa and Tang, Jie and Wang, Hongning and
		  Zamani, Hamed and Zhang, Mingyang},
  title		= {WSDM 2024 Workshop on Large Language Models for
		  Individuals, Groups, and Society},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3635726},
  doi		= {10.1145/3616855.3635726},
  abstract	= {This workshop discusses the cutting-edge developments in
		  research and applications of personalizing large language
		  models (LLMs) and adapting them to the demands of diverse
		  user populations and societal needs. The full-day workshop
		  includes several keynotes and invited talks, a poster
		  session and a panel discussion.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {1206–1207},
  numpages	= {2},
  keywords	= {large language models, personalization, workshop},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@InProceedings{	  10.1145/3613905.3650949,
  author	= {Oelen, Allard and Auer, S\"{o}ren},
  title		= {Leveraging Large Language Models for Realizing Truly
		  Intelligent User Interfaces},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3650949},
  doi		= {10.1145/3613905.3650949},
  abstract	= {The number of published scholarly articles is growing at a
		  significant rate, making scholarly knowledge organization
		  increasingly important. Various approaches have been
		  proposed to organize scholarly information, including
		  describing scholarly knowledge semantically leveraging
		  knowledge graphs. Transforming unstructured knowledge,
		  presented within articles, to structured and semantically
		  represented knowledge generally requires human intelligence
		  and labor since natural language processing methods alone
		  typically do not render sufficient precision and recall for
		  many applications. With the recent developments of Large
		  Language Models (LLMs), it becomes increasingly possible to
		  provide truly intelligent user interfaces guiding humans in
		  the transformation process. We present an approach to
		  integrate non-intrusive LLMs guidance into existing user
		  interfaces. More specifically, we integrate LLM-supported
		  user interface components into an existing scholarly
		  knowledge infrastructure. Additionally, we provide our
		  experiences with LLM integration, detailing best practices
		  and obstacles. Finally, we evaluate the approach using a
		  small-scale user evaluation with domain experts.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {222},
  numpages	= {8},
  keywords	= {Intelligent User Interface, LLM Interface, Scholarly
		  Knowledge Graphs},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@Article{	  10.1109/taslp.2024.3419438,
  author	= {Li, Pijian and Huang, Qingbao and Li, Zhigang and Cai, Yi
		  and Shuang, Feng and Li, Qing},
  title		= {Multi-Granularity Feature Fusion for Image-Guided Story
		  Ending Generation},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3419438},
  doi		= {10.1109/TASLP.2024.3419438},
  abstract	= {Image-guided Story Ending Generation aims at generating a
		  reasonable and logical ending given a story context and an
		  ending-related image. The existing models have achieved
		  some success by fusing global image features with story
		  context through an attention mechanism. However, they
		  ignore the logical relationship between the story context
		  and the image regions, and have not considered the
		  high-level semantic features of the image such as visual
		  sentiment. This may cause the generated ending inconsistent
		  with the logic or sentiment of the given information. In
		  this paper, we propose a
		  &lt;bold&gt;M&lt;/bold&gt;ulti-&lt;bold&gt;G&lt;/bold&gt;ranularity
		  feature &lt;bold&gt;F&lt;/bold&gt;usion (MGF) model to
		  solve this problem. Concretely, we first employ an image
		  sentiment extractor to grasp the sentiment features of the
		  image as part of the global image features. We then design
		  a scene subgraph selector to capture the image features of
		  the key region by picking the scene subgraph most relevant
		  to the context. Finally, we fuse the textual and visual
		  features from object level, region level, and global level,
		  respectively. Our model is thereby capable of effectively
		  capturing the key region features and visual sentiment of
		  the image, so as to generate a more logical and sentimental
		  ending. Experimental results show that our MGF model
		  outperforms the state-of-the-art models on most metrics.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jun,
  pages		= {3437–3449},
  numpages	= {13}
}

@InProceedings{	  10.1145/3639631.3639665,
  author	= {Li, Peihong and Cai, Fei and Wang, Siyuan and Liu, Shixian
		  and Liu, Dengfeng},
  title		= {A Review: Data and Semantic Augmentation for Relation
		  Classification in Low Resource},
  year		= {2024},
  isbn		= {9798400709203},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639631.3639665},
  doi		= {10.1145/3639631.3639665},
  abstract	= {Relation Classification (RC) is a significant study
		  component in Natural Language Processing (NLP) that focuses
		  on matching pairings of entities in natural utterances.
		  Both traditional methods relying on rule matching and
		  statistical features, as well as more contemporary methods
		  utilizing deep learning and Pre-trained Language Model
		  (PLM), excessively depends on vast quantities of data. In
		  reality, numerous domains or subjects sometimes suffer from
		  a scarcity of accessible data. Consequently, numerous
		  academics have shifted their attention towards conducting
		  research in low-resource domains, namely in areas such as
		  semi-supervised learning and weakly supervised learning.
		  However, both of these approaches bring a significant
		  amount of noisy input into the model. Errors may arise in
		  methods utilizing metric learning as a result of
		  inappropriate metric selections. Prompt Learning (PL) has
		  expanded its success in few-shot learning to also include
		  RC tasks. Studies have been carried out to investigate the
		  utilization of PL in enhancing the model’s capacity to
		  comprehend and learn textual content. This includes
		  augmenting the sample data with prompt templates to enhance
		  the model’s ability to learn from a small amount of
		  labeled data. This study presents a comprehensive overview
		  of the latest research advancements in low-resource reading
		  comprehension (RC). Additionally, it provides a summary of
		  the few-shot RC technique based on pre-training and
		  fine-tuning language models (PL). Lastly, the present
		  challenges in research are examined, and the future
		  trajectory of work on few-shot RC based on pre-training and
		  language models are envisioned.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  pages		= {195–201},
  numpages	= {7},
  keywords	= {few-shot, prompt learning, relation classification},
  location	= {Sanya, China},
  series	= {ACAI '23}
}

@Article{	  10.1145/3663482,
  author	= {Gilman, Ekaterina and Bugiotti, Francesca and Khalid,
		  Ahmed and Mehmood, Hassan and Kostakos, Panos and Tuovinen,
		  Lauri and Ylipulli, Johanna and Su, Xiang and Ferreira,
		  Denzil},
  title		= {Addressing Data Challenges to Drive the Transformation of
		  Smart Cities},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {5},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3663482},
  doi		= {10.1145/3663482},
  abstract	= {Cities serve as vital hubs of economic activity and
		  knowledge generation and dissemination. As such, cities
		  bear a significant responsibility to uphold environmental
		  protection measures while promoting the welfare and living
		  comfort of their residents. There are diverse views on the
		  development of smart cities, from integrating Information
		  and Communication Technologies into urban environments for
		  better operational decisions to supporting sustainability,
		  wealth, and comfort of people. However, for all these
		  cases, data are the key ingredient and enabler for the
		  vision and realization of smart cities. This article
		  explores the challenges associated with smart city data. We
		  start with gaining an understanding of the concept of a
		  smart city, how to measure that the city is a smart one,
		  and what architectures and platforms exist to develop one.
		  Afterwards, we research the challenges associated with the
		  data of the cities, including availability, heterogeneity,
		  management, analysis, privacy, and security. Finally, we
		  discuss ethical issues. This article aims to serve as a
		  “one-stop shop” covering data-related issues of smart
		  cities with references for diving deeper into particular
		  topics of interest.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= nov,
  articleno	= {88},
  numpages	= {65},
  keywords	= {Big data, smart city, urban computing, machine learning,
		  data analysis}
}

@Article{	  10.1145/3649449,
  author	= {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie,
		  Jian-Yun and Wen, Ji-Rong},
  title		= {Pre-Trained Language Models for Text Generation: A
		  Survey},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3649449},
  doi		= {10.1145/3649449},
  abstract	= {Text Generation aims to produce plausible and readable
		  text in human language from input data. The resurgence of
		  deep learning has greatly advanced this field, in
		  particular, with the help of neural generation models based
		  on pre-trained language models (PLMs). Text generation
		  based on PLMs is viewed as a promising approach in both
		  academia and industry. In this article, we provide a survey
		  on the utilization of PLMs in text generation. We begin
		  with introducing two key aspects of applying PLMs to text
		  generation: (1) how to design an effective PLM to serve as
		  the generation model; and (2) how to effectively optimize
		  PLMs given the reference text and to ensure that the
		  generated texts satisfy special text properties. Then, we
		  show the major challenges that have arisen in these
		  aspects, as well as possible solutions for them. We also
		  include a summary of various useful resources and typical
		  text generation applications based on PLMs. Finally, we
		  highlight the future research directions which will further
		  improve these PLMs for text generation. This comprehensive
		  survey is intended to help researchers interested in text
		  generation problems to learn the core concepts, the main
		  techniques and the latest developments in this area based
		  on PLMs.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {230},
  numpages	= {39},
  keywords	= {Pre-trained language models, natural language processing}
}

@InProceedings{	  10.1145/3626772.3657755,
  author	= {Dao, Huy and Deng, Yang and Le, Dung D. and Liao, Lizi},
  title		= {Broadening the View: Demonstration-augmented Prompt
		  Learning for Conversational Recommendation},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657755},
  doi		= {10.1145/3626772.3657755},
  abstract	= {Conversational Recommender Systems (CRSs) leverage natural
		  language dialogues to provide tailored recommendations.
		  Traditional methods in this field primarily focus on
		  extracting user preferences from isolated dialogues. It
		  often yields responses with a limited perspective, confined
		  to the scope of individual conversations. Recognizing the
		  potential in collective dialogue examples, our research
		  proposes an expanded approach for CRS models, utilizing
		  selective analogues from dialogue histories and responses
		  to enrich both generation and recommendation processes.
		  This introduces significant research challenges, including:
		  (1) How to secure high-quality collections of
		  recommendation dialogue exemplars? (2) How to effectively
		  leverage these exemplars to enhance CRS models?To tackle
		  these challenges, we introduce a novel
		  Demonstration-enhanced Conversational Recommender System
		  (DCRS), which aims to strengthen its understanding on the
		  given dialogue contexts by retrieving and learning from
		  demonstrations. In particular, we first propose a
		  knowledge-aware contrastive learning method that adeptly
		  taps into the mentioned entities and the dialogue's
		  contextual essence for pretraining the demonstration
		  retriever. Subsequently, we further develop two adaptive
		  demonstration-augmented prompt learning approaches,
		  involving contextualized prompt learning and
		  knowledge-enriched prompt learning, to bridge the gap
		  between the retrieved demonstrations and the two end tasks
		  of CRS, i.e., response generation and item recommendation,
		  respectively. Rigorous evaluations on two established
		  benchmark datasets underscore DCRS's superior performance
		  over existing CRS methods in both item recommendation and
		  response generation.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {785–795},
  numpages	= {11},
  keywords	= {conversational recommendation, demonstration-based
		  learning},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3643489.3661114,
  author	= {Tran, Quang-Linh and Nguyen, Binh and Jones, Gareth J. F.
		  and Gurrin, Cathal},
  title		= {MemoriEase 2.0: A Conversational Lifelog Retrieve System
		  for LSC'24},
  year		= {2024},
  isbn		= {9798400705502},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643489.3661114},
  doi		= {10.1145/3643489.3661114},
  abstract	= {Lifelog retrieval plays an important role in memory
		  support for lifeloggers. It helps the lifeloggers to
		  browse, search and navigate their life moments from the
		  lifelog data. However, the volume and variety of lifelog
		  data are enormous and range in multiple modalities so they
		  impose a big challenge to retrieve accurate lifelog
		  moments. The Lifelog Search Challenges (LSCs) are a
		  benchmark challenge for evaluating lifelog retrieval
		  systems in different tasks. In this paper, we introduce the
		  MemoriEase 2.0 lifelog retrieval system that participates
		  in LSC'24. This system not only inherits core functions
		  from the precedent system but also incorporates new
		  components such as conversational search, visual similarity
		  search and retrieval-augmented generation for
		  question-answering tasks. The new functions are expected to
		  help expert and novice users solve all topics in three
		  tasks of LSC'24. We evaluate MemoriEase 2.0 in KIS topics
		  in LSC'23 and the system achieves promising results with
		  Recall@1 is 40% at the first hint and it solves 8 over 10
		  topics.},
  booktitle	= {Proceedings of the 7th Annual ACM Workshop on the Lifelog
		  Search Challenge},
  pages		= {12–17},
  numpages	= {6},
  keywords	= {lifelog retrieval, conversational search, personal
		  archive},
  location	= {Phuket, Thailand},
  series	= {LSC '24}
}

@Article{	  10.1613/jair.1.15280,
  author	= {Liu, Bin and Yin, Guosheng},
  title		= {Graphmax for Text Generation},
  year		= {2024},
  issue_date	= {Jan 2024},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {78},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.15280},
  doi		= {10.1613/jair.1.15280},
  abstract	= {In text generation, a large language model (LM) makes a
		  choice of each new word based only on the former selection
		  of its context using the softmax function. Nevertheless,
		  the link statistics information of concurrent words based
		  on a scene-specific corpus is valuable in choosing the next
		  word, which can help to ensure the topic of the generated
		  text to be aligned with the current task. To fully explore
		  the co-occurrence information, we propose a graphmax
		  function for task-specific text generation. Using the
		  graph-based regularization, graphmax enables the final word
		  choice to be determined by both the global knowledge from
		  the LM and the local knowledge from the scene-specific
		  corpus. The traditional softmax function is regularized
		  with a graph total variation (GTV) term, which incorporates
		  the local knowledge into the LM and encourages the model to
		  consider the statistical relationships between words in a
		  scene-specific corpus. The proposed graphmax is versatile
		  and can be readily plugged into any large pre-trained LM
		  for text generation and machine translation. Through
		  extensive experiments, we demonstrate that the new
		  GTV-based regularization can improve performances in
		  various natural language processing (NLP) tasks in
		  comparison with existing methods. Moreover, through human
		  experiments, we observe that participants can easily
		  distinguish the text generated by graphmax or softmax.},
  journal	= {J. Artif. Int. Res.},
  month		= jan,
  numpages	= {26}
}

@Proceedings{	  10.1145/3687311,
  title		= {IECT '24: Proceedings of the 2024 International Conference
		  on Intelligent Education and Computer Technology},
  year		= {2024},
  isbn		= {9798400709920},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guilin, China}
}

@InProceedings{	  10.1145/3672919.3672982,
  author	= {Zou, Feifei and Hu, Su and Yu, Wei and Yan, Zejun and
		  Chan, Sijun},
  title		= {Research on Financial Fraud Text Classification Based on
		  PET-BiLSTM},
  year		= {2024},
  isbn		= {9798400718212},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3672919.3672982},
  doi		= {10.1145/3672919.3672982},
  abstract	= {To address challenges associated with obscure features,
		  unbalanced data, and diverse fraud types in Internet
		  financial texts, this paper proposes the PET-BiLSTM
		  financial fraud text classification model to enhance the
		  semantic understanding and reasoning of financial fraud
		  texts. Crawling news, comments and short messages about
		  financial fraud from open source websites, desensitizing
		  and cleaning the texts, labeling them automatically through
		  LDA topic distribution model and financial fraud thesaurus,
		  constructing prompt templates containing mask positions,
		  forming cloze sentences for each sample, and transforming
		  them into multi-classification tasks. The pre-trained BERT
		  model is used to learn semantic information, combined with
		  bidirectional LSTM (BiLSTM) to extract fraudulent text
		  features, and supervised training is carried out.
		  Experimental results show PET-BiLSTM achieving 89.11% and
		  84.54% F1 values in multi-classifying fraud types and
		  three-classifying fraud degree identification tasks,
		  surpassing deep learning baselines by 4.21% to 15.35% and
		  2.21% to 9.12%, respectively. PET-BiLSTM also outperforms
		  ChatGPT in zero-shot prompt learning by 67.11% and 48.54%.
		  The model demonstrates superior performance in
		  understanding knowledge facts and fraud reasoning in the
		  financial fraud text classification task, effectively
		  addressing complex text fraud detection challenges within
		  the financial field.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Cyber Security, Artificial Intelligence and Digital
		  Economy},
  pages		= {341–345},
  numpages	= {5},
  location	= {Nanjing, China},
  series	= {CSAIDE '24}
}

@InProceedings{	  10.1145/3627673.3679915,
  author	= {Abdi, Samireh},
  title		= {Enhancing Event Detection with Inter-Event Dependencies in
		  Large Ontologies},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679915},
  doi		= {10.1145/3627673.3679915},
  abstract	= {Event Detection (ED), a crucial component of comprehensive
		  text analysis tools, is a well-established task within the
		  fields of Natural Language Processing (NLP) and Information
		  Extraction (IE). Current state-of-the-art models for ED
		  primarily focus on identifying a limited set of predefined
		  event types. Recently, the challenge of detecting a broad
		  array of predefined event types has garnered increasing
		  interest within the IE community. However, a significant
		  gap in existing research on ED with extensive ontologies is
		  the inadequate exploration of how interactions between
		  event types affect ED model performance. One of the
		  hindrances for this purpose is the lack of resources to
		  encode event-event dependencies for large ontologies. This
		  study introduces a novel approach that leverages existing
		  inter-event dependency resources to provide this
		  information for extensive ontologies. Specifically, a
		  solution based on Optimal Transport is proposed to map
		  event-event dependency from existing resources to a large
		  ontology. We conduct extensive experiments on multiple
		  benchmark datasets to assess the effectiveness of our
		  approach. Our findings, supported by a thorough analysis,
		  demonstrate that this innovative technique significantly
		  enhances the performance of ED models, especially for
		  ontologies with a large number of event types.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3612–3616},
  numpages	= {5},
  keywords	= {event detection, large ontology, optimal transport},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Proceedings{	  10.1145/3649165,
  title		= {SIGCSE Virtual 2024: Proceedings of the 2024 on ACM
		  Virtual Global Computing Education Conference V. 1},
  year		= {2024},
  isbn		= {9798400705984},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {On behalf of SIGCSE Virtual 2024 Steering, Organization,
		  and Program Committees, we would like to welcome you to
		  this wonderful event. SIGCSE Virtual 2024, 1st ACM Virtual
		  Global Computing Education Conference is now a reality
		  after over a year of work by all the committee members. We
		  like to send our special thanks to the SIGCSE Board and ACM
		  for their continued support, encouragement and
		  facilitation.One of the major goals of SIGCSE Virtual is to
		  promote an inclusive and easily accessible conference to
		  all interested in CS education research and practice. The
		  hope is to allow those who are not able to easily travel to
		  SIGCSE conferences to participate virtually from around the
		  world. For this reason, the core of the conference follows
		  all other SIGCSE conferences by providing papers, panels,
		  posters/lightning talks, working groups, and doctoral
		  consortium sessions dedicated to CS education research and
		  practice.The conference has different themes based on the
		  global aspects of CS education while considering regional
		  circumstances. The sessions are offered considering
		  time-zone constraints. The online program adjusts to time
		  zones.Several different activities are provided besides the
		  technical sessions by conference sponsors as well as for
		  social engagements. All these activities are included in
		  the program.},
  location	= {Virtual Event, NC, USA}
}

@Proceedings{	  10.1145/3700297,
  title		= {ISAIE '24: Proceedings of the 2024 International Symposium
		  on Artificial Intelligence for Education},
  year		= {2024},
  isbn		= {9798400707100},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3642979.3642997,
  author	= {Kille, Benjamin and Lommatzsch, Andreas and
		  \"{O}zg\"{o}bek, \"{O}zlem and Liu, Peng and Zhang, Lemei
		  and Eide, Simen},
  title		= {Report on the 11th International Workshop on News
		  Recommendation and Analytics (INRA 2023) at ACM RecSys
		  2023},
  year		= {2024},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {2},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3642979.3642997},
  doi		= {10.1145/3642979.3642997},
  abstract	= {News remains a challenging domain for personalization.
		  INRA'23 allowed experts and practitioners to get together
		  and discuss recent trends and future directions. Peer
		  review selected six contributions for presentation covering
		  a wide array of topics including beyond accuracy
		  evaluation, the use of large language models,
		  misinformation, and emotions in news.Date: 18 September
		  2023.Website:
		  https://research.idi.ntnu.no/NewsTech/INRA/.},
  journal	= {SIGIR Forum},
  month		= jan,
  articleno	= {15},
  numpages	= {4}
}

@InProceedings{	  10.1145/3687311.3687323,
  author	= {Hu, Bingying and Ni, Qin and Gao, Rong},
  title		= {Development Strategies and Practical Insights of Teachers'
		  Artificial Intelligence Competence},
  year		= {2024},
  isbn		= {9798400709920},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3687311.3687323},
  doi		= {10.1145/3687311.3687323},
  abstract	= {Artificial Intelligence (AI) in education not only changes
		  the education model but also transforms the role of
		  teachers from traditional "knowledge transmitters" to
		  "learning guides" and "skills trainers". This
		  transformation underscores the importance of teachers in
		  leveraging AI for educational enhancement and professional
		  growth, highlighting the need for improved AI competencies.
		  We adhere to human-centered principles and follow ethical
		  and responsible AI ethical standards to ensure that
		  technology truly serves education. Through a literature
		  review from an international perspective, this study
		  examines global trends in AI educational practices,
		  comparing policies, curricula, and research across Asia,
		  Europe, and Africa. This comparison facilitates a deep
		  understanding of AI's application and challenges within
		  various cultural and educational settings, offering
		  insights into enhancing teacher's AI competencies. The
		  paper concludes by suggesting a detailed strategy for
		  bolstering teachers' AI skills, employing a stepwise
		  approach to foster professional development and elevate
		  educational quality in the AI era while providing guidance
		  for teachers to use AI critically and transformatively.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Intelligent Education and Computer Technology},
  pages		= {1–0},
  location	= {Guilin, China},
  series	= {IECT '24}
}

@Proceedings{	  10.1145/3686397,
  title		= {ICISDM '24: Proceedings of the 2024 8th International
		  Conference on Information System and Data Mining},
  year		= {2024},
  isbn		= {9798400717345},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3641237,
  title		= {SIGDOC '24: Proceedings of the 42nd ACM International
		  Conference on Design of Communication},
  year		= {2024},
  isbn		= {9798400705199},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Fairfax, VA, USA}
}

@InProceedings{	  10.1145/3652583.3657599,
  author	= {Nguyen, Van-Loc and Nguyen, Bao-Tin and Nguyen, Thanh-Son
		  and Dang-Nguyen, Duc-Tien and Tran, Minh-Triet},
  title		= {A Unified Network for Detecting Out-Of-Context Information
		  Using Generative Synthetic Data},
  year		= {2024},
  isbn		= {9798400706196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652583.3657599},
  doi		= {10.1145/3652583.3657599},
  abstract	= {In our modern world, the manipulation of digital content,
		  especially the usage of out-of-context images known as
		  Cheapfakes, has become a significant challenge for the
		  endorsement of integrity and trustworthiness for
		  information on the Internet. This highlights an urgent need
		  for effective detection of the misuse of images and their
		  accompanied captions. Motivated by this issue, our research
		  presents an innovative approach to the solution of this
		  problem. Participating in the ACM ICMR 2024 Grand Challenge
		  on Detecting Cheapfakes, we leverage a unified end-to-end
		  network, integrated with generative synthetic data for
		  training. After complete evaluation, our proposed network
		  demonstrated a remarkable accuracy of 95.60% on the public
		  test dataset for Task 1, as well as efficiency in Task 2.
		  This paper highlights the notable potential of employing an
		  end-to-end network for Cheapfakes detection, which composes
		  a significant contribution to the advancement of multimedia
		  content integrity. Our source code is publicly available at
		  https://github.com/thanhson28/cheapfakes_detection_icmr2024.git},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Multimedia Retrieval},
  pages		= {1300–1305},
  numpages	= {6},
  keywords	= {cheapfakes detection, miscontextualization,
		  misinformation, out-of-context, visual entailment},
  location	= {Phuket, Thailand},
  series	= {ICMR '24}
}

@InProceedings{	  10.1145/3631085.3631302,
  author	= {De Lima, Edirlei Soares and Feij\'{o}, Bruno and
		  Cassanova, Marco A. and Furtado, Antonio L.},
  title		= {ChatGeppetto - an AI-powered Storyteller},
  year		= {2024},
  isbn		= {9798400716270},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631085.3631302},
  doi		= {10.1145/3631085.3631302},
  abstract	= {In this paper we introduce a novel highly interactive
		  process to generate natural language narratives on the
		  basis of our ongoing work on semiotic relations. To the two
		  basic components of interactive systems, namely, a software
		  tool and a user interface, we add a third component – AI
		  agents, understood as an upgraded rendition of software
		  agents. Our semiotic relations approach considers four ways
		  of composing new narratives from existing narratives. Along
		  what semioticians call the horizontal syntagmatic axis, one
		  can form the new narrative by combining two or more
		  previous narratives. Along the vertical paradigmatic axis,
		  the new narrative may emerge as a similar version, which
		  imitates the previous one, possibly in a different context.
		  Along the depth meronymic axis, the hierarchic narrative
		  levels, such as plot, event, and scene, are explored,
		  allowing either expansion or summarization. Lastly, the
		  antithetic consideration, rather than adding a dimension,
		  aims at some form of reversal, through the adoption of
		  opposite values. A fully operational prototype is
		  described. Its name, ChatGeppetto, conflates the skilled
		  Geppetto, who fashioned Pinocchio, an early case of
		  artisanship-produced human level intelligence, with
		  ChatGPT, which operates as the main AI agent component. To
		  run the experiments, we concentrated on book narratives.},
  booktitle	= {Proceedings of the 22nd Brazilian Symposium on Games and
		  Digital Entertainment},
  pages		= {28–37},
  numpages	= {10},
  keywords	= {Artificial Intelligence, Book Narratives, ChatGPT,
		  Chatbots, Interactive Story Composition, Semiotic
		  Relations, Storyboards},
  location	= {Rio Grande (RS), Brazil},
  series	= {SBGames '23}
}

@InProceedings{	  10.1145/3637528.3671503,
  author	= {Lakkaraju, Himabindu and Mei, Qiaozhu and Tan, Chenhao and
		  Tang, Jie and Xie, Yutong},
  title		= {The First Workshop on AI Behavioral Science},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671503},
  doi		= {10.1145/3637528.3671503},
  abstract	= {This workshop initiates a new study field which may be
		  named AI behavioral science. It discusses recent findings,
		  methodologies, applications, and potential societal impacts
		  that are related to analyzing, understanding, and directing
		  the behaviors of AI models, especially those built upon
		  large language models. This half-day workshop includes
		  several keynote and invited talks, a poster session, and a
		  panel discussion.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6724–6725},
  numpages	= {2},
  keywords	= {ai behavioral science, large language models},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3698590,
  author	= {Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng,
		  Zaiyi and Chen, Chen and Li, Jundong},
  title		= {Knowledge Editing for Large Language Models: A Survey},
  year		= {2024},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3698590},
  doi		= {10.1145/3698590},
  abstract	= {Large Language Models (LLMs) have recently transformed
		  both the academic and industrial landscapes due to their
		  remarkable capacity to understand, analyze, and generate
		  texts based on their vast knowledge and reasoning ability.
		  Nevertheless, one major drawback of LLMs is their
		  substantial computational cost for pre-training due to
		  their unprecedented amounts of parameters. The disadvantage
		  is exacerbated when new knowledge frequently needs to be
		  introduced into the pre-trained model. Therefore, it is
		  imperative to develop effective and efficient techniques to
		  update pre-trained LLMs. Traditional methods encode new
		  knowledge in pre-trained LLMs through direct fine-tuning.
		  However, naively re-training LLMs can be computationally
		  intensive and risks degenerating valuable pre-trained
		  knowledge irrelevant to the update in the model. Recently,
		  Knowledge-based Model Editing (KME), also known as
		  Knowledge Editing or Model Editing, has attracted
		  increasing attention, which aims at precisely modifying the
		  LLMs to incorporate specific knowledge, without negatively
		  influencing other irrelevant knowledge. In this survey, we
		  aim at providing a comprehensive and in-depth overview of
		  recent advances in the field of KME. We first introduce a
		  general formulation of KME to encompass different KME
		  strategies. Afterward, we provide an innovative taxonomy of
		  KME techniques based on how the new knowledge is introduced
		  into pre-trained LLMs, and investigate existing KME
		  strategies while analyzing key insights, advantages, and
		  limitations of methods from each category. Moreover,
		  representative metrics, datasets, and applications of KME
		  are introduced accordingly. Finally, we provide an in-depth
		  analysis regarding the practicality and remaining
		  challenges of KME and suggest promising research directions
		  for further advancement in this field.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {59},
  numpages	= {37},
  keywords	= {Model editing, knowledge update, fine-tuning, large
		  language models}
}

@InProceedings{	  10.1145/3626772.3657767,
  author	= {Wang, Jie and Karatzoglou, Alexandros and Arapakis,
		  Ioannis and Jose, Joemon M.},
  title		= {Reinforcement Learning-based Recommender Systems with
		  Large Language Models for State Reward and Action
		  Modeling},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657767},
  doi		= {10.1145/3626772.3657767},
  abstract	= {Reinforcement Learning (RL)-based recommender systems have
		  demonstrated promising performance in session-based and
		  sequential recommendation tasks. Existing offline RL-based
		  sequential recommendation methods face the challenge of
		  obtaining effective user feedback from the environment.
		  Developing a model for the user state and shaping an
		  appropriate reward for recommendation remains a challenge.
		  In this paper, we leverage language understanding
		  capabilities and adapt large language models (LLMs) as an
		  environment (LE) to enhance RL-based recommenders. The LE
		  is learned from a subset of user-item interaction data,
		  thus reducing the need for large training data, and can
		  synthesize user feedback for offline data by: (i) acting as
		  a state model that produces high-quality states that enrich
		  the user representation, and (ii) functioning as a reward
		  model to accurately capture nuanced user preferences on
		  actions. Moreover, the LE allows us to generate positive
		  actions that augment the limited offline training data. We
		  propose a LE Augmentation (LEA) method to further improve
		  recommendation performance by optimising jointly the
		  supervised component and the RL policy, using the augmented
		  actions and historical user signals. We use LEA, the state,
		  and reward models in conjunction with state-of-the-art RL
		  recommenders and report experimental results on two
		  publicly available datasets.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {375–385},
  numpages	= {11},
  keywords	= {augmentation, large language models, reinforcement
		  learning, sequential recommendation},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Proceedings{	  10.1145/3661304,
  title		= {GRADES-NDA '24: Proceedings of the 7th Joint Workshop on
		  Graph Data Management Experiences &amp; Systems (GRADES)
		  and Network Data Analytics (NDA)},
  year		= {2024},
  isbn		= {9798400706530},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {We are delighted to present the papers from the 7th
		  GRADES-NDA Joint Workshop on Graph Data Management
		  Experiences &amp; Systems and Network Data Analytics which
		  took place on 14th June 2024 co-located with ACM SIGMOD
		  held in Santiago, Chile.},
  location	= {Santiago, AA, Chile}
}

@Proceedings{	  10.1145/3648188,
  title		= {HT '24: Proceedings of the 35th ACM Conference on
		  Hypertext and Social Media},
  year		= {2024},
  isbn		= {9798400705953},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Poznan, Poland}
}

@Proceedings{	  10.1145/3637907,
  title		= {ICETM '23: Proceedings of the 2023 6th International
		  Conference on Educational Technology Management},
  year		= {2023},
  isbn		= {9798400716676},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guangzhou, China}
}

@Proceedings{	  10.1145/3643562,
  title		= {CI '24: Proceedings of the ACM Collective Intelligence
		  Conference},
  year		= {2024},
  isbn		= {9798400705540},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Boston, MA, USA}
}

@Proceedings{	  10.1145/3641399,
  title		= {ISEC '24: Proceedings of the 17th Innovations in Software
		  Engineering Conference},
  year		= {2024},
  isbn		= {9798400717673},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bangalore, India}
}

@InProceedings{	  10.1109/sc41406.2024.00013,
  author	= {Dharuman, Gautham and Hippe, Kyle and Brace, Alexander and
		  Foreman, Sam and Hatanp\"{a}\"{a}, V\"{a}in\"{o} and
		  Sastry, Varuni K. and Zheng, Huihuo and Ward, Logan and
		  Muralidharan, Servesh and Vasan, Archit and Kale, Bharat
		  and Mann, Carla M. and Ma, Heng and Cheng, Yun-Hsuan and
		  Zamora, Yuliana and Liu, Shengchao and Xiao, Chaowei and
		  Emani, Murali and Gibbs, Tom and Tatineni, Mahidhar and
		  Canchi, Deepak and Mitchell, Jerome and Yamada, Koichi and
		  Garzaran, Maria and Papka, Michael E. and Foster, Ian and
		  Stevens, Rick and Anandkumar, Anima and Vishwanath,
		  Venkatram and Ramanathan, Arvind},
  title		= {MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal
		  Protein Design Workflows with Direct Preference
		  Optimization},
  year		= {2024},
  isbn		= {9798350352917},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/SC41406.2024.00013},
  doi		= {10.1109/SC41406.2024.00013},
  abstract	= {We present a scalable, end-to-end workflow for protein
		  design. By augmenting protein sequences with natural
		  language descriptions of their biochemical properties, we
		  train generative models that can be preferentially aligned
		  with protein fitness landscapes. Through complex
		  experimental- and simulation-based observations, we
		  integrate these measures as preferred parameters for
		  generating new protein variants and demonstrate our
		  workflow on five diverse supercomputers. We achieve &gt;1
		  ExaFLOPS sustained performance in mixed precision on each
		  supercomputer and a maximum sustained performance of 4.11
		  ExaFLOPS and peak performance of 5.57 ExaFLOPS. We
		  establish the scientific performance of our model on two
		  tasks: (1) across a predetermined benchmark dataset of deep
		  mutational scanning experiments to optimize the
		  fitness-determining mutations in the yeast protein HIS7,
		  and (2) in optimizing the design of the enzyme malate
		  dehydrogenase to achieve lower activation barriers (and
		  therefore increased catalytic rates) using simulation data.
		  Our implementation thus sets high watermarks for multimodal
		  protein design workflows.},
  booktitle	= {Proceedings of the International Conference for High
		  Performance Computing, Networking, Storage, and Analysis},
  articleno	= {7},
  numpages	= {13},
  keywords	= {AI, HPC, Large language models, protein design},
  location	= {Atlanta, GA, USA},
  series	= {SC '24}
}

@InProceedings{	  10.1145/3644116.3644226,
  author	= {Zhou, Chunfang and Gong, Qingyue and Zhu, Jinyang and
		  Luan, Huidan},
  title		= {Research and Application of Large Language Models in
		  HealthcareCurrent Development of Large Language Models in
		  the Healthcare FieldA Framework for Applying Large Language
		  Models and the Opportunities and Challenges of Large
		  Language Models in Healthcare: A Framework for Applying
		  Large Language Models and the Opportunities and Challenges
		  of Large Language Models in Healthcare},
  year		= {2024},
  isbn		= {9798400708138},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644116.3644226},
  doi		= {10.1145/3644116.3644226},
  abstract	= {The burgeoning field of Large Language Models (LLM) within
		  the realm of medical research has garnered significant
		  attention. However, there exists a noticeable dearth of
		  scholarly exploration concerning the practical utility of
		  LLM in addressing substantive task-oriented issues within
		  specific medical domains. In light of this lacuna, the
		  present study endeavors to furnish a comprehensive
		  examination and evaluation of the framework underpinning
		  LLM applications and their corresponding research and
		  implementation within the medical domain.Commencing with a
		  concise explication of the foundational tenets of the LLM
		  application framework, the paper proceeds to expound upon
		  the evolutionary trajectory of LLM and their current
		  research landscape within the medical sphere. Subsequently,
		  it delves into the prospective avenues for LLM application
		  within the medical field. Lastly, the study scrutinizes the
		  multifaceted challenges confronting LLM in the medical
		  domain and offers prescriptive insights aimed at mitigating
		  these impediments.This scholarly endeavor not only serves
		  as a foundational cornerstone for the development of a
		  practical business system predicated on LLM by elucidating
		  the architectural intricacies of their application but also
		  furnishes a comprehensive overview of the research and
		  deployment of LLM in the medical domain, thereby affording
		  invaluable points of reference for future endeavors within
		  this domain.},
  booktitle	= {Proceedings of the 2023 4th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {664–670},
  numpages	= {7},
  location	= {Chengdu, China},
  series	= {ISAIMS '23}
}

@Proceedings{	  10.1145/3680530,
  title		= {SA '24: SIGGRAPH Asia 2024 Art Papers},
  year		= {2024},
  isbn		= {9798400711336},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3641032,
  title		= {ICISE '23: Proceedings of the 2023 8th International
		  Conference on Information Systems Engineering},
  year		= {2023},
  isbn		= {9798400709173},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bangkok, Thailand}
}

@Proceedings{	  10.1145/3657054,
  title		= {dg.o '24: Proceedings of the 25th Annual International
		  Conference on Digital Government Research},
  year		= {2024},
  isbn		= {9798400709883},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Taipei, Taiwan}
}

@InProceedings{	  10.1145/3652620.3688206,
  author	= {Rabbi, Fazle},
  title		= {A Model-Based Framework for Exploring Conflict Dynamics},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688206},
  doi		= {10.1145/3652620.3688206},
  abstract	= {This paper introduces a novel framework for conflict
		  analysis that leverages advanced visual modeling
		  techniques. By employing comparative analysis, key
		  variables influencing armed conflicts are identified and
		  analyzed. The framework includes a meta-model representing
		  domain concepts such as the goals and strategies of
		  conflicting parties, escalating stages, and impacts of
		  conflicts.Conflict escalation is a complex process
		  characterized by interactions between opposing parties.
		  This paper presents a structured model that outlines how
		  conflicts evolve and intensify over time. We adapt a
		  meta-modeling framework called the Diagram Predicate
		  Framework (DPF) to represent conflict-related concepts and
		  extend it to support abstract view generation. This
		  framework facilitates the analysis of conflict trends and
		  the study of dynamics across various levels of
		  abstraction.A computational model based on category theory
		  is proposed for trend analysis, enabling the extraction of
		  patterns of conflict evolution and the comparison of
		  strategies and goals at different escalation stages.
		  Categorical operations such as pullback and limit
		  construction are employed to compute conflict evolution and
		  identify common structures among conflict instances,
		  providing insights into conflict dynamics across diverse
		  zones.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {745–754},
  numpages	= {10},
  keywords	= {conflict analysis, computational journalism, category
		  theory, metamodeling},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.1145/3703454,
  author	= {Sicari, Sabrina and Cevallos M., Jesus F. and Rizzardi,
		  Alessandra and Coen-Porisini, Alberto},
  title		= {Open-Ethical AI: Advancements in Open-Source Human-Centric
		  Neural Language Models},
  year		= {2024},
  issue_date	= {April 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3703454},
  doi		= {10.1145/3703454},
  abstract	= {This survey summarises the most recent methods for
		  building and assessing helpful, honest, and harmless neural
		  language models, considering small, medium, and large-size
		  models. Pointers to open-source resources that help to
		  align pre-trained models are given, including methods that
		  use parameter-efficient techniques, specialized prompting
		  frameworks, adapter modules, case-specific knowledge
		  injection, and adversarially robust training techniques.
		  Special care is given to evidencing recent progress on
		  value alignment, commonsense reasoning, factuality
		  enhancement, and abstract reasoning of language models.
		  Most reviewed works in this survey publicly shared their
		  code and related data and were accepted in world-leading
		  Machine Learning venues. This work aims at helping
		  researchers and practitioners accelerate their entrance
		  into the field of human-centric neural language models,
		  which might be a cornerstone of the contemporary and
		  near-future industrial and societal revolution.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {83},
  numpages	= {47},
  keywords	= {Neural language models, open-source, large-language
		  models, human-centric AI}
}

@Proceedings{	  10.1145/3678884,
  title		= {CSCW Companion '24: Companion Publication of the 2024
		  Conference on Computer-Supported Cooperative Work and
		  Social Computing},
  year		= {2024},
  isbn		= {9798400711145},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 27th ACM Conference on Computer-Supported
		  Cooperative Work and Social Computing (CSCW 2024). This
		  year's conference is particularly special, as it marks the
		  first time CSCW to be held in Latin America - a highly
		  anticipated milestone for our Latin American community. We
		  are excited to gather in San Jose, Costa Rica, and host a
		  hybrid event that allows remote participation from
		  community members worldwide.As in previous years, CSCW 2024
		  brings together a variety of disciplines, from system
		  design to critical analysis, to propose, examine, and
		  reimagine technologies that support groups and communities.
		  As our discipline evolves, new topics continuously emerge
		  and are embraced by our community. This year, we see an
		  emphasis on issues centered around AI, including
		  explainability, fairness, and AI-human collaboration. At
		  the same time, our long-standing concerns remain well
		  represented, including work on group dynamics and
		  decision-making, social media, inclusive and culturally
		  aware design, co-design with marginalized communities, and
		  the creation of socially responsible tools. This year, we
		  invited 387 PACM-HCI, TSC, and TOCHI papers to be presented
		  alongside a diverse lineup of workshops, posters, demos,
		  SIGs, panels, and the doctoral consortium. Below are the
		  numbers of reviewed and accepted submissions for each track
		  featured in this conference companion.},
  location	= {San Jose, Costa Rica}
}

@InProceedings{	  10.1145/3626772.3657717,
  author	= {Qin, Weicong and Cao, Zelin and Yu, Weijie and Si, Zihua
		  and Chen, Sirui and Xu, Jun},
  title		= {Explicitly Integrating Judgment Prediction with Legal
		  Document Retrieval: A Law-Guided Generative Approach},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657717},
  doi		= {10.1145/3626772.3657717},
  abstract	= {Legal document retrieval and judgment prediction are
		  crucial tasks in intelligent legal systems. In practice,
		  determining whether two documents share the same judgments
		  is essential for establishing their relevance in legal
		  retrieval. However, existing legal retrieval studies either
		  ignore the vital role of judgment prediction or rely on
		  implicit training objectives, expecting a proper alignment
		  of legal documents in vector space based on their
		  judgments. Neither approach provides explicit evidence of
		  judgment consistency for relevance modeling, leading to
		  inaccuracies and a lack of transparency in retrieval. To
		  address this issue, we propose a law-guided method, namely
		  GEAR, within the generative retrieval framework. GEAR
		  explicitly integrates judgment prediction with legal
		  document retrieval in a sequence-to-sequence manner.
		  Specifically, given the intricate nature of legal
		  documents, we first extract rationales from documents based
		  on the definition of charges in law. We then employ these
		  rationales as queries, ensuring efficiency and producing a
		  shared, informative document representation for both tasks.
		  Second, in accordance with the inherent hierarchy of law,
		  we construct a law structure constraint tree and represent
		  each candidate document as a hierarchical semantic ID based
		  on this tree. This empowers GEAR to perform dual
		  predictions for judgment and relevant documents in a single
		  inference, i.e., traversing the tree from the root through
		  intermediate judgment nodes, to document-specific leaf
		  nodes. Third, we devise the revision loss that jointly
		  minimizes the discrepancy between the IDs of predicted and
		  labeled judgments, as well as retrieved documents, thus
		  improving accuracy and consistency for both tasks.
		  Extensive experiments on two Chinese legal case retrieval
		  datasets show the superiority of GEAR over state-of-the-art
		  methods while maintaining competitive judgment prediction
		  performance. Moreover, we validate the effectiveness of
		  GEAR on a French statutory article retrieval dataset,
		  reaffirming its robustness across languages and domains.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2210–2220},
  numpages	= {11},
  keywords	= {generative retrieval, legal document retrieval, legal
		  judgment prediction},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3671127.3698792,
  author	= {Mulayim, Ozan Baris and Paul, Lazlo and Pritoni, Marco and
		  Prakash, Anand Krishnan and Sudarshan, Malavikha and
		  Fierro, Gabe},
  title		= {Large Language Models for the Creation and Use of Semantic
		  Ontologies in Buildings: Requirements and Challenges},
  year		= {2024},
  isbn		= {9798400707063},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3671127.3698792},
  doi		= {10.1145/3671127.3698792},
  abstract	= {Semantic ontologies offer a formalized, machine-readable
		  framework for representing knowledge, enabling the
		  structured description of complex systems. In the building
		  domain, the adoption of ontologies like the Brick schema
		  has transformed how buildings and their systems are modeled
		  by providing a standardized, interoperable language.
		  However, the complexity and the steep learning curve
		  involved in developing and querying semantic models present
		  substantial challenges, often requiring a workforce with
		  specialized expertise. This paper builds on our experience
		  in investigating how Large Language Models (LLMs) can help
		  address these challenges, focusing on their role in
		  constructing and querying of semantic models, particularly
		  using the Brick Schema. Our study outlines the requirements
		  and metrics for evaluating the scalability and
		  effectiveness of LLM-based tools, while also discussing the
		  current challenges and limitations in developing such
		  tools. Ultimately, this paper aims to orient research
		  efforts as various groups experiment with diverse
		  techniques, while enabling more effective comparison of
		  emerging solutions and fostering collaboration across the
		  field.},
  booktitle	= {Proceedings of the 11th ACM International Conference on
		  Systems for Energy-Efficient Buildings, Cities, and
		  Transportation},
  pages		= {312–317},
  numpages	= {6},
  keywords	= {Knowledge Graphs, Large Language Models, Semantic
		  Ontology},
  location	= {Hangzhou, China},
  series	= {BuildSys '24}
}

@InProceedings{	  10.1145/3657604.3664683,
  author	= {Yan, Zhenting and Zhang, Rui},
  title		= {Enhancing Knowledge Tracing Efficacy with Expert-defined
		  Graphs: A Case Study in Introductory Physics Classes},
  year		= {2024},
  isbn		= {9798400706332},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3657604.3664683},
  doi		= {10.1145/3657604.3664683},
  abstract	= {Knowledge Tracing (KT) is essential in online education
		  for tracking student progress and forecasting future
		  performance. Despite the effectiveness of existing KT
		  models, enhancing their educational interpretability and
		  reliability remains crucial for both academic and practical
		  applications. This study introduces an improved Graph-based
		  Knowledge Tracing (GKT) model, enriched with domain
		  expertise, instructional insights, and contextual features,
		  to overcome current limitations. Our enhanced GKT model
		  employs an expert-defined graph structure for more accurate
		  domain knowledge representation. It integrates critical
		  contextual features, like question difficulty and prompt
		  usage, into the response matrix for a comprehensive
		  context. Additionally, the model leverages second-order
		  neighborhood features to more effectively capture complex
		  interrelations between knowledge concepts. Validation using
		  an Introductory Physics assignment dataset demonstrated
		  that our updated GKT model surpasses its predecessor in
		  both Area Under the Curve (AUC) and accuracy (ACC) metrics.
		  These improvements are instrumental in refining knowledge
		  graphs and developing personalized teaching strategies,
		  thereby facilitating more effective and personalized
		  educational experiences.},
  booktitle	= {Proceedings of the Eleventh ACM Conference on Learning @
		  Scale},
  pages		= {433–437},
  numpages	= {5},
  keywords	= {adaptive learning, educational interpretability, explicit
		  graph structure, graph-based knowledge tracing, modified
		  GNN},
  location	= {Atlanta, GA, USA},
  series	= {L@S '24}
}

@Book{		  10.1145/3674127,
  editor	= {Alonso, Omar and Baeza-Yates, Ricardo},
  title		= {Information Retrieval: Advanced Topics and Techniques},
  year		= {2024},
  isbn		= {9798400710506},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  edition	= {1},
  volume	= {60},
  abstract	= {In the last decade, deep learning and word embeddings have
		  made significant impacts on information retrieval (IR) by
		  adding techniques based in neural networks and language
		  models. At the same time, certain search modalities such as
		  neural IR and conversational search have become more
		  popular. This book, written by international academic and
		  industry experts, brings the field up to date with detailed
		  discussions of these new approaches and techniques. The
		  book is organized in three sections: Foundations,
		  Adaptations and Concerns, and Verticals.Under Foundations,
		  we address topics that form the basic structure of any
		  modern IR system, including recommender systems. These new
		  techniques are developed to augment indexing, retrieval,
		  and ranking. Neural IR, recommender systems, evaluation,
		  query-driven functionality, and knowledge graphs are
		  covered in this section.IR systems need to adapt to
		  specific user characteristics and preferences, and
		  techniques that were considered too niche a few years ago
		  are now a matter of system design consideration. The
		  Adaptations and Concerns section covers the following
		  topics: conversational search, cross-language retrieval,
		  temporal extraction and retrieval, bias in retrieval
		  systems, and privacy in search.While web search engines are
		  the most popular information access point, there are cases
		  where specific verticals provide a better experience in
		  terms of content and relevance. The Verticals section
		  describes eCommerce, professional search, personal
		  collections, music retrieval, and biomedicine as
		  examples.}
}

@Proceedings{	  10.1145/3614407,
  title		= {CSLAW '24: Proceedings of the Symposium on Computer
		  Science and Law},
  year		= {2024},
  isbn		= {9798400703331},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Boston, MA, USA}
}

@Proceedings{	  10.1145/3663649,
  title		= {DataEd '24: Proceedings of the 3rd International Workshop
		  on Data Systems Education: Bridging education practice with
		  education research},
  year		= {2024},
  isbn		= {9798400706783},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Santiago, AA, Chile}
}

@Article{	  10.1109/taslp.2024.3434469,
  author	= {Wang, Yujie and Zhang, Hu and Liang, Jiye and Li, Ru},
  title		= {Heterogeneous-Graph Reasoning With Context Paraphrase for
		  Commonsense Question Answering},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3434469},
  doi		= {10.1109/TASLP.2024.3434469},
  abstract	= {Commonsense question answering (CQA) generally means that
		  the machine uses its mastered commonsense to answer
		  questions without relevant background material, which is a
		  challenging task in natural language processing. Existing
		  methods focus on retrieving relevant subgraphs from
		  knowledge graphs based on key entities and designing
		  complex graph neural networks to perform reasoning over the
		  subgraphs. However, they have the following problems: i)
		  the nested entities in key entities lead to the
		  introduction of irrelevant knowledge; ii) the QA context is
		  not well integrated with the subgraphs; and iii)
		  insufficient context knowledge hinders subgraph nodes
		  understanding. In this paper, we present a
		  heterogeneous-graph reasoning with context paraphrase
		  method (HCP), which introduces the paraphrase knowledge
		  from the dictionary into key entity recognition and
		  subgraphs construction, and effectively fuses QA context
		  and subgraphs during the encoding phase of the pre-trained
		  language model (PTLM). Specifically, HCP filters the nested
		  entities through the dictionary's vocabulary and constructs
		  the Heterogeneous Path-Paraphrase (HPP) graph by connecting
		  the paraphrase descriptions&lt;xref ref-type="fn"
		  rid="fn1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/xref&gt;&lt;fn
		  id="fn1"&gt;&lt;label&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/label&gt;&lt;p&gt;The
		  paraphrase descriptions are English explanations of words
		  or phrases in WordNet and Wiktionary.&lt;/p&gt;&lt;/fn&gt;
		  with the key entity nodes in the subgraphs. Then, by
		  constructing the visible matrices in the PTLM encoding
		  phase, we fuse the QA context representation into the HPP
		  graph. Finally, to get the answer, we perform reasoning on
		  the HPP graph by Mask Self-Attention. Experimental results
		  on CommonsenseQA and OpenBookQA show that fusing QA context
		  with HPP graph in the encoding stage and enhancing the HPP
		  graph representation by using context paraphrase can
		  improve the machine's commonsense reasoning ability.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jul,
  pages		= {3759–3770},
  numpages	= {12}
}

@InProceedings{	  10.1145/3637528.3671469,
  author	= {Zhang, Yunyi and Zhong, Ming and Ouyang, Siru and Jiao,
		  Yizhu and Zhou, Sizhe and Ding, Linyi and Han, Jiawei},
  title		= {Automated Mining of Structured Knowledge from Text in the
		  Era of Large Language Models},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671469},
  doi		= {10.1145/3637528.3671469},
  abstract	= {Massive amount of unstructured text data are generated
		  daily, ranging from news articles to scientific papers. How
		  to mine structured knowledge from the text data remains a
		  crucial research question. Recently, large language models
		  (LLMs) have shed light on the text mining field with their
		  superior text understanding and instruction-following
		  ability. There are typically two ways of utilizing LLMs:
		  fine-tune the LLMs with human-annotated training data,
		  which is labor intensive and hard to scale; prompt the LLMs
		  in a zero-shot or few-shot way, which cannot take advantage
		  of the useful information in the massive text data.
		  Therefore, it remains a challenge on automated mining of
		  structured knowledge from massive text data in the era of
		  large language models. In this tutorial, we cover the
		  recent advancements in mining structured knowledge using
		  language models with very weak supervision. We will
		  introduce the following topics in this tutorial: (1)
		  introduction to large language models, which serves as the
		  foundation for recent text mining tasks, (2) ontology
		  construction, which automatically enriches an ontology from
		  a massive corpus, (3) weakly-supervised text classification
		  in flat and hierarchical label space, (4) weakly-supervised
		  information extraction, which extracts entity and relation
		  structures.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6644–6654},
  numpages	= {11},
  keywords	= {large language models, text mining, weak supervision},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3652598,
  author	= {Wang, Jian and Lin, Dongding and Li, Wenjie},
  title		= {Target-constrained Bidirectional Planning for Generation
		  of Target-oriented Proactive Dialogue},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {5},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3652598},
  doi		= {10.1145/3652598},
  abstract	= {Target-oriented proactive dialogue systems aim at leading
		  conversations from a dialogue context toward a
		  pre-determined target, such as making recommendations on
		  designated items or introducing new specific topics. To
		  this end, it is critical for such dialogue systems to plan
		  reasonable actions to drive the conversation proactively,
		  and meanwhile, to plan appropriate topics to move the
		  conversation forward to the target topic smoothly. In this
		  work, we mainly focus on effective dialogue planning for
		  target-oriented dialogue generation. Inspired by
		  decision-making theories in cognitive science, we propose a
		  novel target-constrained bidirectional planning (TRIP)
		  approach, which plans an appropriate dialogue path by
		  looking ahead and looking back. By formulating the planning
		  as a generation task, our TRIP bidirectionally generates a
		  dialogue path consisting of a sequence of &lt;action,
		  topic&gt; pairs using two Transformer decoders. They are
		  expected to supervise each other and converge on consistent
		  actions and topics by minimizing the decision gap and
		  contrastive generation of targets. Moreover, we propose a
		  target-constrained decoding algorithm with a bidirectional
		  agreement to better control the planning process.
		  Subsequently, we adopt the planned dialogue paths to guide
		  dialogue generation in a pipeline manner, where we explore
		  two variants: prompt-based generation and plan-controlled
		  generation. Extensive experiments are conducted on two
		  challenging dialogue datasets, which are re-purposed for
		  exploring target-oriented dialogue. Our automatic and human
		  evaluations demonstrate that the proposed methods
		  significantly outperform various baseline models.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= apr,
  articleno	= {124},
  numpages	= {27},
  keywords	= {Target-oriented dialogue, dialogue generation,
		  bidirectional planning}
}

@InProceedings{	  10.1145/3678717.3691312,
  author	= {Qi, Xiaoqian and Chai, Haoye and Yu, Li and Li, Yong and
		  Wang, Zhaocheng},
  title		= {Regional Features Conditioned Diffusion Models for 5G
		  Network Traffic Generation},
  year		= {2024},
  isbn		= {9798400711077},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3678717.3691312},
  doi		= {10.1145/3678717.3691312},
  abstract	= {The fifth-generation (5G) mobile network has significantly
		  enhanced people's lives with faster internet speed and more
		  reliable connections. However, there is still insufficient
		  coverage of 5G networks worldwide, requiring telecom
		  operators to deploy more base stations to meet the
		  increasing demand for 5G's further commercialization. In
		  this regard, a major challenge is understanding user
		  network behaviors and traffic demands in target areas where
		  5G has not yet been deployed, which is crucial for
		  developing a more efficient base station deployment
		  strategy. Mobile traffic generation is a potential approach
		  that enables operators to preemptively estimate user
		  network demands in target areas, thereby specifying
		  corresponding deployment strategies to enhance network
		  performance. However, existing methods have limitations in
		  capturing spatio-temporal features of 5G mobile traffic,
		  particularly in areas with insufficient 5G coverage and
		  limited historical 5G traffic data. To fill this gap, we
		  introduce a regional feature conditioned diffusion
		  framework for 5G network traffic generation. Our models
		  explore the relationship between 5G traffic and existing 4G
		  traffic, utilizing a customized cross attention mechanism
		  and graph convolutional networks (GCN) to capture the
		  correlation between network traffic and regional features.
		  Based on this relationship, the framework can characterize
		  mobile network traffic demands, thereby achieving
		  high-fidelity 5G traffic generation in target regions with
		  insufficient 5G coverage. Extensive experiments on
		  real-world datasets have shown that the proposed scheme
		  outperforms state-of-the-art baselines by more than 10%,
		  demonstrating its high-fidelity generation capability,
		  controllability, and generalizability. Moreover, we have
		  deployed our scheme on China Mobile's Jiutian Platform as a
		  network traffic simulator to improve 5G base station
		  deployment strategies.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Advances in Geographic Information Systems},
  pages		= {396–409},
  numpages	= {14},
  keywords	= {5G network traffic generation, Cross attention, Diffusion
		  models, GCN},
  location	= {Atlanta, GA, USA},
  series	= {SIGSPATIAL '24}
}

@Proceedings{	  10.1145/3644815,
  title		= {CAIN '24: Proceedings of the IEEE/ACM 3rd International
		  Conference on AI Engineering - Software Engineering for
		  AI},
  year		= {2024},
  isbn		= {9798400705915},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The goal of the CAIN Conference Series is to bring
		  together researchers and practitioners in software
		  engineering, data science, and artificial intelligence (AI)
		  as part of a growing community that is targeting the
		  challenges of Software Engineering for AI-enabled
		  systems.},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3656156,
  title		= {DIS '24 Companion: Companion Publication of the 2024 ACM
		  Designing Interactive Systems Conference},
  year		= {2024},
  isbn		= {9798400706325},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {IT University of Copenhagen, Denmark}
}

@Proceedings{	  10.1145/3639856,
  title		= {AIMLSystems '23: Proceedings of the Third International
		  Conference on AI-ML Systems},
  year		= {2023},
  isbn		= {9798400716492},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bangalore, India}
}

@InProceedings{	  10.1145/3664647.3681542,
  author	= {Liao, Xinyao and Wei, Wei and Chen, Dangyang and Fu,
		  Yuanyuan},
  title		= {UniQ: Unified Decoder with Task-specific Queries for
		  Efficient Scene Graph Generation},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681542},
  doi		= {10.1145/3664647.3681542},
  abstract	= {Scene Graph Generation(SGG) is a scene understanding task
		  that aims at identifying object entities and reasoning
		  their relationships within a given image. In contrast to
		  prevailing two-stage methods based on a large object
		  detector (e.g., Faster R-CNN), one-stage methods integrate
		  a fixed-size set of learnable queries to jointly reason
		  relational triplets &lt;subject, predicate, object&gt;.
		  This paradigm demonstrates robust performance with
		  significantly reduced parameters and computational
		  overhead. However, the challenge in one-stage methods stems
		  from the issue of weak entanglement, wherein entities
		  involved in relationships require both coupled features
		  shared within triplets and decoupled visual features.
		  Previous methods either adopt a single decoder for coupled
		  triplet feature modeling or multiple decoders for separate
		  visual feature extraction but fail to consider both. In
		  this paper, we introduce UniQ, a Unified decoder with
		  task-specific Queries architecture, where task-specific
		  queries generate decoupled visual features for subjects,
		  objects, and predicates respectively, and unified decoder
		  enables coupled feature modeling within relational
		  triplets. Experimental results on the Visual Genome dataset
		  demonstrate that UniQ has superior performance to both
		  one-stage and two-stage methods.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {8815–8824},
  numpages	= {10},
  keywords	= {one-stage model, scene graph generation, visual
		  relationship detection},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Proceedings{	  10.1145/3631700,
  title		= {UMAP Adjunct '24: Adjunct Proceedings of the 32nd ACM
		  Conference on User Modeling, Adaptation and
		  Personalization},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Cagliari, Italy}
}

@Proceedings{	  10.1145/3649405,
  title		= {ITiCSE 2024: Proceedings of the 2024 on Innovation and
		  Technology in Computer Science Education V. 2},
  year		= {2024},
  isbn		= {9798400706035},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 29th annual conference on Innovation and
		  Technology in Computer Science Education (ITiCSE 2024),
		  hosted by Universita degli Studi di Milano in Milan,
		  Italy.ITiCSE 2024 will take place from Friday July 5 to
		  Wednesday July 10. The conference program includes a
		  keynote address, paper sessions, a panel, tips, techniques
		  &amp; courseware demonstrations, posters, a doctoral
		  consortium, and working group presentations. Working groups
		  meet July 5-7 and will submit draft reports before the
		  conference begins on July 8.The submissions to ITiCSE 2024
		  were reviewed by 446 researchers and practitioners from
		  computing education and related fields, including 44
		  program committee members and 402 reviewers. Thanks to
		  their outstanding effort and commitment, every submission
		  received a metareview and most received at least three
		  reviews, providing authors of all submissions with
		  constructive feedback. Although no review process is
		  flawless, we are confident that this effort led to a
		  vibrant conference program, capturing multiple voices and
		  perspectives in the field.},
  location	= {Milan, Italy}
}

@Proceedings{	  10.1145/3675417,
  title		= {DEAI '24: Proceedings of the 2024 Guangdong-Hong
		  Kong-Macao Greater Bay Area International Conference on
		  Digital Economy and Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400717147},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hongkong, China}
}

@Article{	  10.1145/3654795,
  author	= {Lai, Huiyuan and Nissim, Malvina},
  title		= {A Survey on Automatic Generation of Figurative Language:
		  From Rule-based Systems to Large Language Models},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {10},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3654795},
  doi		= {10.1145/3654795},
  abstract	= {Figurative language generation (FLG) is the task of
		  reformulating a given text to include a desired figure of
		  speech, such as a hyperbole, a simile, and several others,
		  while still being faithful to the original context. This is
		  a fundamental, yet challenging task in Natural Language
		  Processing (NLP), which has recently received increased
		  attention due to the promising performance brought by
		  pre-trained language models. Our survey provides a
		  systematic overview of the development of FLG, mostly in
		  English, starting with the description of some common
		  figures of speech, their corresponding generation tasks,
		  and datasets. We then focus on various modelling approaches
		  and assessment strategies, leading us to discussing some
		  challenges in this field, and suggesting some potential
		  directions for future research. To the best of our
		  knowledge, this is the first survey that summarizes the
		  progress of FLG including the most recent development in
		  NLP. We also organize corresponding resources, e.g.,
		  article lists and datasets, and make them accessible in an
		  open repository. We hope this survey can help researchers
		  in NLP and related fields to easily track the academic
		  frontier, providing them with a landscape and a roadmap of
		  this area.},
  journal	= {ACM Comput. Surv.},
  month		= may,
  articleno	= {244},
  numpages	= {34},
  keywords	= {Figurative language, language generation, systematic
		  review}
}

@Article{	  10.1145/3642979.3643007,
  author	= {Wang, Haixun and Na, Taesik},
  title		= {Rethinking E-Commerce Search},
  year		= {2024},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {2},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3642979.3643007},
  doi		= {10.1145/3642979.3643007},
  abstract	= {E-commerce search and recommendation usually operate on
		  structured data such as product catalogs and taxonomies.
		  However, creating better search and recommendation systems
		  often requires a large variety of unstructured data
		  including customer reviews and articles on the web.
		  Traditionally, the solution has always been converting
		  unstructured data into structured data through information
		  extraction, and conducting search over the structured data.
		  However, this is a costly approach that often has low
		  quality. In this paper, we envision a solution that does
		  entirely the opposite. Instead of converting unstructured
		  data (web pages, customer reviews, etc) to structured data,
		  we instead convert structured data (product inventory,
		  catalogs, taxonomies, etc) into textual data, which can be
		  easily integrated into the text corpus that trains LLMs.
		  Then, search and recommendation can be performed through a
		  Q/A mechanism through an LLM instead of using traditional
		  information retrieval methods over structured data.},
  journal	= {SIGIR Forum},
  month		= jan,
  articleno	= {24},
  numpages	= {19}
}

@Article{	  10.1109/taslp.2024.3358053,
  author	= {Chen, Xiang and Li, Lei and Zhu, Yuqi and Deng, Shumin and
		  Tan, Chuanqi and Huang, Fei and Si, Luo and Zhang, Ningyu
		  and Chen, Huajun},
  title		= {Sequence Labeling as Non-Autoregressive Dual-Query Set
		  Generation},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3358053},
  doi		= {10.1109/TASLP.2024.3358053},
  abstract	= {Sequence labeling is a crucial task in the NLP community
		  that aims at identifying and assigning spans within the
		  input sentence. It has wide applications in various fields
		  such as information extraction, dialogue system, and
		  sentiment analysis. However, previously proposed span-based
		  or sequence-to-sequence models conduct locating and
		  assigning in order, resulting in problems of error
		  propagation and unnecessary training loss, respectively.
		  This paper addresses the problem by reformulating the
		  sequence labeling as a non-autoregressive set generation to
		  realize locating and assigning in parallel. Herein, we
		  propose a
		  &lt;bold&gt;D&lt;/bold&gt;ual-&lt;bold&gt;Q&lt;/bold&gt;uery
		  &lt;bold&gt;Set&lt;/bold&gt;
		  &lt;bold&gt;Gen&lt;/bold&gt;eration
		  (&lt;monospace&gt;&lt;bold&gt;DQSetGen&lt;/bold&gt;&lt;/monospace&gt;)
		  model for unified sequence labeling tasks. Specifically,
		  the dual-query set, including a prompted type query and a
		  positional query with anchor span, is fed into the
		  non-autoregressive decoder to probe the spans which
		  correspond to the positional query and have similar
		  patterns with the type query. By avoiding the
		  autoregressive nature of previous approaches, our method
		  significantly improves efficiency and reduces error
		  propagation. Experimental results illustrate that our
		  approach can obtain superior performance on 5 sub-tasks
		  across 11 benchmark datasets. The non-autoregressive nature
		  of our method allows for parallel computation, achieving
		  faster inference speed than compared baselines. In
		  conclusion, our proposed non-autoregressive dual-query set
		  generation method offers a more efficient and accurate
		  approach to sequence labeling tasks in NLP. Its advantages
		  in terms of performance and efficiency make it a promising
		  solution for various applications in data mining and other
		  related fields.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= feb,
  pages		= {1546–1558},
  numpages	= {13}
}

@InProceedings{	  10.1145/3677052.3698671,
  author	= {Sarmah, Bhaskarjit and Mehta, Dhagash and Hall, Benika and
		  Rao, Rohan and Patel, Sunil and Pasquali, Stefano},
  title		= {HybridRAG: Integrating Knowledge Graphs and Vector
		  Retrieval Augmented Generation for Efficient Information
		  Extraction},
  year		= {2024},
  isbn		= {9798400710810},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677052.3698671},
  doi		= {10.1145/3677052.3698671},
  abstract	= {Extraction and interpretation of intricate information
		  from unstructured text data arising in financial
		  applications, such as earnings call transcripts, present
		  substantial challenges to large language models (LLMs) even
		  using the current best practices to use Retrieval Augmented
		  Generation (RAG) (referred to as VectorRAG techniques which
		  utilize vector databases for information retrieval) due to
		  challenges such as domain specific terminology and complex
		  formats of the documents. We introduce a novel approach
		  based on a combination, called HybridRAG, of the Knowledge
		  Graphs (KGs) based RAG techniques (called GraphRAG) and
		  VectorRAG techniques to enhance question-answer (Q&amp;A)
		  systems for information extraction from financial documents
		  that is shown to be capable of generating accurate and
		  contextually relevant answers. Using experiments on a set
		  of financial earning call transcripts documents which come
		  in the form of Q&amp;A format, and hence provide a natural
		  set of pairs of ground-truth Q&amp;As, we show that
		  HybridRAG which retrieves context from both vector database
		  and KG outperforms both traditional VectorRAG and GraphRAG
		  individually when evaluated at both the retrieval and
		  generation stages in terms of retrieval accuracy and answer
		  generation. The proposed technique has applications beyond
		  the financial domain.},
  booktitle	= {Proceedings of the 5th ACM International Conference on AI
		  in Finance},
  pages		= {608–616},
  numpages	= {9},
  location	= {Brooklyn, NY, USA},
  series	= {ICAIF '24}
}

@Proceedings{	  10.1145/3689217,
  title		= {LAMPS '24: Proceedings of the 1st ACM Workshop on Large AI
		  Systems and Models with Privacy and Safety Analysis},
  year		= {2024},
  isbn		= {9798400712098},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {This volume contains papers presented at the 1st ACM
		  Workshop on Large AI Systems and Models with Privacy and
		  Safety Analysis (LAMPS), which was held on October 14,
		  2024, in Salt Lake City, USA.This year we received 18 paper
		  submissions from Australia, China, Italy, Luxembourg,
		  Singapore, South Korea, USA, and Vietnam, and 11
		  high-quality papers were accepted. Each contributed paper
		  was rigorously peer-reviewed by reviewers who were drawn
		  from a pool of expert technical committee members in
		  machine learning security and privacy. Each paper received
		  two detailed review comments and one meta review comments
		  that summarise the weaknesses/issues to be addressed in the
		  camera-ready revision or future work.},
  location	= {Salt Lake City, UT, USA}
}

@Proceedings{	  10.1145/3677892,
  title		= {DSAI '24: Proceedings of the 2024 International Conference
		  on Digital Society and Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400709838},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Qingdao, China}
}

@Article{	  10.1145/3645099,
  author	= {He, Weidong and Li, Zhi and Wang, Hao and Xu, Tong and
		  Wang, Zhefeng and Huai, Baoxing and Yuan, Nicholas Jing and
		  Chen, Enhong},
  title		= {Multimodal Dialogue Systems via Capturing Context-aware
		  Dependencies and Ordinal Information of Semantic Elements},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {3},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3645099},
  doi		= {10.1145/3645099},
  abstract	= {The topic of multimodal conversation systems has recently
		  garnered significant attention across various industries,
		  including travel and retail, among others. While pioneering
		  works in this field have shown promising performance, they
		  often focus solely on context information at the utterance
		  level, overlooking the context-aware dependencies of
		  multimodal semantic elements like words and images.
		  Furthermore, the ordinal information of images, which
		  indicates the relevance between visual context and users’
		  demands, remains underutilized during the integration of
		  visual content. Additionally, the exploration of how to
		  effectively utilize corresponding attributes provided by
		  users when searching for desired products is still largely
		  unexplored. To address these challenges, we propose PMATE,
		  a Position-aware Multimodal diAlogue system with semanTic
		  Elements. Specifically, to obtain semantic representations
		  at the element level, we first unfold the multimodal
		  historical utterances and devise a position-aware
		  multimodal element-level encoder. This component considers
		  all images that may be relevant to the current turn and
		  introduces a novel position-aware image selector to choose
		  related images before fusing the information from the two
		  modalities. Finally, we present a knowledge-aware two-stage
		  decoder and an attribute-enhanced image searcher for the
		  tasks of generating textual responses and selecting image
		  responses, respectively. We extensively evaluate our model
		  on two large-scale multimodal dialogue datasets, and the
		  results of our experiments demonstrate that our approach
		  outperforms several baseline methods.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= apr,
  articleno	= {45},
  numpages	= {25},
  keywords	= {Multimodal dialogue system, natural language generation,
		  conversational image search}
}

@InProceedings{	  10.1145/3626772.3661372,
  author	= {Jiang, Cong and Chen, Zhongde and Zhang, Bo and Ren,
		  Yankun and Dong, Xin and Cheng, Lei and Yang, Xinxing and
		  Li, Longfei and Zhou, Jun and Mo, Linjian},
  title		= {GATS: Generative Audience Targeting System for Online
		  Advertising},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3661372},
  doi		= {10.1145/3626772.3661372},
  abstract	= {This paper presents GATS (&lt;u&gt;G&lt;/u&gt;enerative
		  &lt;u&gt;A&lt;/u&gt;udience &lt;u&gt;T&lt;/u&gt;argeting
		  &lt;u&gt;S&lt;/u&gt; ystem for Online Advertising), a new
		  framework using large language models (LLMs) to improve
		  audience targeting in online advertising. GATS overcomes
		  the shortcomings of rule-based, look-alike, and graph-based
		  methods by facilitating flexible and interpretable audience
		  criteria expression. The framework integrates intent
		  recognition, knowledge mining, and Data Management Platform
		  (DMP) mapping to translate advertiser demands into
		  actionable user tags and correlate them within a DMP. A
		  small, white-box model called LightGATS (base on QWen-14B),
		  fine-tuned with a high-quality LLM corpus, ensures the
		  framework's safety and efficiency, operating within a
		  scalable hybrid online-offline architecture. GATS's
		  effectiveness is validated through extensive experiments,
		  marking a significant advancement in audience targeting
		  technology.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2920–2924},
  numpages	= {5},
  keywords	= {audience targeting, large language models, multi-task
		  fine-tuning, online advertising},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Proceedings{	  10.1145/3643795,
  title		= {LLM4Code '24: Proceedings of the 1st International
		  Workshop on Large Language Models for Code},
  year		= {2024},
  isbn		= {9798400705793},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the first edition of the InternationalWorkshop
		  on Large Language Models for Code (LLM4Code). Large
		  Language Models (LLMs), which are large-scale models being
		  trained on massive textual corpora, have achieved
		  significant advances in various domains, including Software
		  Engineering (SE). Recently, there has been a growing
		  interest in applying LLMs to assist software development
		  and maintenance, such as code generation and comprehension,
		  test generation, and program repair. Although the
		  application of LLMs on code-relevant tasks has shown very
		  promising performance, there is a huge potential to explore
		  this growing domain further. The motivation of the LLM4Code
		  workshop is to provide a platform for academics and
		  practitioners to discuss and share their ideas on applying
		  and developing LLMs to solve code-relevant problems in SE
		  activities.The LLM4Code workshop is concerned with the
		  research on how to better apply LLMs to solve code-relevant
		  tasks, how to design better LLMs for code-relevant tasks,
		  and how to better benchmark LLMs on code-relevant tasks.
		  The workshop aims to achieve multiple goals as follows.
		  Firstly, the workshop aims to provide an opportunity for
		  participants to discuss novel ideas and preliminary results
		  on LLMs for solving code-relevant SE problems, to exchange
		  the latest progress in this domain. Secondly, the workshop
		  aims to encourage participants to discuss the open
		  challenges and problems of LLM4code, to identify important
		  future directions in this domain. Finally, the workshop
		  aims to encourage participants to share infrastructures and
		  benchmarks that are foundational and beneficial for future
		  research in this domain.},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3613905,
  title		= {CHI EA '24: Extended Abstracts of the CHI Conference on
		  Human Factors in Computing Systems},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Honolulu, HI, USA}
}

@Proceedings{	  10.1145/3678698,
  title		= {VINCI '24: Proceedings of the 17th International Symposium
		  on Visual Information Communication and Interaction},
  year		= {2024},
  isbn		= {9798400709678},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3630106,
  title		= {FAccT '24: Proceedings of the 2024 ACM Conference on
		  Fairness, Accountability, and Transparency},
  year		= {2024},
  isbn		= {9798400704505},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rio de Janeiro, Brazil}
}

@InProceedings{	  10.1145/3652620.3688224,
  author	= {Khalilipour, Alireza and Challenger, Moharram},
  title		= {Towards Intelligent Model Management: An Exploratory Study
		  and Road-mapping},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688224},
  doi		= {10.1145/3652620.3688224},
  abstract	= {With data science entering various domains, new branches
		  are emerging due to the extraction of latent knowledge from
		  each domain's data. Model-based engineering and modeling
		  are no exceptions. Now is the time to open a new chapter in
		  this field by leveraging advanced artificial intelligence
		  techniques. As the number and complexity of models
		  increase, NP-complete problems arise that cannot be
		  effectively addressed through deterministic management
		  solutions. An effective way to address these challenges is
		  by applying non-deterministic intelligent methodologies and
		  data science-derived solutions. The increasing number of
		  models and the formation of large model repositories
		  necessitate intelligent model management, which aims to
		  recognize hidden patterns and knowledge within these
		  repositories using data science, machine learning
		  techniques, and statistical and probabilistic methods for
		  reuse. Despite the progress made in this area, both
		  theoretically and practically, intelligent model management
		  has not yet secured a prominent place in the body of
		  knowledge of model-driven engineering. In this paper, we
		  aim to clarify the exact position of intelligent model
		  management by providing precise definitions, distinguishing
		  it from conventional management, and identifying associated
		  challenges. The above objective outlines the research
		  approach in this area, making it easy for researchers to
		  comprehend the procedures they employ for conducting their
		  investigations.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {1015–1024},
  numpages	= {10},
  keywords	= {intelligent model management, model repositories, machine
		  learning, model-driven engineering, model reuse},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Proceedings{	  10.1145/3675812,
  title		= {ICDEL '24: Proceedings of the 2024 9th International
		  Conference on Distance Education and Learning},
  year		= {2024},
  isbn		= {9798400716805},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guangzhou, China}
}

@Proceedings{	  10.1145/3632754,
  title		= {FIRE '23: Proceedings of the 15th Annual Meeting of the
		  Forum for Information Retrieval Evaluation},
  year		= {2023},
  isbn		= {9798400716324},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Panjim, India}
}

@Proceedings{	  10.1145/3680127,
  title		= {ICEGOV '24: Proceedings of the 17th International
		  Conference on Theory and Practice of Electronic
		  Governance},
  year		= {2024},
  isbn		= {9798400717802},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3689492,
  title		= {Onward! '24: Proceedings of the 2024 ACM SIGPLAN
		  International Symposium on New Ideas, New Paradigms, and
		  Reflections on Programming and Software},
  year		= {2024},
  isbn		= {9798400712159},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 2024 ACM SIGPLAN International Symposium on
		  New Ideas, New Paradigms, and Reflections on Programming
		  and Software (Onward! 2024), the premier multidisciplinary
		  conference focused on everything to do with programming and
		  software, including processes, methods, languages,
		  communities and applications. Onward! is more radical, more
		  visionary and more open than other conferences to ideas
		  that are well-argued but not yet proven. We welcome
		  different ways of thinking about, approaching, and
		  reporting on programming language and software engineering
		  research.},
  location	= {Pasadena, CA, USA}
}

@Proceedings{	  10.1145/3671127,
  title		= {BuildSys '24: Proceedings of the 11th ACM International
		  Conference on Systems for Energy-Efficient Buildings,
		  Cities, and Transportation},
  year		= {2024},
  isbn		= {9798400707063},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Book{		  10.1145/3664191,
  author	= {Kumar, Amruth N. and Raj, Rajendra K. and Aly, Sherif G.
		  and Anderson, Monica D. and Becker, Brett A. and
		  Blumenthal, Richard L. and Eaton, Eric and Epstein, Susan
		  L. and Goldweber, Michael and Jalote, Pankaj and Lea,
		  Douglas and Oudshoorn, Michael and Pias, Marcelo and
		  Reiser, Susan and Servin, Christian and Simha, Rahul and
		  Winters, Titus and Xiang, Qiao},
  title		= {Computer Science Curricula 2023},
  year		= {2024},
  isbn		= {9798400710339},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA}
}

@Proceedings{	  10.1145/3691720,
  title		= {EKI '24: Proceedings of the 2nd International Conference
		  on Educational Knowledge and Informatization},
  year		= {2024},
  isbn		= {9798400710230},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shanghai, China}
}

@Proceedings{	  10.1145/3679058,
  title		= {HUMAN '24: Proceedings of the 7th Workshop on Human
		  Factors in Hypertext},
  year		= {2024},
  isbn		= {9798400711206},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Poznan, Poland}
}

@InProceedings{	  10.1145/3664475.3664559,
  author	= {Malicki-S\'{a}nchez, Keram and Morie, Jacquelyn Ford and
		  Panos, Gregory},
  title		= {Beyond Life and Death: Exploring Digital Legacy with
		  Spatial Media, Emerging Technologies, and Evolving Ethics},
  year		= {2024},
  isbn		= {9798400706837},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664475.3664559},
  doi		= {10.1145/3664475.3664559},
  abstract	= {This course covers how we use technology to capture and
		  preserve ourselves and others, and the philosophical, legal
		  and ethical considerations involved.},
  booktitle	= {ACM SIGGRAPH 2024 Courses},
  articleno	= {9},
  numpages	= {29},
  location	= {Denver, CO, USA},
  series	= {SIGGRAPH Courses '24}
}

@Proceedings{	  10.1145/3697355,
  title		= {BDIOT '24: Proceedings of the 2024 8th International
		  Conference on Big Data and Internet of Things},
  year		= {2024},
  isbn		= {9798400717529},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3613905.3650791,
  author	= {Ma, Donghyeok and Lee, Joon Hyub and Bae, Seok-Hyung},
  title		= {Understanding Visual, Integrated, and Flexible Workspace
		  for Comprehensive Literature Reviews with
		  SketchingRelatedWork},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3650791},
  doi		= {10.1145/3613905.3650791},
  abstract	= {Writing an academic paper requires significant time and
		  effort to find, read, and organize many related papers,
		  which are complex knowledge tasks. We present a novel
		  interactive system that allows users to perform these tasks
		  quickly and easily on the 2D canvas with pen and multitouch
		  inputs, turning users’ sketches and handwriting into
		  node-link diagrams of papers and citations that users can
		  iteratively expand in situ toward constructing a coherent
		  narrative when writing Related Work sections. Through a
		  pilot study involving researchers experienced in publishing
		  academic papers, we show that our system can serve as a
		  visual, integrated, and flexible workspace for conducting
		  comprehensive literature reviews.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {348},
  numpages	= {7},
  keywords	= {Related work, inking, node-link diagram},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@Proceedings{	  10.1145/3688268,
  title		= {ICCCM '24: Proceedings of the 2024 12th International
		  Conference on Computer and Communications Management},
  year		= {2024},
  isbn		= {9798400718038},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3639477.3639743,
  author	= {Fakih, Mohamad and Dharmaji, Rahul and Moghaddas, Yasamin
		  and Quiros, Gustavo and Ogundare, Oluwatosin and Al
		  Faruque, Mohammad Abdullah},
  title		= {LLM4PLC: Harnessing Large Language Models for Verifiable
		  Programming of PLCs in Industrial Control Systems},
  year		= {2024},
  isbn		= {9798400705014},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639477.3639743},
  doi		= {10.1145/3639477.3639743},
  abstract	= {Although Large Language Models (LLMs) have established
		  predominance in automated code generation, they are not
		  devoid of shortcomings. The pertinent issues primarily
		  relate to the absence of execution guarantees for generated
		  code, a lack of explainability, and suboptimal support for
		  essential but niche programming languages. State-of-the-art
		  LLMs such as GPT-4 and LLaMa2 fail to produce valid
		  programs for Industrial Control Systems (ICS) operated by
		  Programmable Logic Controllers (PLCs). We propose LLM4PLC,
		  a user-guided iterative pipeline leveraging user feedback
		  and external verification tools - including grammar
		  checkers, compilers and SMV verifiers - to guide the LLM's
		  generation. We further enhance the generation potential of
		  LLM by employing Prompt Engineering and model fine-tuning
		  through the creation and usage of LoRAs. We validate this
		  system using a FischerTechnik Manufacturing TestBed (MFTB),
		  illustrating how LLMs can evolve from generating
		  structurally-flawed code to producing verifiably correct
		  programs for industrial applications. We run a complete
		  test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned
		  Code Llama-7B model, Code Llama-34B, and a fine-tuned Code
		  Llama-34B model. The proposed pipeline improved the
		  generation success rate from 47% to 72%, and the
		  Survey-of-Experts code quality from 2.25/10 to 7.75/10.To
		  promote open research, we share the complete experimental
		  setup, the LLM Fine-Tuning Weights, and the video
		  demonstrations of the different programs on our dedicated
		  webpage1.},
  booktitle	= {Proceedings of the 46th International Conference on
		  Software Engineering: Software Engineering in Practice},
  pages		= {192–203},
  numpages	= {12},
  keywords	= {industrial control, verifiable synthesis, large language
		  models, prompt engineering},
  location	= {Lisbon, Portugal},
  series	= {ICSE-SEIP '24}
}

@InProceedings{	  10.1145/3632754.3632943,
  author	= {Santra, Payel and Ghosh, Madhusudan and Mukherjee, Shrimon
		  and Ganguly, Debasis and Basuchowdhuri, Partha and Naskar,
		  Sudip Kumar},
  title		= {Unleashing the Power of Large Language Models: A Hands-On
		  Tutorial},
  year		= {2024},
  isbn		= {9798400716324},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632754.3632943},
  doi		= {10.1145/3632754.3632943},
  abstract	= {LLMs have opened up possibilities for advancing the
		  state-of-the-art in natural language processing (NLP). In
		  this tutorial, we present the audience with an introduction
		  to LLMs and the associated challenges. The tutorial is
		  structured in the following manner. First, we provide a
		  brief preface that outlines the fundamental principles of
		  NLP, following which, we explore the area of distributional
		  representation learning for NLP. Then, we delve into the
		  essential component of transformer-based pretrained
		  language models. We then follow this up with the concept of
		  prompt learning or in-context learning (ICL) and discuss
		  how it is emerging as a popular methodology replacing the
		  conventional supervised learning workflow comprised of
		  pretraining and fine-tuning. We outline the research
		  challenges in ICL, which usually involves finding the
		  correct set of examples and contexts for the purpose of
		  guiding the LLM decoder towards effective predictions.
		  Afterwards, a hands-on coding and demonstration session
		  will be carried out to impart practical knowledge about
		  LLMs and ICL to the tutorial participants.},
  booktitle	= {Proceedings of the 15th Annual Meeting of the Forum for
		  Information Retrieval Evaluation},
  pages		= {149–152},
  numpages	= {4},
  keywords	= {In-context Learning, Large Language Models, Natural
		  Language Inferencing, Sentiment Analysis, Text
		  Summarization},
  location	= {Panjim, India},
  series	= {FIRE '23}
}

@Proceedings{	  10.1145/3626772,
  title		= {SIGIR '24: Proceedings of the 47th International ACM SIGIR
		  Conference on Research and Development in Information
		  Retrieval},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 47th Annual International ACM SIGIR
		  Conference on Research and Development in Information
		  Retrieval (SIGIR 2024), taking place in Washington D.C.,
		  USA, from July 14 to 18, 2024.SIGIR serves as the foremost
		  international forum for the presentation of groundbreaking
		  research findings, the demonstration of innovative systems
		  and techniques, and the exploration of forwardthinking
		  research directions in the field of information
		  retrieval.This year's SIGIR is an in-person conference. We
		  believe that an in-person conference is beneficial for
		  several reasons: it fosters direct engagement and
		  networking opportunities, enhances the exchange of research
		  ideas, contributes to a more dynamic and productive
		  conference experience, and nurtures our research community
		  by welcoming newcomers, providing them with the opportunity
		  to become acquainted with SIGIR traditions. This decision
		  has not been made lightly. We understand the challenges
		  that can pose in the aftermath of a pandemic and amidst the
		  uncertainties of the world around us. To accommodate those
		  who cannot attend, we have implemented a series of measures
		  such as proxy presenters, livestreaming, and recording
		  sessions. These steps are taken to ensure that everyone has
		  access to the valuable content that the conference
		  offers.},
  location	= {Washington DC, USA}
}

@InProceedings{	  10.1145/3664647.3680760,
  author	= {Liu, Rui and Li, Mingjie and Zhao, Shen and Chen, Ling and
		  Chang, Xiaojun and Yao, Lina},
  title		= {In-Context Learning for Zero-shot Medical Report
		  Generation},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3680760},
  doi		= {10.1145/3664647.3680760},
  abstract	= {Medical report generation (MRG) has emerged as a pivotal
		  research topic in the medical multi-modal field, given its
		  potential to alleviate the heavy workloads of radiologists.
		  Recently, advancements have been made with MRG systems that
		  leverage large multimodal models (LMMs) to generate
		  high-quality reports. To address the challenge of
		  collecting large amounts of paired medical image-report
		  data for training, this paper proposes a zero-shot report
		  generation model based on in-context learning, we call it
		  MCVGen. Departing from traditional in-context learning
		  approaches that directly feed all demonstrations to a
		  pre-trained large model, this work innovates by employing a
		  multi-modal contextual vector (MCV) to represent the
		  contextual information of demonstrations. Initially, we
		  pre-train a medical large multi-modal model (Med-LMM) and
		  secure the last hidden state of each demonstration through
		  the forward pass in Med-LMM. Benefits from the
		  auto-regressive mechanism, the last hidden state garners
		  critical information to the targeted scenarios.
		  Subsequently, we average the multiple MCVs and integrate
		  them with the first hidden state on the new query, thereby
		  shifting the latent states and guiding the model toward
		  acquiring previously unlearned multi-modal contextual
		  information. This approach has the advantage of regulating
		  the number of prompts, thus reducing computational costs.
		  We tested our model on the publicly available IU X-ray and
		  MIMIC datasets, demonstrating its exceptional zero-shot
		  capability on both cross-center and cross-disease
		  evaluations. We hope it could be a viable solution for
		  practical clinical applications.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {8721–8730},
  numpages	= {10},
  keywords	= {large multi-modal model, medical report generation,
		  multi-modal in-context learning, zero-shot},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Article{	  10.1145/3649451,
  author	= {Du, Kelvin and Xing, Frank and Mao, Rui and Cambria,
		  Erik},
  title		= {Financial Sentiment Analysis: Techniques and
		  Applications},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {9},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3649451},
  doi		= {10.1145/3649451},
  abstract	= {Financial Sentiment Analysis (FSA) is an important domain
		  application of sentiment analysis that has gained
		  increasing attention in the past decade. FSA research falls
		  into two main streams. The first stream focuses on defining
		  tasks and developing techniques for FSA, and its main
		  objective is to improve the performances of various FSA
		  tasks by advancing methods and using/curating
		  human-annotated datasets. The second stream of research
		  focuses on using financial sentiment, implicitly or
		  explicitly, for downstream applications on financial
		  markets, which has received more research efforts. The main
		  objective is to discover appropriate market applications
		  for existing techniques. More specifically, the application
		  of FSA mainly includes hypothesis testing and predictive
		  modeling in financial markets. This survey conducts a
		  comprehensive review of FSA research in both the technique
		  and application areas and proposes several frameworks to
		  help understand the two areas’ interactive relationship.
		  This article defines a clearer scope for FSA studies and
		  conceptualizes the FSA-investor sentiment-market sentiment
		  relationship. Major findings, challenges, and future
		  research directions for both FSA techniques and
		  applications have also been summarized and discussed.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {220},
  numpages	= {42},
  keywords	= {Financial sentiment analysis, financial forecasting,
		  natural language processing, information system, machine
		  learning, deep learning}
}

@InProceedings{	  10.1145/3680127.3680130,
  author	= {Maratsi, Maria Ioanna and Alexopoulos, Charalampos and
		  Charalabidis, Yannis},
  title		= {On the Semantic Analysis of Open (Government) Data
		  Portals’ Metadata Provision and Schema},
  year		= {2024},
  isbn		= {9798400717802},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3680127.3680130},
  doi		= {10.1145/3680127.3680130},
  abstract	= {The ever-increasing amount of data available through Open
		  Data portals follows the momentum and proliferation of Open
		  Data initiatives and, apart from the numerous benefits
		  offered, poses a reality which brings forth several
		  challenges. Metadata quality and management are key aspects
		  in this regard; in their absence affecting the
		  interoperability, findability, and consequently, open data
		  consumption. The present study aims to analyse several
		  semantic aspects of 3 major international data portals and
		  their metadata schemas mapped to the main DCAT schema
		  classes in order to assess the current status regarding
		  metadata provision and aiming towards prospective
		  standardisation and improved semantic interoperability of
		  the datasets available. The analysis revealed several
		  persisting challenges, including the lack of standardised
		  information (e.g., controlled vocabularies) as accepted
		  values of the identified mandatory metadata fields in use,
		  or, in the case of existing standardisation schemas, a
		  misalignment and lack of consensus among the different
		  portals. The study also pinpoints future research lines
		  centred around the elicitation of guidelines and practices
		  to standardise not only descriptive but also
		  domain-specific metadata provision across the portals,
		  facilitating the process towards the identification of
		  minimum sets of metadata descriptions applicable to various
		  contexts, and reaching one step closer to the envisioned
		  interoperable ODPs paradigm.},
  booktitle	= {Proceedings of the 17th International Conference on Theory
		  and Practice of Electronic Governance},
  pages		= {147–157},
  numpages	= {11},
  location	= { },
  series	= {ICEGOV '24}
}

@Proceedings{	  10.1145/3629606,
  title		= {CHCHI '23: Proceedings of the Eleventh International
		  Symposium of Chinese CHI},
  year		= {2023},
  isbn		= {9798400716454},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Denpasar, Bali, Indonesia}
}

@Proceedings{	  10.1145/3689218,
  title		= {PRIS '24: Proceedings of the 2024 6th International
		  Conference on Pattern Recognition and Intelligent Systems},
  year		= {2024},
  isbn		= {9798400718250},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hong Kong, Hong Kong}
}

@Proceedings{	  10.1145/3688671,
  title		= {SETN '24: Proceedings of the 13th Hellenic Conference on
		  Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400709821},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3674971,
  author	= {Ellouze, Mourad and Hadrich Belguith, Lamia},
  title		= {Artificial Intelligence application for the analysis of
		  personality traits and disorders in social media: A
		  Survey},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3674971},
  doi		= {10.1145/3674971},
  abstract	= {Personality analysis has a positive influence on humanity
		  as it aids in identifying personality traits and disorders.
		  In addition, it facilitates the monitoring of cases and
		  enriches doctors’ knowledge bases, particularly in
		  decision-making processes. This study includes a
		  comprehensive literature review on personality analysis
		  approaches from social media, aiming to gain a thorough
		  understanding of the current studies on personality
		  therapy. Moreover, the objective of this study is to
		  identify various limitations present in these studies and
		  explore potential avenues for enhancement. More
		  specifically, this research begins with an introduction
		  that discusses the main concepts of traits and personality
		  disorders, as well as the importance of psychological
		  analysis. Following that, four cluster studies related to
		  personality analysis on social media are presented:
		  personality traits, personality disorders, detection of
		  links between diseases, and monitoring patient status.
		  Then, the majority of the currently available works for
		  each cluster are exposed. Afterward, a comparative study of
		  the different presented works is proposed. Finally, an
		  outline of plans for further research in this area is
		  provided, detailing potential paths for exploration.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  keywords	= {Social Media, Personality Traits, Personality Disorders,
		  Artificial Intelligence, Text Mining, Natural Language
		  Processing}
}

@Proceedings{	  10.1145/3654777,
  title		= {UIST '24: Proceedings of the 37th Annual ACM Symposium on
		  User Interface Software and Technology},
  year		= {2024},
  isbn		= {9798400706288},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Pittsburgh, PA, USA}
}

@Article{	  10.1145/3651983,
  author	= {Rong, Huan and Chen, Zhongfeng and Lu, Zhenyu and Xu, Fan
		  and Sheng, Victor S},
  title		= {Multization: Multi-Modal Summarization Enhanced by
		  Multi-Contextually Relevant and Irrelevant Attention
		  Alignment},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3651983},
  doi		= {10.1145/3651983},
  abstract	= {This article focuses on the task of Multi-Modal
		  Summarization with Multi-Modal Output for China JD.COM
		  e-commerce product description containing both source text
		  and source images. In the context learning of multi-modal
		  (text and image) input, there exists a semantic gap between
		  text and image, especially in the cross-modal semantics of
		  text and image. As a result, capturing shared cross-modal
		  semantics earlier becomes crucial for multi-modal
		  summarization. However, when generating the multi-modal
		  summarization, based on the different contributions of
		  input text and images, the relevance and irrelevance of
		  multi-modal contexts to the target summary should be
		  considered, so as to optimize the process of learning
		  cross-modal context to guide the summary generation process
		  and to emphasize the significant semantics within each
		  modality. To address the aforementioned challenges,
		  Multization has been proposed to enhance multi-modal
		  semantic information by multi-contextually relevant and
		  irrelevant attention alignment. Specifically, a Semantic
		  Alignment Enhancement mechanism is employed to capture
		  shared semantics between different modalities (text and
		  image), so as to enhance the importance of crucial
		  multi-modal information in the encoding stage.
		  Additionally, the IR-Relevant Multi-Context Learning
		  mechanism is utilized to observe the summary generation
		  process from both relevant and irrelevant perspectives, so
		  as to form a multi-modal context that incorporates both
		  text and image semantic information. The experimental
		  results in the China JD.COM e-commerce dataset demonstrate
		  that the proposed Multization method effectively captures
		  the shared semantics between the input source text and
		  source images, and highlights essential semantics. It also
		  successfully generates the multi-modal summary (including
		  image and text) that comprehensively considers the
		  semantics information of both text and image.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {69},
  numpages	= {29},
  keywords	= {Business intelligence, multi-modal summarization, semantic
		  enhancement and attention, multi-modal cross learning}
}

@InProceedings{	  10.1145/3640310.3674085,
  author	= {Ben Chaaben, Meriem and Ben Sghaier, Oussama and Dhaouadi,
		  Mouna and Elrasheed, Nafisa and Darif, Ikram and Jaoua,
		  Imen and Oakes, Bentley and Syriani, Eugene and Hamdaqa,
		  Mohammad},
  title		= {Toward Intelligent Generation of Tailored Graphical
		  Concrete Syntax},
  year		= {2024},
  isbn		= {9798400705045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640310.3674085},
  doi		= {10.1145/3640310.3674085},
  abstract	= {In model-driven engineering, the concrete syntax of a
		  domain-specific modeling language (DSML) is fundamental as
		  it constitutes the primary point of interaction between the
		  user and the DSML. Nevertheless, the conventional
		  one-size-fits-all approach to concrete syntax often
		  undermines the effectiveness of DSMLs, as it fails to
		  accommodate the diverse constraints and specific
		  requirements inherent to diverse users and usage contexts.
		  Such shortcomings can lead to a significant decline in the
		  performance, usability, and efficiency of DSMLs. This
		  vision paper proposes a conceptual framework to generate
		  concrete syntax intelligently. Our framework considers
		  multiple concerns of users and aims to align the concrete
		  syntax with the context of the DSML usage. Additionally, we
		  detail a baseline process to employ our framework in
		  practice, leveraging large language models to expedite the
		  generation of tailored concrete syntax. We illustrate the
		  potential of our vision with two concrete examples and
		  discuss the shortcomings and research challenges of current
		  intelligent generation techniques.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {160–171},
  numpages	= {12},
  keywords	= {Artificial Intelligence, Concrete Syntax, Domain-specific
		  Modeling Languages, Large Language Models},
  location	= {Linz, Austria},
  series	= {MODELS '24}
}

@InProceedings{	  10.1145/3689236.3696039,
  author	= {Wu, Jing and Guo, Zhenxin and Wang, Zhicheng and Zhang,
		  Haotian and Kang, Xaolin and Ma, Xiaoguang},
  title		= {Design and Implementation of Embedded Qinghai Tourism
		  Customer Service Question-and-Answer Robot Based on
		  ChatGPT},
  year		= {2024},
  isbn		= {9798400718137},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689236.3696039},
  doi		= {10.1145/3689236.3696039},
  abstract	= {The application of big data technology has brought new
		  impetus and possibilities to the development of tourism.
		  The design of an embedded Q&amp;A robot system for Qinghai
		  tourism customer service based on ChatGPT system aims to
		  provide an intelligent customer service solution to help
		  users obtain information about Qinghai tourism and answer
		  questions. The system uses ChatGPT as the core dialogue
		  model, combines the knowledge and data in the field of
		  Qinghai tourism, and realizes the intelligent answer to
		  user questions. This study application in the field of
		  natural language processing, and analyzes the design
		  requirements and implementation scheme of embedded travel
		  question answering system in detail. The function module
		  includes user login and registration, travel tickets, hotel
		  information query, scenic spot ticket price query and other
		  essential basic function modules of the tourism website.
		  Whether the design is reasonable or not will also directly
		  affect the user experience. The results show that ChatGPT
		  embedded in Qinghai tourism Q&amp;A system can not only
		  provide more intelligent and accurate answers, but also
		  enhance the interactivity and practicability of the system,
		  and provide users with more convenient and personalized
		  tourism information services.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Cyber Security and Information Engineering},
  pages		= {871–882},
  numpages	= {12},
  keywords	= {ChatGPT, Chatbot, Generative Artificial Intelligence,
		  Qinghai Tourism},
  location	= { },
  series	= {ICCSIE '24}
}

@InProceedings{	  10.1145/3589334.3645623,
  author	= {Bai, Yuyang and Feng, Shangbin and Balachandran, Vidhisha
		  and Tan, Zhaoxuan and Lou, Shiqi and He, Tianxing and
		  Tsvetkov, Yulia},
  title		= {KGQuiz: Evaluating the Generalization of Encoded Knowledge
		  in Large Language Models},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645623},
  doi		= {10.1145/3589334.3645623},
  abstract	= {Large language models (LLMs) demonstrate remarkable
		  performance on knowledge-intensive tasks, suggesting that
		  real-world knowledge is encoded in their model parameters.
		  However, besides explorations on a few probing tasks in
		  limited knowledge domains, it is not well understood how to
		  evaluate LLMs' knowledge systematically and how well their
		  knowledge abilities generalize, across a spectrum of
		  knowledge domains and progressively complex task formats.
		  To this end, we propose KGQuiz, a knowledge-intensive
		  benchmark to comprehensively investigate the knowledge
		  generalization abilities of LLMs. KGQuiz is a scalable
		  framework constructed from triplet-based knowledge, which
		  covers three knowledge domains and consists of five tasks
		  with increasing complexity: true-or-false, multiple-choice
		  QA, blank filling, factual editing, and open-ended
		  knowledge generation. To gain a better understanding of
		  LLMs' knowledge abilities and their generalization, we
		  evaluate 10 open-source and black-box LLMs on the KGQuiz
		  benchmark across the five knowledge-intensive tasks and
		  knowledge domains. Extensive experiments demonstrate that
		  LLMs achieve impressive performance in straightforward
		  knowledge QA tasks, while settings and contexts requiring
		  more complex reasoning or employing domain-specific facts
		  still present significant challenges. We envision KGQuiz as
		  a testbed to analyze such nuanced variations in performance
		  across domains and task formats, and ultimately to
		  understand, evaluate, and improve LLMs' knowledge abilities
		  across a wide spectrum of knowledge domains and tasks.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2226–2237},
  numpages	= {12},
  keywords	= {knowledge probing, large language models},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3678717,
  title		= {SIGSPATIAL '24: Proceedings of the 32nd ACM International
		  Conference on Advances in Geographic Information Systems},
  year		= {2024},
  isbn		= {9798400711077},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {These proceedings contain the papers from the 32nd ACM
		  SIGSPATIAL International Conference on Advances in
		  Geographic Information Systems (ACM SIGSPATIAL 2024), held
		  as an in-person event in Atlanta, GA, USA on October 29-
		  November 01, 2024. SIGSPATIAL academics, students, and
		  industry practitioners could attend the technical talks
		  in-person, meet in the hallway for further discussions, and
		  boost their professional network over lunch.},
  location	= {Atlanta, GA, USA}
}

@Proceedings{	  10.1145/3655497,
  title		= {ICIAI '24: Proceedings of the 2024 International
		  Conference on Innovation in Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400709302},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tokyo, Japan}
}

@InProceedings{	  10.1145/3679058.3688633,
  author	= {Atzenbeck, Claus and Eidloth, Lisa},
  title		= {Harnessing Hypertext Paradigms to Augment VR Spaces},
  year		= {2024},
  isbn		= {9798400711206},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3679058.3688633},
  doi		= {10.1145/3679058.3688633},
  abstract	= {This paper explores the integration of hypertext
		  structures within Virtual Reality (VR) environments,
		  differentiating between two distinct design philosophies:
		  VR as a native framework for 3D embodiment-enabled spaces
		  similar to traditional 2D spatial hypertext, and utilizing
		  hypertext to enhance VR experiences. Focusing on the latter
		  approach, we propose an abstract knowledge layer that
		  bridges typical VR systems and human thinking, thus
		  facilitating the integration of human cognitive
		  capabilities. Finally, we explore ethical implications of
		  VR systems that arise in the presented context and propose
		  hypertext as a paradigm to address some of these
		  concerns.},
  booktitle	= {Proceedings of the 7th Workshop on Human Factors in
		  Hypertext},
  articleno	= {5},
  numpages	= {10},
  keywords	= {Mother, VR, archeology, augmentation, cognitive maps,
		  collaboration, digital twins, ethics, hypertext, note
		  taking, spatial hypertext, virtual reality},
  location	= {Poznan, Poland},
  series	= {HUMAN '24}
}

@InProceedings{	  10.1145/3641234.3671068,
  author	= {Lee, Sin-Fei and Chi, Ming-Te},
  title		= {Illusory Eyescape - Exploring the Variations of
		  Consciousness through Generative Art and Eye-Tracking
		  Techniques},
  year		= {2024},
  isbn		= {9798400705168},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3641234.3671068},
  doi		= {10.1145/3641234.3671068},
  abstract	= {Historically, art has been considered a form of expression
		  uniquely reserved for humans. However, technological
		  advances, particularly in computer graphics, have expanded
		  the boundaries of artistic creation beyond human
		  exclusivity. The emergence of Generative Art in recent
		  years has not only transformed the artistic process but has
		  also initiated extensive dialogues on the interactions
		  between humans and machines. To delve deeper into this
		  theme, we developed an interactive art installation titled
		  "Illusory Eyescape." This installation integrates
		  Generative Art and Eye-tracking Technologies to probe the
		  fundamental nature of consciousness in humans and machines,
		  questioning the role of art in this new epoch of
		  human-machine synergy. This exploration broadens our
		  comprehension of the potential amalgamations of art and
		  technology and challenges our conventional perceptions of
		  art.},
  booktitle	= {ACM SIGGRAPH 2024 Posters},
  articleno	= {13},
  numpages	= {2},
  keywords	= {Eye-tracking, Generative Art, Interactive Art},
  location	= {Denver, CO, USA},
  series	= {SIGGRAPH '24}
}

@InProceedings{	  10.1145/3613904.3642887,
  author	= {Chen, Liuqing and Jiang, Zhaojun and Xia, Duowei and Cai,
		  Zebin and Sun, Lingyun and Childs, Peter and Zuo, Haoyu},
  title		= {BIDTrainer: An LLMs-driven Education Tool for Enhancing
		  the Understanding and Reasoning in Bio-inspired Design},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642887},
  doi		= {10.1145/3613904.3642887},
  abstract	= {Bio-inspired design (BID) fosters innovations in
		  engineering. Learning BID is crucial for developing
		  multidisciplinary innovation skills of designers and
		  engineers. Current BID education aims to enhance
		  learners’ understanding and analogical reasoning skills.
		  However, it often heavily relies on the teachers’
		  expertise. When learners pursue independent learning using
		  some educational tools, they face challenges in
		  understanding and reasoning practice within this
		  multidisciplinary field. Additionally, evaluating their
		  learning outcomes comprehensively becomes problematic.
		  Addressing these challenges, we introduce a LLMs-driven BID
		  education method based on a structured ontology and three
		  strategies: enhancing understanding through LLMs-enpowered
		  "learning by asking", assisting reasoning by providing
		  hints and feedback, and assessing learning outcomes through
		  benchmarking against existing BID cases. Implementing the
		  method, we developed BIDTrainer, a BID education tool. User
		  studies indicate that learners using BIDTrainer understood
		  BID knowledge better, reason faster with higher
		  interactivity than the baseline, and BIDTrainer assessed
		  the learning outcomes consistent with experts.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {676},
  numpages	= {20},
  keywords	= {Analogy training, Bio-inspired design, Design education,
		  Design evaluation},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3701571.3701588,
  author	= {Kronhardt, Kirill and Rolfes, Kevin and Gerken, Jens},
  title		= {Trickery: Exploring a Serious Game Approach to Raise
		  Awareness of Deceptive Patterns},
  year		= {2024},
  isbn		= {9798400712838},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701571.3701588},
  doi		= {10.1145/3701571.3701588},
  abstract	= {Deceptive patterns are often used in interface design to
		  manipulate users into taking actions they would not
		  otherwise take, such as consenting to excessive data
		  collection. We present Trickery, a narrative serious game
		  that incorporates seven gamified deceptive patterns. We
		  designed the game as a potential mechanism for raising
		  awareness of, and increasing resistance to, deceptive
		  patterns through direct consequences of player actions. We
		  conducted an explorative gameplay study to examine player
		  behavior when confronted with the game Trickery. In
		  addition, we conducted an online survey to shed light on
		  the perceived helpfulness of our gamified deceptive
		  patterns. Our results reveal different player motivations
		  and driving forces that players used to justify their
		  behavior when confronted with deceptive patterns in the
		  Trickery game. In addition, we identified several
		  influencing factors that need to be considered when
		  adapting deceptive patterns into gameplay. Overall, the
		  approach appears to be a promising solution for increasing
		  user understanding and awareness of deceptive patterns.},
  booktitle	= {Proceedings of the International Conference on Mobile and
		  Ubiquitous Multimedia},
  pages		= {133–147},
  numpages	= {15},
  keywords	= {education, privacy, dark patterns, deceptive patterns,
		  serious games, awareness},
  location	= { },
  series	= {MUM '24}
}

@InProceedings{	  10.1145/3637528.3671454,
  author	= {Zheng, Lecheng and Jing, Baoyu and Li, Zihao and Tong,
		  Hanghang and He, Jingrui},
  title		= {Heterogeneous Contrastive Learning for Foundation Models
		  and Beyond},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671454},
  doi		= {10.1145/3637528.3671454},
  abstract	= {In the era of big data and Artificial Intelligence, an
		  emerging paradigm is to utilize contrastive self-supervised
		  learning to model large-scale heterogeneous data. Many
		  existing foundation models benefit from the generalization
		  capability of contrastive self-supervised learning by
		  learning compact and high-quality representations without
		  relying on any label information. Amidst the explosive
		  advancements in foundation models across multiple domains,
		  including natural language processing and computer vision,
		  a thorough survey on heterogeneous contrastive learning for
		  the foundation model is urgently needed. In response, this
		  survey critically evaluates the current landscape of
		  heterogeneous contrastive learning for foundation models,
		  highlighting the open challenges and future trends of
		  contrastive learning. In particular, we first present how
		  the recent advanced contrastive learning-based methods deal
		  with view heterogeneity and how contrastive learning is
		  applied to train and fine-tune the multi-view foundation
		  models. Then, we move to contrastive learning methods for
		  task heterogeneity, including pretraining tasks and
		  downstream tasks, and show how different tasks are combined
		  with contrastive learning loss for different purposes.
		  Finally, we conclude this survey by discussing the open
		  challenges and shedding light on the future directions of
		  contrastive learning.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6666–6676},
  numpages	= {11},
  keywords	= {contrastive learning, foundation model, multi-task
		  learning, heterogeneous learning, multi-view learning},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Proceedings{	  10.1145/3627508,
  title		= {CHIIR '24: Proceedings of the 2024 Conference on Human
		  Information Interaction and Retrieval},
  year		= {2024},
  isbn		= {9798400704345},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sheffield, United Kingdom}
}

@Article{	  10.1109/taslp.2024.3487409,
  author	= {Wei, Yuting and Hu, Linmei and Zhu, Yangfu and Zhao, Jiaqi
		  and Wu, Bin},
  title		= {Knowledge-Guided Transformer for Joint Theme and Emotion
		  Classification of Chinese Classical Poetry},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3487409},
  doi		= {10.1109/TASLP.2024.3487409},
  abstract	= {The classifications of the theme and emotion are essential
		  for understanding and organizing Chinese classical poetry.
		  Existing works often overlook the rich semantic knowledge
		  derived from poem annotations, which contain crucial
		  insights into themes and emotions and are instrumental in
		  semantic understanding. Additionally, the complex
		  interdependence and diversity of themes and emotions within
		  poems are frequently disregarded. Hence, this paper
		  introduces a Poetry Knowledge-augmented Joint Model (Poka)
		  specifically designed for the multi-label classification of
		  themes and emotions in Chinese classical poetry.
		  Specifically, we first employ an automated approach to
		  construct two semantic knowledge graphs for theme and
		  emotion. These graphs facilitate a deeper understanding of
		  the poems by bridging the semantic gap between the obscure
		  ancient words and their modern Chinese counterparts.
		  Representations related to themes and emotions are then
		  acquired through a knowledge-guided mask-transformer.
		  Moreover, Poka leverages the inherent correlations between
		  themes and emotions by adopting a joint classification
		  strategy with shared training parameters. Extensive
		  experiments demonstrate that our model achieves
		  state-of-the-art performance on both theme and emotion
		  classifications, especially on tail labels.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= oct,
  pages		= {4783–4794},
  numpages	= {12}
}

@InProceedings{	  10.1145/3677525.3678695,
  author	= {Altammami, Alaa and Dimitrova, Vania and Pournaras,
		  Evangelos},
  title		= {What you see, What you get? Mapping Inconsistencies of
		  Sustainability Judgements among Experts and Consumers},
  year		= {2024},
  isbn		= {9798400710940},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677525.3678695},
  doi		= {10.1145/3677525.3678695},
  abstract	= {Addressing sustainability issues requires collective
		  action, with individuals playing a crucial role. Despite a
		  willingness to shop responsibly, people lack the knowledge
		  needed to make informed decisions, this information gap
		  underpins the intention-behaviour gap. Online shopping, now
		  dominant, can offer rich sustainability information.
		  However, consumers, especially those new to the
		  sustainability domain, face challenges in comprehending
		  this information due to its complexity and volume,
		  including confusion over the meaning of ecolabels. Product
		  descriptions are a key decision-making resource, yet there
		  is no significant research analysing their sustainability
		  content or their potential for seamless in-situ/situated
		  learning. We propose a framework using a Taxonomy for
		  Product Sustainability (TPS) to automatically extract and
		  analyse sustainability profiles from product descriptions.
		  By comparing these profiles with expert judgements, we
		  identify how alignments can be seen as an opportunity to
		  enhance consumer awareness and how misalignments can
		  introduce cognitive biases that impede ethical shopping.
		  Our analysis of food product descriptions reveals distinct
		  patterns of agreement and disagreement, highlighting
		  cognitive biases that affect consumer decisions. These
		  biases, driven by misalignment and information overload,
		  contribute to the intention-behaviour gap in sustainable
		  shopping. By identifying specific areas of confusion, we
		  suggest targeted interventions, such as informative
		  prompts, to facilitate seamless learning and improve
		  consumer knowledge, eventually promoting more informed and
		  ethical choices.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Information Technology for Social Good},
  pages		= {443–452},
  numpages	= {10},
  keywords	= {Consumer Decision Making, Ethical Consumption, Learning.,
		  Online Information, Sustainability Taxonomy},
  location	= {Bremen, Germany},
  series	= {GoodIT '24}
}

@InProceedings{	  10.1145/3648188.3675152,
  author	= {Li, Ge and Vachtsevanou, Danai and Lem\'{e}e,
		  J\'{e}r\'{e}my and Mayer, Simon and Strecker, Jannis},
  title		= {Reader-aware Writing Assistance through Reader Profiles},
  year		= {2024},
  isbn		= {9798400705953},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3648188.3675152},
  doi		= {10.1145/3648188.3675152},
  abstract	= {Establishing rapport between authors and readers of
		  scientific texts is essential for supporting readers in
		  understanding texts as intended, facilitating
		  socio-discursive practices within disciplinary communities,
		  and helping in identifying interdisciplinary links among
		  scientific writings. We propose a Reader-aware Congruence
		  Assistant (RaCA), which supports writers to create texts
		  that are adapted to target readers. Similar to
		  user-centered design which is based on user profiles, RaCA
		  features reader-centered writing through reader profiles
		  that are dynamically computed from information discovered
		  through academic search engines. Our assistant then
		  leverages large language models to measure the congruence
		  of a written text with a given reader profile, and provides
		  feedback to the writer. We demonstrate our approach with an
		  implemented prototype that illustrates how RaCA exploits
		  information available on the Web to construct reader
		  profiles, assesses writer-reader congruence and offers
		  writers color-coded visual feedback accordingly. We argue
		  that our approach to reader-oriented scientific writing
		  paves the way towards the more personalized interaction of
		  readers and writers with scientific content, and discuss
		  how integration with Semantic Web technologies and Adaptive
		  User Interface design can help materialize this vision
		  within an ever-growing Web of scientific ideas, proof, and
		  discourse.},
  booktitle	= {Proceedings of the 35th ACM Conference on Hypertext and
		  Social Media},
  pages		= {344–350},
  numpages	= {7},
  keywords	= {Natural Language Processing, Personalized Text Adaptation,
		  Reader Profile, Text Congruence},
  location	= {Poznan, Poland},
  series	= {HT '24}
}

@InProceedings{	  10.1145/3589334.3645678,
  author	= {Sun, Qi and Huang, Kun and Yang, Xiaocui and Tong, Rong
		  and Zhang, Kun and Poria, Soujanya},
  title		= {Consistency Guided Knowledge Retrieval and Denoising in
		  LLMs for Zero-shot Document-level Relation Triplet
		  Extraction},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645678},
  doi		= {10.1145/3589334.3645678},
  abstract	= {Document-level Relation Triplet Extraction (DocRTE) is a
		  fundamental task in information systems that aims to
		  simultaneously extract entities with semantic relations
		  from a document. Existing methods heavily rely on a
		  substantial amount of fully labeled data. However,
		  collecting and annotating data for newly emerging relations
		  is time-consuming and labor-intensive. Recent advanced
		  Large Language Models (LLMs), such as ChatGPT and LLaMA,
		  exhibit impressive long-text generation capabilities,
		  inspiring us to explore an alternative approach for
		  obtaining auto-labeled documents with new relations. In
		  this paper, we propose a Zero-shot Document-level Relation
		  Triplet Extraction (ZeroDocRTE) framework, which Generates
		  labeled data by Retrieval and Denoising Knowledge from
		  LLMs, called GenRDK. Specifically, we propose a
		  chain-of-retrieval prompt to guide ChatGPT to generate
		  labeled long-text data step by step. To improve the quality
		  of synthetic data, we propose a denoising strategy based on
		  the consistency of cross-document knowledge. Leveraging our
		  denoised synthetic data, we proceed to fine-tune the
		  LLaMA2-13B-Chat for extracting document-level relation
		  triplets. We perform experiments for both zero-shot
		  document-level relation and triplet extraction on two
		  public datasets. The experimental results illustrate that
		  our GenRDK framework outperforms strong baselines.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {4407–4416},
  numpages	= {10},
  keywords	= {document-level relation triplet extraction, knowledge
		  denoising, large language models, synthetic data, zero-shot
		  learning},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3652988.3696195,
  author	= {Origlia, Antonio and Di Maro, Maria},
  title		= {A Linguistically Motivated Approach to Hybrid
		  Conversational AI with the FANTASIA Plugin},
  year		= {2024},
  isbn		= {9798400706257},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652988.3696195},
  doi		= {10.1145/3652988.3696195},
  abstract	= {This paper presents an installation showcasing the basic
		  concepts of the FANTASIA tool and the Interaction Model
		  built around it. The tool itself can be used in a variety
		  of ways, through the modular set of components it provides
		  to extend the capabilities of a powerful engine for Real
		  Time Interactive 3D applications, the Unreal Engine. The
		  Interaction Model represents how FANTASIA can be used to
		  build linguistically motivated models for dialogue
		  management, going beyond the use of statistical learning
		  only. Specifically, an explicit separation between language
		  modelling capabilities and decision making will be
		  described for movie recommendations.},
  booktitle	= {Proceedings of the 24th ACM International Conference on
		  Intelligent Virtual Agents},
  articleno	= {46},
  numpages	= {3},
  keywords	= {Conversational AI, Hybrid AI, Linguistics, Unreal Engine},
  location	= {GLASGOW, United Kingdom},
  series	= {IVA '24}
}

@Proceedings{	  10.1145/3663548,
  title		= {ASSETS '24: Proceedings of the 26th International ACM
		  SIGACCESS Conference on Computers and Accessibility},
  year		= {2024},
  isbn		= {9798400706776},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {St. John's, NL, Canada}
}

@InProceedings{	  10.1145/3637528.3671466,
  author	= {Deng, Songgaojun and de Rijke, Maarten and Ning, Yue},
  title		= {Advances in Human Event Modeling: From Graph Neural
		  Networks to Language Models},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671466},
  doi		= {10.1145/3637528.3671466},
  abstract	= {Human events such as hospital visits, protests, and
		  epidemic outbreaks directly affect individuals,
		  communities, and societies. These events are often
		  influenced by factors such as economics, politics, and
		  public policies of our society. The abundance of online
		  data sources such as social networks, official news
		  articles, and personal blogs chronicle societal events,
		  facilitating the development of AI models for social
		  science, public health care, and decision making. Human
		  event modeling generally comprises both the forecasting
		  stage, which estimates future events based on historical
		  data, and interpretation, which seeks to identify
		  influential factors of such events to understand their
		  causative attributes. Recent achievements, fueled by deep
		  learning and the availability of public data, have
		  significantly advanced the field of human event
		  modeling.This survey offers a systematic overview of deep
		  learning technologies for forecasting and interpreting
		  human events, with a primary focus on political events. We
		  first introduce the existing challenges and background in
		  this domain. We then present the problem formulation of
		  event forecasting and interpretation. We investigate recent
		  achievements in graph neural networks, owing to the
		  prevalence of relational data and the efficacy of graph
		  learning models. We also discuss the latest studies that
		  utilize large language models for event reasoning. Lastly,
		  we provide summaries of data resources, open challenges,
		  and future research directions in the study of human event
		  modeling.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6459–6469},
  numpages	= {11},
  keywords	= {event forecasting, graph neural networks, language
		  models},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3675417.3675447,
  author	= {Lv, Han and Wang, Kun},
  title		= {A study of AIGC-enabled international marketing},
  year		= {2024},
  isbn		= {9798400717147},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3675417.3675447},
  doi		= {10.1145/3675417.3675447},
  abstract	= {The intelligent transformation and development of domestic
		  small and medium-sized manufacturing enterprises is of
		  great significance for transferring internal production
		  capacity, enhancing the level of opening up to the outside
		  world, and promoting the high-quality development of the
		  foreign trade industry.The development of AIGC (Artificial
		  Intelligence Generated Content) is not only a major
		  technological breakthrough in the field of artificial
		  intelligence, but also its highly efficient content
		  production drives the generation of new intelligent export
		  methods in the manufacturing industry. Through the mass
		  generation of text, the use of pre-training models as well
		  as the development and optimization of cue words, to the
		  manufacturing and exporting enterprises to output content
		  in line with SEO (Search Engine Optimization) rules,
		  through the optimization of internal and external chains to
		  improve search engine rankings, so as to enhance the
		  overseas visibility of the enterprise's products, and to
		  open up the sales of the products. Based on the mechanism
		  analysis of economics and management perspective, AIGC can
		  provide professional and in-depth website enhancement
		  diagnostic solutions and key optimization for foreign trade
		  enterprises on a regular basis through the revolution of
		  content production, so as to enhance the export efficiency
		  and competitiveness of foreign trade enterprises.
		  Therefore, the application of AIGC technology in foreign
		  trade enterprises should be promoted, and AIGC technology
		  providers should be encouraged to strengthen the training
		  of large models in different industries, so as to provide
		  specialized technical support for export enterprises.},
  booktitle	= {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater
		  Bay Area International Conference on Digital Economy and
		  Artificial Intelligence},
  pages		= {179–191},
  numpages	= {13},
  location	= {Hongkong, China},
  series	= {DEAI '24}
}

@Proceedings{	  10.1145/3643796,
  title		= {IDE '24: Proceedings of the 1st ACM/IEEE Workshop on
		  Integrated Development Environments},
  year		= {2024},
  isbn		= {9798400705809},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Despite the research community's desire to improve the
		  productivity of software developers, it is challenging for
		  research to move beyond papers into the everyday practice
		  of software development. Since IDEs are one of the most
		  widely used tools in developers' toolkit, they remain a
		  crucial venue for research to reach the practitioners. To
		  close the gap between research and adoption in practice, we
		  launched the first edition of the IDE Workshop.},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3637528.3671742,
  author	= {Jiang, Wenyuan and Wu, Wenwei and Zhang, Le and Yuan,
		  Zixuan and Xiang, Jian and Zhou, Jingbo and Xiong, Hui},
  title		= {Killing Two Birds with One Stone: Cross-modal Reinforced
		  Prompting for Graph and Language Tasks},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671742},
  doi		= {10.1145/3637528.3671742},
  abstract	= {In recent years, Graph Neural Networks (GNNs) and Large
		  Language Models (LLMs) have exhibited remarkable capability
		  in addressing different graph learning and natural language
		  tasks, respectively. Motivated by this, integrating LLMs
		  with GNNs has been increasingly studied to acquire
		  transferable knowledge across modalities, which leads to
		  improved empirical performance in language and graph
		  domains. However, existing studies mainly focused on a
		  single-domain scenario by designing complicated integration
		  techniques to manage multimodal data effectively.
		  Therefore, a concise and generic learning framework for
		  multi-domain tasks, i.e., graph and language domains, is
		  highly desired yet remains under-exploited due to two major
		  challenges. First, the language corpus of downstream tasks
		  differs significantly from graph data, making it hard to
		  bridge the knowledge gap between modalities. Second, not
		  all knowledge demonstrates immediate benefits for
		  downstream tasks, potentially introducing disruptive noise
		  to context-sensitive models like LLMs. To tackle these
		  challenges, we propose a novel plug-and-play framework for
		  incorporating a lightweight cross-domain prompting method
		  into both language and graph learning tasks. Specifically,
		  we first convert the textual input into a domain-scalable
		  prompt, which not only preserves the semantic and logical
		  contents of the textual input, but also highlights related
		  graph information as external knowledge for different
		  domains. Then, we develop a reinforcement learning-based
		  method to learn the optimal edge selection strategy for
		  useful knowledge extraction, which profoundly sharpens the
		  multi-domain model capabilities. In addition, we introduce
		  a joint multi-view optimization module to regularize
		  agent-level collaborative learning across two domains.
		  Finally, extensive empirical justifications over 23 public
		  and synthetic datasets demonstrate that our approach can be
		  applied to diverse multi-domain tasks more accurately,
		  robustly, and reasonably, and improve the performances of
		  the state-of-the-art graph and language models in different
		  learning paradigms.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1301–1312},
  numpages	= {12},
  keywords	= {graph neural networks, large language models, prompt
		  learning, reinforcement learning},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Proceedings{	  10.1145/3698387,
  title		= {SocialMeta '24: Proceedings of the Third International
		  Workshop on Social and Metaverse Computing, Sensing and
		  Networking},
  year		= {2024},
  isbn		= {9798400712999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Proceedings{	  10.1145/3640457,
  title		= {RecSys '24: Proceedings of the 18th ACM Conference on
		  Recommender Systems},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bari, Italy}
}

@Article{	  10.1145/3676279,
  author	= {Konstantinidis, Ioannis and Kapantai, Eleni and
		  Michailidis, Alexios and Deligiannis, Athanasios and
		  Berberidis, Christos and Magnisalis, Ioannis and
		  Peristeras, Vassilios},
  title		= {From document-centric to data-centric public service
		  provision},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {3},
  url		= {https://doi.org/10.1145/3676279},
  doi		= {10.1145/3676279},
  abstract	= {The profound digitization of public administration over
		  recent decades has not eliminated information exchange via
		  paper or electronic documents and certificates. We argue
		  that a paradigm shift from document-centric to data-centric
		  public service provision is needed and is feasible today
		  with the exploitation of emerging technologies. We explore
		  frameworks, architectures, benefits, and challenges in
		  transforming document-centric administration processes into
		  integrated, granular data exchange. A conceptual
		  architecture for public service provision is proposed to
		  extract preconditions from legislation, map the needed
		  evidence to the service requirements, standardize evidence
		  types, and integrate authoritative data sources. While
		  promoting efficiency, privacy, and innovation, this shift
		  faces technical and organizational challenges as the
		  analysis of the “National Registry of Administrative
		  Public Services” in Greece reveals. Further research on
		  aligning policies, upholding trust, and coordinating
		  institutional processes is warranted.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= sep,
  articleno	= {28},
  numpages	= {27},
  keywords	= {data, public service, digital transformation, AI, LLMs,
		  Knowledge Graphs}
}

@InProceedings{	  10.1145/3675417.3675513,
  author	= {Li, Shiye and Yi, Li},
  title		= {A Few-Shot Entity Relation Extraction Method in the Legal
		  Domain Based on Large Language Models},
  year		= {2024},
  isbn		= {9798400717147},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3675417.3675513},
  doi		= {10.1145/3675417.3675513},
  abstract	= {With the increasing transparency of judicial information,
		  extracting implicit legal information from a massive corpus
		  of legal documents becomes more academically valuable and
		  practically significant. Large Language Models (LLMs) have
		  demonstrated outstanding performance in many NLP tasks,
		  particularly in generative tasks. However, satisfactory
		  results are often elusive in vertical domains like legal
		  entity relation extraction tasks. Due to the scarcity of
		  well-annotated training data in the legal domain, and the
		  expensive and time-consuming nature of labeling such data,
		  research on few-shot learning becomes particularly crucial.
		  Leveraging the advantage of large models pre-trained on
		  extensive datasets, capable of acquiring vast prior
		  knowledge of various tasks and adapting quickly to new
		  tasks, this paper proposes a few-shot entity relation
		  extraction method in the legal domain based on large
		  language models. The proposed method is evaluated on two
		  publicly available legal entity relation extraction
		  datasets through relevant experiments. The research results
		  indicate that the proposed approach reduces the cost of
		  constructing training data and exhibits excellent
		  performance in few-shot legal entity relation extraction
		  tasks. The F1 score on two public datasets is improved by
		  2.8% and 3.1%, respectively, compared to traditional deep
		  learning models, while maintaining better generalization
		  capabilities.},
  booktitle	= {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater
		  Bay Area International Conference on Digital Economy and
		  Artificial Intelligence},
  pages		= {580–586},
  numpages	= {7},
  location	= {Hongkong, China},
  series	= {DEAI '24}
}

@Proceedings{	  10.1145/3673791,
  title		= {SIGIR-AP 2024: Proceedings of the 2024 Annual
		  International ACM SIGIR Conference on Research and
		  Development in Information Retrieval in the Asia Pacific
		  Region},
  year		= {2024},
  isbn		= {9798400707247},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the 2024 Annual
		  International ACM SIGIR Conference on Research and
		  Development in Information Retrieval in the Asia Pacific
		  Region - the Second SIGIR-AP, hosted at Waseda University
		  Nishiwaseda Campus, Tokyo, Japan, in December 2024.
		  Following the great success of the very first SIGIR-AP held
		  in Beijing in November 2023, we have worked very hard with
		  our organising team over a year so that SIGIR-AP 2024 will
		  live up to its high expectations.As noted in the SIGIR-AP
		  Charter and Bylaws, SIGIR-AP cares about sustainability,
		  and is fully hybrid so that participants may choose to
		  enjoy the conference online to avoid taking
		  earth-unfriendly longdistance flights. Moreover, we are
		  experimenting with a few novel approaches to running a
		  sustainable conference, thanks to the generous support from
		  the Tokyo Convention and Visitors Bureau: we will provide
		  sustainable seafood lunch boxes, as well as a conference
		  bag and a mug that are made from recycled plastic! In
		  addition, a substantial portion of the banquet dishes will
		  be vegetarian, which is known to be better for the
		  environment than meat dishes (See Greta Thunberg: The
		  Climate Book, p.249, Penguin Press, 2023).The SIGIR-AP 2024
		  features two keynotes from Japan: one by Dr. Momoe Makino
		  (Institute of Developing Economies Japan External Trade
		  Organization), titled Information Experiment: What Does
		  Empirical Microeconomics Tell Us? and the other by
		  Professor Sadao Kurohashi (National Institute of
		  Informatics) titled From Data Platforms to Knowledge
		  Infrastructure. We thank Dr. Makino and Professor Kurohashi
		  for boosting the brilliance of our conference program.},
  location	= {Tokyo, Japan}
}

@InProceedings{	  10.1145/3626772.3657904,
  author	= {Xie, Yuzhang and Lu, Jiaying and Ho, Joyce and Nahab, Fadi
		  and Hu, Xiao and Yang, Carl},
  title		= {PromptLink: Leveraging Large Language Models for
		  Cross-Source Biomedical Concept Linking},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657904},
  doi		= {10.1145/3626772.3657904},
  abstract	= {Linking (aligning) biomedical concepts across diverse data
		  sources enables various integrative analyses, but it is
		  challenging due to the discrepancies in concept naming
		  conventions. Various strategies have been developed to
		  overcome this challenge, such as those based on
		  string-matching rules, manually crafted thesauri, and
		  machine learning models. However, these methods are
		  constrained by limited prior biomedical knowledge and can
		  hardly generalize beyond the limited amounts of rules,
		  thesauri, or training samples. Recently, large language
		  models (LLMs) have exhibited impressive results in diverse
		  biomedical NLP tasks due to their unprecedentedly rich
		  prior knowledge and strong zero-shot prediction abilities.
		  However, LLMs suffer from issues including high costs,
		  limited context length, and unreliable predictions. In this
		  research, we propose PromptLink, a novel biomedical concept
		  linking framework that leverages LLMs. Empirical results on
		  the concept linking task between two EHR datasets and an
		  external biomedical KG demonstrate the effectiveness of
		  PromptLink. Furthermore, PromptLink is a generic framework
		  without reliance on additional prior knowledge, context, or
		  training data, making it well-suited for concept linking
		  across various types of data sources. The source code of
		  this study is available at
		  https://github.com/constantjxyz/PromptLink.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2589–2593},
  numpages	= {5},
  keywords	= {biomedical concept linking, few-shot prompting, large
		  language models for resource-constrained field, retrieve
		  &amp; re-rank},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Proceedings{	  10.1145/3677052,
  title		= {ICAIF '24: Proceedings of the 5th ACM International
		  Conference on AI in Finance},
  year		= {2024},
  isbn		= {9798400710810},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Brooklyn, NY, USA}
}

@InProceedings{	  10.1145/3589334.3645404,
  author	= {Baek, Jinheon and Chandrasekaran, Nirupama and Cucerzan,
		  Silviu and Herring, Allen and Jauhar, Sujay Kumar},
  title		= {Knowledge-Augmented Large Language Models for Personalized
		  Contextual Query Suggestion},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645404},
  doi		= {10.1145/3589334.3645404},
  abstract	= {Large Language Models (LLMs) excel at tackling various
		  natural language tasks. However, due to the significant
		  costs involved in re-training or fine-tuning them, they
		  remain largely static and difficult to personalize.
		  Nevertheless, a variety of applications could benefit from
		  generations that are tailored to users' preferences, goals,
		  and knowledge. Among them is web search, where knowing what
		  a user is trying to accomplish, what they care about, and
		  what they know can lead to improved search experiences. In
		  this work, we propose a novel and general approach that
		  augments an LLM with relevant context from users'
		  interaction histories with a search engine in order to
		  personalize its outputs. Specifically, we construct an
		  entity-centric knowledge store for each user based on their
		  search and browsing activities on the web, which is then
		  leveraged to provide contextually relevant LLM prompt
		  augmentations. This knowledge store is light-weight, since
		  it only produces user-specific aggregate projections of
		  interests and knowledge onto public knowledge graphs, and
		  leverages existing search log infrastructure, thereby
		  mitigating the privacy, compliance, and scalability
		  concerns associated with building deep user profiles for
		  personalization. We validate our approach on the task of
		  contextual query suggestion, which requires understanding
		  not only the user's current search context but also what
		  they historically know and care about. Through a number of
		  experiments based on human evaluation, we show that our
		  approach is significantly better than several other
		  LLM-powered baselines, generating query suggestions that
		  are contextually more relevant, personalized, and useful.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {3355–3366},
  numpages	= {12},
  keywords	= {contextual query suggestion, entity-centric knowledge,
		  large language models, personalization},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3688868,
  title		= {MCHM'24: Proceedings of the 1st International Workshop on
		  Multimedia Computing for Health and Medicine},
  year		= {2024},
  isbn		= {9798400711954},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to The 1st
		  International Workshop on Multimedia Computing for Health
		  and Medicine MCHM '24. This year we created the first
		  international workshop on multimedia computing for health
		  and medicine, a premier forum for presentation of research
		  results on leading edge issues of multimedia-based
		  health/medicine computing. The mission of the workshop is
		  to share novel multimedia computing solutions that fulfill
		  the needs of health and medicine problems. It gives
		  researchers a unique opportunity to share their
		  perspectives with others interested in multimedia computing
		  for health and medicine.},
  location	= {Melbourne VIC, Australia}
}

@InProceedings{	  10.1145/3637528.3671793,
  author	= {Gyurek, Croix and Talukder, Niloy and Hasan, Mohammad Al},
  title		= {Binder: Hierarchical Concept Representation through Order
		  Embedding of Binary Vectors},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671793},
  doi		= {10.1145/3637528.3671793},
  abstract	= {For natural language understanding and generation,
		  embedding concepts using an order-based representation is
		  an essential task. Unlike traditional point vector based
		  representation, an order-based representation imposes
		  geometric constraints on the representation vectors for
		  explicitly capturing various semantic relationships that
		  may exist between a pair of concepts. In existing
		  literature, several approaches on order-based embedding
		  have been proposed, mostly focusing on capturing
		  hierarchical relationships; examples include vectors in
		  Euclidean space, complex, Hyperbolic, order, and Box
		  Embedding. Box embedding creates region-based rich
		  representation of concepts, but along the process it
		  sacrifices simplicity, requiring a custom-made optimization
		  scheme for learning the representation. Hyperbolic
		  embedding improves embedding quality by exploiting the
		  ever-expanding property of Hyperbolic space, but it also
		  suffers from the same fate as box embedding as gradient
		  descent like optimization is not simple in the Hyperbolic
		  space. In this work, we propose Binder, a novel approach
		  for order-based representation. Binder uses binary vectors
		  for embedding, so the embedding vectors are compact with an
		  order of magnitude smaller footprint than other methods.
		  Binder uses a simple and efficient optimization scheme for
		  learning representation vectors with a linear time
		  complexity. Our comprehensive experimental results show
		  that Binder is very accurate, yielding competitive results
		  on the representation task. But Binder stands out from its
		  competitors on the transitive closure link prediction task
		  as it can learn concept embeddings just from the direct
		  edges, whereas all existing order-based approaches rely on
		  the indirect edges. In particular, Binder achieves a
		  whopping 70% higher F1-score than the second best method
		  (98.6% vs 29%) in our largest dataset, WordNet Nouns
		  (743,241 edges), when using only direct edges during
		  training.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {980–991},
  numpages	= {12},
  keywords	= {binary vector embedding, concept graph, hierarchical
		  embedding, order embedding},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3671151.3671324,
  author	= {Wang, Yaohui and Zhao, Yuxian and Hao, Qiang and Zhang,
		  Jian and Sun, Xiaohu and Lyu, Xueqiang},
  title		= {Question-and-answer intention classification in the coal
		  mine production field based on prompt learning},
  year		= {2024},
  isbn		= {9798400718106},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3671151.3671324},
  doi		= {10.1145/3671151.3671324},
  abstract	= {Intelligent Q&amp;A is an essential part of the
		  intelligent construction of coal mines. However, in
		  practical applications, there is a lack of Q&amp;A related
		  data sets in the field of coal mine production, and it is
		  difficult to obtain annotated data. The existing Q&amp;A
		  system in the field of coal mine production has problems
		  such as inability to solve polysemy issues and accurately
		  understand user needs. To solve these problems, this paper
		  introduces prompt learning into the field of coal mine
		  production for the first time for Q&amp;A intent
		  classification. By introducing multiple prompt templates to
		  fine-tune the BERT pre-trained model, the accuracy of
		  Q&amp;A intent classification is improved in the case of
		  low data volume. Experimental results show that after
		  introducing the prompt template, the accuracy, recall rate,
		  and F1 value of the model are increased by 0.66, 1.21, and
		  1.29 percentage points, respectively. In addition, we have
		  also conducted experiments in low-sample scenarios, proving
		  that prompt learning methods can quickly adapt to new
		  domain tasks in low-sample scenarios. The proposed method
		  has good application value in Q&amp;A intent classification
		  in the field of coal mine production.},
  booktitle	= {Proceedings of the 5th International Conference on
		  Computer Information and Big Data Applications},
  pages		= {992–998},
  numpages	= {7},
  location	= {Wuhan, China},
  series	= {CIBDA '24}
}

@Proceedings{	  10.1145/3675249,
  title		= {ICCMT '24: Proceedings of the 2024 International
		  Conference on Computer and Multimedia Technology},
  year		= {2024},
  isbn		= {9798400718267},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sanming, China}
}

@Proceedings{	  10.1145/3677045,
  title		= {NordiCHI '24 Adjunct: Adjunct Proceedings of the 2024
		  Nordic Conference on Human-Computer Interaction},
  year		= {2024},
  isbn		= {9798400709654},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Uppsala, Sweden}
}

@Article{	  10.1145/3681802,
  author	= {McDonald, Nora and Badillo-Urquiola, Karla and Razi,
		  Afsaneh and Seberger, John S. and Agosto, Denise E. and
		  Wisniewski, Pamela},
  title		= {AI Through Gen Z: Partnerships Toward New Research
		  Agendas},
  year		= {2024},
  issue_date	= {September - October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {31},
  number	= {5},
  issn		= {1072-5520},
  url		= {https://doi.org/10.1145/3681802},
  doi		= {10.1145/3681802},
  journal	= {Interactions},
  month		= aug,
  pages		= {28–31},
  numpages	= {4}
}

@Proceedings{	  10.1145/3696409,
  title		= {MMAsia '24: Proceedings of the 6th ACM International
		  Conference on Multimedia in Asia},
  year		= {2024},
  isbn		= {9798400712739},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3640794,
  title		= {CUI '24: Proceedings of the 6th ACM Conference on
		  Conversational User Interfaces},
  year		= {2024},
  isbn		= {9798400705113},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Luxembourg, Luxembourg}
}

@InProceedings{	  10.1145/3640794.3665537,
  author	= {Seaborn, Katie and Gessinger, Iona and Yoshida, Suzuka and
		  Cowan, Benjamin R. and Doyle, Philip R.},
  title		= {Cross-Cultural Validation of Partner Models for Voice User
		  Interfaces},
  year		= {2024},
  isbn		= {9798400705113},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640794.3665537},
  doi		= {10.1145/3640794.3665537},
  abstract	= {Recent research has begun to assess people’s perceptions
		  of voice user interfaces (VUIs) as dialogue partners,
		  termed partner models. Current self-report measures are
		  only available in English, limiting research to
		  English-speaking users. To improve the diversity of user
		  samples and contexts that inform partner modelling
		  research, we translated, localized, and evaluated the
		  Partner Modelling Questionnaire (PMQ) for non-English
		  speaking Western (German, n=185) and East Asian (Japanese,
		  n=198) cohorts where VUI use is popular. Through
		  confirmatory factor analysis (CFA), we find that the scale
		  produces equivalent levels of “goodness-to-fit” for
		  both our German and Japanese translations, confirming its
		  cross-cultural validity. Still, the structure of the
		  communicative flexibility factor did not replicate directly
		  across Western and East Asian cohorts. We discuss how our
		  translations can open up critical research on cultural
		  similarities and differences in partner model use and
		  design, whilst highlighting the challenges for ensuring
		  accurate translation across cultural contexts.},
  booktitle	= {Proceedings of the 6th ACM Conference on Conversational
		  User Interfaces},
  articleno	= {19},
  numpages	= {10},
  keywords	= {conversational user interfaces, cross-cultural research,
		  human-computer interaction, human-machine dialogue, mental
		  models, partner models, speech interfaces, voice user
		  interfaces},
  location	= {Luxembourg, Luxembourg},
  series	= {CUI '24}
}

@InProceedings{	  10.1145/3665601.3669846,
  author	= {Feng, Yanlin and Rahman, Sajjadur and Feng, Aaron and
		  Chen, Vincent and Kandogan, Eser},
  title		= {CMDBench: A Benchmark for Coarse-to-fine Multimodal Data
		  Discovery in Compound AI Systems},
  year		= {2024},
  isbn		= {9798400706943},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665601.3669846},
  doi		= {10.1145/3665601.3669846},
  abstract	= {Compound AI systems (CASs) that employ LLMs as agents to
		  accomplish knowledge-intensive tasks via interactions with
		  tools and data retrievers have garnered significant
		  interest within database and AI communities. While these
		  systems have the potential to supplement typical analysis
		  workflows of data analysts in enterprise data platforms,
		  unfortunately, CASs are subject to the same data discovery
		  challenges that analysts have encountered over the years
		  — silos of multimodal data sources, created across teams
		  and departments within an organization, make it difficult
		  to identify appropriate data sources for accomplishing the
		  task at hand. Existing data discovery benchmarks do not
		  model such multimodality and multiplicity of data sources.
		  Moreover, benchmarks of CASs prioritize only evaluating
		  end-to-end task performance. To catalyze research on
		  evaluating the data discovery performance of multimodal
		  data retrievers in CASs within a real-world setting, we
		  propose CMDBench, a benchmark modeling the complexity of
		  enterprise data platforms. We adapt existing datasets and
		  benchmarks in open-domain — from question answering and
		  complex reasoning tasks to natural language querying over
		  structured data — to evaluate coarse- and fine-grained
		  data discovery and task execution performance. Our
		  experiments reveal the impact of data retriever design on
		  downstream task performance — 46% drop in task accuracy
		  on average — across various modalities, data sources, and
		  task difficulty. The results indicate the need to develop
		  optimization strategies to identify appropriate LLM agents
		  and retrievers for efficient execution of CASs over
		  enterprise data.},
  booktitle	= {Proceedings of the Conference on Governance, Understanding
		  and Integration of Data for Effective and Responsible AI},
  pages		= {16–25},
  numpages	= {10},
  keywords	= {Benchmark, Compound AI Systems., Data Discovery, LLMs},
  location	= {Santiago, AA, Chile},
  series	= {GUIDE-AI '24}
}

@Proceedings{	  10.1145/3674399,
  title		= {ACM-TURC '24: Proceedings of the ACM Turing Award
		  Celebration Conference - China 2024},
  year		= {2024},
  isbn		= {9798400710117},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Changsha, China}
}

@InProceedings{	  10.1145/3664647.3680613,
  author	= {Deng, Ruoxi and Yu, Bin and Lu, Jinxuan and Zhou, Caixia
		  and Chen, Zhao-Min and Hu, Jie},
  title		= {Advancing Semantic Edge Detection through Cross-Modal
		  Knowledge Learning},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3680613},
  doi		= {10.1145/3664647.3680613},
  abstract	= {Semantic edge detection (SED) is pivotal for the precise
		  demarcation of object boundaries, yet it faces ongoing
		  challenges due to the prevalence of low-quality labels in
		  current methods. In this paper, we present a novel solution
		  to bolster SED through the encoding of both language and
		  image data. Distinct from antecedent language-driven
		  techniques, which predominantly utilize static elements
		  such as dataset labels, our method taps into the dynamic
		  language content that details the objects in each image and
		  their interrelations. By encoding this varied input, we
		  generate integrated features that utilize semantic insights
		  to refine the high-level image features and the ultimate
		  mask representations. This advancement improves the quality
		  of these features and elevates SED performance.
		  Experimental evaluation on benchmark datasets, including
		  SBD and Cityscape, showcases the efficacy of our method,
		  achieving leading ODS F-scores of 79.0 and 76.0,
		  respectively. Our approach signifies a notable advancement
		  in SED technology by seamlessly integrating multimodal
		  textual information, embracing both static and dynamic
		  aspects.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {4524–4532},
  numpages	= {9},
  keywords	= {contour detection, deep convolutional neural networks,
		  low-level vision, semantic edge detection},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3702038.3702094,
  author	= {Oliveira, Alberto Dumont Alves and Eler, Marcelo
		  Medeiros},
  title		= {Exploring Accessibility of Mobile Applications Through
		  User Feedback: Insights from App Reviews in a Systematic
		  Literature Review},
  year		= {2024},
  isbn		= {9798400712241},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3702038.3702094},
  doi		= {10.1145/3702038.3702094},
  abstract	= {Mobile applications have become essential tools for daily
		  activities, encompassing communication, productivity, and
		  entertainment, serving a diverse user base that includes
		  people with disabilities. Despite this broad usage,
		  ensuring accessibility for users with varying needs remains
		  a significant challenge. This paper presents a systematic
		  literature review (SLR) examining the accessibility of
		  mobile applications through user reviews. An initial search
		  across major academic digital libraries identified 638
		  papers, which were narrowed down to 16 key studies
		  published since 2013 based on specific inclusion and
		  exclusion criteria. The articles analyzed featured a wide
		  range of review counts, from 173 to over 179 million. Our
		  analysis reveals various purposes, methodologies, and
		  approaches that researchers have employed to explore
		  digital accessibility. The SLR identifies four main
		  strategies for collecting and analyzing accessibility
		  reviews, with commonly referenced standards including BBC
		  guidelines, Google Material Design, and WCAG. The results
		  also identify persistent barriers for users with
		  disabilities and synthesize 31 recommendations for future
		  research directions in areas such as machine learning and
		  the automatic extraction and classification of user
		  reviews. These findings underscore the critical importance
		  of integrating user feedback and perspectives into the
		  design and development of mobile applications to ensure
		  they are inclusive and accessible to all users.},
  booktitle	= {Proceedings of the XXIII Brazilian Symposium on Human
		  Factors in Computing Systems},
  articleno	= {56},
  numpages	= {15},
  keywords	= {accessibility reviews, app reviews, user reviews, mobile,
		  application, systematic literature review},
  location	= { },
  series	= {IHC '24}
}

@Proceedings{	  10.1145/3613904,
  title		= {CHI '24: Proceedings of the 2024 CHI Conference on Human
		  Factors in Computing Systems},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Honolulu, HI, USA}
}

@InProceedings{	  10.1145/3687123.3698284,
  author	= {Tsiligkaridis, Athanasios and Kalinowski, Nicholas and Li,
		  Zhongheng and Hou, Elizabeth},
  title		= {Encoding Agent Trajectories as Representations with
		  Sequence Transformers},
  year		= {2024},
  isbn		= {9798400711763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3687123.3698284},
  doi		= {10.1145/3687123.3698284},
  abstract	= {Spatiotemporal data faces many analogous challenges to
		  natural language text including the ordering of locations
		  (words) in a sequence, long range dependencies between
		  locations, and locations having multiple meanings. In this
		  work, we propose a novel model for representing high
		  dimensional spatiotemporal trajectories as sequences of
		  discrete locations and encoding them with a
		  Transformer-based neural network architecture. Similar to
		  language models, our Sequence Transformer for Agent
		  Representation Encodings (STARE) model can learn
		  representations and structure in trajectory data through
		  both supervisory tasks (e.g., classification), and
		  self-supervisory tasks (e.g., masked modelling). We present
		  experimental results on various synthetic and real
		  trajectory datasets and show that our proposed model can
		  learn meaningful encodings that are useful for many
		  downstream tasks including discriminating between labels
		  and indicating similarity between locations. Using these
		  encodings, we also learn relationships between agents and
		  locations present in spatiotemporal data.},
  booktitle	= {Proceedings of the 7th ACM SIGSPATIAL International
		  Workshop on AI for Geographic Knowledge Discovery},
  pages		= {38–49},
  numpages	= {12},
  keywords	= {Transformers, encoders, human mobility, spatiotemporal
		  data, trajectory modeling},
  location	= {Atlanta, GA, USA},
  series	= {GeoAI '24}
}

@Proceedings{	  10.1145/3675094,
  title		= {UbiComp '24: Companion of the 2024 on ACM International
		  Joint Conference on Pervasive and Ubiquitous Computing},
  year		= {2024},
  isbn		= {9798400710582},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to UbiComp/ISWC 2024, the companion program of two
		  premier conferences: The 2024 ACM International Joint
		  Conference on Pervasive and Ubiquitous Computing (UbiComp
		  2024) and the 2024 International Symposium on Wearable
		  Computers (ISWC 2024). UbiComp and ISWC are premier
		  interdisciplinary venues for international researchers,
		  designers, developers, practitioners and educators in the
		  field to present and discuss novel and impactful research
		  in interactive, mobile, wearable and ubiquitous computing.
		  The companion program has traditionally been a very
		  important part of the UbiComp/ISWC conference
		  series.UbiComp/ISWC 2024 is held from October 5 to 9, 2024
		  in Melbourne, Australia. Originally, UbiComp/ISWC was
		  scheduled to take place in Melbourne in 2021. However, due
		  to the significant impact of COVID-19, our community
		  decided to postpone conferences taking place in their
		  traditional form until last year, when UbiComp took place
		  as an in-person event in Mexico. Now, in 2024 we look to
		  consolidate the strength and ties in our community by
		  having another fully in-person event and hoping to welcome
		  a new generation of researchers to meet and explore the
		  wonderful people that make up our community.},
  location	= {Melbourne VIC, Australia}
}

@InProceedings{	  10.1145/3626772.3657705,
  author	= {Wang, Pancheng and Li, Shasha and Li, Dong and Long, Kehan
		  and Tang, Jintao and Wang, Ting},
  title		= {Disentangling Instructive Information from Ranked Multiple
		  Candidates for Multi-Document Scientific Summarization},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657705},
  doi		= {10.1145/3626772.3657705},
  abstract	= {Automatically condensing multiple topic-related scientific
		  papers into a succinct and concise summary is referred to
		  as Multi-Document Scientific Summarization (MDSS).
		  Currently, while commonly used abstractive MDSS methods can
		  generate flexible and coherent summaries, the difficulty in
		  handling global information and the lack of guidance during
		  decoding still make it challenging to generate better
		  summaries. To alleviate these two shortcomings, this paper
		  introduces summary candidates into MDSS, utilizing the
		  global information of the document set and additional
		  guidance from the summary candidates to guide the decoding
		  process. Our insights are twofold: Firstly, summary
		  candidates can provide instructive information from both
		  positive and negative perspectives, and secondly, selecting
		  higher-quality candidates from multiple options contributes
		  to producing better summaries. Drawing on the insights, we
		  propose a summary candidates fusion framework -
		  Disentangling Instructive information from Ranked
		  candidates (DIR) for MDSS. Specifically, DIR first uses a
		  specialized pairwise comparison method towards multiple
		  candidates to pick out those of higher quality. Then DIR
		  disentangles the instructive information of summary
		  candidates into positive and negative latent variables with
		  Conditional Variational Autoencoder. These variables are
		  further incorporated into the decoder to guide generation.
		  We evaluate our approach with three different types of
		  Transformer-based models and three different types of
		  candidates, and consistently observe noticeable performance
		  improvements according to automatic and human evaluation.
		  More analyses further demonstrate the effectiveness of our
		  model in handling global information and enhancing decoding
		  controllability.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2028–2037},
  numpages	= {10},
  keywords	= {disentangled representation learning, multi-document
		  scientific summarization, summary candidates, summary
		  ranking},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Article{	  10.1145/3695986,
  author	= {Zheng, Yu and Hao, Qianyue and Wang, Jingwei and Gao,
		  Changzheng and Chen, Jinwei and Jin, Depeng and Li, Yong},
  title		= {A Survey of Machine Learning for Urban Decision Making:
		  Applications in Planning, Transportation, and Healthcare},
  year		= {2024},
  issue_date	= {April 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3695986},
  doi		= {10.1145/3695986},
  abstract	= {Developing smart cities is vital for ensuring sustainable
		  development and improving human well-being. One critical
		  aspect of building smart cities is designing intelligent
		  methods to address various decision-making problems that
		  arise in urban areas. As machine learning techniques
		  continue to advance rapidly, a growing body of research has
		  been focused on utilizing these methods to achieve
		  intelligent urban decision-making. In this survey, we
		  conduct a systematic literature review on the application
		  of machine learning methods in urban decision-making, with
		  a focus on planning, transportation, and healthcare. First,
		  we provide a taxonomy based on typical applications of
		  machine learning methods for urban decision-making. We then
		  present background knowledge on these tasks and the machine
		  learning techniques that have been adopted to solve them.
		  Next, we examine the challenges and advantages of applying
		  machine learning in urban decision-making, including issues
		  related to urban complexity, urban heterogeneity, and
		  computational cost. Afterward and primarily, we elaborate
		  on the existing machine learning methods that aim at
		  solving urban decision-making tasks in planning,
		  transportation, and healthcare, highlighting their
		  strengths and limitations. Finally, we discuss open
		  problems and the future directions of applying machine
		  learning to enable intelligent urban decision-making, such
		  as developing foundation models and combining reinforcement
		  learning algorithms with human feedback. We hope this
		  survey can help researchers in related fields understand
		  the recent progress made in existing works, and inspire
		  novel applications of machine learning in smart cities.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {99},
  numpages	= {41},
  keywords	= {machine learning, urban planning, optimization, decision
		  making}
}

@InProceedings{	  10.1145/3627673.3680025,
  author	= {Huang, Jia-Hong and Yang, Chao-Chun and Shen, Yixian and
		  Pacces, Alessio M. and Kanoulas, Evangelos},
  title		= {Optimizing Numerical Estimation and Operational Efficiency
		  in the Legal Domain through Large Language Models},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680025},
  doi		= {10.1145/3627673.3680025},
  abstract	= {The legal landscape encompasses a wide array of lawsuit
		  types, presenting lawyers with challenges in delivering
		  timely and accurate information to clients, particularly
		  concerning critical aspects like potential imprisonment
		  duration or financial repercussions. Compounded by the
		  scarcity of legal experts, there's an urgent need to
		  enhance the efficiency of traditional legal workflows.
		  Recent advances in deep learning, especially Large Language
		  Models (LLMs), offer promising solutions to this challenge.
		  Leveraging LLMs' mathematical reasoning capabilities, we
		  propose a novel approach integrating LLM-based
		  methodologies with specially designed prompts to address
		  precision requirements in legal Artificial Intelligence
		  (LegalAI) applications. The proposed work seeks to bridge
		  the gap between traditional legal practices and modern
		  technological advancements, paving the way for a more
		  accessible, efficient, and equitable legal system. To
		  validate this method, we introduce a curated dataset
		  tailored to precision-oriented LegalAI tasks, serving as a
		  benchmark for evaluating LLM-based approaches. Extensive
		  experimentation confirms the efficacy of our methodology in
		  generating accurate numerical estimates within the legal
		  domain, emphasizing the role of LLMs in streamlining legal
		  processes and meeting the evolving demands of LegalAI.
		  Github:
		  https://github.com/Jhhuangkay/Optimizing-Numerical-Estimation-and-Operational-Efficiency-in-the-Legal-Domain.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4554–4562},
  numpages	= {9},
  keywords	= {large language models, precision-oriented legal artificial
		  intelligence, tailored prompt design},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3664647.3681472,
  author	= {Sun, Luoyi and Xu, Xuenan and Wu, Mengyue and Xie, Weidi},
  title		= {Auto-ACD: A Large-scale Dataset for Audio-Language
		  Representation Learning},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681472},
  doi		= {10.1145/3664647.3681472},
  abstract	= {Recently, the AI community has made significant strides in
		  developing powerful foundation models, driven by
		  large-scale multimodal datasets. However, for audio
		  representation learning, existing datasets suffer from
		  limitations in the following aspects: insufficient volume,
		  simplistic content, and arduous collection procedures. To
		  establish an audio dataset with high-quality captions, we
		  propose an innovative, automatic approach leveraging
		  multimodal inputs, such as video frames, audio streams.
		  Specifically, we construct a large-scale, high-quality,
		  audio-language dataset, named as Auto-ACD, comprising over
		  1.5M audio-text pairs. We exploit a series of pre-trained
		  models or APIs, to determine audio-visual synchronisation,
		  generate image captions, object detection, or audio tags
		  for specific videos. Subsequently, we employ LLM to
		  paraphrase a congruent caption for each audio, guided by
		  the extracted multi-modality clues. To demonstrate the
		  effectiveness of the proposed dataset, we train widely used
		  models on our dataset and show performance improvement on
		  various downstream tasks, for example, audio-language
		  retrieval, audio captioning, zero-shot classification. In
		  addition, we establish a novel benchmark with environmental
		  information and provide a benchmark for audio-text tasks.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {5025–5034},
  numpages	= {10},
  keywords	= {audio captioning, audio-language dataset, audio-language
		  representation learning},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Article{	  10.1145/3664615,
  author	= {Ji, Shaoxiong and Li, Xiaobo and Sun, Wei and Dong, Hang
		  and Taalas, Ara and Zhang, Yijia and Wu, Honghan and
		  Pitk\"{a}nen, Esa and Marttinen, Pekka},
  title		= {A Unified Review of Deep Learning for Automated Medical
		  Coding},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {12},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3664615},
  doi		= {10.1145/3664615},
  abstract	= {Automated medical coding, an essential task for healthcare
		  operation and delivery, makes unstructured data manageable
		  by predicting medical codes from clinical documents. Recent
		  advances in deep learning and natural language processing
		  have been widely applied to this task. However, deep
		  learning–based medical coding lacks a unified view of the
		  design of neural network architectures. This review
		  proposes a unified framework to provide a general
		  understanding of the building blocks of medical coding
		  models and summarizes recent advanced models under the
		  proposed framework. Our unified framework decomposes
		  medical coding into four main components, i.e., encoder
		  modules for text feature extraction, mechanisms for
		  building deep encoder architectures, decoder modules for
		  transforming hidden representations into medical codes, and
		  the usage of auxiliary information. Finally, we introduce
		  the benchmarks and real-world usage and discuss key
		  research challenges and future directions.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {306},
  numpages	= {41},
  keywords	= {Medical coding, deep learning, unified framework}
}

@InProceedings{	  10.1145/3637528.3671453,
  author	= {Zhang, Weijia and Han, Jindong and Xu, Zhao and Ni, Hang
		  and Liu, Hao and Xiong, Hui},
  title		= {Urban Foundation Models: A Survey},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671453},
  doi		= {10.1145/3637528.3671453},
  abstract	= {Machine learning techniques are now integral to the
		  advancement of intelligent urban services, playing a
		  crucial role in elevating the efficiency, sustainability,
		  and livability of urban environments. The recent emergence
		  of foundation models such as ChatGPT marks a revolutionary
		  shift in the fields of machine learning and artificial
		  intelligence. Their unparalleled capabilities in contextual
		  understanding, problem solving, and adaptability across a
		  wide range of tasks suggest that integrating these models
		  into urban domains could have a transformative impact on
		  the development of smart cities. Despite growing interest
		  in Urban Foundation Models (UFMs), this burgeoning field
		  faces challenges such as a lack of clear definitions and
		  systematic reviews. To this end, this paper first
		  introduces the concept of UFMs and discusses the unique
		  challenges involved in building them. We then propose a
		  data-centric taxonomy that categorizes and clarifies
		  current UFM-related works, based on urban data modalities
		  and types. Furthermore, we explore the application
		  landscape of UFMs, detailing their potential impact in
		  various urban contexts. Relevant papers and open-source
		  resources have been collated and are continuously updated
		  at:
		  https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6633–6643},
  numpages	= {11},
  keywords	= {geospatial artificial intelligence, spatio-temporal data
		  mining, urban foundation models, urban general
		  intelligence},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3673229,
  author	= {Taylor, Jordan and Deng, Wesley Hanwen and Holstein,
		  Kenneth and Fox, Sarah and Zhu, Haiyi},
  title		= {Carefully Unmaking the “Marginalized User”: A
		  Diffractive Analysis of a Gay Online Community},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {31},
  number	= {6},
  issn		= {1073-0516},
  url		= {https://doi.org/10.1145/3673229},
  doi		= {10.1145/3673229},
  abstract	= {HCI scholars are increasingly engaging in research about
		  “marginalized groups,” such as LGBTQ+ people. While
		  normative habitual readings of marginalized people in HCI
		  often highlight real problems, this work has been
		  criticized for flattening heterogeneous experiences and
		  overemphasizing harms. Some have advocated for expanding
		  how we approach research on marginalized people (e.g.,
		  assets-based design, the everyday, and joy). Sensitized by
		  unmaking literature, we explore this tension between
		  conditions, experiences, and representations of marginality
		  in HCI scholarship. To do so, we perform a diffractive
		  analysis of posts in a gay online community by bringing two
		  readings of the same data together: a normative habitual
		  reading of marginalization and an expanded reading. By
		  examining the relationship between empirical material and
		  its representations by HCI researchers, we explore how to
		  carefully unmake HCI research, thus maintaining and
		  repairing our research community. We discuss the political
		  and designerly implications of different readings of
		  marginalized people and offer considerations for attending
		  to the processes and afterlives of HCI research.},
  journal	= {ACM Trans. Comput.-Hum. Interact.},
  month		= dec,
  articleno	= {81},
  numpages	= {30},
  keywords	= {Unmaking, Diffraction, Marginalized Groups, Marginalized
		  Communities, Gay Men, LGBTQ, LGBTQ+ People}
}

@InProceedings{	  10.1145/3589335.3641306,
  author	= {Mao, Haitao and Zhao, Jianan and He, Xiaoxin and Chen,
		  Zhikai and Huang, Qian and Zhu, Zhaocheng and Tang, Jian
		  and Bronstein, Micheal and Bresson, Xavier and Hooi, Bryan
		  and Zhang, Haiyang and Tang, Xianfeng and Chen, Luo and
		  Tang, Jiliang},
  title		= {The 1st International Workshop on Graph Foundation Models
		  (GFM)},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3641306},
  doi		= {10.1145/3589335.3641306},
  abstract	= {Foundation models such as GPT-4 for natural language
		  processing (NLP), Flamingo for computer vision (CV), have
		  set new benchmarks in AI by delivering state-of-the-art
		  results across various tasks with minimal task-specific
		  data. Despite their success, the application of these
		  models to the graph domain is challenging due to the
		  relational nature of graph-structured data. To address this
		  gap, we propose the Graph Foundation Model (GFM) Workshop,
		  the first workshop for GFMs, dedicated to exploring the
		  adaptation and development of foundation models
		  specifically designed for graph data. The GFM workshop
		  focuses on two critical questions: (1) How can the
		  underlying capabilities of existing foundation models be
		  effectively applied to graph data? (2) What foundational
		  principles should guide the creation of models tailored to
		  the graph domain? Through a curated set of panel sections,
		  keynote talks, and paper presentations, our workshop
		  intends to catalyze innovative approaches and theoretical
		  frameworks for Graph Foundation Models (GFMs). We target a
		  broad audience, encompassing researchers, practitioners,
		  and students, and aim to lay the groundwork for the next
		  wave of breakthroughs in integrating graph data with
		  foundation models.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1789–1792},
  numpages	= {4},
  keywords	= {data mining, foundation model, graph machine learning},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3687123,
  title		= {GeoAI '24: Proceedings of the 7th ACM SIGSPATIAL
		  International Workshop on AI for Geographic Knowledge
		  Discovery},
  year		= {2024},
  isbn		= {9798400711763},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Advances in artificial intelligence, hardware
		  accelerators, and data processing architectures continue to
		  reach the geospatial information sciences, with a
		  transformative impact on many societal challenges. Recent
		  breakthroughs in deep learning have brought forward an
		  automated capability to learn representational features
		  from massive and complex data, including text, images, and
		  videos. In tandem, rapid innovations in sensing
		  technologies are supporting the collection of geospatial
		  data in even higher resolution and throughput, supporting
		  the observation, mapping, and analysis of different
		  events/phenomena over the Earth's surface with
		  unprecedented detail. Combined, these developments are
		  offering the potential for breakthroughs in geographic
		  knowledge discovery, impacting decision-making in areas
		  such as humanitarian mapping, intelligent transport
		  systems, urban expansion analysis, health data analysis and
		  epidemiology, the study of climate change, handling natural
		  disasters, the general monitoring of the Earth's surface,
		  and achieving sustainability.},
  location	= {Atlanta, GA, USA}
}

@InProceedings{	  10.1145/3589335.3651242,
  author	= {Lian, Jianxun and Lei, Yuxuan and Huang, Xu and Yao, Jing
		  and Xu, Wei and Xie, Xing},
  title		= {RecAI: Leveraging Large Language Models for
		  Next-Generation Recommender Systems},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651242},
  doi		= {10.1145/3589335.3651242},
  abstract	= {This paper introduces RecAI, a practical toolkit designed
		  to augment or even revolutionize recommender systems with
		  the advanced capabilities of Large Language Models (LLMs).
		  RecAI provides a suite of tools, including Recommender AI
		  Agent, Recommendation-oriented Language Models, Knowledge
		  Plugin, RecExplainer, and Evaluator, to facilitate the
		  integration of LLMs into recommender systems from
		  multifaceted perspectives. The new generation of
		  recommender systems, empowered by LLMs, are expected to be
		  more versatile, explainable, conversational, and
		  controllable, paving the way for more intelligent and
		  user-centric recommendation experiences. We hope the
		  open-source of RecAI can help accelerate evolution of new
		  advanced recommender systems. The source code of RecAI is
		  available at https://github.com/microsoft/RecAI.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1031–1034},
  numpages	= {4},
  keywords	= {large language models, recommender systems},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3614419.3644001,
  author	= {Pl\"{o}tzky, Florian and Kiehne, Niklas and Balke,
		  Wolf-Tilo},
  title		= {Lost in Recursion: Mining Rich Event Semantics in
		  Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400703348},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3614419.3644001},
  doi		= {10.1145/3614419.3644001},
  abstract	= {Our world is shaped by events of various complexity. This
		  includes both small-scale local events like local farmer
		  markets and large complex events like political and
		  military conflicts. The latter are typically not observed
		  directly but through the lenses of intermediaries like
		  newspapers or social media. In other words, we do not
		  witness the unfolding of such events directly but are
		  confronted with narratives surrounding them. Such
		  narratives capture different aspects of a complex event and
		  may also differ with respect to the narrator. Thus, they
		  provide a rich semantics concerning real-world events. In
		  this paper, we show how narratives concerning complex
		  events can be constructed and utilized. We provide a formal
		  representation of narratives based on recursive nodes to
		  represent multiple levels of detail and discuss how
		  narratives can be bound to event-centric knowledge graphs.
		  Additionally, we provide an algorithm based on incremental
		  prompting techniques that mines such narratives from texts
		  to account for different perspectives on complex events.
		  Finally, we show the effectiveness and future research
		  directions in a proof of concept.},
  booktitle	= {Proceedings of the 16th ACM Web Science Conference},
  pages		= {354–364},
  numpages	= {11},
  keywords	= {Events, Narratives, Recursive Narrative Mining},
  location	= {Stuttgart, Germany},
  series	= {WEBSCI '24}
}

@InProceedings{	  10.1145/3637528.3671678,
  author	= {Hu, Qi and Li, Haoran and Bai, Jiaxin and Wang, Zihao and
		  Song, Yangqiu},
  title		= {Privacy-Preserved Neural Graph Databases},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671678},
  doi		= {10.1145/3637528.3671678},
  abstract	= {In the era of large language models (LLMs), efficient and
		  accurate data retrieval has become increasingly crucial for
		  the use of domain-specific or private data in the retrieval
		  augmented generation (RAG). Neural graph databases (NGDBs)
		  have emerged as a powerful paradigm that combines the
		  strengths of graph databases (GDBs) and neural networks to
		  enable efficient storage, retrieval, and analysis of
		  graph-structured data which can be adaptively trained with
		  LLMs. The usage of neural embedding storage and Complex
		  neural logical Query Answering (CQA) provides NGDBs with
		  generalization ability. When the graph is incomplete, by
		  extracting latent patterns and representations, neural
		  graph databases can fill gaps in the graph structure,
		  revealing hidden relationships and enabling accurate query
		  answering. Nevertheless, this capability comes with
		  inherent trade-offs, as it introduces additional privacy
		  risks to the domain-specific or private databases.
		  Malicious attackers can infer more sensitive information in
		  the database using well-designed queries such as from the
		  answer sets of where Turing Award winners born before 1950
		  and after 1940 lived, the living places of Turing Award
		  winner Hinton are probably exposed, although the living
		  places may have been deleted in the training stage due to
		  the privacy concerns. In this work, we propose a
		  privacy-preserved neural graph database (P-NGDB) framework
		  to alleviate the risks of privacy leakage in NGDBs. We
		  introduce adversarial training techniques in the training
		  stage to enforce the NGDBs to generate indistinguishable
		  answers when queried with private information, enhancing
		  the difficulty of inferring sensitive information through
		  combinations of multiple innocuous queries. Extensive
		  experimental results on three datasets show that our
		  framework can effectively protect private information in
		  the graph database while delivering high-quality public
		  answers responses to queries. The code is available at
		  https://github.com/HKUST-KnowComp/PrivateNGDB.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1108–1118},
  numpages	= {11},
  keywords	= {complex query answering (cqa), knowledge graphs (kgs),
		  neural graph databases (ngdbs), privacy preserving},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.14778/3654621.3654639,
  author	= {He, Wenjia and Sabek, Ibrahim and Lou, Yuze and Cafarella,
		  Michael},
  title		= {Optimizing Video Selection LIMIT Queries with Commonsense
		  Knowledge},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {7},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3654621.3654639},
  doi		= {10.14778/3654621.3654639},
  abstract	= {Video is becoming a major part of contemporary data
		  collection. It is increasingly important to process video
		  selection queries --- selecting videos that contain target
		  objects. Advances in neural networks allow us to detect the
		  objects in an image, and thereby offer query systems to
		  examine the content of the video. Unfortunately, neural
		  network-based approaches have long inference times.
		  Processing this type of query through a standard scan would
		  be time-consuming and would involve applying complex
		  detectors to numerous irrelevant videos. It is tempting to
		  try to improve query times by computing an index in
		  advance. But unfortunately, many frames will never be
		  beneficial for any query. Time spent processing them,
		  whether at index time or at query time, is simply wasted
		  computation.We propose a novel index mechanism to optimize
		  video selection queries with commonsense knowledge.
		  Commonsense knowledge consists of fundamental information
		  about the world, such as the fact that a tennis racket is a
		  tool designed for hitting a tennis ball. To save
		  computation, an inexpensive but lossy index can be
		  intentionally created, but this may result in missed target
		  objects and suboptimal query time performance. Our
		  mechanism addresses this issue by constructing
		  probabilistic models from commonsense knowledge to patch
		  the lossy index and then prioritizing predicate-related
		  videos at query time. This method can achieve significant
		  performance improvements comparable to those of a full
		  index while keeping the construction costs of a lossy
		  index. We describe our prototype system, Paine, plus
		  experiments on two video corpora. We show our best
		  optimization method can process up to 97.79% fewer videos
		  compared to baselines. Even the model constructed without
		  any video content can yield a 75.39% improvement over
		  baselines.},
  journal	= {Proc. VLDB Endow.},
  month		= mar,
  pages		= {1751–1764},
  numpages	= {14}
}

@InProceedings{	  10.1145/3627673.3679917,
  author	= {Wang, Xiaotong and Liu, Xuanning and Zhong, Shuai and
		  Chen, Xinming and Wu, Bin},
  title		= {Enhancing Temporal and Geographical Named Entity
		  Recognition in Chinese Ancient Texts with External
		  Time-series Knowledge Bases},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679917},
  doi		= {10.1145/3627673.3679917},
  abstract	= {In the field of ancient Chinese text, extracting and
		  analysing temporal and geographic information are crucial
		  for understanding the personal experiences of historical
		  figures, the development of historical events, and the
		  overall historical background. Currently, named entity
		  recognition(NER) strategies such as BERT+CRF are used to
		  extract temporal and geographic information from ancient
		  Chinese text. However, ancient Chinese text covers a vast
		  time span, and the temporal and geographic entities
		  constantly evolve and change, making it difficult to
		  extract these entities from text. This paper proposes a
		  temporal and geographic extraction model for ancient
		  Chinese text, enhanced by time-series external knowledge
		  base. The extraction of proprietary nouns and general
		  structures are divided into two independent networks. An
		  external database is applied to enhance extraction of
		  proprietary nouns and reduce noise for general structure
		  inference. We constructed address trees and chronological
		  tables containing commonly used places and time-related
		  keywords from different periods and collected 12,000 texts
		  spanning 3,000 years for extensive training. Overall, our
		  research highlights the importance of external knowledge
		  base for ancient Chinese NER, and provides new ideas for
		  research in related fields.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4107–4112},
  numpages	= {6},
  keywords	= {ancient Chinese texts, external knowledge enhancement,
		  named entity recognition, time-series knowledge base},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3643834.3661498,
  author	= {Sivertsen, Christian and L\o{}vlie, Anders Sundnes},
  title		= {Exploring Aesthetic Qualities of Deep Generative Models
		  through Technological (Art) Mediation},
  year		= {2024},
  isbn		= {9798400705830},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643834.3661498},
  doi		= {10.1145/3643834.3661498},
  abstract	= {Deep Generative Models (DGM) have had a great impact both
		  on visual art and broader visual culture. In this
		  research-through-design project we investigate the use of a
		  DGM for helping museum visitors explore the aesthetics of
		  Edvard Munch’s art. We designed and built an interactive
		  drawing table that allows a user to explore a StyleGAN
		  model trained on sketches by Edvard Munch. The paper makes
		  two novel contributions: 1. It presents a system that
		  allows users to interact with a DGM by drawing on paper
		  (rather than the typical text prompts used by most current
		  systems). 2. We demonstrate how this mode and quality of
		  interaction establish a unique perspective on Munch’s
		  drawings as a practice. Through qualitative evaluation, we
		  discuss how this setup led users towards a specific
		  hermeneutic drawing strategy that enables building
		  competency with the model and by proxy the data it is
		  trained on. We suggest that the resulting interaction may
		  contribute to an "education of attention" helping museum
		  visitors to become attentive to certain visual qualities in
		  Munch’s drawing practice. Finally, we discuss how the
		  concepts of technological mediation and relationality are
		  useful for designing how the output of a DGM is understood
		  by its users.},
  booktitle	= {Proceedings of the 2024 ACM Designing Interactive Systems
		  Conference},
  pages		= {2738–2752},
  numpages	= {15},
  keywords	= {aesthetics, deep generative model, drawing, fine art,
		  interaction design, machine learning, postphenomenology,
		  stylegan},
  location	= {Copenhagen, Denmark},
  series	= {DIS '24}
}

@InProceedings{	  10.1145/3630106.3658917,
  author	= {Srinivasan, Ramya},
  title		= {To See or Not to See: Understanding the Tensions of
		  Algorithmic Curation for Visual Arts},
  year		= {2024},
  isbn		= {9798400704505},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3630106.3658917},
  doi		= {10.1145/3630106.3658917},
  abstract	= {Algorithmic recommendation is one of the most popular
		  applications of machine learning (ML) systems. While the
		  implication of algorithmic recommendation has been studied
		  in the context of high-stakes domains such as finance and
		  healthcare, there has been very little focus in
		  understanding its impacts with respect to the arts domain.
		  Given that ML is increasingly finding place in the arts
		  domain such as in generative arts and content analysis, in
		  this paper, we examine the tensions of algorithmic curation
		  in the context of visual arts. Through case studies, we
		  describe how curatorial algorithms that are oblivious of
		  broader socio-cultural contexts could potentially result in
		  ethical concerns such as over-representation and
		  misattribution, to name a few. Towards addressing some of
		  these concerns, the paper offers design guidelines.
		  Specifically, the paper outlines repair strategies that
		  suggest ways 1) to engage with cultural stakeholders in
		  building visual art curatorial algorithms, 2) to unlearn
		  biases embedded in digital artworks and their meta-data,
		  and 3) emphasize the need to establish regulatory norms
		  specific to the use of ML in visual art curation. Taking
		  cue from the process employed by artwork curators, the
		  paper also describes how authenticity can be prioritized by
		  re-calibrating visual art curatorial algorithms. The paper
		  also suggest ways through which the potential of
		  state-of-the-art ML curatorial algorithms can be
		  re-imagined towards empowering the audience of artworks. We
		  hope the insights presented in the paper spark
		  interdisciplinary discussions and pave way for fostering
		  reformation in algorithmic curation of visual arts.},
  booktitle	= {Proceedings of the 2024 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {444–455},
  numpages	= {12},
  keywords	= {algorithmic recommendation, case studies, curation,
		  machine learning, visual arts},
  location	= {Rio de Janeiro, Brazil},
  series	= {FAccT '24}
}

@InProceedings{	  10.1145/3640457.3688191,
  author	= {Xie, Zhouhang and Wu, Junda and Jeon, Hyunsik and He,
		  Zhankui and Steck, Harald and Jha, Rahul and Liang, Dawen
		  and Kallus, Nathan and Mcauley, Julian},
  title		= {Neighborhood-Based Collaborative Filtering for
		  Conversational Recommendation},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688191},
  doi		= {10.1145/3640457.3688191},
  abstract	= {Conversational recommender systems (CRS) should understand
		  users’ expressed interests, which are frequently
		  semantically rich and knowledge-intensive. Prior works
		  attempt to address this challenge by using external
		  knowledge bases or parametric knowledge in large language
		  models (LLMs). In this paper, we study a complementary
		  solution, exploiting item knowledge in the training data.
		  We hypothesize that many inference-time user requests can
		  be answered by reusing popular crowd-written answers
		  associated with similar training queries. Following this
		  intuition, we define a class of neighborhood-based CRS that
		  makes recommendations by identifying items commonly
		  associated with similar training dialogue contexts.
		  Experiments on Inspired, Redial, and Reddit-Movie
		  benchmarks show our method outperforms state-of-the-art
		  LLMs with 2 billion parameters, and offers on-par
		  performance to 7 billion parameter models while using over
		  170 times less GPU memory. We also show neighborhood and
		  model-based predictions can be combined to achieve further
		  performance improvements1.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {1045–1050},
  numpages	= {6},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@Article{	  10.1145/3702315,
  author	= {Fan, Wenfei and Pang, Kehan and Lu, Ping and Tian, Chao},
  title		= {Making It Tractable to Detect and Correct Errors in
		  Graphs},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {49},
  number	= {4},
  issn		= {0362-5915},
  url		= {https://doi.org/10.1145/3702315},
  doi		= {10.1145/3702315},
  abstract	= {This article develops Hercules, a system for entity
		  resolution (ER), conflict resolution (CR), timeliness
		  deduction (TD), and missing value/link imputation (MI) in
		  graphs. It proposes GCR+s, a class of graph cleaning rules
		  (GCR) that support not only predicates for ER and CR but
		  also temporal orders to deduce timeliness and data
		  extraction to impute missing data. As opposed to previous
		  graph rules, GCR+s are defined with a dual graph pattern to
		  accommodate irregular structures of schemaless graphs and
		  adopt patterns of a star form to reduce the complexity. We
		  show that while the implication and satisfiability problems
		  are intractable for GCR+s, it is in polynomial time to
		  detect and correct errors with GCR+s. Underlying Hercules,
		  we train a ranking model to predict the temporal orders on
		  attributes and embed it as a predicate of GCR+s. We provide
		  an algorithm for discovering GCR+s by combining the
		  generations of patterns and predicates. We also develop a
		  method for conducting ER, CR, TD, and MI in the same
		  process to improve the overall quality of graphs by
		  leveraging their interactions and chasing with GCR+s; we
		  show that the method has the Church–Rosser property under
		  certain conditions. Using real-life and synthetic graphs,
		  we empirically verify that Hercules is 53% more accurate
		  than the state-of-the-art graph cleaning systems and
		  performs comparably in efficiency and scalability.},
  journal	= {ACM Trans. Database Syst.},
  month		= dec,
  articleno	= {16},
  numpages	= {75},
  keywords	= {Entity resolution, conflict resolution, timeliness
		  deduction, missing data imputation, graph cleaning rules}
}

@Proceedings{	  10.1145/3639474,
  title		= {ICSE-SEET '24: Proceedings of the 46th International
		  Conference on Software Engineering: Software Engineering
		  Education and Training},
  year		= {2024},
  isbn		= {9798400704987},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3677454,
  title		= {ARAEML '24: Proceedings of the 2024 International
		  Conference on Advanced Robotics, Automation Engineering and
		  Machine Learning},
  year		= {2024},
  isbn		= {9798400717116},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Article{	  10.1145/3664930,
  author	= {Fan, Lizhou and Li, Lingyao and Ma, Zihui and Lee, Sanggyu
		  and Yu, Huizi and Hemphill, Libby},
  title		= {A Bibliometric Review of Large Language Models Research
		  from 2017 to 2023},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {5},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3664930},
  doi		= {10.1145/3664930},
  abstract	= {Large language models (LLMs), such as OpenAI's Generative
		  Pre-trained Transformer (GPT), are a class of language
		  models that have demonstrated outstanding performance
		  across a range of natural language processing (NLP) tasks.
		  LLMs have become a highly sought-after research area
		  because of their ability to generate human-like language
		  and their potential to revolutionize science and
		  technology. In this study, we conduct bibliometric and
		  discourse analyses of scholarly literature on LLMs.
		  Synthesizing over 5,000 publications, this article serves
		  as a roadmap for researchers, practitioners, and
		  policymakers to navigate the current landscape of LLMs
		  research. We present the research trends from 2017 to early
		  2023, identifying patterns in research paradigms and
		  collaborations. We start with analyzing the core algorithm
		  developments and NLP tasks that are fundamental in LLMs
		  research. We then investigate the applications of LLMs in
		  various fields and domains, including medicine,
		  engineering, social science, and humanities. Our review
		  also reveals the dynamic, fast-paced evolution of LLMs
		  research. Overall, this article offers valuable insights
		  into the current state, impact, and potential of LLMs
		  research and its applications.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  articleno	= {91},
  numpages	= {25},
  keywords	= {Bibliometric analysis, large language models, discourse
		  analysis, scholarly collaboration networks, topic
		  modeling}
}

@InProceedings{	  10.1145/3652620.3687806,
  author	= {Burgue\~{n}o, Lola and Keet, Maria and Kienzle, J\"{o}rg
		  and Michael, Judith and Babur, \"{O}nder},
  title		= {A Human Behavior Exploration Approach Using LLMs for
		  Cyber-Physical Systems},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687806},
  doi		= {10.1145/3652620.3687806},
  abstract	= {In the early phases of Cyber-Physical Systems (CPS)
		  development, scoping human behavior plays a significant
		  role, especially when interactions extend beyond expected
		  behavior. Here, it is especially challenging to develop
		  cases that capture the full spectrum of human behavior. Up
		  to now, identifying such behavior of humans remains a task
		  for domain experts. We explore how one can use Large
		  Languages Models (LLMs) in the design phase of systems to
		  provide additional information about human-CPS interaction.
		  Our approach proposes a preliminary ontology describing a
		  hierarchy of types of behavior and relevant CPS components
		  as input for prompt templates. It uses them to generate
		  parts of human behavior descriptions, as well as a canned
		  prompt with one variable about behavior. For demonstration,
		  we take a smart building with a Home Energy System as the
		  use case.An initial user evaluation shows that the behavior
		  descriptions generated with standard and ontology-driven
		  prompts complement each other and are useful when assisting
		  humans. The discovered uncommon behaviors can be used to
		  complete interaction scenarios that eventually result in a
		  more robust CPS implementation.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {578–586},
  numpages	= {9},
  keywords	= {human behavior, large language models, cyber-physical
		  systems, user scenario, digital twin},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Article{	  10.1145/3656579,
  author	= {Liang, Wanying and Meo, Pasquale De and Tang, Yong and
		  Zhu, Jia},
  title		= {A Survey of Multi-modal Knowledge Graphs: Technologies and
		  Trends},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {11},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3656579},
  doi		= {10.1145/3656579},
  abstract	= {In recent years, Knowledge Graphs (KGs) have played a
		  crucial role in the development of advanced
		  knowledge-intensive applications, such as recommender
		  systems and semantic search. However, the human sensory
		  system is inherently multi-modal, as objects around us are
		  often represented by a combination of multiple signals,
		  such as visual and textual. Consequently, Multi-modal
		  Knowledge Graphs (MMKGs), which combine structured
		  knowledge representation with multiple modalities,
		  represent a powerful extension of KGs. Although MMKGs can
		  handle certain types of tasks (e.g., visual query
		  answering) or queries that standard KGs cannot process, and
		  they can effectively tackle some standard problems (e.g.,
		  entity alignment), we lack a widely accepted definition of
		  MMKG. In this survey, we provide a rigorous definition of
		  MMKGs along with a classification scheme based on how
		  existing approaches address four fundamental challenges:
		  representation, fusion, alignment, and translation, which
		  are crucial to improving an MMKG. Our classification scheme
		  is flexible and allows for easy incorporation of new
		  approaches, as well as a comparison of two approaches in
		  terms of how they address one of the fundamental challenges
		  mentioned above. As the first comprehensive survey of MMKG,
		  this article aims at inspiring and provide a reference for
		  relevant researchers in the field of Artificial
		  Intelligence.},
  journal	= {ACM Comput. Surv.},
  month		= jun,
  articleno	= {273},
  numpages	= {41},
  keywords	= {Multi-modal knowledge graphs, four fundamental challenges,
		  pre-training in MMKGs}
}

@Proceedings{	  10.1145/3641234,
  title		= {SIGGRAPH '24: ACM SIGGRAPH 2024 Posters},
  year		= {2024},
  isbn		= {9798400705168},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Denver, CO, USA}
}

@Proceedings{	  10.1145/3614419,
  title		= {WEBSCI '24: Proceedings of the 16th ACM Web Science
		  Conference},
  year		= {2024},
  isbn		= {9798400703348},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Stuttgart, Germany}
}

@InProceedings{	  10.1145/3676581.3676593,
  author	= {Zhao, Xiaoyan and He, Yao and Gao, Yankun and Luo, Xu and
		  Ke, Wenjun},
  title		= {Techniques for Fine-Grained Analysis of Scientific and
		  Technological Intelligence},
  year		= {2024},
  isbn		= {9798400716898},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3676581.3676593},
  doi		= {10.1145/3676581.3676593},
  abstract	= {In today's world, scientific and technological
		  intelligence information presents challenges such as
		  insufficient annotated samples in interdisciplinary and
		  specialized fields, the continuous emergence of new
		  technologies, and complex interwoven dependencies of
		  technical points. These aspects pose significant obstacles
		  for the fine-grained acquisition and analysis of scientific
		  and technological intelligence. To address these issues,
		  this paper focuses on three areas: domain-aware generation
		  of scientific and technological intelligence samples,
		  extraction of intelligence entity relationships based on
		  continual learning, and mining of technological dependency
		  chains using causal graphs. This research aims to provide
		  foundational techniques and algorithms for the acquisition
		  and analysis of fine-grained scientific and technological
		  intelligence, thereby enhancing our nation's capability in
		  technological trend assessment and development planning in
		  the context of global competition.},
  booktitle	= {Proceedings of the 2024 2nd International Conference on
		  Communications, Computing and Artificial Intelligence},
  pages		= {64–70},
  numpages	= {7},
  keywords	= {dependency chain mining, relationship extraction, sample
		  generation},
  location	= {Jeju, Republic of Korea},
  series	= {CCCAI '24}
}

@InProceedings{	  10.1145/3627673.3679783,
  author	= {Anand, Avinash and Nair, Ashwin R and Prasad, Kritarth and
		  Narayan, Vrinda and Lal, Naman and Mahata, Debanjan and
		  Singla, Yaman K and Shah, Rajiv Ratn},
  title		= {Advances in Citation Text Generation: Leveraging
		  Multi-Source Seq2Seq Models and Large Language Models},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679783},
  doi		= {10.1145/3627673.3679783},
  abstract	= {Citation Text Generation (CTG) in scientific documents
		  often relies on standard summarization techniques, which
		  may not fully capture the nuanced relationship between the
		  citing and cited papers. To address this, we present a
		  Multi-Source Citation Text Generation (M-CTG) architecture,
		  leveraging a Seq2Seq transformer framework enhanced with
		  keyphrase embeddings, graph embeddings, and text
		  representations. This approach aims to produce more
		  contextually relevant and accurate citation texts by
		  integrating multiple sources of information. Our
		  methodology is tested using the newly created CTG-S2ORC
		  dataset, consisting of English-language computer science
		  research papers. In a comparative analysis, we explore the
		  performance of traditional Language Models (LMs) and
		  demonstrate how Large Language Models (LLMs), particularly
		  when integrated with various prompting techniques and
		  Knowledge Graphs, offer superior capabilities in analyzing
		  and generating citation texts. In addition to traditional
		  evaluation metrics, we introduce a custom metric that
		  emphasizes the overlap of key terms and semantic
		  similarity, providing a more comprehensive assessment of
		  our model's performance. Our code and data are available at
		  https://github.com/midas-research/M-CTG/tree/main.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {56–64},
  numpages	= {9},
  keywords	= {S2ORC, citation text generation, graph embeddings,
		  knowledge graphs, language models, large language models},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Proceedings{	  10.1145/3641142,
  title		= {ACSW '24: Proceedings of the 2024 Australasian Computer
		  Science Week},
  year		= {2024},
  isbn		= {9798400717307},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sydney, NSW, Australia}
}

@Proceedings{	  10.1145/3639592,
  title		= {AICCC '23: Proceedings of the 2023 6th Artificial
		  Intelligence and Cloud Computing Conference},
  year		= {2023},
  isbn		= {9798400716225},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Kyoto, Japan}
}

@Proceedings{	  10.1145/3652583,
  title		= {ICMR '24: Proceedings of the 2024 International Conference
		  on Multimedia Retrieval},
  year		= {2024},
  isbn		= {9798400706196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {We are pleased to present the 2024 edition of the ACM
		  International Conference on Multimedia Retrieval, ACM ICMR
		  2024, that took place from 10-14 June 2024, in Phuket,
		  Thailand.Effectively and efficiently retrieving information
		  from multimedia collections (e.g., text, image, video,
		  audio, sensor data, 3D) based on user needs is one of the
		  most exciting areas in multimedia research. The Annual ACM
		  International Conference on Multimedia Retrieval (ICMR)
		  offers a great opportunity for exchanging leading-edge
		  multimedia retrieval ideas among researchers,
		  practitioners, and other potential users of multimedia
		  retrieval systems. ACM ICMR was created in 2011 in a merger
		  of ACM CIVR (International Conference on Image and Video
		  Retrieval) and ACM MIR (International Conference on
		  Multimedia Information Retrieval). ACM ICMR serves to
		  illuminate the state of the art in multimedia retrieval.
		  ACM ICMR 2024 in Phuket follows the successful previous
		  editions of ICMR in Trento, Italy 2011; Hong Kong, China
		  2012; Dallas, USA 2013; Glasgow, UK 2014; Shanghai, China
		  2015; New York, USA 2016; Bucharest, Romania 2017;
		  Yokohama, Japan 2018; Ottawa, Canada 2019; Dublin, Ireland
		  2020 (online); Taipei, Taiwan 2021 (online); Newark, USA
		  2022 (hybrid); and Thessaloniki, Greece 2023 (hybrid).},
  location	= {Phuket, Thailand}
}

@InProceedings{	  10.1145/3613905.3651112,
  author	= {Zhao, Yijun and Pan, Jiangyu and Dong, Yan and Dong,
		  Tianshu and Wang, Guanyun and Ying, Fangtian and Shen,
		  Qihang and Cao, Jiacheng},
  title		= {Language Urban Odyssey: A Serious Game for Enhancing
		  Second Language Acquisition through Large Language Models},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3651112},
  doi		= {10.1145/3613905.3651112},
  abstract	= {Traditional second language acquisition (SLA) often lacks
		  deep immersion in authentic environments, presenting high
		  learning and resource challenges. To overcome this, we
		  introduced "Language Urban Odyssey" (LUO), a serious game
		  designed to offer an affordable language practice
		  environment. LUO combines Large Language Models (LLMs) with
		  game-based learning, creating an immersive and interactive
		  experience. Players interact with AI-driven characters in a
		  fictional city, leveraging ChatGPT 3.5’s capabilities for
		  simulating real language use and cultural diversity. The
		  game aims to reduce language learning barriers, ignite
		  interest, and provide practical scenarios. Test results
		  show LUO significantly boosts interest and proficiency in
		  language learning. Players praise its engaging narrative,
		  interactive dialogues, and adaptive experience. However,
		  while LUO is beneficial, it’s crucial to recognize
		  gamified learning’s limits; genuine language fluency
		  still requires real-life communication practice and
		  validation.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {219},
  numpages	= {7},
  keywords	= {Educational Games, Human-AI Interaction, Large Language
		  Models, Second Language Acquisition},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@Proceedings{	  10.1145/3703187,
  title		= {CISAI '24: Proceedings of the 2024 7th International
		  Conference on Computer Information Science and Artificial
		  Intelligence},
  year		= {2024},
  isbn		= {9798400707254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3652628,
  title		= {ICAICE '23: Proceedings of the 4th International
		  Conference on Artificial Intelligence and Computer
		  Engineering},
  year		= {2023},
  isbn		= {9798400708831},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dalian, China}
}

@Proceedings{	  10.1145/3643916,
  title		= {ICPC '24: Proceedings of the 32nd IEEE/ACM International
		  Conference on Program Comprehension},
  year		= {2024},
  isbn		= {9798400705861},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {ICPC is the premier (CORE A) venue for research on program
		  comprehension. Research on program comprehension
		  encompasses both human activities for comprehending the
		  software and technologies for supporting such
		  comprehension.},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3651671.3651778,
  author	= {Liu, Lijuan and Shi, Li},
  title		= {Application case study, security challenges and
		  countermeasures of AIGC in the context of metaverse},
  year		= {2024},
  isbn		= {9798400709234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3651671.3651778},
  doi		= {10.1145/3651671.3651778},
  abstract	= {With the rapid development of artificial intelligence
		  technology, the era of AIGC is coming. This paper relies on
		  the Metaverse theoretical system to conduct analysis and
		  research from the perspective of AIGC core technology as
		  well as AIGC application cases. Take ChatGPT and ERNIE Bot
		  as products examples. Besides, security challenges are
		  taken into consideration, it brings the risk of sensitive
		  content, it leads to lower costs for criminal activities,
		  it brings cross-border data security risk, and it will even
		  reduce the public's thinking ability. To deal with the
		  above security challenges, the security response methods
		  are discussed from the perspective of decision-makers,
		  program developers, and product users, in the hope that
		  AIGC can better serve society.},
  booktitle	= {Proceedings of the 2024 16th International Conference on
		  Machine Learning and Computing},
  pages		= {228–232},
  numpages	= {5},
  keywords	= {ChatGPT, Metaverse, intelligent applications, security
		  challenges},
  location	= {Shenzhen, China},
  series	= {ICMLC '24}
}

@Article{	  10.1145/3659942,
  author	= {Mashayekhi, Yoosof and Li, Nan and Kang, Bo and Lijffijt,
		  Jefrey and De Bie, Tijl},
  title		= {A Challenge-based Survey of E-recruitment Recommendation
		  Systems},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {10},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3659942},
  doi		= {10.1145/3659942},
  abstract	= {E-recruitment recommendation systems recommend jobs to job
		  seekers and job seekers to recruiters. The recommendations
		  are generated based on the suitability of job seekers for
		  positions and on job seekers’ and recruiters’
		  preferences. Therefore, e-recruitment recommendation
		  systems may greatly impact people’s careers. Moreover, by
		  affecting the hiring processes of the companies,
		  e-recruitment recommendation systems play an important role
		  in shaping the competitive edge of companies. Hence, it
		  seems prudent to consider what (unique) challenges there
		  are for recommendation systems in e-recruitment. Existing
		  surveys on this topic discuss past studies from the
		  algorithmic perspective, e.g., by categorizing them into
		  collaborative filtering, content-based, and hybrid methods.
		  This survey, instead, takes a complementary,
		  challenge-based approach. We believe this is more practical
		  for developers facing a concrete e-recruitment design task
		  with a specific set of challenges, and also for researchers
		  that look for impactful research projects in this domain.
		  In this survey, we first identify the main challenges in
		  the e-recruitment recommendation research. Next, we discuss
		  how those challenges have been studied in the literature.
		  Finally, we provide future research directions that we
		  consider most promising in the e-recruitment recommendation
		  domain.},
  journal	= {ACM Comput. Surv.},
  month		= jun,
  articleno	= {252},
  numpages	= {33},
  keywords	= {Job recommendation, e-recruitment recommendation}
}

@InProceedings{	  10.1145/3637528.3671491,
  author	= {Alam, Mehwish and Buscaldi, Davide and Cochez, Michael and
		  Gesese, Genet Asefa and Osborne, Francesco and Reforgiato
		  Recupero, Diego},
  title		= {Workshop on Deep Learning and Large Language Models for
		  Knowledge Graphs (DL4KG)},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671491},
  doi		= {10.1145/3637528.3671491},
  abstract	= {The use of Knowledge Graphs (KGs) which constitute large
		  networks of real-world entities and their
		  interrelationships, has grown rapidly. A substantial body
		  of research has emerged, exploring the integration of deep
		  learning (DL) and large language models (LLMs) with KGs.
		  This workshop aims to bring together leading researchers in
		  the field to discuss and foster collaborations on the
		  intersection of KG and DL/LLMs.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6704–6705},
  numpages	= {2},
  keywords	= {artificial intelligence, deep learning, knowledge graphs,
		  large language models},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3653984,
  author	= {Zhuang, Haojie and Zhang, Wei and Chen, Weitong and Yang,
		  Jian and Sheng, Quan Z.},
  title		= {Improving Faithfulness and Factuality with Contrastive
		  Learning in Explainable Recommendation},
  year		= {2024},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {1},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3653984},
  doi		= {10.1145/3653984},
  abstract	= {Recommender systems have become increasingly important in
		  navigating the vast amount of information and options
		  available in various domains. By tailoring and
		  personalizing recommendations to user preferences and
		  interests, these systems improve the user experience,
		  efficiency, and satisfaction. With a growing demand for
		  transparency and understanding of recommendation outputs,
		  explainable recommender systems have gained growing
		  attention in recent years. Additionally, as user reviews
		  could be considered the rationales behind why the user
		  likes (or dislikes) the products, generating informative
		  and reliable reviews alongside recommendations has thus
		  emerged as a research focus in explainable recommendation.
		  However, the model-generated reviews might contain
		  factually inconsistent contents (i.e., the hallucination
		  issue), which would thus compromise the recommendation
		  rationales. To address this issue, we propose a contrastive
		  learning framework to improve the faithfulness and
		  factuality in explainable recommendation in this article.
		  We further develop different strategies of generating
		  positive and negative examples for contrastive learning,
		  such as back-translation or synonym substitution for
		  positive examples, and editing positive examples or
		  utilizing model-generated texts for negative examples. Our
		  proposed method optimizes the model to distinguish faithful
		  explanations (i.e., positive examples) and unfaithful ones
		  with factual errors (i.e., negative examples), which thus
		  drives the model to generate faithful reviews as
		  explanations while avoiding inconsistent contents.
		  Extensive experiments and analysis on three benchmark
		  datasets show that our proposed model outperforms other
		  review generation baselines in faithfulness and factuality.
		  In addition, the proposed contrastive learning component
		  could be easily incorporated into other explainable
		  recommender systems in a plug-and-play manner.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= dec,
  articleno	= {9},
  numpages	= {23},
  keywords	= {Recommender systems, explainable recommendation, review
		  generation}
}

@Article{	  10.1145/3700748,
  author	= {Mostafa, Mohamed and Almogren, Ahmad S and Al-Qurishi,
		  Muhammad and Alrubaian, Majed},
  title		= {Modality Deep-learning Frameworks for Fake News Detection
		  on Social Networks: A Systematic Literature Review},
  year		= {2024},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {3},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3700748},
  doi		= {10.1145/3700748},
  abstract	= {Fake news on social networks is a challenging problem due
		  to the rapid dissemination and volume of information, as
		  well as the ease of creating and sharing content
		  anonymously. Fake news stories are problematic not only for
		  the credibility of online journalism, but also due to their
		  detrimental real-world consequences. The primary research
		  objective of this study is to identify recent
		  state-of-the-art deep learning methods used to detect fake
		  news in social networks. This article presents a systematic
		  literature review of deep learning-based fake news
		  detection models in social networks. The methodology
		  followed a rigorous approach, including predefined criteria
		  for study selection of deep learning modalities. This study
		  focuses on the types of deep learning modalities: unimodal
		  (refers to the use of a single model for analysis or
		  modeling purposes) and multimodal models (refers to the
		  integration of multiple models). The results of this review
		  reveal the strengths and weaknesses of modalities
		  approaches, as well as the limitations of low-resource
		  languages datasets. Furthermore, it provides insights into
		  future directions for deep learning models and different
		  fact-checking techniques. At the end of this study, we
		  discuss the problem of fake news detection in the era of
		  large language models in terms of advantages, drawbacks,
		  and challenges.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {77},
  numpages	= {50},
  keywords	= {Social computing, deep learning, modality architectures,
		  unimodal, multimodal, fake news detection, text
		  classification}
}

@InProceedings{	  10.1145/3613904.3642149,
  author	= {Liu, Michael Xieyang and Wu, Tongshuang and Chen, Tianying
		  and Li, Franklin Mingzhe and Kittur, Aniket and Myers, Brad
		  A},
  title		= {Selenite: Scaffolding Online Sensemaking with
		  Comprehensive Overviews Elicited from Large Language
		  Models},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642149},
  doi		= {10.1145/3613904.3642149},
  abstract	= {Sensemaking in unfamiliar domains can be challenging,
		  demanding considerable user effort to compare different
		  options with respect to various criteria. Prior research
		  and our formative study found that people would benefit
		  from reading an overview of an information space upfront,
		  including the criteria others previously found useful.
		  However, existing sensemaking tools struggle with the
		  “cold-start” problem — it not only requires
		  significant input from previous users to generate and share
		  these overviews, but such overviews may also turn out to be
		  biased and incomplete. In this work, we introduce a novel
		  system, Selenite, which leverages Large Language Models
		  (LLMs) as reasoning machines and knowledge retrievers to
		  automatically produce a comprehensive overview of options
		  and criteria to jumpstart users’ sensemaking processes.
		  Subsequently, Selenite also adapts as people use it,
		  helping users find, read, and navigate unfamiliar
		  information in a systematic yet personalized manner.
		  Through three studies, we found that Selenite produced
		  accurate and high-quality overviews reliably, significantly
		  accelerated users’ information processing, and
		  effectively improved their overall comprehension and
		  sensemaking experience.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {837},
  numpages	= {26},
  keywords	= {Human-AI Collaboration, Large Language Models, Natural
		  Language Processing, Sensemaking},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3705677.3705701,
  author	= {Cai, Zhengna and Fan, Yujing and Xin, Jianfeng},
  title		= {Research on Medical Text Named Entity Recognition Model
		  Based on Prompt Contrastive Learning},
  year		= {2025},
  isbn		= {9798400711848},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3705677.3705701},
  doi		= {10.1145/3705677.3705701},
  abstract	= {Medical Named Entity Recognition (NER) aims to
		  automatically identify entities like diseases, drugs, and
		  symptoms from medical texts, supporting medical knowledge
		  graphs, clinical decision-making, and intelligent systems.
		  Current research relies on deep learning models,
		  particularly pre-trained language models like BERT,
		  improving recognition accuracy. Challenges include data
		  sparsity, ambiguous boundaries, and synonym diversity,
		  affecting generalization on specific datasets. To address
		  this, we enhance BERT with prompt learning and contrastive
		  learning, using learnable entity class embeddings and
		  similarity computations to improve classification and
		  recognition on few-shot datasets. Our model achieves F1
		  scores of 90.19% on the CCKS 2019 dataset and 83.30% on the
		  JNLPBA dataset.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Computer, Internet of Things and Control Engineering},
  pages		= {139–144},
  numpages	= {6},
  keywords	= {Contrast Learning, Named Entity Recognition, Prompt
		  Learning},
  location	= { },
  series	= {CITCE '24}
}

@Article{	  10.1145/3664597,
  author	= {Wan, Yao and Bi, Zhangqian and He, Yang and Zhang, Jianguo
		  and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Jin,
		  Hai and Yu, Philip},
  title		= {Deep Learning for Code Intelligence: Survey, Benchmark and
		  Toolkit},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {12},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3664597},
  doi		= {10.1145/3664597},
  abstract	= {Code intelligence leverages machine learning techniques to
		  extract knowledge from extensive code corpora, with the aim
		  of developing intelligent tools to improve the quality and
		  productivity of computer programming. Currently, there is
		  already a thriving research community focusing on code
		  intelligence, with efforts ranging from software
		  engineering, machine learning, data mining, natural
		  language processing, and programming languages. In this
		  paper, we conduct a comprehensive literature review on deep
		  learning for code intelligence, from the aspects of code
		  representation learning, deep learning techniques, and
		  application tasks. We also benchmark several
		  state-of-the-art neural models for code intelligence, and
		  provide an open-source toolkit tailored for the rapid
		  prototyping of deep-learning-based code intelligence
		  models. In particular, we inspect the existing code
		  intelligence models under the basis of code representation
		  learning, and provide a comprehensive overview to enhance
		  comprehension of the present state of code intelligence.
		  Furthermore, we publicly release the source code and data
		  resources to provide the community with a ready-to-use
		  benchmark, which can facilitate the evaluation and
		  comparison of existing and future code intelligence models
		  (https://xcodemind.github.io). At last, we also point out
		  several challenging and promising directions for future
		  research.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {309},
  numpages	= {41},
  keywords	= {Code intelligence, code representation, deep learning,
		  large language models, survey, benchmark, toolkit}
}

@InProceedings{	  10.1145/3687311.3687339,
  author	= {Yang, Xin and Zhao, Fengjuan},
  title		= {Integrating AI with Pedagogies: Drama, Multimodal and the
		  Production-oriented Approach- a Study Based on the 6th
		  SFLEP Intercultural Competence Contest},
  year		= {2024},
  isbn		= {9798400709920},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3687311.3687339},
  doi		= {10.1145/3687311.3687339},
  abstract	= {Cultural studies have garnered significant attention in
		  China for many decades, and the cultivation of
		  intercultural competence within the academic sphere has
		  been meticulously developed, particularly within the
		  university context. Although intercultural competence is
		  inherently multidisciplinary, its cultivation is
		  predominantly integrated into the pedagogy of foreign
		  language instruction. The discourse surrounding
		  intercultural communication is mainly led by educators and
		  scholars in the field of foreign language studies. The
		  SFLEP Intercultural Competence Contest comprises three
		  pivotal tasks: the development of intercultural case
		  studies, scenario analysis, and the narration of Chinese
		  stories. The rapid development of AI has opened new avenues
		  in the field of education, particularly in language
		  learning. The integration of AI with traditional pedagogies
		  like drama, multimodal learning, and the
		  production-oriented approach has been observed to enrich
		  the learning experience and improve intercultural
		  competence. A thorough examination of the 6th iteration of
		  the contest provides the foundation for this exploration.
		  The study not only highlights the potential of AI
		  integrated approaches but also underscores their
		  significant relevance in enhancing scaffolding techniques
		  in foreign language education.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Intelligent Education and Computer Technology},
  pages		= {151–157},
  numpages	= {7},
  location	= {Guilin, China},
  series	= {IECT '24}
}

@Proceedings{	  10.1145/3640310,
  title		= {MODELS '24: Proceedings of the ACM/IEEE 27th International
		  Conference on Model Driven Engineering Languages and
		  Systems},
  year		= {2024},
  isbn		= {9798400705045},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Linz, Austria}
}

@InProceedings{	  10.1145/3589335.3651480,
  author	= {Ahn, Dawon and Shiao, William and Khaled, Arindam and
		  Bauer, Andrew and Poulis, Stefanos and Papalexakis,
		  Evangelos E.},
  title		= {Compact Interpretable Tensor Graph Multi-Modal News
		  Embeddings},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651480},
  doi		= {10.1145/3589335.3651480},
  abstract	= {Online news articles encompass a variety of modalities
		  such as text and images. How can we learn a representation
		  that incorporates information from all those modalities in
		  a compact and interpretable manner? In this paper, we
		  propose CITEM (Compact Interpretable Tensor graph
		  multi-modal news EMbedding), a tensor-based framework for
		  compact and interpretable multi-modal news representations.
		  CITEM generates a tensor graph consisting of a news
		  similarity graph for each modality and employs a tensor
		  decomposition to produce compact and interpretable
		  embeddings, each dimension of which is a heterogeneous
		  co-cluster of news articles and corresponding modalities.
		  We extensively validate CITEM compared to baselines on two
		  news classification tasks: misinformation news detection
		  and news categorization. The experimental results show that
		  CITEM performs within the same range of AUC as
		  state-of-the-art baselines while producing 7x to 10.5x more
		  compact embeddings. In addition, each embedding dimension
		  of CITEM is interpretable, representing a latent co-cluster
		  of articles.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {847–850},
  numpages	= {4},
  keywords	= {interpretable multi-modal embeddings, tensor
		  decomposition},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1109/tcbb.2024.3412174,
  author	= {Dai, Yuanfei and Zhang, Bin and Wang, Shiping},
  title		= {Distantly Supervised Biomedical Relation Extraction via
		  Negative Learning and Noisy Student Self-Training},
  year		= {2024},
  issue_date	= {Nov.-Dec. 2024},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {21},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2024.3412174},
  doi		= {10.1109/TCBB.2024.3412174},
  abstract	= {Biomedical relation extraction aims to identify underlying
		  relationships among entities, such as gene associations and
		  drug interactions, within biomedical texts. Despite
		  advancements in relation extraction in general knowledge
		  domains, the scarcity of labeled training data remains a
		  significant challenge in the biomedical field. This paper
		  provides a novel approach for biomedical relation
		  extraction that leverages a noisy student self-training
		  strategy combined with negative learning. This method
		  addresses the challenge of data insufficiency by utilizing
		  distantly supervised data to generate high-quality labeled
		  samples. Negative learning, as opposed to traditional
		  positive learning, offers a more robust mechanism to
		  discern and relabel noisy samples, preventing model
		  overfitting. The integration of these techniques ensures
		  enhanced noise reduction and relabeling capabilities,
		  leading to improved performance even with noisy datasets.
		  Experimental results demonstrate the effectiveness of the
		  proposed framework in mitigating the impact of noisy data
		  and outperforming existing benchmarks.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jun,
  pages		= {1697–1708},
  numpages	= {12}
}

@InProceedings{	  10.1145/3639479.3639496,
  author	= {Shen, Yingli and Zhao, Xiaobing},
  title		= {Reinforcement Learning in Natural Language Processing: A
		  Survey},
  year		= {2024},
  isbn		= {9798400709241},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639479.3639496},
  doi		= {10.1145/3639479.3639496},
  abstract	= {Reinforcement learning (RL) is a powerful technique for
		  learning from data and feedback, but its effective
		  application to natural language processing (NLP) tasks
		  remains an open question. Consequently, this paper first
		  introduces the general concepts of RL and the common
		  approaches. Subsequently, we review the task construction
		  settings and the application of RL for various NLP
		  problems, such as machine translation, dialogue system, and
		  text generation. Finally, we discuss some promising
		  research directions and challenges of RL in NLP. We hope
		  that our work can provide a comprehensive overview and
		  inspire more research on this promising yet challenging
		  topic.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Machine Learning and Natural Language Processing},
  pages		= {84–90},
  numpages	= {7},
  keywords	= {Application, Natural Language Processing, Reinforcement
		  Learning, Survey},
  location	= {Sanya, China},
  series	= {MLNLP '23}
}

@Article{	  10.1145/3697838,
  author	= {Qu, Shilin and Wang, Weiqing and Zhou, Xin and Zhan,
		  Haolan and Li, Zhuang and Qu, Lizhen and Luo, Linhao and
		  Li, Yuan-Fang and Haffari, Gholamreza},
  title		= {Scalable Frame-based Construction of Sociocultural
		  NormBases for Socially-Aware Dialogues},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3697838},
  doi		= {10.1145/3697838},
  abstract	= {Sociocultural norms serve as guiding principles for
		  personal conduct in social interactions, emphasizing
		  respect, cooperation, and appropriate behavior, which is
		  able to benefit tasks including conversational information
		  retrieval, contextual information retrieval and
		  retrieval-enhanced machine learning. We propose a scalable
		  approach for constructing a Sociocultural Norm (Scn) Base
		  using Large Language Models (LLMs) for socially aware
		  dialogues. We construct a comprehensive and publicly
		  accessible Chinese Sociocultural NormBase
		  (ChineseNormBase). Our approach utilizes socially-aware
		  dialogues, enriched with contextual frames, as the primary
		  data source to constrain the generating process and reduce
		  the hallucinations. This enables extracting of high-quality
		  and nuanced natural-language norm statements, leveraging
		  the pragmatic implications of utterances with respect to
		  the situation. As real dialogue annotated with gold frames
		  are not readily available, we propose using synthetic data.
		  Our empirical results show: (i) the quality of the Scns
		  derived from synthetic data is comparable to that from real
		  dialogues annotated with gold frames, and (ii) the quality
		  of the Scns extracted from real data, annotated with either
		  silver (predicted) or gold frames, surpasses that without
		  the frame annotations. We further show the effectiveness of
		  the extracted Scns in a RAG-based (Retrieval-Augmented
		  Generation) model to reason about multiple downstream
		  dialogue tasks.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= oct,
  keywords	= {Social-culrutal Norm Base, Chinese Culture,
		  Retrieval-Augmented Generation, Norm Construction}
}

@InProceedings{	  10.1145/3637528.3671873,
  author	= {Yan, Mengyi and Wang, Yaoshu and Pang, Kehan and Xie, Min
		  and Li, Jianxin},
  title		= {Efficient Mixture of Experts based on Large Language
		  Models for Low-Resource Data Preprocessing},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671873},
  doi		= {10.1145/3637528.3671873},
  abstract	= {Data preprocessing (DP) that transforms erroneous and raw
		  data to a clean version is a cornerstone of the data mining
		  pipeline. Due to the diverse requirements of downstream
		  tasks, data scientists and domain experts have to handcraft
		  domain-specific rules or train ML models with annotated
		  examples, which is costly/time-consuming. In this paper, we
		  present MELD (&lt;u&gt;M&lt;/u&gt;ixture of
		  &lt;u&gt;E&lt;/u&gt;xperts on &lt;u&gt;L&lt;/u&gt;arge
		  Language Models for &lt;u&gt;D&lt;/u&gt;ata Preprocessing),
		  a universal solver for low-resource DP. MELD adopts a
		  Mixture-of-Experts (MoE) architecture that enables the
		  amalgamation and enhancement of domain-specific experts
		  trained on limited annotated examples. To fine-tune MELD,
		  we develop a suite of expert-tuning and MoE-tuning
		  techniques, including a retrieval augmented generation
		  (RAG) system, meta-path search for data augmentation,
		  expert refinement and router network training based on
		  information bottleneck. To further verify the effectiveness
		  of MELD, we theoretically prove that MoE in MELD is
		  superior than a single expert and the router network is
		  able to dispatch data to the right experts. Finally, we
		  conducted extensive experiments on 19 datasets over 10 DP
		  tasks to show that MELD outperforms the state-of-the-art
		  methods in both effectiveness and efficiency. More
		  importantly, MELD is able to be fine-tuned in a
		  low-resource environment, e.g. a local, single and
		  low-priced 3090 GPU.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3690–3701},
  numpages	= {12},
  keywords	= {LLMs, data preprocessing, low-resource, mixture of
		  expert},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Proceedings{	  10.1145/3678610,
  title		= {ICSLT '24: Proceedings of the 2024 10th International
		  Conference on e-Society, e-Learning and e-Technologies
		  (ICSLT)},
  year		= {2024},
  isbn		= {9798400716799},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3679318,
  title		= {NordiCHI '24: Proceedings of the 13th Nordic Conference on
		  Human-Computer Interaction},
  year		= {2024},
  isbn		= {9798400709661},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Uppsala, Sweden}
}

@InProceedings{	  10.1145/3627673.3679883,
  author	= {Yang, Xinjie and Gong, Xiaocheng and Tang, Binghao and
		  Lei, Yang and Deng, Yayue and Ouyang, Huan and Zhao, Gang
		  and Luo, Lei and Feng, Yunling and Duan, Bin and Li, Si and
		  Xu, Yajing},
  title		= {CAG: A Consistency-Adaptive Text-Image Alignment
		  Generation for Joint Multimodal Entity-Relation
		  Extraction},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679883},
  doi		= {10.1145/3627673.3679883},
  abstract	= {Joint Multimodal Entity-Relation Extraction (JMERE) aims
		  to extract entity-relationship triples in texts from given
		  image-text pairs. As a joint multimodal information
		  extraction task, it has attracted increasing research
		  interest. Previous works of JMERE typically utilize graph
		  networks to align textual entities and visual objects and
		  achieve promising performance. However, these methods do
		  not pay attention to the inconsistency between text and
		  image and the straight alignment could limit the
		  performance of JMERE models. In this paper, we propose a
		  Consistency-adaptive text-image Alignment Generation (CAG)
		  framework for various text-image consistency scenarios.
		  Specifically, we propose a Consistency Factor (CF) to
		  measure the consistency between images and texts. We also
		  design consistency-adaptive contrastive learning based on
		  CF, which can reduce the impact of inconsistent visual and
		  textual information. Additionally, we adopt
		  JMERE-specifical instruction tuning for better
		  entity-relationship triplet generation. Experimental
		  results on the JMERE dataset demonstrate that our proposed
		  CAG is effective and achieves state-of-the-art
		  performance.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4183–4187},
  numpages	= {5},
  keywords	= {contrastive learning, instruction tuning, joint multimodal
		  entity-relation extraction, multimodal alignment},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Proceedings{	  10.1145/3698587,
  title		= {BCB '24: Proceedings of the 15th ACM International
		  Conference on Bioinformatics, Computational Biology and
		  Health Informatics},
  year		= {2024},
  isbn		= {9798400713026},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shenzhen, China}
}

@InProceedings{	  10.1145/3663976.3664023,
  author	= {Sun, Yaru and Yang, Ying and Fu, Wenhao},
  title		= {Exploring Synergies between Causal Models and
		  LargeLanguage Models for Enhanced Understanding and
		  Inference},
  year		= {2024},
  isbn		= {9798400716607},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3663976.3664023},
  doi		= {10.1145/3663976.3664023},
  abstract	= {Large language models (LLMs) have sparked a new wave of
		  excitement in the field of artificial intelligence, thanks
		  to their robust generative capabilities. However, they fall
		  short when it comes to comprehending factual knowledge and
		  logical reasoning. In contrast, causal models demonstrate
		  superior interpretability, resilience against disturbances,
		  and decision-support capabilities. By integrating event
		  generation mechanisms and external knowledge, causal models
		  can enhance the reasoning and interpretability of LLMs.
		  Nevertheless, the complex construction and iterative nature
		  of causal models pose challenges that push the boundaries
		  of current frameworks. Thus, leveraging the strengths of
		  both LLMs and causal models can effectively address the
		  limitations of LLMs in logical reasoning, complex
		  inference, and causal deduction, as well as tackle the
		  complexities and difficulties encountered in the
		  establishment and analysis of causal models. These
		  challenges include distinguishing between relevance and
		  causality, handling reverse relationships, and managing
		  interactions. This paper proposes a technical roadmap for a
		  collaborative approach between LLMs and causal models,
		  exploring four different methods of collaboration: causal
		  relationship modeling, causal knowledge injection, causal
		  perception, and causal relationship constraints. We review
		  and summarize existing work while identifying future
		  research directions in harnessing the synergy between LLMs
		  and causal models.},
  booktitle	= {Proceedings of the 2024 2nd Asia Conference on Computer
		  Vision, Image Processing and Pattern Recognition},
  articleno	= {38},
  numpages	= {8},
  keywords	= {Causal Knowledge Infusion, Causal Models, Causal
		  Perception, Large Language Models},
  location	= {Xiamen, China},
  series	= {CVIPPR '24}
}

@InProceedings{	  10.5555/3635637.3663263,
  author	= {Rodriguez, Sebastian and Thangarajah, John},
  title		= {Explainable Agents (XAg) by Design},
  year		= {2024},
  isbn		= {9798400704864},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {The likes of ChatGPT has propelled the use of AI
		  techniques beyond our community's expectations. Along with
		  this, the fear of AI has also risen, in particular around
		  the ability, or lack thereof, of the AI system to explain
		  its behaviours. Explainability is a key element of building
		  trust and an important issue for our community. In this
		  paper we advocate for agents that are
		  explainable-by-design, that is, explainability is built
		  into the development of agents rather than an afterthought.
		  We propose key features of an explainable agent (XAg)
		  system and propose a general framework that enables
		  explainability. We advocate the use of design patterns to
		  develop XAgs and propose a general design pattern that can
		  be used for any agent architecture. We instantiate our
		  framework for goal-based agents and implement the framework
		  for the SARL agent programming language coupled with a
		  state-of-the-art event management system. We make a call to
		  the developers of other agent programming languages (APLs)
		  in our community to follow suit by instantiating the
		  general framework we propose into their APL, perhaps even
		  enhancing the framework we present. We also propose an open
		  repository of design patterns and examples for agent
		  systems. If nothing else, we hope this paper will inspire
		  further work on XAg from the design perspective as it is
		  critical that multi agent systems are explainable by
		  design!},
  booktitle	= {Proceedings of the 23rd International Conference on
		  Autonomous Agents and Multiagent Systems},
  pages		= {2712–2716},
  numpages	= {5},
  keywords	= {aose, emas, explainable ai},
  location	= {Auckland, New Zealand},
  series	= {AAMAS '24}
}

@Article{	  10.1109/taslp.2024.3394778,
  author	= {Hu, Maodi and Qian, Li and Chang, Zhijun and Zhang,
		  Zhixiong},
  title		= {KDPG-Enhanced MRC Framework for Scientific Entity
		  Recognition in Survey Papers},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3394778},
  doi		= {10.1109/TASLP.2024.3394778},
  abstract	= {Scientific survey papers play a pivotal role in advancing
		  knowledge and scientific progress by providing concise
		  summaries and analyses of research trends and findings. To
		  facilitate better knowledge organization and analysis, we
		  have undertaken the challenge of defining the scientific
		  entity recognition task for survey papers and carefully
		  curated a dataset that closely emulates real-world
		  scenarios. The scientific entity recognition task presents
		  unique challenges, including multi-label, low-resource, and
		  nested scenarios. To address these challenges, we propose a
		  unified framework based on the machine reading
		  comprehension (MRC) paradigm. This framework not only
		  supports nested and multi-label settings but also enables
		  the effective transfer of information from high-resource
		  categories to low-resource ones, ensuring adaptability and
		  robustness. To further enhance performance, we introduce
		  the Knowledge-Driven Prototype Guidance (KDPG) module,
		  seamlessly integrated into a two-phase learning strategy.
		  The KDPG module leverages prior knowledge and acts as an
		  initial prototype-based manifold constraint, effectively
		  harnessing the power of few-shot learning capabilities.
		  Through this integration, our approach complements the
		  classification learning tasks for entity recognition,
		  resulting in improved accuracy and efficiency. Our
		  experimental results validate the effectiveness of the
		  proposed KDPG-enhanced MRC framework, showcasing its
		  leading performance on publicly available datasets and our
		  collected scientific survey paper dataset.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= apr,
  pages		= {2532–2543},
  numpages	= {12}
}

@Article{	  10.1145/3696379,
  author	= {Bui, Minh-Thanh and Boffa, Matteo and Valentim, Rodolfo
		  Vieira and Navarro, Jose Manuel and Chen, Fuxing and Bao,
		  Xiaosheng and Houidi, Zied Ben and Rossi, Dario},
  title		= {A Systematic Comparison of Large Language Models
		  Performance for Intrusion Detection},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {CoNEXT4},
  url		= {https://doi.org/10.1145/3696379},
  doi		= {10.1145/3696379},
  abstract	= {We explore the capabilities of Large Language Models
		  (LLMs) to assist or substitute devices (i.e., firewalls)
		  and humans (i.e., security experts) respectively in the
		  detection and analysis of security incidents. We leverage
		  transformer-based technologies, from relatively small to
		  foundational sizes, to address the problem of correctly
		  identifying the attack severity (and accessorily
		  identifying and explaining the attack type). We contrast a
		  broad range of LLM techniques (prompting, retrieval
		  augmented generation, and fine-tuning of several models)
		  using state-of-the-art machine learning models as a
		  baseline. Using proprietary data from commercial
		  deployment, our study provides an unbiased picture of the
		  strengths and weaknesses of LLM for intrusion detection.},
  journal	= {Proc. ACM Netw.},
  month		= nov,
  articleno	= {22},
  numpages	= {23},
  keywords	= {computing methodologies, firewalls, intrusion detection
		  systems, machine learning, natural language processing,
		  security and privacy}
}

@Proceedings{	  10.1145/3651671,
  title		= {ICMLC '24: Proceedings of the 2024 16th International
		  Conference on Machine Learning and Computing},
  year		= {2024},
  isbn		= {9798400709234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shenzhen, China}
}

@InProceedings{	  10.1145/3660043.3660126,
  author	= {Cui, Xiaofeng and Li, Liang},
  title		= {Conversational Recommender Systems based on Topic
		  Prediction and Retrieval},
  year		= {2024},
  isbn		= {9798400716157},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3660043.3660126},
  doi		= {10.1145/3660043.3660126},
  abstract	= {The goal of conversation recommendation systems is to
		  provide users with high-quality, personalized responses by
		  analyzing conversation history, understanding user intent
		  and context, and utilizing relevant knowledge and data.
		  Existing conversation recommendation systems based on
		  prompt learning, which generate response templates from
		  fused knowledge representations generated by pre-trained
		  semantic fusion modules, task-specific soft tokens and
		  conversation contexts, utilize response templates generated
		  from conversation sub-tasks as an important part of the
		  prompts to enhance the recommendation subtask. However, the
		  prompt information in existing methods is limited and may
		  not fit well with the recommendation results when
		  generating the final conversation. To this end, this
		  article proposes a method, TPRCRS (Conversational
		  Recommender Systems based on Topic Prediction and
		  Retrieval), which predicts topics through the conversation
		  context, the previous round of conversation topics, and the
		  behavior of users and systems. Subsequently, through
		  conversation topics, vocabulary, entities and semantic
		  fusion and pre-training, using the fused topics to search
		  in the datasets. When a result is found, it is treated as a
		  conversation template and applied to the recommendation
		  task; otherwise, the prompt is used to generate a
		  conversation template. Finally, the optimal reply is
		  generated through a hint learning method. Experiments show
		  that TPRCRS achieves significantly improved results in two
		  tasks.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Information Education and Artificial Intelligence},
  pages		= {466–471},
  numpages	= {6},
  location	= {Xiamen, China},
  series	= {ICIEAI '23}
}

@Proceedings{	  10.1145/3698300,
  title		= {ICBDT '24: Proceedings of the 2024 7th International
		  Conference on Big Data Technologies},
  year		= {2024},
  isbn		= {9798400717512},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3678726,
  title		= {ICEMT '24: Proceedings of the 2024 8th International
		  Conference on Education and Multimedia Technology},
  year		= {2024},
  isbn		= {9798400717611},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tokyo, Japan}
}

@Article{	  10.1145/3655103.3655110,
  author	= {Chen, Zhikai and Mao, Haitao and Li, Hang and Jin, Wei and
		  Wen, Hongzhi and Wei, Xiaochi and Wang, Shuaiqiang and Yin,
		  Dawei and Fan, Wenqi and Liu, Hui and Tang, Jiliang},
  title		= {Exploring the Potential of Large Language Models (LLMs)in
		  Learning on Graphs},
  year		= {2024},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {25},
  number	= {2},
  issn		= {1931-0145},
  url		= {https://doi.org/10.1145/3655103.3655110},
  doi		= {10.1145/3655103.3655110},
  abstract	= {Learning on Graphs has attracted immense attention due to
		  its wide real-world applications. The most popular pipeline
		  for learning on graphs with textual node attributes
		  primarily relies on Graph Neural Networks (GNNs), and
		  utilizes shallow text embedding as initial node
		  representations, which has limitations in general knowledge
		  and profound semantic understanding. In recent years, Large
		  Language Models (LLMs) have been proven to possess
		  extensive common knowledge and powerful semantic
		  comprehension abilities that have revolutionized existing
		  workflows to handle text data. In this paper, we aim to
		  explore the potential of LLMs in graph machine learning,
		  especially the node classification task, and investigate
		  two possible pipelines: LLMs-as-Enhancers and
		  LLMs-as-Predictors. The former leverages LLMs to enhance
		  nodes' text attributes with their massive knowledge and
		  then generate predictions through GNNs. The latter attempts
		  to directly employ LLMs as standalone predictors. We
		  conduct comprehensive and systematical studies on these two
		  pipelines under various settings. From comprehensive
		  empirical results, we make original observations and find
		  new insights that open new possibilities and suggest
		  promising directions to leverage LLMs for learning on
		  graphs. Our codes and datasets are available at:
		  https://github.com/CurryTang/Graph-LLM .},
  journal	= {SIGKDD Explor. Newsl.},
  month		= mar,
  pages		= {42–61},
  numpages	= {20}
}

@Proceedings{	  10.1145/3698322,
  title		= {EuroPLoP '24: Proceedings of the 29th European Conference
		  on Pattern Languages of Programs, People, and Practices},
  year		= {2024},
  isbn		= {9798400716836},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3664647.3681522,
  author	= {Chen, Jiali and Cai, Yi and Xu, Ruohang and Wang, Jiexin
		  and Xie, Jiayuan and Li, Qing},
  title		= {Deconfounded Emotion Guidance Sticker Selection with
		  Causal Inference},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681522},
  doi		= {10.1145/3664647.3681522},
  abstract	= {With the increasing popularity of online social
		  applications, stickers have become common in online chats.
		  Teaching a model to select the appropriate sticker from a
		  set of candidate stickers based on dialogue context is
		  important for optimizing the user experience. Existing
		  methods have proposed leveraging emotional information to
		  facilitate the selection of appropriate stickers. However,
		  considering the frequent co-occurrence among sticker
		  images, words with emotional preference in the dialogue and
		  emotion labels, these methods tend to over-rely on such
		  dataset bias, inducing spurious correlations during
		  training. As a result, these methods may select
		  inappropriate stickers that do not match users' intended
		  expression. In this paper, we introduce a causal graph to
		  explicitly identify the spurious correlations in the
		  sticker selection task. Building upon the analysis, we
		  propose a Causal Knowledge-Enhanced Sticker Selection model
		  to mitigate spurious correlations. Specifically, we design
		  a knowledge-enhanced emotional utterance extractor to
		  identify emotional information within dialogues. Then an
		  interventional visual feature extractor is employed to
		  obtain unbiased visual features, aligning them with the
		  emotional utterances representation. Finally, a standard
		  transformer encoder fuses the multimodal information for
		  emotion recognition and sticker selection. Extensive
		  experiments on the MOD dataset show that our CKS model
		  significantly outperforms the baseline models.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {3084–3093},
  numpages	= {10},
  keywords	= {causal inference, emotion recognition, sticker selection},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Proceedings{	  10.1145/3636555,
  title		= {LAK '24: Proceedings of the 14th Learning Analytics and
		  Knowledge Conference},
  year		= {2024},
  isbn		= {9798400716188},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Kyoto, Japan}
}

@Proceedings{	  10.1145/3674912,
  title		= {CompSysTech '24: Proceedings of the International
		  Conference on Computer Systems and Technologies 2024},
  year		= {2024},
  isbn		= {9798400716843},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Ruse, Bulgaria}
}

@Proceedings{	  10.1145/3639477,
  title		= {ICSE-SEIP '24: Proceedings of the 46th International
		  Conference on Software Engineering: Software Engineering in
		  Practice},
  year		= {2024},
  isbn		= {9798400705014},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lisbon, Portugal}
}

@Article{	  10.1109/taslp.2024.3374060,
  author	= {Wu, Yuxia and Dai, Tianhao and Zheng, Zhedong and Liao,
		  Lizi},
  title		= {Active Discovering New Slots for Task-Oriented
		  Conversation},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3374060},
  doi		= {10.1109/TASLP.2024.3374060},
  abstract	= {Existing task-oriented conversational systems heavily rely
		  on domain ontologies with pre-defined slots and candidate
		  values. In practical settings, these prerequisites are hard
		  to meet, due to the emerging new user requirements and
		  ever-changing scenarios. To mitigate these issues for
		  better interaction performance, there are efforts working
		  towards detecting out-of-vocabulary values or discovering
		  new slots under unsupervised or semi-supervised learning
		  paradigms. However, overemphasizing on the conversation
		  data patterns alone induces these methods to yield noisy
		  and arbitrary slot results. To facilitate the pragmatic
		  utility, real-world systems tend to provide a stringent
		  amount of human labeling quota, which offers an
		  authoritative way to obtain accurate and meaningful slot
		  assignments. Nonetheless, it also brings forward the high
		  requirement of utilizing such quota efficiently. Hence, we
		  formulate a general new slot discovery task in an
		  information extraction fashion and incorporate it into an
		  active learning framework to realize human-in-the-loop
		  learning. Specifically, we leverage existing language tools
		  to extract value candidates where the corresponding labels
		  are further leveraged as weak supervision signals. Based on
		  these, we propose a bi-criteria selection scheme which
		  incorporates two major strategies, namely,
		  uncertainty-based and diversity-based sampling to
		  efficiently identify terms of interest. We conduct
		  extensive experiments on several public datasets and
		  compare with a bunch of competitive baselines to
		  demonstrate the effectiveness of our method.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {2062–2072},
  numpages	= {11}
}

@Article{	  10.14778/3648160.3648162,
  author	= {Fan, Wenfei and Liu, Muyang and Liu, Shuhao and Tian,
		  Chao},
  title		= {Capturing More Associations by Referencing External
		  Graphs},
  year		= {2024},
  issue_date	= {February 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {6},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3648160.3648162},
  doi		= {10.14778/3648160.3648162},
  abstract	= {This paper studies association rule discovery in a graph
		  G1 by referencing an external graph G2 with overlapping
		  information. The objective is to enrich G1 with relevant
		  properties and links from G2. As a testbed, we consider
		  Graph Association Rules (GARs). We propose a notion of
		  graph joins to enrich G1 by aligning entities across G1 and
		  G2. We also introduce a graph filtering method to support
		  graph joins, by fetching only the data of G2 that pertains
		  to the entities of G1, to reduce noise and the size of the
		  fused data. Based on these we develop a parallel algorithm
		  to discover GARs across G1 and G2. Moreover, we provide an
		  incremental GAR discovery algorithm in response to updates
		  to G1 and G2. We show that both algorithms guarantee to
		  reduce parallel runtime when given more processors. Better
		  yet, the incremental algorithm is bounded relative to the
		  batch one. Using real-life and synthetic data, we
		  empirically verify that the methods improve the accuracy of
		  association analyses by 30.4% on average, and scale well
		  with large graphs.},
  journal	= {Proc. VLDB Endow.},
  month		= feb,
  pages		= {1173–1186},
  numpages	= {14}
}

@Article{	  10.1145/3641001,
  author	= {Jing, Felicia S. and Berger, Sara E. and Becerra Sandoval,
		  Juana Catalina and Pepper, Kristin and Wheeler, April M.
		  and Mayoral, Paula Redondo and Lokesh, Divya and Feng,
		  Alice and Mijalkovic, Marija and Bao, Chaoyun and Dholakia,
		  Sara and Goyal, Mohit},
  title		= {Designing for Agonism: 12 Workers' Perspectives on
		  Contesting Technology Futures},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW1},
  url		= {https://doi.org/10.1145/3641001},
  doi		= {10.1145/3641001},
  abstract	= {In this paper, we gather 12 workers from a large
		  technology company, as recent participants of a research
		  initiative on the social impact of emerging technologies,
		  to present a collaborative analysis of the opportunities
		  and limitations of dissensus-based approaches to technology
		  research and design. We introduce a series of speculative
		  and deconstructive probes and present findings from their
		  use in four collaborative design sessions. We then draw on
		  the theoretical tradition of Agonism to identify moments of
		  friction, refusal, and disagreement over the course of
		  these sessions. We contend that this approach offers a
		  politically important alternative to consensus-based
		  collaborative design methods and can even surface new
		  rhetorics of contestation within discourses on technology
		  futures. We conclude with a discussion of the importance of
		  worker-authored research and an initial set opportunities,
		  challenges, and paradoxes as a resource for future efforts
		  to "Design for Agonism."},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= apr,
  articleno	= {162},
  numpages	= {25},
  keywords	= {agonism, agonistic participatory design, collaborative
		  design, deconstruction, dissensus, emerging technologies,
		  speculative design}
}

@InProceedings{	  10.1109/ase56229.2023.00070,
  author	= {Wang, Tao and Chen, Wei and Liu, Liwei and Wu, Guoquan and
		  Wei, Jun and Huang, Tao},
  title		= {Detecting Smart Home Automation Application Interferences
		  with Domain Knowledge},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00070},
  doi		= {10.1109/ASE56229.2023.00070},
  abstract	= {Trigger-action programming (TAP) is a widely used
		  development paradigm that simplifies the Internet of Things
		  (IoT) automation. However, the exceptional interactions
		  between automation applications may result in
		  interferences, such as conflicts and infinite loops, which
		  cause undesirable consequences and even security and safety
		  risks. While several techniques have been proposed to
		  address this problem, they are often restricted in handling
		  explicit and simple conflicts without considering
		  contextual influences. In addition, they suffer from
		  performance issues when applying to large-scale
		  applications.To address these challenges, we design an
		  effective and practical tool KnowDetector with
		  comprehensive domain knowledge to detect application
		  interferences. To detect application interferences,
		  KnowDetector constructs an automation graph with 1) events,
		  conditions, and actions from automation applications, 2)
		  vertices representing physical environment channels, and 3)
		  edges derived from potential semantic relations between the
		  vertices. In order to make the graph extensively capture
		  the interactions between automation applications, we
		  propose a knowledge model named KnowIoT that accurately
		  characterizes IoT devices with command-level IoT services
		  and the intricate relations between these services and the
		  contextual environment. We abstract the interference
		  detection into a graph pattern-matching problem and
		  summarize ten application interference patterns of four
		  types. Finally, KnowDetector can efficiently detect
		  application interferences by searching for sub-graphs
		  matching the patterns within the automation graph. We
		  evaluated KnowDetector on three real-world datasets. The
		  results demonstrated that it outperformed the other
		  state-of-the-art tools with the highest precision, recall,
		  and F-measure. In addition, KnowDetector is scalable to
		  detect application interferences within a large number of
		  applications with a minimal time overhead.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {1086–1097},
  numpages	= {12},
  keywords	= {smart home platform, TAP, automation application
		  interference, internet of things},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@InProceedings{	  10.1145/3630106.3658981,
  author	= {Kraft, Angelie and Soulier, Elo\"{\i}se},
  title		= {Knowledge-Enhanced Language Models Are Not Bias-Proof:
		  Situated Knowledge and Epistemic Injustice in AI},
  year		= {2024},
  isbn		= {9798400704505},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3630106.3658981},
  doi		= {10.1145/3630106.3658981},
  abstract	= {The factual inaccuracies ("hallucinations") of large
		  language models have recently inspired more research on
		  knowledge-enhanced language modeling approaches. These are
		  often assumed to enhance the overall trustworthiness and
		  objectivity of language models. Meanwhile, the issue of
		  bias is usually only mentioned as a limitation of
		  statistical representations. This dissociation of
		  knowledge-enhancement and bias is in line with previous
		  research on AI engineers’ assumptions about knowledge,
		  which indicate that knowledge is commonly understood as
		  objective and value-neutral by this community. We argue
		  that claims and practices by actors of the field still
		  reflect this underlying conception of knowledge. We
		  contrast this assumption with literature from social and,
		  in particular, feminist epistemology, which argues that the
		  idea of a universal disembodied knower is blind to the
		  reality of knowledge practices and seriously challenges
		  claims of "objective" or "neutral" knowledge. Knowledge
		  enhancement techniques commonly use Wikidata and Wikipedia
		  as their sources for knowledge, due to their large scales,
		  public accessibility, and assumed trustworthiness. In this
		  work, they serve as a case study for the influence of the
		  social setting and the identity of knowers on epistemic
		  processes. Indeed, the communities behind Wikidata and
		  Wikipedia are known to be male-dominated and many instances
		  of hostile behavior have been reported in the past decade.
		  In effect, the contents of these knowledge bases are highly
		  biased. It is therefore doubtful that these knowledge bases
		  would contribute to bias reduction. In fact, our empirical
		  evaluations of RoBERTa, KEPLER, and CoLAKE, demonstrate
		  that knowledge enhancement may not live up to the hopes of
		  increased objectivity. In our study, the average
		  probability for stereotypical associations was preserved on
		  two out of three metrics and performance-related gender
		  gaps on knowledge-driven task were also preserved. We build
		  on these results and critical literature to argue that the
		  label of "knowledge" and the commonly held beliefs about it
		  can obscure the harm that is still done to marginalized
		  groups. Knowledge enhancement is at risk of perpetuating
		  epistemic injustice, and AI engineers’ understanding of
		  knowledge as objective per se conceals this injustice.
		  Finally, to get closer to trustworthy language models, we
		  need to rethink knowledge in AI and aim for an agenda of
		  diversification and scrutiny from outgroup members.},
  booktitle	= {Proceedings of the 2024 ACM Conference on Fairness,
		  Accountability, and Transparency},
  pages		= {1433–1445},
  numpages	= {13},
  keywords	= {bias, epistemology, fairness, feminism, knowledge
		  enhancement, knowledge graphs, language models, natural
		  language processing, representation},
  location	= {Rio de Janeiro, Brazil},
  series	= {FAccT '24}
}

@Article{	  10.1145/3652865,
  author	= {Sun, Zhu and Feng, Kaidong and Yang, Jie and Fang, Hui and
		  Qu, Xinghua and Ong, Yew-Soon and Liu, Wenyuan},
  title		= {Revisiting Bundle Recommendation for Intent-aware Product
		  Bundling},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {3},
  url		= {https://doi.org/10.1145/3652865},
  doi		= {10.1145/3652865},
  abstract	= {Product bundling represents a prevalent marketing strategy
		  in both offline stores and e-commerce systems. Despite its
		  widespread use, previous studies on bundle recommendation
		  face two significant limitations. Firstly, they rely on
		  noisy datasets, where bundles are defined by heuristics,
		  e.g., products co-purchased in the same session. Secondly,
		  they target specific tasks by holding unrealistic
		  assumptions, e.g., the availability of bundles for
		  recommendation directly. This paper proposes to take a step
		  back and considers the process of bundle recommendation
		  from a holistic user experience perspective. We first
		  construct high-quality bundle datasets with rich metadata,
		  particularly bundle intents, through a carefully designed
		  crowd-sourcing task. We then define a series of tasks that
		  together, support all key steps in a typical bundle
		  recommendation process, from bundle detection, completion
		  and ranking, to explanation and auto-naming, whereby 19
		  research questions are raised correspondingly to guide the
		  analysis. Finally, we conduct extensive experiments and
		  analyses with representative recommendation models and
		  large language models (LLMs), demonstrating the challenges
		  and opportunities, especially with the emergence of LLMs.
		  To summarize, our study contributes by introducing novel
		  data sources, paving the way for new research avenues, and
		  offering insights to guide product bundling in real
		  e-commerce platforms.},
  journal	= {ACM Trans. Recomm. Syst.},
  month		= jun,
  articleno	= {24},
  numpages	= {34},
  keywords	= {Product bundling, crowd-sourcing task, bundle datasets,
		  bundle recommendation}
}

@InProceedings{	  10.1145/3627673.3679852,
  author	= {Zha, Zhiwei and Wang, Jiaan and Li, Zhixu and Zhu, Xiangru
		  and Song, Wei and Xiao, Yanghua},
  title		= {M2ConceptBase: A Fine-Grained Aligned Concept-Centric
		  Multimodal Knowledge Base},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679852},
  doi		= {10.1145/3627673.3679852},
  abstract	= {Multimodal knowledge bases (MMKBs) provide cross-modal
		  aligned knowledge crucial for multimodal tasks. However,
		  the images in existing MMKBs are generally collected for
		  entities in encyclopedia knowledge graphs. Therefore,
		  detailed groundings of visual semantics with linguistic
		  concepts are lacking, which are essential for the visual
		  concept cognition ability of multimodal models. Addressing
		  this gap, we introduce M2 ConceptBase, the first
		  concept-centric MMKB. M2 ConceptBase models concepts as
		  nodes with associated images and detailed textual
		  descriptions. We propose a context-aware multimodal symbol
		  grounding approach to align concept-image and
		  concept-description pairs using context information from
		  image-text datasets. Comprising 951K images and 152K
		  concepts, M2 ConceptBase links each concept to an average
		  of 6.27 images and a single description, ensuring
		  comprehensive visual and textual semantics. Human studies
		  confirm more than 95% alignment accuracy, underscoring its
		  quality. Additionally, our experiments demonstrate that M2
		  ConceptBase significantly enhances VQA model performance on
		  the OK-VQA task. M2 ConceptBase also substantially improves
		  the fine-grained concept understanding capabilities of
		  multimodal large language models through retrieval
		  augmentation in two concept-related tasks, highlighting its
		  value.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {3113–3123},
  numpages	= {11},
  keywords	= {knowledge base, multimodal knowledge base, multimodal
		  symbol grounding, visual question answering},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3613904.3642157,
  author	= {Meyer, Louie and Aaen, Johanne Engel and Tranberg,
		  Anitamalina Regitse and Kun, Peter and Freiberger, Matthias
		  and Risi, Sebastian and L\o{}vlie, Anders Sundnes},
  title		= {Algorithmic Ways of Seeing: Using Object Detection to
		  Facilitate Art Exploration},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642157},
  doi		= {10.1145/3613904.3642157},
  abstract	= {This Research through Design paper explores how object
		  detection may be applied to a large digital art museum
		  collection to facilitate new ways of encountering and
		  experiencing art. We present the design and evaluation of
		  an interactive application called SMKExplore, which allows
		  users to explore a museum’s digital collection of
		  paintings by browsing through objects detected in the
		  images, as a novel form of open-ended exploration. We
		  provide three contributions. First, we show how an object
		  detection pipeline can be integrated into a design process
		  for visual exploration. Second, we present the design and
		  development of an app that enables exploration of an art
		  museum’s collection. Third, we offer reflections on
		  future possibilities for museums and HCI researchers to
		  incorporate object detection techniques into the
		  digitalization of museums.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {29},
  numpages	= {18},
  keywords	= {Art, Computer Vision, Experience Design, Exploratory
		  Search, Object Detection},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@Article{	  10.1145/3679200,
  author	= {Liao, Weibin and Zhu, Yifan and Li, Yanyan and Zhang, Qi
		  and Ou, Zhonghong and Li, Xuesong},
  title		= {RevGNN: Negative Sampling Enhanced Contrastive Graph
		  Learning for Academic Reviewer Recommendation},
  year		= {2024},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3679200},
  doi		= {10.1145/3679200},
  abstract	= {Acquiring reviewers for academic submissions is a
		  challenging recommendation scenario. Recent graph
		  learning-driven models have made remarkable progress in the
		  field of recommendation, but their performance in the
		  academic reviewer recommendation task may suffer from a
		  significant false negative issue. This arises from the
		  assumption that unobserved edges represent negative
		  samples. In fact, the mechanism of anonymous review results
		  in inadequate exposure of interactions between reviewers
		  and submissions, leading to a higher number of unobserved
		  interactions compared to those caused by reviewers
		  declining to participate. Therefore, investigating how to
		  better comprehend the negative labeling of unobserved
		  interactions in academic reviewer recommendations is a
		  significant challenge. This study aims to tackle the
		  ambiguous nature of unobserved interactions in academic
		  reviewer recommendations. Specifically, we propose an
		  unsupervised Pseudo Neg-Label strategy to enhance graph
		  contrastive learning (GCL) for recommending reviewers for
		  academic submissions, which we call RevGNN. RevGNN utilizes
		  a two-stage encoder structure that encodes both scientific
		  knowledge and behavior using Pseudo Neg-Label to
		  approximate review preference. Extensive experiments on
		  three real-world datasets demonstrate that RevGNN
		  outperforms all baselines across four metrics.
		  Additionally, detailed further analyses confirm the
		  effectiveness of each component in RevGNN.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= nov,
  articleno	= {1},
  numpages	= {26},
  keywords	= {Academic reviewer recommendation, expert finding,
		  GNN-based recommendation, negative sampling in GCL}
}

@InProceedings{	  10.1145/3660853.3660886,
  author	= {Tabaza, Abdulrahman and Quishawi, Omar and Yaghi,
		  Abdelrahman and Qawasmeh, Omar},
  title		= {Binding Text, Images, Graphs, and Audio for Music
		  Representation Learning},
  year		= {2024},
  isbn		= {9798400716928},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3660853.3660886},
  doi		= {10.1145/3660853.3660886},
  abstract	= {Abstract In the field of Information Retrieval and Natural
		  Language Processing, text embeddings play a significant
		  role in tasks such as classification, clustering, and topic
		  modeling. However, extending these embeddings to abstract
		  concepts such as music, which involves multiple modalities,
		  presents a unique challenge. Our work addresses this
		  challenge by integrating rich multi-modal data into a
		  unified joint embedding space. This space includes: (1)
		  textual, (2) visual, (3) acoustic, and (4) graph-based
		  modality features. By doing so, we mirror cognitive
		  processes associated with music interaction and overcome
		  the disjoint nature of individual modalities. The resulting
		  joint low-dimensional vector space facilitates retrieval,
		  clustering, embedding space arithmetic, and cross-modal
		  retrieval tasks. Importantly, our approach carries
		  implications for music information retrieval and
		  recommendation systems. Furthermore, we propose a novel
		  multi-modal model that integrates various data
		  types—text, images, graphs, and audio—for music
		  representation learning. Our model aims to capture the
		  complex relationships between different modalities,
		  enhancing the overall understanding of music. By combining
		  textual descriptions, visual imagery, graph-based
		  structures, and audio signals, we create a comprehensive
		  representation that can be leveraged for a wide range of
		  music-related tasks. Notably, our model demonstrates
		  promising results in music classification, and
		  recommendation systems. Code Availability: The source code
		  for the multi-modal music representation model described in
		  this paper is available on GitHub. Access and further
		  details can be found at the following repository link:
		  //github.com/a-tabaza/binding_music/},
  booktitle	= {Proceedings of the Cognitive Models and Artificial
		  Intelligence Conference},
  pages		= {139–146},
  numpages	= {8},
  location	= {undefinedstanbul, Turkiye},
  series	= {AICCONF '24}
}

@Proceedings{	  10.1145/3665348,
  title		= {GAIIS '24: Proceedings of the 2024 International
		  Conference on Generative Artificial Intelligence and
		  Information Security},
  year		= {2024},
  isbn		= {9798400709562},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Kuala Lumpur, Malaysia}
}

@InProceedings{	  10.1145/3626772.3657877,
  author	= {Leventidis, Aristotelis and Christensen, Martin Pek\'{a}r
		  and Lissandrini, Matteo and Di Rocco, Laura and Hose, Katja
		  and Miller, Ren\'{e}e J.},
  title		= {A Large Scale Test Corpus for Semantic Table Search},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657877},
  doi		= {10.1145/3626772.3657877},
  abstract	= {Table search aims to answer a query with a ranked list of
		  tables. Unfortunately, current test corpora have focused
		  mostly on needle-in-the-haystack tasks, where only a few
		  tables are expected to exactly match the query intent.
		  Instead, table search tasks often arise in response to the
		  need for retrieving new datasets or augmenting existing
		  ones, e.g., for data augmentation within data science or
		  machine learning pipelines. Existing table repositories and
		  benchmarks are limited in their ability to test retrieval
		  methods for table search tasks. Thus, to close this gap, we
		  introduce a novel dataset for query-by-example Semantic
		  Table Search. This novel dataset consists of two snapshots
		  of the large-scale Wikipedia tables collection from 2013
		  and 2019 with two important additions: (1) a page and topic
		  aware ground truth relevance judgment and (2) a large-scale
		  DBpedia entity linking annotation. Moreover, we generate a
		  novel set of entity-centric queries that allows testing
		  existing methods under a novel search scenario: semantic
		  exploratory search. The resulting resource consists of
		  9,296 novel queries, 610,553 query-table relevance
		  annotations, and 238,038 entity-linked tables from the 2013
		  snapshot. Similarly, on the 2019 snapshot, the resource
		  consists of 2,560 queries, 958,214 relevance annotations,
		  and 457,714 total tables. This makes our resource the
		  largest annotated table-search corpus to date (97 times
		  more queries and 956 times more annotated tables than any
		  existing benchmark). We perform a user study among domain
		  experts and prove that these annotators agree with the
		  automatically generated relevance annotations. As a result,
		  we can re-evaluate some basic assumptions behind existing
		  table search approaches identifying their shortcomings
		  along with promising novel research directions.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1142–1151},
  numpages	= {10},
  keywords	= {benchmark, query-by-example, semantic search, table
		  search},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Proceedings{	  10.1145/3677779,
  title		= {CMNM '24: Proceedings of the International Conference on
		  Modeling, Natural Language Processing and Machine
		  Learning},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xi'an, China}
}

@InProceedings{	  10.1145/3626772.3657666,
  author	= {Prieur, Maxime and Du Mouza, C\'{e}dric and Gadek,
		  Guillaume and Grilheres, Bruno},
  title		= {Shadowfax: Harnessing Textual Knowledge Base Population},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657666},
  doi		= {10.1145/3626772.3657666},
  abstract	= {Knowledge base population (KBP) from texts involves the
		  extraction and organization of information from
		  unstructured textual data to enhance or create a structured
		  knowledge base. This process is crucial for various
		  applications, such as natural language understanding,
		  question-answering systems, and knowledge-driven
		  decision-making. However the difficulty lies in the
		  complexity of natural language, which is nuanced,
		  ambiguous, and context-dependent. Extracting accurate and
		  reliable information requires overcoming challenges such as
		  entity disambiguation and relation extraction which are
		  time-consuming tasks for users.Shadowfax is an interactive
		  platform designed to support users by streamlining the
		  process of knowledge base population (KPB) from text
		  documents. Unlike other existing tools, it relies on a
		  unified machine learning model to extract relevant
		  information from unstructured text, enabling operational
		  agents to gain a quick overview. The proposed system
		  supports a variety of natural language processing (NLP)
		  tasks using a single architecture, while presenting
		  information in the most comprehensive way possible to the
		  end user.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2796–2800},
  numpages	= {5},
  keywords	= {data mining, deep-learning, end-to-end, information
		  extraction, knowledge base population, user in the loop},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Article{	  10.1145/3688841,
  author	= {Latendresse, Jasmine and Abedu, Samuel and Abdellatif,
		  Ahmad and Shihab, Emad},
  title		= {An Exploratory Study on Machine Learning Model
		  Management},
  year		= {2024},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {34},
  number	= {1},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3688841},
  doi		= {10.1145/3688841},
  abstract	= {Effective model management is crucial for ensuring
		  performance and reliability in Machine Learning (ML)
		  systems, given the dynamic nature of data and operational
		  environments. However, standard practices are lacking,
		  often resulting in ad hoc approaches. To address this, our
		  research provides a clear definition of ML model management
		  activities, processes, and techniques. Analyzing 227 ML
		  repositories, we propose a taxonomy of 16 model management
		  activities and identify 12 unique challenges. We find that
		  57.9% of the identified activities belong to the
		  maintenance category, with activities like refactoring
		  (20.5%) and documentation (18.3%) dominating. Our findings
		  also reveal significant challenges in documentation
		  maintenance (15.3%) and bug management (14.9%), emphasizing
		  the need for robust versioning tools and practices in the
		  ML pipeline. Additionally, we conducted a survey that
		  underscores a shift toward automation, particularly in
		  data, model, and documentation versioning, as key to
		  managing ML models effectively. Our contributions include a
		  detailed taxonomy of model management activities, a mapping
		  of challenges to these activities, practitioner-informed
		  solutions for challenge mitigation, and a publicly
		  available dataset of model management activities and
		  challenges. This work aims to equip ML developers with
		  knowledge and best practices essential for the robust
		  management of ML models.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= dec,
  articleno	= {16},
  numpages	= {31},
  keywords	= {Software engineering, machine learning, model management}
}

@Proceedings{	  10.1145/3610978,
  title		= {HRI '24: Companion of the 2024 ACM/IEEE International
		  Conference on Human-Robot Interaction},
  year		= {2024},
  isbn		= {9798400703232},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome one and all to the 19th Annual ACM/IEEE
		  International Conference on Human-Robot Interaction
		  (HRI)!We are so pleased to re-welcome the HRI community to
		  Boulder, Colorado, where HRI 2021 would have been held, had
		  the COVID pandemic not interfered. Following up on the
		  successful in-person conference held last year in Sweden,
		  this year's theme is "HRI in the Real World," and focuses
		  on advances that aim to bring human-robot interaction out
		  of the lab and into everyday life.One aspect of this that
		  we are very excited about is the introduction of a robot
		  challenge to the conference activities, where teams from
		  around the world will showcase their research and
		  development via actual, interactive robots in the "real
		  world" of an academic conference. It is our hope that this
		  feature will grow and develop over the coming years into a
		  staple of the HRI conference.This year's HRI conference saw
		  an impressive surge in global interest, with 352 full paper
		  submissions from around the world, marking a significant
		  40% increase compared to the previous year. These papers
		  were categorized under relevant thematic subcommittees and
		  underwent a double-blind review process, a rebuttal phase,
		  and selective shepherding by the HRI program committee.
		  From this process, 87 outstanding papers (24.7%) were
		  chosen for full presentation at the conference. Reflecting
		  our joint sponsorship with IEEE and ACM, all accepted
		  papers will be accessible in the ACM Digital Library and
		  IEEE Xplore.},
  location	= {Boulder, CO, USA}
}

@InProceedings{	  10.1145/3633637.3633677,
  author	= {Wang, Peng and Liu, Jingju},
  title		= {A Cyber Threat Entity Recognition Method Based on Robust
		  Feature Representation and Adversarial Training},
  year		= {2024},
  isbn		= {9798400707988},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3633637.3633677},
  doi		= {10.1145/3633637.3633677},
  abstract	= {With the development of Internet, cybersecurity attracts
		  people's attention. In order to better protect
		  cybersecurity, we can comprehensively analyze the security
		  events based on cyber threat intelligence. We aim to
		  identify correlations between security events to
		  proactively address potential threats. However, there are
		  still many challenges when people use cyber threat
		  intelligence. Cyber threat intelligence mainly exists in
		  unstructured form. It is necessary to extract the important
		  elements from it. We design a cyber threat entity
		  recognition method to help the analysis of cyber threat
		  intelligence. The formation of accurate and robust feature
		  representation is the key to realize the task of cyber
		  threat entity recognition, but the feature representation
		  of text is susceptible to noise interference. In order to
		  form an accurate representation of the text, we design a
		  robust feature representation method which extracts
		  features based on multiple perspectives and adopts a mutual
		  learning mechanism to promote feature interaction. It
		  adopts iterative fusion to form the final feature
		  representation. And we use an adversarial training
		  framework that can learn attack strategies to alleviate the
		  problem of noise interference. We conduct relevant
		  experiments on the cyber threat intelligence dataset DNRTI.
		  The experimental results show that our method can be used
		  in cyber threat intelligence analysis.},
  booktitle	= {Proceedings of the 2023 12th International Conference on
		  Computing and Pattern Recognition},
  pages		= {255–259},
  numpages	= {5},
  keywords	= {Adversarial training, Cyber threat intelligence, Feature
		  representation, Threat entity recognition},
  location	= {Qingdao, China},
  series	= {ICCPR '23}
}

@InProceedings{	  10.1145/3626772.3657659,
  author	= {Lee, Yuan-Chi and Yen, An-Zi and Huang, Hen-Hsen and Chen,
		  Hsin-Hsi},
  title		= {ConvLogRecaller: Real-Time Conversational Lifelog
		  Recaller},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657659},
  doi		= {10.1145/3626772.3657659},
  abstract	= {The popularization of networks fosters the convenience of
		  communication. People can easily share their life
		  experiences and thoughts with relatives and friends via
		  instant messaging software. As time passes, individuals may
		  forget certain details of life events, leading to
		  difficulties in effectively communicating with others. The
		  propensity of individuals to forget or mix up life events
		  highlights the importance of services aimed at retrieving
		  information about past experiences. This paper presents a
		  conversational information recall system, ConvLogRecaller,
		  which proactively supports real-time memory recall
		  assistance during online conversations. Given a
		  conversation of the user with others, ConvLogRecaller
		  suggests a message if the user forgets the details of the
		  life experiences. The services provided by our system can
		  avoid hesitations or memory lapses that might hinder the
		  efficiency of a conversation.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2724–2728},
  numpages	= {5},
  keywords	= {conversational lifelogs retrieval, lifelogging, proactive
		  information recall},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Proceedings{	  10.1145/3663529,
  title		= {FSE 2024: Companion Proceedings of the 32nd ACM
		  International Conference on the Foundations of Software
		  Engineering},
  year		= {2024},
  isbn		= {9798400706585},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {We are pleased to welcome all delegates to FSE 2024, the
		  ACM International Conference on the Foundations of Software
		  Engineering (FSE) 2024. The conference now has a shorter
		  name! FSE is an internationally renowned forum for
		  researchers, practitioners, and educators to present and
		  discuss the most recent innovations, trends, experiences,
		  and challenges in the field of software engineering. FSE
		  brings together experts from academia and industry to
		  exchange the latest research results and trends as well as
		  their practical application in all areas of software
		  engineering.},
  location	= {Porto de Galinhas, Brazil}
}

@Article{	  10.1145/3654984,
  author	= {Chen, Kaiwen and Koudas, Nick},
  title		= {Unstructured Data Fusion for Schema and Data Extraction},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {3},
  url		= {https://doi.org/10.1145/3654984},
  doi		= {10.1145/3654984},
  abstract	= {Recently, there has been significant interest in
		  extracting actionable insights from the abundance of
		  unstructured textual data. In this paper, we introduce a
		  novel problem, which we term Semistructured Schema and Data
		  Extraction (SDE). This task aims to enhance and complete
		  tables using information discovered from textual
		  repositories, given partial table specifications in the
		  form of queries. To effectively solve SDE, several
		  challenges must be overcome, which involve transforming the
		  partial table specifications into effective queries,
		  retrieving relevant documents, discerning values for
		  partially specified attributes, inferring additional
		  attributes, and constructing an enriched output table while
		  mitigating the influence of false positives from the
		  retrieval.We propose an end-to-end pipeline for SDE, which
		  consists of a retrieval component and an augmentation
		  component, to address each of the challenges. In the
		  retrieval component, we serialize the partial table
		  specifications into a query and employ a dense passage
		  retrieval algorithm to extract the top-k relevant results
		  from the text repository. Subsequently, the augmentation
		  component ingests the output documents from the retrieval
		  phase and generates an enriched table. We formulate this
		  table enrichment task as a unique sequence-to-sequence
		  task, distinct from traditional approaches, as it operates
		  on multiple documents during generation. Utilizing an
		  interpolation mechanism on the encoder output, our model
		  maintains a nearly constant context length while
		  automatically prioritizing the importance of documents
		  during the generation. Due to the novelty of SDE, we
		  establish a validation methodology, adapting and expanding
		  existing benchmarks with the use of powerful large language
		  models. Our extensive experiments show that our method
		  achieves high accuracy in enriching query tables through
		  multi-document fusion, while also surpassing baseline
		  methods in both accuracy and computational efficiency.},
  journal	= {Proc. ACM Manag. Data},
  month		= may,
  articleno	= {181},
  numpages	= {26},
  keywords	= {data fusion, information extraction, schema extraction}
}

@Proceedings{	  10.1145/3681716,
  title		= {Mindtrek '24: Proceedings of the 27th International
		  Academic Mindtrek Conference},
  year		= {2024},
  isbn		= {9798400718236},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tampere, Finland}
}

@Article{	  10.1145/3696117.3696122,
  author	= {Simensen, John Eidar and Esnoul, Coralie and Jee,
		  Eunkyoung and Babar, Ali and Minh Le, Triet Huynh and
		  Rashid, Awais},
  title		= {Report on the 5th International Workshop on Engineering
		  and Cybersecurity of Critical Systems and 2nd International
		  Workshop on Software Vulnerability Management (EnCyCriS/SVM
		  - 2024)},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {49},
  number	= {4},
  issn		= {0163-5948},
  url		= {https://doi.org/10.1145/3696117.3696122},
  doi		= {10.1145/3696117.3696122},
  abstract	= {Increasing system interconnectivity, decentralization, and
		  introduction of new, more intelligent technologies, result
		  in critical infrastructures becoming exposed to increased
		  risk of cyber, physical, and combine cyber-physical
		  attacks. Cyber-attacks on critical systems can inflict
		  severe consequences on to people, society, economy, and
		  national security, and can have adverse effects on safety
		  and reliability of critical infrastructures. The joint
		  EnCyCriS-SVM workshop facilitates discourse and discussion
		  among researchers, practitioner, and students who are
		  working on challenges and solutions related to the
		  industrial revolution. Focus is given on sharing industry
		  experience and project results pertaining to cyber threats
		  on critical systems; secure software engineering; and
		  attack detection and response mechanisms.},
  journal	= {SIGSOFT Softw. Eng. Notes},
  month		= oct,
  pages		= {18–21},
  numpages	= {4},
  keywords	= {critical infrastructures., cybersecurity, safety, software
		  engineering, systems engineering}
}

@InProceedings{	  10.1145/3677779.3677802,
  author	= {Yue, Liu and Liu, Shengquan and Zhao, Ming and Guo,
		  Quanjiang},
  title		= {Research on Distant Supervision Relation Extraction based
		  on Attention Graph Enhancement and Dynamic Loss},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677779.3677802},
  doi		= {10.1145/3677779.3677802},
  abstract	= {Distant supervision relation extraction utilizes alignment
		  with existing knowledge bases to collect training data. In
		  current research, entity types and relation types are often
		  embedded separately in sentences or bags, ignoring the
		  semantic connections between entity types and relation
		  types. To address this issue, entity types, and relation
		  types are concatenated to form an entity-relation graph. In
		  this paper, the entity-relation graph is integrated into
		  the relation extraction model using Attention Graph
		  Enhancement (Attention Graph Enhancement, AGE) to identify
		  relations. Furthermore, the imbalance in the number of
		  training instances in distant supervision, with fewer
		  training instances for long-tail relations, leads to
		  insufficient extraction of long-tail relations. In this
		  paper, a dynamic loss (Dynamic Loss, DL) is constructed
		  based on the number of relation instances to design a
		  weight-based dynamic loss function to optimize the
		  proportion of head and long-tail relations instances during
		  training. Each training round dynamically updates the
		  training instance weights based on the loss, emphasizing
		  long-tail relation training in the following training
		  stage. The experimental results show that the proposed
		  method improves the AUC values by 1.9% and 2.4% on the
		  NYT-520K and NYT-570K datasets.},
  booktitle	= {Proceedings of the International Conference on Modeling,
		  Natural Language Processing and Machine Learning},
  pages		= {139–146},
  numpages	= {8},
  location	= {Xi'an, China},
  series	= {CMNM '24}
}

@Proceedings{	  10.1145/3625007,
  title		= {ASONAM '23: Proceedings of the 2023 IEEE/ACM International
		  Conference on Advances in Social Networks Analysis and
		  Mining},
  year		= {2023},
  isbn		= {9798400704093},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The ASONAM conference series brings together researchers
		  from around the world to share the latest advances in the
		  attractive field of Social Networks Analysis and Mining.},
  location	= {Kusadasi, Turkiye}
}

@Proceedings{	  10.1145/3627043,
  title		= {UMAP '24: Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  year		= {2024},
  isbn		= {9798400704338},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Cagliari, Italy}
}

@Proceedings{	  10.1145/3605098,
  title		= {SAC '24: Proceedings of the 39th ACM/SIGAPP Symposium on
		  Applied Computing},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {On behalf of the Organizing Committee, I extend a warm
		  welcome to you at the 39th Annual ACM Symposium on Applied
		  Computing (SAC 2024), taking place in \'{A}vila, Spain, and
		  hosted by the University of Salamanca. For more than three
		  decades, this international forum has been dedicated to
		  computer scientists, engineers, and practitioners,
		  providing a platform for presenting their research findings
		  and results in various areas of applied computing. The
		  organizing committee sincerely appreciates your
		  participation in this exciting international event, and we
		  hope that the conference proves interesting and beneficial
		  for all attendees.},
  location	= {Avila, Spain}
}

@Proceedings{	  10.1145/3659677,
  title		= {NISS '24: Proceedings of the 7th International Conference
		  on Networking, Intelligent Systems and Security},
  year		= {2024},
  isbn		= {9798400709296},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Meknes, AA, Morocco}
}

@Proceedings{	  10.1145/3696500,
  title		= {ICBDDM '24: Proceedings of the 2024 International
		  Conference on Big Data and Digital Management},
  year		= {2024},
  isbn		= {9798400710278},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shanghai, China}
}

@InProceedings{	  10.1109/jcdl57899.2023.00012,
  author	= {Chekuri, Satvik and Chandrasekar, Prashant and Banerjee,
		  Bipasha and Park, Sung Hee and Masrourisaadat, Nila and
		  Ahuja, Aman and Ingram, William A. and Fox, Edward A.},
  title		= {Integrated Digital Library System for Long Documents and
		  their Elements},
  year		= {2024},
  isbn		= {9798350399318},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL57899.2023.00012},
  doi		= {10.1109/JCDL57899.2023.00012},
  abstract	= {We describe a next-generation integrated Digital Library
		  (DL) system that addresses the numerous goals associated
		  with long documents such as Electronic Theses and
		  Dissertations (ETDs). Our extensible workflow-centric
		  design supports a variety of users/personas (e.g.,
		  researchers, curators, and experimenters) who can benefit
		  from improved access to ETDs and the content buried
		  therein. Our approach leverages natural language
		  processing, deep learning, information retrieval, and
		  software engineering methods. The services cover ingesting,
		  storing, curating, analyzing, detecting, extracting,
		  classifying, summarizing, topic modeling, browsing,
		  searching, retrieving, recommending, visualizing/reporting,
		  and interacting with ETDs and derivative text/image-based
		  elements/objects. Workflows connect the services and their
		  APIs, along with UI-based access. We believe our approach
		  can guide others to combine tailored user support,
		  research, and education by way of extensible DLs.},
  booktitle	= {Proceedings of the 2023 ACM/IEEE Joint Conference on
		  Digital Libraries},
  pages		= {13–24},
  numpages	= {12},
  keywords	= {digital library, information system, information
		  retrieval, deep learning, NLP},
  location	= {Santa Fe, New Mexico, USA},
  series	= {JCDL '23}
}

@InProceedings{	  10.1145/3652620.3687820,
  author	= {Moln\'{a}r, Vince and Graics, Bence and V\"{o}r\"{o}s,
		  Andr\'{a}s and Tonetta, Stefano and Cristoforetti, Luca and
		  Kimberly, Greg and Dyer, Pamela and Giammarco, Kristin and
		  Koethe, Manfred and Hester, John and Smith, Jamie and
		  Grimm, Christoph},
  title		= {Towards the Formal Verification of SysML v2 Models},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3687820},
  doi		= {10.1145/3652620.3687820},
  abstract	= {Systems Modeling Language (SysML) is the de facto standard
		  in the industry for modeling complex systems. SysML v2 is
		  the new version of the language with reworked fundamentals.
		  In this paper, we explore how the new formal semantics of
		  SysML v2 can enable formal verification and various forms
		  of automated reasoning. Formal verification involves
		  mathematically proving the correctness of a system's design
		  with respect to certain specifications or properties. This
		  rigorous approach ensures that models behave as intended
		  under all possible conditions. Through a detailed
		  examination, we demonstrate how five specific tools -
		  Gamma, MP-Firebird, Imandra, SAVVS, and SysMD - can
		  formally analyze SysML v2 models. We show how these tools
		  support the different concepts in the language, as well as
		  the set of features and technologies they provide to users
		  of SysML v2, such as model checking, theorem proving,
		  contract-based design, or automatic fault injections. We
		  propose a workflow for applying formal methods on SysML v2
		  models, illustrated by example models and artifacts
		  generated by the above tools.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {1086–1095},
  numpages	= {10},
  keywords	= {SysML V2, systems modeling, formal methods, verification
		  and validation, automated reasoning, tools},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3670105.3670124,
  author	= {Pang, Ronghui},
  title		= {Research on Text Classification Applications Based on NLP
		  Technology},
  year		= {2024},
  isbn		= {9798400716751},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670105.3670124},
  doi		= {10.1145/3670105.3670124},
  abstract	= {Text classification is a fundamental task in enterprise
		  applications research based on natural language process-
		  ing (NLP) technology. Faced with massive text data
		  generated by enterprise users on the Internet, how to
		  effectively classify and apply them is one of the
		  challenges that enterprises face in management. This
		  article takes user-generated content (UGC) on e-commerce
		  platforms as the research background and uses the BERT
		  model to classify the textual content generated by users.
		  The model achieves an accuracy rate of up to 81%. The
		  article demonstrates that compared with the W2V+SVM model,
		  the BERT model can effectively address the problem of
		  overly complex and disordered web text. This research
		  provides a viable solution for enterprise managers to more
		  effectively manage and utilize massive text data. Through
		  text classification technology, enterprises can promptly
		  understand consumer sentiment, product preferences, and
		  feedback, thereby guiding decisions in marketing
		  strategies, product development, and customer service.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computing, Networks and Internet of Things},
  pages		= {108–111},
  numpages	= {4},
  keywords	= {BERT, SVM, Text Classification, Word Embedding},
  location	= {Tokyo, Japan},
  series	= {CNIOT '24}
}

@Proceedings{	  10.1145/3634814,
  title		= {ASSE '23: Proceedings of the 2023 4th Asia Service
		  Sciences and Software Engineering Conference},
  year		= {2023},
  isbn		= {9798400708534},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Aizu-Wakamatsu City, Japan}
}

@InProceedings{	  10.1145/3670105.3670127,
  author	= {Li, Peng and Liu, Zhiqi and Pang, WeiJian and Cao, Jiang},
  title		= {Semantic Collaboration: A Collaborative Approach for
		  Multi-Agent Systems Based on Semantic Communication},
  year		= {2024},
  isbn		= {9798400716751},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670105.3670127},
  doi		= {10.1145/3670105.3670127},
  abstract	= {Abstract. In order to meet the practical needs of
		  different intelligent agents collaborating to execute
		  tasks, this paper has studied the multi-agent collaboration
		  mode based on semantic communication and proposed a
		  multi-agent collaboration method based on semantic
		  communication. Firstly, the basic concept, basic framework,
		  typical process and application of semantic communication
		  are systematically described. On this basis, the basic
		  concept and framework of multi-agent collaboration based
		  are proposed, and the differences and relations between
		  semantic communication and semantic collaboration are
		  analyzed. Subsequently, the paper provides a detailed
		  introduction to the typical process of semantic
		  collaboration. Next, the key supporting technologies of
		  semantic collaboration are analyzed. At last, taking the
		  multi-agent cooperative search and rescue task as an
		  example, the application mode of semantic collaboration is
		  introduced.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computing, Networks and Internet of Things},
  pages		= {123–132},
  numpages	= {10},
  keywords	= {Multi-Agent Systems, Semantic Collaboration, Semantic
		  Communication},
  location	= {Tokyo, Japan},
  series	= {CNIOT '24}
}

@Article{	  10.1109/taslp.2024.3350905,
  author	= {Jiang, Shu and Li, Zuchao and Zhao, Hai and Ding,
		  Weiping},
  title		= {Entity-Relation Extraction as Full Shallow Semantic
		  Dependency Parsing},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3350905},
  doi		= {10.1109/TASLP.2024.3350905},
  abstract	= {Entity-relation extraction is the essential information
		  extraction task and can be decomposed into Named Entity
		  Recognition (NER) and Relation Extraction (RE) subtasks.
		  This paper proposes a novel joint entity-relation
		  extraction method that models the entity-relation
		  extraction task as full shallow semantic dependency graph
		  parsing. Specifically, it jointly and simultaneously
		  converts the entities and relation mentions as the edges of
		  the semantic dependency graph to be parsed and their types
		  as the labels. This model also integrates the advantages of
		  multiple feature tagging methods and enriches the token
		  representation. Furthermore, second-order scoring is
		  introduced to exploit the relationships between entities
		  and relations, which improves the model performance. Our
		  work is the first time to fully model entities and
		  relations into a graph and uses higher-order modules to
		  address their interaction problems. Compared with
		  state-of-the-art scores on five benchmarks (ACE04, ACE05,
		  CoNLL04, ADE, and SciERC), empirical results show that our
		  proposed model makes significant improvements and
		  demonstrates its effectiveness and practicability.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jan,
  pages		= {1088–1099},
  numpages	= {12}
}

@Proceedings{	  10.1145/3643991,
  title		= {MSR '24: Proceedings of the 21st International Conference
		  on Mining Software Repositories},
  year		= {2024},
  isbn		= {9798400705878},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {MSR is a thriving research community that organizes a
		  yearly conference with a solid reputation amongst software
		  engineering researchers.},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3687272,
  title		= {HAI '24: Proceedings of the 12th International Conference
		  on Human-Agent Interaction},
  year		= {2024},
  isbn		= {9798400711787},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Swansea, United Kingdom}
}

@Article{	  10.1145/3678879,
  author	= {Xu, Jingyun and Yu, Junnan and Cai, Yi and Chua,
		  Tat-Seng},
  title		= {Dual Contrastive Learning for Cross-Domain Named Entity
		  Recognition},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3678879},
  doi		= {10.1145/3678879},
  abstract	= {Benefiting many information retrieval applications, named
		  entity recognition (NER) has shown impressive progress.
		  Recently, there has been a growing trend to decompose
		  complex NER tasks into two subtasks (e.g., entity span
		  detection (ESD) and entity type classification (ETC), to
		  achieve better performance. Despite the remarkable success,
		  from the perspective of representation, existing methods do
		  not explicitly distinguish non-entities and entities, which
		  may lead to ESD errors. Meanwhile, they do not explicitly
		  distinguish entities with different entity types, which may
		  lead to entity type misclassification. As such, the limited
		  representation abilities may challenge some competitive NER
		  methods, leading to unsatisfactory performance, especially
		  in the low-resource setting (e.g., cross-domain NER). In
		  light of these challenges, we propose to utilize
		  contrastive learning to refine the original chaotic
		  representations and learn the generalized representations
		  for cross-domain NER. In particular, this article proposes
		  a dual contrastive learning model (Dual-CL), which
		  respectively utilizes a token-level contrastive learning
		  module and a sentence-level contrastive learning module to
		  enhance ESD, ETC for cross-domain NER. Empirical results on
		  10 domain pairs under two different settings show that
		  Dual-CL achieves better performances than compared
		  baselines in terms of several standard metrics. Moreover,
		  we conduct detailed analyses to are presented to better
		  understand each component’s effectiveness.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= oct,
  articleno	= {163},
  numpages	= {33},
  keywords	= {Named Entity Recognition, Cross-domain, Contrastive
		  Learning}
}

@Proceedings{	  10.1145/3674558,
  title		= {ICCTA '24: Proceedings of the 2024 10th International
		  Conference on Computer Technology Applications},
  year		= {2024},
  isbn		= {9798400716386},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Vienna, Austria}
}

@Proceedings{	  10.1145/3695719,
  title		= {ICDLT '24: Proceedings of the 2024 8th International
		  Conference on Deep Learning Technologies},
  year		= {2024},
  isbn		= {9798400716867},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3670085.3670104,
  author	= {Yao, zhan'ao and Yang, Hongxin and Chen, Tingwei},
  title		= {A pruning-based word-centered context fragment extraction
		  method for relation extraction},
  year		= {2024},
  isbn		= {9798400717284},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3670085.3670104},
  doi		= {10.1145/3670085.3670104},
  abstract	= {Neural relationship extraction is an important task in
		  natural language processing, aimed at extracting
		  relationships between target entity pairs from a given
		  text. In recent years, with the development of deep neural
		  networks, various types of neural networks to extract
		  sentence entity-level, fragment-level, and sentence-level
		  features for relationship extraction have become a
		  mainstream research direction. Most existing studies use
		  the BERT model to embed sentences and then use CNN to
		  manipulate all words in the entire sentence to obtain
		  fragment-level features. This article proposes a new
		  word-centered context fragment-level method based on
		  pruning the shortest dependency path between entity pairs.
		  We demonstrate that using a pruning method based on the
		  shortest dependency path between entity pairs can
		  effectively improve the ability of model fragments and
		  information extraction. We evaluated our method on a public
		  benchmark: SemEval 2010 Task 8. The experimental results
		  show that our method outperforms the advanced model using
		  BERT as the embedding.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Mathematics and Artificial Intelligence},
  pages		= {97–104},
  numpages	= {8},
  keywords	= {Neural relation extraction, Pruning method, Shortest
		  dependency path, Word-centered context fragment},
  location	= {Beijing, China},
  series	= {ICMAI '24}
}

@Proceedings{	  10.1145/3670474,
  title		= {MLCAD '24: Proceedings of the 2024 ACM/IEEE International
		  Symposium on Machine Learning for CAD},
  year		= {2024},
  isbn		= {9798400706998},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Salt Lake City, UT, USA}
}

@InProceedings{	  10.1145/3613905.3644065,
  author	= {Xu, Chunchen and Ge, Xiao},
  title		= {AI as a Child of Mother Earth: Regrounding Human-AI
		  Interaction in Ecological Thinking},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3644065},
  doi		= {10.1145/3613905.3644065},
  abstract	= {The anthropocentric cultural idea that humans are active
		  agents exerting control over their environments has been
		  largely normalized and inscribed in practices, policies,
		  and products of contemporary industrialized societies. This
		  view underlies a human-ecology relationship based on
		  resource and knowledge extraction. To create a more
		  sustainable and equitable future, it is essential to
		  consider alternative cultural ideas rooted in ecological
		  thinking. This perspective underscores the
		  interconnectedness between humans and more-than-human
		  worlds. We propose a path to reshape the human-ecology
		  relationship by advocating for alternative human-AI
		  interactions. In this paper, we undertake a critical
		  comparison between anthropocentrism and ecological
		  thinking, using storytelling to illustrate various human-AI
		  interactions that embody ecological thinking. We also
		  delineate a set of design principles aimed at guiding AI
		  developments toward fostering a more caring human-ecology
		  relationship.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {546},
  numpages	= {9},
  keywords	= {AI, Anthropocentrism, Culture, Design, Ecological
		  thinking, Environmental justice, Human-ecology
		  relationship, More-than-human, Storytelling,
		  Sustainability},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@Proceedings{	  10.1145/3672919,
  title		= {CSAIDE '24: Proceedings of the 2024 3rd International
		  Conference on Cyber Security, Artificial Intelligence and
		  Digital Economy},
  year		= {2024},
  isbn		= {9798400718212},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Nanjing, China}
}

@InProceedings{	  10.1145/3654777.3676462,
  author	= {Khanal, Nabin and Yu, Chun Meng and Chiu, Jui-Cheng and
		  Chaudhary, Anav and Zhang, Ziyue and Katija, Kakani and
		  Forbes, Angus G.},
  title		= {FathomGPT: A natural language interface for interactively
		  exploring ocean science data},
  year		= {2024},
  isbn		= {9798400706288},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3654777.3676462},
  doi		= {10.1145/3654777.3676462},
  abstract	= {We introduce FathomGPT, an open source system for the
		  interactive investigation of ocean science data via a
		  natural language interface. FathomGPT was developed in
		  close collaboration with marine scientists to enable
		  researchers to explore and analyze the FathomNet image
		  database. FathomGPT provides a custom information retrieval
		  pipeline that leverages OpenAI’s large language models to
		  enable: the creation of complex queries to retrieve images,
		  taxonomic information, and scientific measurements; mapping
		  common names and morphological features to scientific
		  names; generating interactive charts on demand; and
		  searching by image or specified patterns within an image.
		  In designing FathomGPT, particular emphasis was placed on
		  enhancing the user’s experience by facilitating free-form
		  exploration and optimizing response times. We present an
		  architectural overview and implementation details of
		  FathomGPT, along with a series of ablation studies that
		  demonstrate the effectiveness of our approach to name
		  resolution, fine tuning, and prompt modification. We also
		  present usage scenarios of interactive data exploration
		  sessions and document feedback from ocean scientists and
		  machine learning experts.},
  booktitle	= {Proceedings of the 37th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {95},
  numpages	= {15},
  keywords	= {Natural Language Interfaces, Ocean Science, Scientific
		  Databases},
  location	= {Pittsburgh, PA, USA},
  series	= {UIST '24}
}

@Proceedings{	  10.1145/3666094,
  title		= {PDC '24: Proceedings of the Participatory Design
		  Conference 2024: Full Papers - Volume 1},
  year		= {2024},
  isbn		= {9798400708084},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {1},
  location	= {Sibu, Malaysia}
}

@InProceedings{	  10.1145/3639474.3640083,
  author	= {Parthasarathy, P. D. and Joshi, Swaroop},
  title		= {Teaching Digital Accessibility to Industry Professionals
		  using the Community of Practice framework: An Experience
		  Report},
  year		= {2024},
  isbn		= {9798400704987},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639474.3640083},
  doi		= {10.1145/3639474.3640083},
  abstract	= {Despite recent initiatives aimed at improving
		  accessibility, the field of digital accessibility remains
		  markedly behind contemporary advancements in the software
		  industry, as many real-world software and web applications
		  continue to fall short of accessibility requirements. A
		  persisting skills deficit within the existing technology
		  workforce has been an enduring impediment, hindering
		  organizations from delivering truly accessible software
		  products. This, in turn, elevates the risk of isolating and
		  excluding a substantial portion of potential users. In this
		  paper, we report lessons learned from a training program
		  for teaching digital accessibility using the Communities of
		  Practice (CoP) framework to industry professionals. We
		  recruited 66 participants from a large multinational
		  software company and assigned them to two groups: one
		  participating in a CoP and the other using self-paced
		  learning. We report experiences from designing the training
		  program, conducting the actual training, and assessing the
		  efficiency of the two approaches. Based on these findings,
		  we provide recommendations for practitioners in Learning
		  and Development teams and educators in designing
		  accessibility courses for industry professionals.},
  booktitle	= {Proceedings of the 46th International Conference on
		  Software Engineering: Software Engineering Education and
		  Training},
  pages		= {191–200},
  numpages	= {10},
  keywords	= {accessibility, massive open online courses, community of
		  practice, computing education, global computing education},
  location	= {Lisbon, Portugal},
  series	= {ICSE-SEET '24}
}

@InProceedings{	  10.1145/3640457.3688014,
  author	= {Schellingerhout, Roan},
  title		= {Explainable Multi-Stakeholder Job Recommender Systems},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688014},
  doi		= {10.1145/3640457.3688014},
  abstract	= {Public opinion on recommender systems has become
		  increasingly wary in recent years. In line with this trend,
		  lawmakers have also started to become more critical of such
		  systems, resulting in the introduction of new laws focusing
		  on aspects such as privacy, fairness, and explainability
		  for recommender systems and AI at large. These concepts are
		  especially crucial in high-risk domains such as
		  recruitment. In recruitment specifically, decisions carry
		  substantial weight, as the outcomes can significantly
		  impact individuals’ careers and companies’ success.
		  Additionally, there is a need for a multi-stakeholder
		  approach, as these systems are used by job seekers,
		  recruiters, and companies simultaneously, each with its own
		  requirements and expectations. In this paper, I summarize
		  my current research on the topic of explainable,
		  multi-stakeholder job recommender systems and set out a
		  number of future research directions.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {1318–1322},
  numpages	= {5},
  keywords	= {Explainable AI, Graph Neural Networks, Job Recommender
		  Systems, Knowledge Graphs, Multi-Stakeholder
		  Recommendation},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@Proceedings{	  10.1145/3701625,
  title		= {SBQS '24: Proceedings of the XXIII Brazilian Symposium on
		  Software Quality},
  year		= {2024},
  isbn		= {9798400717772},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3637528.3671973,
  author	= {Lin, Fake and Zhao, Ziwei and Zhu, Xi and Zhang, Da and
		  Shen, Shitian and Li, Xueying and Xu, Tong and Zhang,
		  Suojuan and Chen, Enhong},
  title		= {When Box Meets Graph Neural Network in Tag-aware
		  Recommendation},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671973},
  doi		= {10.1145/3637528.3671973},
  abstract	= {Last year has witnessed the re-flourishment of tag-aware
		  recommender systems supported by the LLM-enriched tags.
		  Unfortunately, though large efforts have been made, current
		  solutions may fail to describe the diversity and
		  uncertainty inherent in user preferences with only
		  tag-driven profiles. Recently, with the development of
		  geometry-based techniques, e.g., box embeddings, the
		  diversity of user preferences now could be fully modeled as
		  the range within a box in high dimension space. However,
		  defect still exists as these approaches are incapable of
		  capturing high-order neighbor signals, i.e., semantic-rich
		  multi-hop relations within the user-tag-item tripartite
		  graph, which severely limits the effectiveness of user
		  modeling. To deal with this challenge, in this paper, we
		  propose a novel framework, called BoxGNN, to perform
		  message aggregation via combinations of logical operations,
		  thereby incorporating high-order signals. Specifically, we
		  first embed users, items, and tags as hyper-boxes rather
		  than simple points in the representation space, and define
		  two logical operations, i.e., union and intersection, to
		  facilitate the subsequent process. Next, we perform the
		  message aggregation mechanism via the combination of
		  logical operations, to obtain the corresponding high-order
		  box representations. Finally, we adopt a volume-based
		  learning objective with Gumbel smoothing techniques to
		  refine the representation of boxes. Extensive experiments
		  on two publicly available datasets and one LLM-enhanced
		  e-commerce dataset have validated the superiority of BoxGNN
		  compared with various state-of-the-art baselines. The code
		  is released online: https://github.com/critical88/BoxGNN.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {1770–1780},
  numpages	= {11},
  keywords	= {box embedding, graph neural networks, recommendation
		  system, tag-aware recommendation},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Proceedings{	  10.1145/3694811,
  title		= {GNNet '24: Proceedings of the 3rd GNNet Workshop on Graph
		  Neural Networking Workshop},
  year		= {2024},
  isbn		= {9798400712548},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the Third
		  International Workshop on Graph Neural Networking - GNNet
		  2024, co-located with ACM CoNEXT 2024.Graphs are emerging
		  as an abstraction to represent complex data. Computer
		  Networks are fundamentally graphs, and many of their
		  relevant characteristics - such as topology and routing -
		  are represented as graph-structured data. Machine learning,
		  especially deep representation learning on graphs, is an
		  emerging field with a wide array of applications. Within
		  this field, Graph Neural Networks (GNNs) have been recently
		  proposed to model and learn over graph-structured data. Due
		  to their unique ability to generalize over graph data, GNNs
		  are a central tool to apply AI/ML techniques to networking
		  applications.The GNNet workshop continues its tradition of
		  providing the first dedicated venue to present and discuss
		  the latest advancements on the emerging topic of GNNs
		  applied to computer networking problems. GNNet brings
		  together leaders from academia and industry to showcase
		  recent methodological advances of GNNs and their
		  application to computer networks, covering a wide range of
		  applications and practical challenges for training and
		  deployment. The GNNet workshop serves as the meeting point
		  for the growing community on this fascinating domain, which
		  previously did not have a specific forum for sharing ideas
		  and discussion.The third edition of the GNNet workshop is
		  co-located with ACM CoNEXT 2024 and held in Los Angeles,
		  CA, USA, in December 2024. The GNNet 2024 technical program
		  consists of 9 quality papers. The TPC was composed of 21
		  well-recognized researchers and practitioners in the areas
		  of GNN and AI/ML applied to computer networks.},
  location	= {Los Angeles, CA, USA}
}

@Proceedings{	  10.1145/3671151,
  title		= {CIBDA '24: Proceedings of the 5th International Conference
		  on Computer Information and Big Data Applications},
  year		= {2024},
  isbn		= {9798400718106},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Wuhan, China}
}

@Proceedings{	  10.1145/3643662,
  title		= {EnCyCriS/SVM '24: Proceedings of the 2024 ACM/IEEE 4th
		  International Workshop on Engineering and Cybersecurity of
		  Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second
		  International Workshop on Software Vulnerability},
  year		= {2024},
  isbn		= {9798400705656},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Increasing system interconnectivity, decentralization, and
		  introduction of new, more intelligent technologies, result
		  in critical infrastructures becoming exposed to increased
		  risk of cyber, physical, and combined cyber-physical
		  attacks. Cyberattacks on critical systems can inflict
		  severe consequences to people, society, economy, and
		  national security, and can have adverse effects on safety
		  and reliability of critical infrastructures. The joint
		  EnCyCriS-SVM workshop facilitates discourse and discussions
		  among researchers, practitioners, and students who are
		  working on challenges and solutions related to the
		  industrial revolution. Focus is given on sharing industry
		  experience and project results pertaining to cyber threats
		  on critical systems; secure software engineering; and
		  attack detection and response mechanisms.},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3652620.3686246,
  author	= {Siddeshwar, Vaishali and Alwidian, Sanaa and Makrehchi,
		  Masoud},
  title		= {A Comparative Study of Large Language Models for Goal
		  Model Extraction},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3686246},
  doi		= {10.1145/3652620.3686246},
  abstract	= {User stories, expressed in snippets of natural language
		  text, are commonly used to elicit stakeholder's needs in
		  agile software development. Requirement engineers model
		  user stories to interpret the relations among goals and
		  requirements. Manual transformation of goal models has
		  challenges such as, difficulty of converting
		  lower-abstraction user stories into higher-level goals, and
		  extraction of goals embedded in user stories depends on the
		  skill of requirements engineers. In this paper we introduce
		  a technique that leverages Large Language Models (LLMs) to
		  automatically generate goal models from user stories. The
		  approach uses Iterative Prompt Engineering that guides LLM
		  to extract intentional elements and generate its
		  XML-compatible representation in Goal-oriented Requirements
		  Language (GRL). The generated models can be visualized
		  using jUCMNav tool. We evaluated our approach using three
		  LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation
		  indicates that GPT-4 or Llama can be used to assist
		  requirements engineers in modeling as they can produce GRL
		  goal models that are understandable. Additionally, these
		  LLMs are capable of exposing soft goals that are not
		  apparent to stakeholders who are new to the domain.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {253–263},
  numpages	= {11},
  keywords	= {goal-oriented requirement language (GRL), goal modeling,
		  user story, agile development, requirements engineering,
		  large language models (LLMS), GPT-4, llama, cohere},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3695080.3695096,
  author	= {Chi, Wenxin and Wei, Shijie},
  title		= {Construction of marketing risk perception framework based
		  on large model agent},
  year		= {2024},
  isbn		= {9798400710223},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3695080.3695096},
  doi		= {10.1145/3695080.3695096},
  abstract	= {The new generation of artificial intelligence technology
		  represented by the Chat-GPT large model is profoundly
		  affecting all walks of life. The marketing risk perception
		  framework based on large model agents brings new
		  opportunities for marketing risk analysis of large
		  enterprises. The author takes China Mobile's large-model
		  intelligent marketing entity risk management and control as
		  an example to discuss the application exploration of
		  large-model intelligent entities in corporate marketing
		  risks. This article first discusses the various
		  opportunities and effects that large model intelligence
		  brings to corporate marketing; secondly, it elaborates on
		  its construction framework, then analyzes its user
		  intentions and assesses risks, and finally elaborates on
		  its operational logic. Through the above steps, the role of
		  the marketing risk perception framework based on large
		  model agents in the field of corporate marketing is
		  systematically analyzed in terms of risk prediction, risk
		  analysis, and risk prevention and control. This will
		  systematically and comprehensively lay a practical
		  foundation for large model agents in the field of marketing
		  risk perception.},
  booktitle	= {Proceedings of the 2024 International Conference on Cloud
		  Computing and Big Data},
  pages		= {93–99},
  numpages	= {7},
  location	= {Dali, China},
  series	= {ICCBD '24}
}

@InProceedings{	  10.1145/3661725.3661766,
  author	= {Nir, Oron and Vidra, Idan and Neeman, Avi and Kinarti,
		  Barak and Shamir, Ariel},
  title		= {VCR: Video representation for Contextual Retrieval},
  year		= {2024},
  isbn		= {9798400716393},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3661725.3661766},
  doi		= {10.1145/3661725.3661766},
  abstract	= {Streamlining content discovery in media archives requires
		  advanced data representations and effective visualization
		  techniques for clear communication of video topics to
		  users. The proposed system addresses the challenge of
		  efficiently navigating large video collections by
		  exploiting a fusion of visual, audio, and textual features
		  to accurately index and categorize video content through a
		  text-based method. Additionally, semantic embeddings are
		  employed to provide contextually relevant information and
		  recommendations to users, resulting in an intuitive and
		  engaging exploratory experience over our topics ontology
		  map using LLMs (GitHub).},
  booktitle	= {Proceedings of the International Conference on Computing,
		  Machine Learning and Data Science},
  articleno	= {39},
  numpages	= {9},
  keywords	= {Archive Exploration, Media Search, Video Representation},
  location	= {Singapore, Singapore},
  series	= {CMLDS '24}
}

@InProceedings{	  10.1145/3678957.3688616,
  author	= {Raingeard de la Bletiere, Paul},
  title		= {A musical Robot for People with Dementia},
  year		= {2024},
  isbn		= {9798400704628},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3678957.3688616},
  doi		= {10.1145/3678957.3688616},
  abstract	= {This doctoral research aims to enhance the Quality of Life
		  (QoL) of People with Dementia (PwD) by developing a
		  personalized musical robot to provide support through music
		  and reminiscence activities. Our research is dedicated to
		  creating and facilitating meaningful activities, while
		  reducing agitation and improving PwD’s mood. Key studies
		  include the development of a music recommender system based
		  on episodic memories, robotic assistance in daily
		  activities through schedule-related music, and
		  collaborative storytelling involving the PwD and their
		  informal caregivers. These interventions are intended to
		  support emotional regulation and communication. This PhD is
		  part of the QoLEAD project, which integrates
		  multidisciplinary research to bridge the gap between AI and
		  warm care in dementia.},
  booktitle	= {Proceedings of the 26th International Conference on
		  Multimodal Interaction},
  pages		= {602–606},
  numpages	= {5},
  keywords	= {Memories, Music, Music Recommendation, People with
		  Dementia, PhD, Robot, Storytelling},
  location	= {San Jose, Costa Rica},
  series	= {ICMI '24}
}

@Article{	  10.1145/3653070,
  author	= {Mai, Gengchen and Huang, Weiming and Sun, Jin and Song,
		  Suhang and Mishra, Deepak and Liu, Ninghao and Gao, Song
		  and Liu, Tianming and Cong, Gao and Hu, Yingjie and Cundy,
		  Chris and Li, Ziyuan and Zhu, Rui and Lao, Ni},
  title		= {On the Opportunities and Challenges of Foundation Models
		  for GeoAI (Vision Paper)},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {10},
  number	= {2},
  issn		= {2374-0353},
  url		= {https://doi.org/10.1145/3653070},
  doi		= {10.1145/3653070},
  abstract	= {Large pre-trained models, also known as foundation models
		  (FMs), are trained in a task-agnostic manner on large-scale
		  data and can be adapted to a wide range of downstream tasks
		  by fine-tuning, few-shot, or even zero-shot learning.
		  Despite their successes in language and vision tasks, we
		  have not yet seen an attempt to develop foundation models
		  for geospatial artificial intelligence (GeoAI). In this
		  work, we explore the promises and challenges of developing
		  multimodal foundation models for GeoAI. We first
		  investigate the potential of many existing FMs by testing
		  their performances on seven tasks across multiple
		  geospatial domains, including Geospatial Semantics, Health
		  Geography, Urban Geography, and Remote Sensing. Our results
		  indicate that on several geospatial tasks that only involve
		  text modality, such as toponym recognition, location
		  description recognition, and US state-level/county-level
		  dementia time series forecasting, the task-agnostic large
		  learning models (LLMs) can outperform task-specific fully
		  supervised models in a zero-shot or few-shot learning
		  setting. However, on other geospatial tasks, especially
		  tasks that involve multiple data modalities (e.g.,
		  POI-based urban function classification, street view
		  image–based urban noise intensity classification, and
		  remote sensing image scene classification), existing FMs
		  still underperform task-specific models. Based on these
		  observations, we propose that one of the major challenges
		  of developing an FM for GeoAI is to address the multimodal
		  nature of geospatial tasks. After discussing the distinct
		  challenges of each geospatial data modality, we suggest the
		  possibility of a multimodal FM that can reason over various
		  types of geospatial data through geospatial alignments. We
		  conclude this article by discussing the unique risks and
		  challenges to developing such a model for GeoAI.},
  journal	= {ACM Trans. Spatial Algorithms Syst.},
  month		= jul,
  articleno	= {11},
  numpages	= {46},
  keywords	= {Foundation models, geospatial artificial intelligence,
		  multimodal learning}
}

@Article{	  10.1145/3708504,
  author	= {Tsakalakis, Niko and Stalla-Bourdillon, Sophie and Huynh,
		  Dong and Moreau, Luc},
  title		= {A Typology of Explanations for Explainability-by-Design},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708504},
  doi		= {10.1145/3708504},
  abstract	= {As automated decision-making permeates almost all aspects
		  of everyday life, capabilities to generate meaningful
		  explanations for various stakeholders (i.e.,
		  decision-makers, addressees of decisions including
		  individuals, auditors, and regulators) should be carefully
		  deployed. This paper presents a typology of explanations
		  intended to support the first pillar of an
		  explainability-by-design strategy. Its production has been
		  achieved by pursuing a responsible innovation approach and
		  introducing a new persona within the research and
		  innovation process, i.e., a legal engineer, whose role is
		  to work at the interface of two teams, the compliance and
		  the engineering teams and to oversee the process of
		  requirement elicitation, which is often opinionated and
		  narrowing. Once explanation requirements have been derived
		  from applicable regulatory requirements, compliance rules
		  or business policies, they have been mapped to the
		  dimensions of the typology to produce fine-grained
		  explanation requirements, forming computable building
		  blocks that can then be translated into system requirements
		  during the technical design phase. The typology has been
		  co-created with industry partners operating in two sectors:
		  finance and education. Two pilot studies have thus been
		  conducted to test both the feasibility of the generation
		  and computation of explanations on the basis of the
		  typology and the usefulness of the outputs in the light of
		  the state of the art. The typology comprises nine
		  hierarchical dimensions. It can be leveraged to operate a
		  stand-alone classifier of explanations that acts as
		  detective controls within a broader partially-automated
		  compliance strategy. A machine-readable format of the
		  typology is provided in the form of a light ontology.},
  note		= {Just Accepted},
  journal	= {ACM J. Responsib. Comput.},
  month		= dec,
  keywords	= {artificial intelligence, explainability, typology, data
		  protection, automated decisions}
}

@Article{	  10.1145/3643505,
  author	= {King, Evan and Yu, Haoxiang and Lee, Sangsu and Julien,
		  Christine},
  title		= {Sasha: Creative Goal-Oriented Reasoning in Smart Homes
		  with Large Language Models},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {1},
  url		= {https://doi.org/10.1145/3643505},
  doi		= {10.1145/3643505},
  abstract	= {Smart home assistants function best when user commands are
		  direct and well-specified---e.g., "turn on the kitchen
		  light"---or when a hard-coded routine specifies the
		  response. In more natural communication, however, human
		  speech is unconstrained, often describing goals (e.g.,
		  "make it cozy in here" or "help me save energy") rather
		  than indicating specific target devices and actions to take
		  on those devices. Current systems fail to understand these
		  under-specified commands since they cannot reason about
		  devices and settings as they relate to human situations. We
		  introduce large language models (LLMs) to this problem
		  space, exploring their use for controlling devices and
		  creating automation routines in response to under-specified
		  user commands in smart homes. We empirically study the
		  baseline quality and failure modes of LLM-created action
		  plans with a survey of age-diverse users. We find that LLMs
		  can reason creatively to achieve challenging goals, but
		  they experience patterns of failure that diminish their
		  usefulness. We address these gaps with Sasha, a smarter
		  smart home assistant. Sasha responds to loosely-constrained
		  commands like "make it cozy" or "help me sleep better" by
		  executing plans to achieve user goals---e.g., setting a
		  mood with available devices, or devising automation
		  routines. We implement and evaluate Sasha in a hands-on
		  user study, showing the capabilities and limitations of
		  LLM-driven smart homes when faced with unconstrained
		  user-generated scenarios.},
  journal	= {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month		= mar,
  articleno	= {12},
  numpages	= {38},
  keywords	= {ambient intelligence, large language models, pervasive
		  computing, smart environments}
}

@InProceedings{	  10.1145/3638530.3664163,
  author	= {Custode, Leonardo Lucio and Caraffini, Fabio and Yaman,
		  Anil and Iacca, Giovanni},
  title		= {An investigation on the use of Large Language Models for
		  hyperparameter tuning in Evolutionary Algorithms},
  year		= {2024},
  isbn		= {9798400704956},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638530.3664163},
  doi		= {10.1145/3638530.3664163},
  abstract	= {Hyperparameter optimization is a crucial problem in
		  Evolutionary Computation. In fact, the values of the
		  hyperparameters directly impact the trajectory taken by the
		  optimization process, and their choice requires extensive
		  reasoning by human operators. Although a variety of
		  self-adaptive Evolutionary Algorithms have been proposed in
		  the literature, no definitive solution has been found. In
		  this work, we perform a preliminary investigation to
		  automate the reasoning process that leads to the choice of
		  hyperparameter values. We employ two open-source Large
		  Language Models (LLMs), namely Llama2-70b and Mixtral, to
		  analyze the optimization logs online and provide novel
		  real-time hyperparameter recommendations. We study our
		  approach in the context of step-size adaptation for (1 +
		  1)-ES. The results suggest that LLMs can be an effective
		  method for optimizing hyperparameters in Evolution
		  Strategies, encouraging further research in this
		  direction.},
  booktitle	= {Proceedings of the Genetic and Evolutionary Computation
		  Conference Companion},
  pages		= {1838–1845},
  numpages	= {8},
  keywords	= {evolutionary algorithms, large language models, landscape
		  analysis, parameter tuning},
  location	= {Melbourne, VIC, Australia},
  series	= {GECCO '24 Companion}
}

@Article{	  10.1145/3698811,
  author	= {Yan, Mengyi and Wang, Yaoshu and Wang, Yue and Miao,
		  Xiaoye and Li, Jianxin},
  title		= {GIDCL: A Graph-Enhanced Interpretable Data Cleaning
		  Framework with Large Language Models},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {6},
  url		= {https://doi.org/10.1145/3698811},
  doi		= {10.1145/3698811},
  abstract	= {Data quality is critical across many applications. The
		  utility of data is undermined by various errors, making
		  rigorous data cleaning a necessity. Traditional data
		  cleaning systems depend heavily on predefined rules and
		  constraints, which necessitate significant domain knowledge
		  and manual effort. Moreover, while configuration-free
		  approaches and deep learning methods have been explored,
		  they struggle with complex error patterns, lacking
		  interpretability, requiring extensive feature engineering
		  or labeled data. This paper introduces GIDCL
		  (Graph-enhanced Interpretable Data Cleaning with Large
		  language models), a pioneering framework that harnesses the
		  capabilities of Large Language Models (LLMs) alongside
		  Graph Neural Network (GNN) to address the challenges of
		  traditional and machine learning-based data cleaning
		  methods. By converting relational tables into graph
		  structures, GIDCL utilizes GNN to effectively capture and
		  leverage structural correlations among data, enhancing the
		  model's ability to understand and rectify complex
		  dependencies and errors. The framework's creator-critic
		  workflow innovatively employs LLMs to automatically
		  generate interpretable data cleaning rules and tailor
		  feature engineering with minimal labeled data. This process
		  includes the iterative refinement of error detection and
		  correction models through few-shot learning, significantly
		  reducing the need for extensive manual configuration. GIDCL
		  not only improves the precision and efficiency of data
		  cleaning but also enhances its interpretability, making it
		  accessible and practical for non-expert users. Our
		  extensive experiments demonstrate that GIDCL significantly
		  outperforms existing methods, improving F1-scores by 10% on
		  average while requiring only 20 labeled tuples.},
  journal	= {Proc. ACM Manag. Data},
  month		= dec,
  articleno	= {236},
  numpages	= {29},
  keywords	= {data quality, graph neural network, interpretable, large
		  language models}
}

@InProceedings{	  10.1145/3657054.3657277,
  author	= {Bachinger, Sarah T. and Feddoul, Leila and Mauch, Marianne
		  Jana and K\"{o}nig-Ries, Birgitta},
  title		= {Extracting Legal Norm Analysis Categories from German Law
		  Texts with Large Language Models},
  year		= {2024},
  isbn		= {9798400709883},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3657054.3657277},
  doi		= {10.1145/3657054.3657277},
  abstract	= {The digitization of public services in Germany is always
		  based on a legal basis (e.g., laws). In the digitization
		  process, first relevant entities in law documents (e.g.,
		  actors) are detected, then a list of possible process steps
		  of their interactions is derived. The final process is
		  constructed and transformed to a digital service for
		  citizens and companies. Today, the discovery of custom
		  entities in German law documents is still manual high
		  effort work. In our study, we investigate the capabilities
		  of Large Language Models (LLMs) to automate this task,
		  choose five LLMs from 61 evaluated candidates, and perform
		  prompt engineering to create five different prompt variants
		  with differing parts. We examine the automatic annotation
		  by two LLMs (LeoLM and BLOOM CLP German) in detail and find
		  that the inclusion of more information in the prompts as
		  well as an increased number of examples per prompt are
		  beneficial. We report micro F1-scores for the optimal
		  scenario of 0.91 for BLOOM CLP German, and 0.82 for LeoLM,
		  with a higher balanced accuracy for LeoLM. The results
		  indicate that LLMs have a good potential to perform named
		  entity recognition, especially for supporting legal norm
		  analysis in the context of the digitization of public
		  administration.},
  booktitle	= {Proceedings of the 25th Annual International Conference on
		  Digital Government Research},
  pages		= {481–493},
  numpages	= {13},
  keywords	= {Digital Transformation, Federal Information Management,
		  Large Language Models, Named Entity Recognition, Public
		  Administration},
  location	= {Taipei, Taiwan},
  series	= {dg.o '24}
}

@InProceedings{	  10.1145/3675249.3675319,
  author	= {Xu, Dapeng and Kang, Qi and Zhang, Wei},
  title		= {Risk Rating Method for Power Grid Production Operations
		  Based on an Improved BERT Model},
  year		= {2024},
  isbn		= {9798400718267},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3675249.3675319},
  doi		= {10.1145/3675249.3675319},
  abstract	= {Enhancing the level of risk rating in power grid
		  operations can effectively ensure the safety and stability
		  of the power grid. Artificial intelligence-based risk
		  rating methods can assist on-site workers in quickly and
		  standardly judging operational risks. This paper proposes
		  an improved model based on the bidirectional encoder
		  representations from transformers(BERT) model. It replaces
		  the gaussian error linear unit (GELU) activation function
		  in the feedforward neural network (FFN) layer with the
		  switched gated linear unit(SwiGLU) activation function to
		  study its performance change pattern. Experiments were
		  conducted on a collected dataset of power grid production
		  operation risks. The results show that the model
		  demonstrates excellent performance in risk rating.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Computer and Multimedia Technology},
  pages		= {401–407},
  numpages	= {7},
  location	= {Sanming, China},
  series	= {ICCMT '24}
}

@InProceedings{	  10.1145/3626246.3653372,
  author	= {Bao, Xianchun and Bao, Zian and Binbin, Bie and Duan,
		  QingSong and Fan, Wenfei and Lei, Hui and Li, Daji and Lin,
		  Wei and Liu, Peng and Lv, Zhicong and Ouyang, Mingliang and
		  Tang, Shuai and Wang, Yaoshu and Wei, Qiyuan and Xie, Min
		  and Zhang, Jing and Zhang, Xin and Zhao, Runxiao and Zhou,
		  Shuping},
  title		= {Rock: Cleaning Data by Embedding ML in Logic Rules},
  year		= {2024},
  isbn		= {9798400704222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626246.3653372},
  doi		= {10.1145/3626246.3653372},
  abstract	= {We introduce Rock, a system for cleaning relational data.
		  Rock implements a framework that unifies machine learning
		  (ML) and logic deduction by embedding ML classifiers in
		  rules as predicates. In a unified process, it identifies
		  tuples that refer to the same real-world entity, catches
		  semantic inconsistencies among the entities, deduces the
		  timeliness of the attribute values of the entities, and
		  imputes missing values by possibly extracting data from
		  knowledge graphs. That is, Rock conducts entity resolution,
		  conflict resolution, incomplete information imputation and
		  timeliness deduction in the same process, makes use of
		  their interactions and improves the overall quality of the
		  data. Moreover, Rock supports methods, batch and
		  incremental, for discovering rules from real-life data,
		  detecting errors with the learned rules, accumulating
		  ground truth, and fixing the errors, such that the
		  corrections are logical consequences of the rules and
		  ground truth. We present the design and implementation of
		  Rock. We evaluate the scalability and accuracy of Rock, and
		  share lessons learned from a variety of real-life
		  applications.},
  booktitle	= {Companion of the 2024 International Conference on
		  Management of Data},
  pages		= {106–119},
  numpages	= {14},
  keywords	= {conflict resolution, data quality, entity resolution,
		  error correction, error detection, missing value
		  imputation, timeliness deduction},
  location	= {Santiago AA, Chile},
  series	= {SIGMOD/PODS '24}
}

@InProceedings{	  10.1145/3603273.3627834,
  author	= {Zhang, Xixiang and Meng, Qi and Tan, Qiwen and Dong, Yun
		  and Chen, Yan and Tan, Zhixiang},
  title		= {GCN-based Entity Relation Extraction Method for Power
		  Marketing Data},
  year		= {2024},
  isbn		= {9798400708268},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3603273.3627834},
  doi		= {10.1145/3603273.3627834},
  abstract	= {Power marketing text data contains more specialized terms
		  in specific fields, and there are multiple nested text
		  entities, therefore the relationship between entity
		  recognition is more difficult. In this paper, a graph
		  convolution-based entity relation extraction model PMRE for
		  power marketing data is proposed. Firstly, using the span
		  representation annotation method, the model extracts the
		  bidirectional representation of text depth by using RoBERTa
		  as the encoder layer, and uses the whole word masking
		  strategy for pre-training. Then the feature representation
		  is passed to the graph convolutional layer, and using
		  multi-head attention mechanisms to capture the weights of
		  relationships between words in the text across multiple
		  dimensions, and the semantic dependency tree is constructed
		  and then fed into the graph convolutional neural network,
		  and the distance dependency problem solved by the spatial
		  features between entities is obtained. Finally, the
		  features are fed into the multilayer perceptron layer for
		  entity relationship classification. In the process of model
		  training, the cross-entropy loss function is used to obtain
		  the maximum likelihood value of the predicted label and the
		  target label. Using the power marketing data supplied by
		  the Southern Power Grid as an example, the precision and F1
		  score of the relation extraction model in this paper
		  reached 83.80% and 82.16%, respectively, compared with the
		  benchmark model, recall, precision and F1 score of this
		  model are significantly enhanced, indicating that the model
		  proposed in this paper has a good solution effect on entity
		  relation extraction of power marketing text data with
		  complex entity nested and many professional terms.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Advances in Artificial Intelligence and Applications},
  pages		= {120–127},
  numpages	= {8},
  keywords	= {GCN, RoBERTa, power marketing, relation extraction, whole
		  word masking},
  location	= {Wuhan, China},
  series	= {AAIA '23}
}

@Proceedings{	  10.1145/3670105,
  title		= {CNIOT '24: Proceedings of the 2024 5th International
		  Conference on Computing, Networks and Internet of Things},
  year		= {2024},
  isbn		= {9798400716751},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tokyo, Japan}
}

@InProceedings{	  10.1145/3658644.3690306,
  author	= {Wen, Rui and Li, Zheng and Backes, Michael and Zhang,
		  Yang},
  title		= {Membership Inference Attacks Against In-Context Learning},
  year		= {2024},
  isbn		= {9798400706363},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3658644.3690306},
  doi		= {10.1145/3658644.3690306},
  abstract	= {Adapting Large Language Models (LLMs) to specific tasks
		  introduces concerns about computational efficiency,
		  prompting an exploration of efficient methods such as
		  In-Context Learning (ICL). However, the vulnerability of
		  ICL to privacy attacks under realistic assumptions remains
		  largely unexplored. In this work, we present the first
		  membership inference attack tailored for ICL, relying
		  solely on generated texts without their associated
		  probabilities. We propose four attack strategies tailored
		  to various constrained scenarios and conduct extensive
		  experiments on four popular large language models.
		  Empirical results show that our attacks can accurately
		  determine membership status in most cases, e.g., 95%
		  accuracy advantage against LLaMA, indicating that the
		  associated risks are much higher than those shown by
		  existing probability-based attacks. Additionally, we
		  propose a hybrid attack that synthesizes the strengths of
		  the aforementioned strategies, achieving an accuracy
		  advantage of over 95% in most cases. Furthermore, we
		  investigate three potential defenses targeting data,
		  instruction, and output. Results demonstrate combining
		  defenses from orthogonal dimensions significantly reduces
		  privacy leakage and offers enhanced privacy assurances.},
  booktitle	= {Proceedings of the 2024 on ACM SIGSAC Conference on
		  Computer and Communications Security},
  pages		= {3481–3495},
  numpages	= {15},
  keywords	= {in-context learning, large language models, membership
		  inference attacks},
  location	= {Salt Lake City, UT, USA},
  series	= {CCS '24}
}

@InProceedings{	  10.1145/3652620.3686251,
  author	= {Zavada, \'{A}rmin and Marussy, Krist\'{o}f and Moln\'{a}r,
		  Vince},
  title		= {From Transpilers to Semantic Libraries: Formal
		  Verification With Pluggable Semantics},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3686251},
  doi		= {10.1145/3652620.3686251},
  abstract	= {In the field of model-based systems engineering, there is
		  an increasing demand for the application of formal methods.
		  However, this requires expertise in formal methods, which
		  cannot be expected from systems engineers. While several
		  attempts have been made to bridge this gap, there are still
		  open questions. (1) With the trend shifting towards
		  ontological languages, systems are modeled as classes of 4D
		  occurrences, rather than a 3D system evolving with time,
		  which hinders the application of state-of-the-art model
		  checking algorithms. (2) Ontological reasoning cannot
		  handle the state space explosion problem, and can even make
		  it harder for verifiers to operate efficiently. (3) When
		  operationalizing ontological languages, we need to validate
		  the conformance of the two semantics, even in the presence
		  of optimizations. (4) On top of all, these challenges must
		  be solved for every new engineering language, version, or
		  variant. In this paper, we propose a new approach to
		  address the aforementioned challenges. To validate its
		  feasibility, we present a prototype tool and evaluate it on
		  a SysML model.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {311–317},
  numpages	= {7},
  keywords	= {model-based systems engineering, kernel modeling language,
		  formal verification, declarative interpretation,
		  metaprogramming, semantic libraries, operational
		  libraries},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@Proceedings{	  10.1145/3660043,
  title		= {ICIEAI '23: Proceedings of the 2023 International
		  Conference on Information Education and Artificial
		  Intelligence},
  year		= {2023},
  isbn		= {9798400716157},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@InProceedings{	  10.1145/3689492.3690054,
  author	= {Marron, Mark},
  title		= {A Programming Language for Data and Configuration!},
  year		= {2024},
  isbn		= {9798400712159},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689492.3690054},
  doi		= {10.1145/3689492.3690054},
  abstract	= {A day in the life of a developer often involves more time
		  working with schemas, configurations, and data description
		  systems than writing code and logic in a classical
		  programming language. As more systems move into distributed
		  worlds, e.g. cloud and microservices, and developers make
		  increasing use of libraries and frameworks, the need to
		  interact with a range of data formats and configuration
		  mechanisms is only increasing. This is a treacherous world,
		  where a misspelled property name or missing field can
		  render an entire service inoperable, a mistake that a
		  number in an API represents
		  
		  seconds instead of milli-seconds can lead to a message
		  being set for delivery in several months instead of in an
		  hour, misconfigured schema can lead to public exposure of
		  sensitive data, and corrupt or erroneous results from a
		  misunderstood data format could result in massive financial
		  and/or reputational damage.
		  
		  To address these challenges this paper casts the problems
		  of data and configuration descriptions, not as a problem of
		  data representation, but as a type system problem, that can
		  be addressed with well understood and highly effective
		  programming language techniques! The novel challenge is
		  that data representation and configuration are universal
		  concerns in a system and, particularly in modern cloud or
		  micro-service systems, these systems may involve many
		  programming languages. In the past this has led to
		  specification systems that use a least-common-denominator
		  set of data types, often little more than strings and
		  numbers, and then rely on conventions or (out-of-date)
		  documentation to ensure that the data is interpreted
		  correctly. This paper shows that, with careful design, it
		  is possible to create a rich universal system that can be
		  used to express data and configuration specifications in a
		  way that is human readable/writable and that can be
		  produced/consumed, much like JSON, by a wide range of
		  programming languages and systems.},
  booktitle	= {Proceedings of the 2024 ACM SIGPLAN International
		  Symposium on New Ideas, New Paradigms, and Reflections on
		  Programming and Software},
  pages		= {147–161},
  numpages	= {15},
  keywords	= {Configuration, Data Specification, Programming Language},
  location	= {Pasadena, CA, USA},
  series	= {Onward! '24}
}

@Proceedings{	  10.1145/3643657,
  title		= {SATrends '24: Proceedings of the 1st International
		  Workshop on New Trends in Software Architecture},
  year		= {2024},
  isbn		= {9798400705601},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {In this workshop, we aim at establishing a forum to
		  collect practitioners' experiences and/or researchers'
		  observations related to trends, and enable practitioners
		  and researchers to exchange opinions, learn from each
		  other, and progress the state of the art in the adoption of
		  new trends.},
  location	= {Lisbon, Portugal}
}

@Article{	  10.1145/3674500,
  author	= {De Sousa Ribeiro, Fabio and Duarte, Kevin and Everett,
		  Miles and Leontidis, Georgios and Shah, Mubarak},
  title		= {Object-centric Learning with Capsule Networks: A Survey},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {11},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3674500},
  doi		= {10.1145/3674500},
  abstract	= {Capsule networks emerged as a promising alternative to
		  convolutional neural networks for learning object-centric
		  representations. The idea is to explicitly model part-whole
		  hierarchies by using groups of neurons called capsules to
		  encode visual entities, then learn the relationships
		  between these entities dynamically from data. However, a
		  major hurdle for capsule network research has been the lack
		  of a reliable point of reference for understanding their
		  foundational ideas and motivations. This survey provides a
		  comprehensive and critical overview of capsule networks,
		  which aims to serve as a main point of reference going
		  forward. To that end, we introduce the fundamental concepts
		  and motivations behind capsule networks, such as
		  equivariant inference. We then cover various technical
		  advances in capsule routing algorithms as well as
		  alternative geometric and generative formulations. We
		  provide a detailed explanation of how capsule networks
		  relate to the attention mechanism in Transformers and
		  uncover non-trivial conceptual similarities between them in
		  the context of object-centric representation learning. We
		  also review the extensive applications of capsule networks
		  in computer vision, video and motion, graph representation
		  learning, natural language processing, medical imaging, and
		  many others. To conclude, we provide an in-depth discussion
		  highlighting promising directions for future work.},
  journal	= {ACM Comput. Surv.},
  month		= jul,
  articleno	= {291},
  numpages	= {291},
  keywords	= {Deep learning, capsule networks, deep neural networks,
		  convolutional neural networks, transformers,
		  routing-by-agreement, self-attention, representation
		  learning, object-centric learning, generative models,
		  computer vision}
}

@Article{	  10.1145/3689629,
  author	= {Yang, Songhua and Zhang, Chenghao and He, Chenyuan and Xu,
		  Hongfei and Zan, Hongying and Jia, Yuxiang},
  title		= {Knowledge-injected Prompt Learning for Chinese Biomedical
		  Entity Normalization},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {10},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3689629},
  doi		= {10.1145/3689629},
  abstract	= {The Biomedical Entity Normalization (BEN) task aims to
		  align raw, unstructured medical entities to standard
		  entities, thus promoting data coherence and facilitating
		  better downstream medical applications. Recently, prompt
		  learning methods have shown promising results in the
		  natural language processing field. However, existing
		  research falls short in tackling the more complex Chinese
		  BEN task, especially in the few-shot scenario with limited
		  medical data, and the vast potential of the external
		  medical knowledge base has not yet been fully exploited. To
		  address these challenges, this article proposes a novel
		  Knowledge-injected Prompt Learning (PL-Knowledge) method.
		  Specifically, the approach consists of five stages:
		  candidate entity matching, knowledge extraction, knowledge
		  encoding, knowledge injection, and prediction output. By
		  effectively encoding the knowledge items contained in
		  medical entities and incorporating them into tailor-made
		  knowledge-injected templates, the additional knowledge
		  enhances the model’s ability to capture latent
		  relationships between medical entities, thus achieving a
		  better match with the standard entities. Comprehensive
		  experiments are conducted on a benchmark dataset in both
		  few-shot and full-scale settings. This method outperforms
		  existing baselines, with an average accuracy improvement of
		  12.96 percentage points in few-shot and 0.94 percentage
		  points in full-data cases, showcasing its excellence in the
		  BEN task.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  articleno	= {144},
  numpages	= {21},
  keywords	= {Biomedical entity normalization, prompt learning,
		  knowledge enhancement, few-shot learning}
}

@InProceedings{	  10.1145/3660395.3660458,
  author	= {Zhao, Jiaqi and Lin, Rongheng and Wang, Baigen and Wang,
		  Ou and Zhao, Qian and Liu, Huizhou},
  title		= {Pruned Contrastive Learning Verbalizer for Prompt-based
		  Few-shot Text Classification},
  year		= {2024},
  isbn		= {9798400716362},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3660395.3660458},
  doi		= {10.1145/3660395.3660458},
  abstract	= {The utilization of pre-trained language models (PLMs) has
		  led to remarkable advancements in the field of few-shot
		  learning through prompt-based fine-tuning. This technique
		  requires the use of templates to structure input text as a
		  cloze question and the implementation of verbalizers to map
		  the PLM's output to the answer space of a given task.
		  However, the manual construction of verbalizers heavily
		  relies on domain knowledge and experience, and the
		  performance of automatic verbalizers searching based on
		  limited samples is notably weaker compared to scenarios
		  with sufficient data. To enhance the performance in
		  low-data settings, we introduce a novel method called
		  Pruned Contrastive Learning Verbalizers (PCV) for
		  automatically generating verbalizers. Our approach
		  comprises two phases: firstly, employing pruned searching
		  to identify the most suitable label words, and secondly,
		  utilizing contrastive learning to acquire class prototypes
		  from the training data. Our experiments on text
		  classification tasks show that PCV outperforms compared to
		  manual and automatic verbalizers in resource-constrained
		  environments. Finally, we applied PCV in a real-world
		  scenario involving electrical safety inspections, further
		  demonstrating its exceptional performance in classification
		  tasks.},
  booktitle	= {Proceedings of the 2023 3rd Guangdong-Hong Kong-Macao
		  Greater Bay Area Artificial Intelligence and Big Data
		  Forum},
  pages		= {370–377},
  numpages	= {8},
  location	= {Guangzhou, China},
  series	= {AIBDF '23}
}

@InProceedings{	  10.1145/3669754.3669784,
  author	= {Jayawardena, Lasal and Yapa, Prasan},
  title		= {Improving Quality and Domain-Relevancy of Paraphrase
		  Generation with Graph-Based Retrieval Augmented
		  Generation},
  year		= {2024},
  isbn		= {9798400717055},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3669754.3669784},
  doi		= {10.1145/3669754.3669784},
  abstract	= {Paraphrase generation is a fundamental area of research in
		  Natural Language Processing (NLP) and Natural Language
		  Generation (NLG), due to its sequence-to-sequence (Seq2Seq)
		  nature. Paraphrasing, spanning across various domains,
		  poses challenges for simpler model architectures due to the
		  extensive knowledge required to generate paraphrases. The
		  added constraint of generating diverse paraphrases further
		  complicates the task for models trained on existing
		  datasets. We present a methodology that leverages
		  Graph-Based Retrieval Augmented Generation (G-RAG), capable
		  of utilizing both entity and phrasal knowledge to address
		  this issue. We demonstrate through experiments that this
		  approach enables both complex models like Large Language
		  models (LLMs) and smaller Seq2Seq models to generate more
		  diverse paraphrases without compromising semantic
		  similarity. Furthermore, this approach’s capacity to
		  integrate domain-specific knowledge makes it particularly
		  effective across different domains, enhancing its
		  applicability in varied contexts. The results are further
		  corroborated by human evaluation and extensive quantitative
		  analysis focusing on semantic similarity, lexical
		  diversity, syntactic diversity, and grammatical correctness
		  to gauge high-quality paraphrases.},
  booktitle	= {Proceedings of the 2024 10th International Conference on
		  Computing and Artificial Intelligence},
  pages		= {196–208},
  numpages	= {13},
  keywords	= {Graph-based Knowledge, Large Language Models, Natural
		  Language Processing, Paraphrase Generation,
		  Sequence-to-Sequence Models},
  location	= {Bali Island, Indonesia},
  series	= {ICCAI '24}
}

@Proceedings{	  10.1145/3695080,
  title		= {ICCBD '24: Proceedings of the 2024 International
		  Conference on Cloud Computing and Big Data},
  year		= {2024},
  isbn		= {9798400710223},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dali, China}
}

@InProceedings{	  10.1145/3589335.3651445,
  author	= {Kasela, Pranav and Braga, Marco and Pasi, Gabriella and
		  Perego, Raffaele},
  title		= {SE-PQA: Personalized Community Question Answering},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651445},
  doi		= {10.1145/3589335.3651445},
  abstract	= {Personalization in Information Retrieval is a topic
		  studied for a long time. Nevertheless, there is still a
		  lack of high-quality, real-world datasets to conduct
		  large-scale experiments and evaluate models for
		  personalized search. This paper contributes to filling this
		  gap by introducing SE-PQA(StackExchange - Personalized
		  Question Answering), a new curated resource to design and
		  evaluate personalized models related to the task of
		  community Question Answering (cQA). The contributed dataset
		  includes more than 1 million queries and 2 million answers,
		  annotated with a rich set of features modeling the social
		  interactions among the users of a popular cQA platform. We
		  describe the characteristics of SE-PQA and detail the
		  features associated with questions and answers. We also
		  provide reproducible baseline methods for the cQA task
		  based on the resource, including deep learning models and
		  personalization approaches. The results of the preliminary
		  experiments conducted show the appropriateness of SE-PQA to
		  train effective cQA models; they also show that
		  personalization remarkably improves the effectiveness of
		  all the methods tested. Furthermore, we show the benefits
		  in terms of robustness and generalization of combining data
		  from multiple communities for personalization purposes.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1095–1098},
  numpages	= {4},
  keywords	= {personalization, question answering, user model},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3701571,
  title		= {MUM '24: Proceedings of the International Conference on
		  Mobile and Ubiquitous Multimedia},
  year		= {2024},
  isbn		= {9798400712838},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3652037,
  title		= {PETRA '24: Proceedings of the 17th International
		  Conference on PErvasive Technologies Related to Assistive
		  Environments},
  year		= {2024},
  isbn		= {9798400717604},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Crete, Greece}
}

@InProceedings{	  10.1145/3652628.3652696,
  author	= {Li, Xianda and Azhati, Baheti},
  title		= {Research on the Application of an OPT Model Integrating
		  Meta-Learning and Prompt Learning for Few-Shot Event
		  Extraction},
  year		= {2024},
  isbn		= {9798400708831},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652628.3652696},
  doi		= {10.1145/3652628.3652696},
  abstract	= {Event extraction plays a pivotal role in natural language
		  processing (NLP), especially in few-shot learning
		  environments where research is increasingly growing. This
		  paper proposes an OPT model that integrates Model-Agnostic
		  Meta-Learning (MAML) and prompt learning to enhance the
		  performance of few-shot event extraction. Our method was
		  tested on the ACE2005 dataset and compared with existing
		  models. The results demonstrate the effectiveness of our
		  approach in improving few-shot event extraction.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Artificial Intelligence and Computer Engineering},
  pages		= {412–415},
  numpages	= {4},
  location	= {Dalian, China},
  series	= {ICAICE '23}
}

@Article{	  10.1145/3659948,
  author	= {Hou, Wenlong and Zhao, Weidong and Liu, Xianhui and Guo,
		  Wenyan},
  title		= {Knowledge-Enriched Prompt for Low-Resource Named Entity
		  Recognition},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3659948},
  doi		= {10.1145/3659948},
  abstract	= {Named Entity Recognition (NER) in low-resource settings
		  aims to identify and categorize entities in a sentence with
		  limited labeled data. Although prompt-based methods have
		  succeeded in low-resource perspectives, challenges persist
		  in effectively harnessing information and optimizing
		  computational efficiency. In this work, we present a novel
		  prompt-based method to enhance low-resource NER without
		  exhaustive template tuning. First, we construct
		  knowledge-enriched prompts by integrating representative
		  entities and background information to provide informative
		  supervision tailored to each entity type. Then, we
		  introduce an efficient reverse generative framework
		  inspired by question answering (QA), which avoids redundant
		  computations. Finally, we reduce costs by generating
		  entities from their types while retaining model reasoning
		  capacity. Experiment results demonstrate that our method
		  outperforms other baselines on three datasets under
		  few-shot settings.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {72},
  numpages	= {15},
  keywords	= {Low-resource NER, Knowledge Injection, Prompt
		  Engineering}
}

@InProceedings{	  10.1145/3703187.3703233,
  author	= {Pan, Xinqi and Wang, Wei and Chen, Yefeng},
  title		= {Theme Classification of Chinese Classical Poetry Based on
		  the GB-MSA-BGTCN Hybrid Model},
  year		= {2024},
  isbn		= {9798400707254},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3703187.3703233},
  doi		= {10.1145/3703187.3703233},
  abstract	= {The theme of Chinese classical poetry reveal its
		  connotations and artistic conceptions, making them
		  essential for understanding the poetry. Therefore, studying
		  the theme classification of Chinese classical poetry is
		  crucial. However, existing methods have struggled to
		  effectively leverage key information and extract deep
		  textual features. Traditional word embedding models have
		  also failed to address polysemy, limiting their ability to
		  convey the complex semantics of classical poetry. To
		  overcome these challenges, we propose the GB-MSA-BGTCN
		  model for theme classification. This model integrates
		  GuwenBERT, a Multi-head Self-Attention (MSA) mechanism,
		  Bidirectional Gated Recurrent Unit (BiGRU), and Temporal
		  Convolutional Network (TCN). GuwenBERT, pre-trained on
		  classical text corpora, is used for word embedding to
		  effectively represent the semantic information of classical
		  poetry. The MSA mechanism enhances classification accuracy
		  by focusing on key textual information at multiple scales.
		  The combined use of BiGRU and TCN captures long-term
		  dependencies and contextual information, enabling deep
		  feature extraction and improving performance. Due to the
		  lack of public datasets on Chinese classical poetry themes,
		  we constructed a new dataset for evaluation. Experimental
		  results show that our model achieved precision, recall, and
		  F1 scores of 84.10%, 81.65%, and 82.64%, respectively,
		  surpassing other comparative models.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Computer Information Science and Artificial Intelligence},
  pages		= {276–280},
  numpages	= {5},
  keywords	= {Bidirectional gated recurrent unit, Chinese classical
		  poetry, GuwenBERT, Multi-head self-attention mechanism,
		  Temporal convolutional network, Theme classification},
  location	= { },
  series	= {CISAI '24}
}

@InProceedings{	  10.1145/3643657.3643910,
  author	= {Cabrera, Christian and Paleyes, Andrei and Lawrence, Neil
		  David},
  title		= {Self-sustaining Software Systems (S4): Towards Improved
		  Interpretability and Adaptation},
  year		= {2024},
  isbn		= {9798400705601},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643657.3643910},
  doi		= {10.1145/3643657.3643910},
  abstract	= {Software systems impact society at different levels as
		  they pervasively solve real-world problems. Modern software
		  systems are often so sophisticated that their complexity
		  exceeds the limits of human comprehension. These systems
		  must respond to changing goals, dynamic data, unexpected
		  failures, and security threats, among other variable
		  factors in real-world environments. Systems' complexity
		  challenges their interpretability and requires autonomous
		  responses to dynamic changes. Two main research areas
		  explore autonomous systems' responses: evolutionary
		  computing and autonomic computing. Evolutionary computing
		  focuses on software improvement based on iterative
		  modifications to the source code. Autonomic computing
		  focuses on optimising systems' performance by changing
		  their structure, behaviour, or environment variables.
		  Approaches from both areas rely on feedback loops that
		  accumulate knowledge from the system interactions to inform
		  autonomous decision-making. However, this knowledge is
		  often limited, constraining the systems' interpretability
		  and adaptability. This paper proposes a new concept for
		  interpretable and adaptable software systems:
		  self-sustaining software systems (S4). S4 builds knowledge
		  loops between all available knowledge sources that define
		  modern software systems to improve their interpretability
		  and adaptability. This paper introduces and discusses the
		  S4 concept.},
  booktitle	= {Proceedings of the 1st International Workshop on New
		  Trends in Software Architecture},
  pages		= {5–9},
  numpages	= {5},
  keywords	= {autonomous systems, software engineering, knowledge
		  graphs, data-oriented architectures, large language
		  models},
  location	= {Lisbon, Portugal},
  series	= {SATrends '24}
}

@Proceedings{	  10.1145/3702250,
  title		= {ICVGIP '24: Proceedings of the Fifteenth Indian Conference
		  on Computer Vision Graphics and Image Processing},
  year		= {2024},
  isbn		= {9798400710759},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3643675,
  author	= {Tao, Wei and Zhou, Yucheng and Wang, Yanlin and Zhang,
		  Hongyu and Wang, Haofen and Zhang, Wenqiang},
  title		= {KADEL: Knowledge-Aware Denoising Learning for Commit
		  Message Generation},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {5},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3643675},
  doi		= {10.1145/3643675},
  abstract	= {Commit messages are natural language descriptions of code
		  changes, which are important for software evolution such as
		  code understanding and maintenance. However, previous
		  methods are trained on the entire dataset without
		  considering the fact that a portion of commit messages
		  adhere to good practice (i.e., good-practice commits),
		  while the rest do not. On the basis of our empirical study,
		  we discover that training on good-practice commits
		  significantly contributes to the commit message generation.
		  Motivated by this finding, we propose a novel
		  knowledge-aware denoising learning method called KADEL.
		  Considering that good-practice commits constitute only a
		  small proportion of the dataset, we align the remaining
		  training samples with these good-practice commits. To
		  achieve this, we propose a model that learns the commit
		  knowledge by training on good-practice commits. This
		  knowledge model enables supplementing more information for
		  training samples that do not conform to good practice.
		  However, since the supplementary information may contain
		  noise or prediction errors, we propose a dynamic denoising
		  training method. This method composes a distribution-aware
		  confidence function and a dynamic distribution list, which
		  enhances the effectiveness of the training process.
		  Experimental results on the whole MCMD dataset demonstrate
		  that our method overall achieves state-of-the-art
		  performance compared with previous methods.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jun,
  articleno	= {133},
  numpages	= {32},
  keywords	= {Commit message generation, knowledge introducing,
		  denoising training}
}

@Proceedings{	  10.1145/3643489,
  title		= {LSC '24: Proceedings of the 7th Annual ACM Workshop on the
		  Lifelog Search Challenge},
  year		= {2024},
  isbn		= {9798400705502},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The LSC workshops are participation workshops, where
		  participants write and present an academic paper describing
		  their prototype lifelog retrieval system, and then take
		  part in a live interactive search competition.
		  Consequently, the workshop is highly interactive and
		  challenging for participants.},
  location	= {Phuket, Thailand}
}

@Proceedings{	  10.1145/3632410,
  title		= {CODS-COMAD '24: Proceedings of the 7th Joint International
		  Conference on Data Science &amp; Management of Data (11th
		  ACM IKDD CODS and 29th COMAD)},
  year		= {2024},
  isbn		= {9798400716348},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bangalore, India}
}

@InProceedings{	  10.1145/3675888.3676107,
  author	= {Kumar, Pratiksh and Gupta, Rishik and Kumar, Bagesh and
		  Kumar, Aman},
  title		= {Bridging the Gap: Leveraging Textual and Visual Contexts
		  for PreciseMedical Visual Question Answering},
  year		= {2024},
  isbn		= {9798400709722},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3675888.3676107},
  doi		= {10.1145/3675888.3676107},
  abstract	= {The advent of Visual Question Answering (VQA) technology
		  has brought significant advancements in the medical field,
		  offering transformative potential in clinical diagnostics
		  and patient care. This research explores the application of
		  VQA within the medical domain, highlighting its critical
		  role in interpreting complex visual data, such as
		  radiological images, pathology slides, and other diagnostic
		  visuals. Traditional diagnostic processes often rely
		  heavily on human expertise, which can be time-consuming and
		  prone to variability. VQA systems, powered by sophisticated
		  machine learning models, provide consistent and accurate
		  interpretations, thus enhancing diagnostic accuracy and
		  efficiency. Visual Question Answering (VQA) in the medical
		  field necessitates extracting information from both textual
		  and visual inputs to provide accurate answers, a critical
		  requirement for supporting medical decision-making. This
		  research introduces a novel approach to address VQA
		  challenges in the medical domain using Bi-Directional
		  Layout with Positional Encoding (BLIP) models. Our
		  methodology seamlessly integrates text and image processing
		  within a unified framework, enabling precise interactions
		  between textual queries and medical imaging data. We
		  commence with textual inputs, encoded by BLIP processors,
		  and medical images, encoded by BLIP image processors. A
		  custom VQA dataset, specifically designed for the medical
		  field, includes textual questions and their corresponding
		  medical image features. We employ a BLIP-based Question
		  Answering architecture, fine-tuned on our medical VQA
		  dataset, and optimized using the AdamW optimizer with a
		  learning rate of 0.00005, ensuring efficient convergence.
		  Additionally, we introduce attention mechanisms using
		  Coarse and Fine Attention blocks for enhanced feature
		  fusion and accurate answer prediction. Our results are
		  highly encouraging, demonstrating competitive metrics in
		  extensive VQA task experiments on both training and
		  validation datasets. Qualitative analysis of sample
		  predictions indicates the model’s capability to provide
		  accurate answers for diverse visual and textual medical
		  inputs. This work holds significant promise for improving
		  automated medical image analysis and supporting clinical
		  decision-making.},
  booktitle	= {Proceedings of the 2024 Sixteenth International Conference
		  on Contemporary Computing},
  pages		= {519–526},
  numpages	= {8},
  keywords	= {Attention Model, BLIP, Medical Visual Question Answering,
		  PathVQA},
  location	= {Noida, India},
  series	= {IC3-2024}
}

@Article{	  10.1145/3705322,
  author	= {Zhang, Jianrong and Fan, Hehe and Yang, Yi},
  title		= {Protein Captioning: Bridging the Gap between Protein
		  Sequences and Natural Languages},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3705322},
  doi		= {10.1145/3705322},
  abstract	= {We introduce the multimodal task of Protein Captioning,
		  which is an easy-to-understand and flexible way for protein
		  analysis. Compared to specific protein recognition or
		  classification tasks, such as enzyme reaction
		  classification and gene ontology term prediction, protein
		  captioning provides comprehensive textural descriptions for
		  proteins, thus playing a key role in bridging the gap
		  between protein sequences and natural languages. To address
		  the problem, we propose a simple yet effective method,
		  Protein-to-Text Generative Pre-trained Transformer
		  (P2T-GPT), to fuse multimodal embeddings and translate the
		  chain of amino acid residues in a protein to a sequence of
		  natural language words, i.e., text. For the evaluation of
		  protein captioning, we collect the ProteinCap dataset that
		  contains 94,454 protein-text pairs. Experiments on
		  ProteinCap demonstrate the effectiveness of the proposed
		  P2T-GPT on protein captioning. For example, our method
		  obtains improvements of 8.74, 10.03, and 11.05 in the
		  BERTScore compared to the baseline model on ProteinCap-
		  (alpha,beta,gamma) , respectively. As minor contributions,
		  first, P2T-GPT provides a way to connect protein science
		  and Large Language Models (LLMs). By appending ChatGPT, our
		  method can interact in a conversational way to answer
		  questions given a protein. Second, we show that protein
		  captioning can be treated as a pre-trained task that can
		  benefit a range of downstream tasks, to a certain extent.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= nov,
  keywords	= {Protein captioning, Natural language processing,
		  Multimodal learning}
}

@InProceedings{	  10.1145/3640771.3640779,
  author	= {Liu, Zhou Yu and Yan, Xin and Luo, Long You},
  title		= {CLPLM-EE: Contrastive Learning Pre-training Model for
		  Event Extraction In New Domain},
  year		= {2024},
  isbn		= {9798400708954},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640771.3640779},
  doi		= {10.1145/3640771.3640779},
  abstract	= {The event extraction task recognizes events in natural
		  language text and extracts the event triggers and
		  arguments. The majority of existing methods that based on
		  closed domains have poor generalization and are difficult
		  to extend to new domains. As a result, event extraction
		  models for new domains requires general event knowledge
		  learned from large amounts of unsupervised data.
		  Fortunately, existing studies have shown that a contrastive
		  pre-training model for information extraction can better
		  obtain prior knowledge from large unannotated data.
		  However, such methods have insufficient perception of event
		  structure, have not learnt enough about event
		  representations, and thier contrastive learning methods
		  used to construct negative samples lead to semantic
		  conflicts. To adress the above problem, we propose
		  CLPLM-EE, a contrastive pre-training MODEL for event
		  extraction in new domians. CLPLM-EE contains has an encoder
		  with event semantics and event structure awareness to learn
		  the most common knowledge of the event. In addition,
		  CLPLM-EE learns high-quality event representations by
		  eliminating false negative samples and using a weighting
		  mechanism to avoid the semantic conflicts generated in
		  Contrastive learning pre-training.},
  booktitle	= {Proceedings of the 2023 2nd International Symposium on
		  Computing and Artificial Intelligence},
  pages		= {28–34},
  numpages	= {7},
  keywords	= {Contrastive learning, Event extraction, Pre-trained
		  model},
  location	= {Shanghai, China},
  series	= {ISCAI '23}
}

@InProceedings{	  10.1145/3632971.3632983,
  author	= {Zheng, Dequan and Zhang, Haoyu and Yu, Feng},
  title		= {Named entity recognition of Chinese electronic medical
		  rec-ords based on adversarial training and feature fusion},
  year		= {2024},
  isbn		= {9798400707704},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632971.3632983},
  doi		= {10.1145/3632971.3632983},
  abstract	= {Abstract. In the discipline of natural language
		  processing, named entity recognition is the foundation for
		  tasks such as information extraction, information
		  retrieval, and knowledge graphs. This paper puts forward an
		  entity recognition model based on adversarial training and
		  feature fusion to address the issues of polysemy and not
		  complete word recognition in Chinese electronic medical
		  record named entity recognition. The above technique
		  results in adversarial samples by infusing disturbance
		  factors into the word embedding layer. These adversarial
		  samples obtained are subsequently used for iterative
		  training in order to optimize the model's parameters. Then,
		  utilize the improved Transform encoder and Bi-GRU to
		  extract global the field of semantics and direction
		  information, add an attention mechanism to merge the
		  extracted context features, and finally implement the
		  entity labelling sequence using CRF. In addition, we use
		  the RoBERTa-WWM pre-training model as the embedding layer
		  of the model in order to offer character-level embedding,
		  picking up more contextual semantic data as well as lexical
		  information, as well as enhance entity recognition
		  performance. Experimental results on the CCKS2017 and
		  CCKS2019 evaluation datasets indicate that the proposed
		  model outperforms the baseline model by 0.9% and 0.74 %,
		  for example, in terms of F1. And comparative experiments
		  demonstrate that the addition of adversarial training and
		  feature fusion will improve the model's predictive ability
		  and robustness.},
  booktitle	= {Proceedings of the 2023 International Joint Conference on
		  Robotics and Artificial Intelligence},
  pages		= {175–179},
  numpages	= {5},
  keywords	= {Adversarial training, Attention mechanism, Electronic
		  medical records, Named entity recognition, RoBERTa-WWM},
  location	= {Shanghai, China},
  series	= {JCRAI '23}
}

@Proceedings{	  10.1145/3629527,
  title		= {ICPE '24 Companion: Companion of the 15th ACM/SPEC
		  International Conference on Performance Engineering},
  year		= {2024},
  isbn		= {9798400704451},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to present the ICPE 2024
		  workshops program. ICPE workshops extend the main
		  conference by providing a forum to foster discussion on hot
		  and emerging topics from the broad field of performance
		  engineering. They offer a highly dynamic venue to exchange
		  ideas, establish new collaborations, and bootstrap debates
		  on novel techniques, methodologies, and their associated
		  early research results. Workshops feature various
		  presentation formats, including research paper
		  presentations, panel discussions, and keynote talks.
		  Through these presentations and discussions with peer
		  researchers, ICPE workshops help shape future research and
		  identify promising research directions for performance
		  engineering.},
  location	= {London, United Kingdom}
}

@InProceedings{	  10.1145/3678717.3691277,
  author	= {Paul, Arpan and Maheshwary, Saket and Sohoney, Saurabh},
  title		= {Accurate Customer Address Matching via Weak Supervision
		  for Geocode Learning},
  year		= {2024},
  isbn		= {9798400711077},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3678717.3691277},
  doi		= {10.1145/3678717.3691277},
  abstract	= {Determining the precise location of customers is important
		  for an efficient and reliable delivery experience, both for
		  customers and delivery associates. Address text is a
		  primary source of information provided by customers about
		  their location. In this paper, we study the important and
		  challenging task of matching free-form customer address
		  text to determine if two addresses represent the same
		  physical building. We introduce a novel address matching
		  framework that leverages transformer-based encoder to
		  prevent tedious and time-consuming efforts spent on manual
		  feature engineering by the baseline model. Furthermore, our
		  proposed framework employs weak supervision to leverage
		  historic delivery information and generate high-quality
		  labeled data. This reduces the requirement for massive
		  amounts of labeled data, typically needed for
		  transformer-based models. Our experiments on manually
		  curated datasets demonstrate the effective and generic
		  nature of our approach, as we achieve 15.57% improvement in
		  recall at 95% precision, on average, compared to the
		  current baseline model across four geographies. We also
		  introduce delivery point (DP) geocode learning for
		  cold-start addresses as a downstream application of
		  customer address matching. In addition to offline
		  experiments, we performed online A/B experiments for DP
		  geocode learning with our proposed approach and observed
		  delivery precision improved by 8.09% and delivery defects
		  reduced by 11.78% on average across four geographies in
		  comparison to the baseline model.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Advances in Geographic Information Systems},
  pages		= {454–464},
  numpages	= {11},
  keywords	= {Entity Matching, Geocoding, Language Models, Weak
		  Supervision},
  location	= {Atlanta, GA, USA},
  series	= {SIGSPATIAL '24}
}

@InProceedings{	  10.1145/3639479.3639525,
  author	= {Jin, Fan and Chang, Qingling and Xu, Zhongwen},
  title		= {MuseumQA: A Fine-Grained Question Answering Dataset for
		  Museums and Artifacts},
  year		= {2024},
  isbn		= {9798400709241},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639479.3639525},
  doi		= {10.1145/3639479.3639525},
  abstract	= {In this paper, we present a fine-grained museum artifact
		  question-answering (QA) dataset, which serves as the
		  cornerstone for developing museum question-answering
		  systems. Creating these systems is essential for the
		  advancement of museums and can enhance the visitor
		  experience. Nevertheless, research reveals the current
		  absence of domestically available datasets for museum
		  artifacts in China. To ensure data authenticity and
		  validity, we meticulously collected and screened 3,416 raw
		  data entries from the official websites of provincial
		  museums across China. Using these raw data, we annotated
		  annotatable QA pair information to create the final QA
		  dataset. Initially, a small batch of QA pairs was generated
		  with the assistance of ChatGPT. Subsequently, the remaining
		  QA pairs were annotated using an enhanced QA generation
		  model, yielding 23,149 QA pairs. To mitigate overfitting
		  due to dataset-model size disparities, a noise factor was
		  incorporated into the enhanced generation model.
		  Additionally, a Chinese grammar correction module was
		  integrated to enhance the accuracy of the generated
		  statements. Ultimately, the model achieved optimal
		  performance, and the dataset demonstrated the highest
		  semantic relevance.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Machine Learning and Natural Language Processing},
  pages		= {221–226},
  numpages	= {6},
  keywords	= {museum artifact dataset, question-answering pair
		  generation},
  location	= {Sanya, China},
  series	= {MLNLP '23}
}

@Proceedings{	  10.1145/3639479,
  title		= {MLNLP '23: Proceedings of the 2023 6th International
		  Conference on Machine Learning and Natural Language
		  Processing},
  year		= {2023},
  isbn		= {9798400709241},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sanya, China}
}

@InProceedings{	  10.1145/3634814.3634837,
  author	= {Bai, Minhao and Huang, Yongfeng and Yang, Jinshuai and
		  Pang, Kaiyi and Li, Songbin},
  title		= {Exploration of the Effectiveness and Characteristics of
		  ChatGPT in Steganalysis Tasks},
  year		= {2024},
  isbn		= {9798400708534},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3634814.3634837},
  doi		= {10.1145/3634814.3634837},
  abstract	= {Text steganography is a method of covert communication
		  that aims to conceal the existence of secret information.
		  Steganography has a long history of development and is
		  widely used. However, its misuse poses a serious threat to
		  information security, such as hiding malicious code to
		  bypass security checks or hiding criminal evidence in
		  network environments. In response to the potential threat
		  of steganographic text, steganalysis techniques have
		  received urgent demand from practical applications and
		  extensive attention from researchers. Currently,
		  steganalysis models for text are mainly based on
		  statistical features of steganographic text to identify
		  such text, and these models require a large amount of
		  training data consisting of steganographic and normal text
		  to achieve good classification performance. The emergence
		  of the large-scale conversational model ChatGPT in November
		  last year has attracted widespread attention. Considering
		  the powerful understanding ability of ChatGPT for text, we
		  expect that ChatGPT can achieve good performance in the
		  task of steganalysis or obtain inspiration about
		  steganographic text features from its results. To evaluate
		  the effectiveness of ChatGPT, we conduct experiments on 2
		  datasets and 3 encoding methods. The experiments show that
		  compared with normal steganalisis method, ChatGPT can
		  achieve similar results with only 32 samples, even without
		  any training or fine-tuning.},
  booktitle	= {Proceedings of the 2023 4th Asia Service Sciences and
		  Software Engineering Conference},
  pages		= {163–170},
  numpages	= {8},
  location	= {Aizu-Wakamatsu City, Japan},
  series	= {ASSE '23}
}

@InProceedings{	  10.1145/3654777.3676390,
  author	= {Fan, Haoxiang and Chen, Guanzheng and Wang, Xingbo and
		  Peng, Zhenhui},
  title		= {LessonPlanner: Assisting Novice Teachers to Prepare
		  Pedagogy-Driven Lesson Plans with Large Language Models},
  year		= {2024},
  isbn		= {9798400706288},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3654777.3676390},
  doi		= {10.1145/3654777.3676390},
  abstract	= {Preparing a lesson plan, e.g., a detailed road map with
		  strategies and materials for instructing a 90-minute class,
		  is beneficial yet challenging for novice teachers. Large
		  language models (LLMs) can ease this process by generating
		  adaptive content for lesson plans, which would otherwise
		  require teachers to create from scratch or search existing
		  resources. In this work, we first conduct a formative study
		  with six novice teachers to understand their needs for
		  support of preparing lesson plans with LLMs. Then, we
		  develop LessonPlanner that assists users to interactively
		  construct lesson plans with adaptive LLM-generated content
		  based on Gagne’s nine events. Our within-subjects study
		  (N = 12) shows that compared to the baseline ChatGPT
		  interface, LessonPlanner can significantly improve the
		  quality of outcome lesson plans and ease users’ workload
		  in the preparation process. Our expert interviews (N = 6)
		  further demonstrate LessonPlanner ’s usefulness in
		  suggesting effective teaching strategies and meaningful
		  educational resources. We discuss concerns on and design
		  considerations for supporting teaching activities with
		  LLMs.},
  booktitle	= {Proceedings of the 37th Annual ACM Symposium on User
		  Interface Software and Technology},
  articleno	= {146},
  numpages	= {20},
  keywords	= {Large language models, lesson plan preparation,
		  pedagogy-driven system},
  location	= {Pittsburgh, PA, USA},
  series	= {UIST '24}
}

@Article{	  10.1145/3578519,
  author	= {Frieder, Ophir and Mele, Ida and Muntean, Cristina Ioana
		  and Nardini, Franco Maria and Perego, Raffaele and
		  Tonellotto, Nicola},
  title		= {Caching Historical Embeddings in Conversational Search},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {4},
  issn		= {1559-1131},
  url		= {https://doi.org/10.1145/3578519},
  doi		= {10.1145/3578519},
  abstract	= {Rapid response, namely, low latency, is fundamental in
		  search applications; it is particularly so in interactive
		  search sessions, such as those encountered in
		  conversational settings. An observation with a potential to
		  reduce latency asserts that conversational queries exhibit
		  a temporal locality in the lists of documents retrieved.
		  Motivated by this observation, we propose and evaluate a
		  client-side document embedding cache, improving the
		  responsiveness of conversational search systems. By
		  leveraging state-of-the-art dense retrieval models to
		  abstract document and query semantics, we cache the
		  embeddings of documents retrieved for a topic introduced in
		  the conversation, as they are likely relevant to successive
		  queries. Our document embedding cache implements an
		  efficient metric index, answering nearest-neighbor
		  similarity queries by estimating the approximate result
		  sets returned. We demonstrate the efficiency achieved using
		  our cache via reproducible experiments based on Text
		  Retrieval Conference Conversational Assistant Track
		  datasets, achieving a hit rate of up to 75% without
		  degrading answer quality. Our achieved high cache hit rates
		  significantly improve the responsiveness of conversational
		  systems while likewise reducing the number of queries
		  managed on the search back-end.},
  journal	= {ACM Trans. Web},
  month		= oct,
  articleno	= {42},
  numpages	= {19},
  keywords	= {Conversational search, similarity search, caching, dense
		  retrieval}
}

@Article{	  10.1145/3699759,
  author	= {Arakawa, Riku and Lehman, Jill Fain and Goel, Mayank},
  title		= {PrISM-Q&amp;A: Step-Aware Voice Assistant on a Smartwatch
		  Enabled by Multimodal Procedure Tracking and Large Language
		  Models},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {4},
  url		= {https://doi.org/10.1145/3699759},
  doi		= {10.1145/3699759},
  abstract	= {Voice assistants capable of answering user queries during
		  various physical tasks have shown promise in guiding users
		  through complex procedures. However, users often find it
		  challenging to articulate their queries precisely,
		  especially when unfamiliar with the specific terminologies
		  required for machine-oriented tasks. We introduce
		  PrISM-Q&amp;A, a novel question-answering (Q&amp;A)
		  interaction termed step-aware Q&amp;A, which enhances the
		  functionality of voice assistants on smartwatches by
		  incorporating Human Activity Recognition (HAR) and
		  providing the system with user context. It continuously
		  monitors user behavior during procedural tasks via audio
		  and motion sensors on the watch and estimates which step
		  the user is performing. When a question is posed, this
		  contextual information is supplied to Large Language Models
		  (LLMs) as part of the context used to generate a response,
		  even in the case of inherently vague questions like "What
		  should I do next with this?" Our studies confirmed that
		  users preferred the convenience of our approach compared to
		  existing voice assistants. Our real-time assistant
		  represents the first Q&amp;A system that provides
		  contextually situated support during tasks without camera
		  use, paving the way for the ubiquitous, intelligent
		  assistant.},
  journal	= {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month		= nov,
  articleno	= {180},
  numpages	= {26},
  keywords	= {context-aware, large language models, procedure tracking,
		  question answering, task assistance}
}

@InProceedings{	  10.1145/3626772.3657693,
  author	= {Tang, Yanran and Qiu, Ruihong and Yin, Hongzhi and Li, Xue
		  and Huang, Zi},
  title		= {CaseLink: Inductive Graph Learning for Legal Case
		  Retrieval},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657693},
  doi		= {10.1145/3626772.3657693},
  abstract	= {In case law, the precedents are the relevant cases that
		  are used to support the decisions made by the judges and
		  the opinions of lawyers towards a given case. This
		  relevance is referred to as the case-to-case reference
		  relation. To efficiently find relevant cases from a large
		  case pool, retrieval tools are widely used by legal
		  practitioners. Existing legal case retrieval models mainly
		  work by comparing the text representations of individual
		  cases. Although they obtain a decent retrieval accuracy,
		  the intrinsic case connectivity relationships among cases
		  have not been well exploited for case encoding, therefore
		  limiting the further improvement of retrieval performance.
		  In a case pool, there are three types of case connectivity
		  relationships: the case reference relationship, the case
		  semantic relationship, and the case legal charge
		  relationship. Due to the inductive manner in the task of
		  legal case retrieval, using case reference as input is not
		  applicable for testing. Thus, in this paper, a CaseLink
		  model based on inductive graph learning is proposed to
		  utilise the intrinsic case connectivity for legal case
		  retrieval, a novel Global Case Graph is incorporated to
		  represent both the case semantic relationship and the case
		  legal charge relationship. A novel contrastive objective
		  with a regularisation on the degree of case nodes is
		  proposed to leverage the information carried by the case
		  reference relationship to optimise the model. Extensive
		  experiments have been conducted on two benchmark datasets,
		  which demonstrate the state-of-the-art performance of
		  CaseLink. The code has been released on
		  https://github.com/yanran-tang/CaseLink.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {2199–2209},
  numpages	= {11},
  keywords	= {graph neural networks, information retrieval, legal case
		  retrieval},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3631700.3665188,
  author	= {Hendrawan, Rully Agus and Brusilovsky, Peter and Lekshmi
		  Narayanan, Arun Balajiee and Barria-Pineda, Jordan},
  title		= {Explanations in Open User Models for Personalized
		  Information Exploration},
  year		= {2024},
  isbn		= {9798400704666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631700.3665188},
  doi		= {10.1145/3631700.3665188},
  abstract	= {Open user models provide affordance for a transparent user
		  control over recommendations based on shared symbolic
		  representation within the system. Users must build their
		  user profile by adding these symbols and tuning their
		  importance to get meaningful recommendations. Since the
		  link between these symbols and the reference explanation is
		  often unavailable, it can be difficult for users to
		  understand them. These symbols are often referred to as
		  concepts, tags, areas, topics, labels, features, or
		  keyphrases. This study showcases an information exploration
		  system that helps students identify potential faculty
		  members to collaborate with. The system works by matching
		  user and faculty profiles that contain keywords or phrases
		  representing topics/areas of interest. Students must
		  develop their understanding of research topics while
		  building their profiles, which can become challenging as
		  they add more keywords. To support students in controlling
		  the recommendation, we introduce post hoc explanations with
		  three levels of detail: no explanations, individual
		  explanation for topics, and explanation of the
		  relationships between topics. This study explores how
		  explanation is associated with the user context / tasks and
		  the exploration process. Our observation suggests that
		  expertise in the field is linked to exploring fewer novel
		  topics and seeking fewer explanations but engaging more
		  with explanations of relationships. In addition, we found
		  that the engagement with faculty information is moderately
		  correlated with the use of more advanced explanations.},
  booktitle	= {Adjunct Proceedings of the 32nd ACM Conference on User
		  Modeling, Adaptation and Personalization},
  pages		= {256–263},
  numpages	= {8},
  keywords	= {Adaptive explanation, Concept graph, Information
		  exploration, Intelligent interface, Open user model},
  location	= {Cagliari, Italy},
  series	= {UMAP Adjunct '24}
}

@Article{	  10.1145/3649506,
  author	= {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han,
		  Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong,
		  Shaochen and Yin, Bing and Hu, Xia},
  title		= {Harnessing the Power of LLMs in Practice: A Survey on
		  ChatGPT and Beyond},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {6},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3649506},
  doi		= {10.1145/3649506},
  abstract	= {This article presents a comprehensive and practical guide
		  for practitioners and end-users working with Large Language
		  Models (LLMs) in their downstream Natural Language
		  Processing (NLP) tasks. We provide discussions and insights
		  into the usage of LLMs from the perspectives of models,
		  data, and downstream tasks. First, we offer an introduction
		  and brief summary of current language models. Then, we
		  discuss the influence of pre-training data, training data,
		  and test data. Most importantly, we provide a detailed
		  discussion about the use and non-use cases of large
		  language models for various natural language processing
		  tasks, such as knowledge-intensive tasks, traditional
		  natural language understanding tasks, generation tasks,
		  emergent abilities, and considerations for specific tasks.
		  We present various use cases and non-use cases to
		  illustrate the practical applications and limitations of
		  LLMs in real-world scenarios. We also try to understand the
		  importance of data and the specific challenges associated
		  with each NLP task. Furthermore, we explore the impact of
		  spurious biases on LLMs and delve into other essential
		  considerations, such as efficiency, cost, and latency, to
		  ensure a comprehensive understanding of deploying LLMs in
		  practice. This comprehensive guide aims to provide
		  researchers and practitioners with valuable insights and
		  best practices for working with LLMs, thereby enabling the
		  successful implementation of these models in a wide range
		  of NLP tasks. A curated list of practical guide resources
		  of LLMs, regularly updated, can be found at . An LLMs
		  evolutionary tree, editable yet regularly updated, can be
		  found at .},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= apr,
  articleno	= {160},
  numpages	= {32},
  keywords	= {Large language models, neural language processing,
		  practical guide, ChatGPT}
}

@Proceedings{	  10.1145/3656650,
  title		= {AVI '24: Proceedings of the 2024 International Conference
		  on Advanced Visual Interfaces},
  year		= {2024},
  isbn		= {9798400717642},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {AVI 2024 is the 17th edition of the International
		  Conference on Advanced Visual Interfaces, held in Arenzano,
		  Genoa (IT), in cooperation with ACM, ACM SIGCHI, ACM SIGMM,
		  and ACM SIGWEB.Every two years since 1992, AVI has gathered
		  a vast international community of experts with a wide range
		  of backgrounds. Throughout three decades, AVI has gained
		  and holds a prestigious position among International HCI
		  conferences, boasting a dedicated nucleus of returning
		  participants, but also providing a venue for young
		  researchers to show their achievements and establish
		  contacts with senior community members.AVI 2024 presents a
		  broad and sound scientific program covering traditional AVI
		  topics on information and data visualization, interaction
		  with multimodal user interfaces, augmented and virtual
		  reality, while also addressing emerging topics including
		  the application of generative artificial intelligence in
		  HCI design and evaluation.The program features the
		  presentation of 21 long research papers and 28 short papers
		  selected through a rigorous double-blind reviewing process
		  and organized into sessions on 13 main topics. Furthermore,
		  it includes the presentation of 48 poster papers, 9 demo
		  papers, and 11 doctoral consortium papers, selected through
		  a single-blind reviewing process. Finally, the rich and
		  vibrant program includes 3 keynote talks, 3 tutorials, and
		  10 workshops addressing some of the most exciting issues in
		  HCI.Submissions to AVI 2024 came from 34 different
		  countries distributed in descending order in Europe, Asia,
		  North America, South America, and Africa.},
  location	= {Arenzano, Genoa, Italy}
}

@InProceedings{	  10.1145/3616855.3635724,
  author	= {Dave, Vachik S. and Pang, Linsey and Cui, Xiquan and Luo,
		  Chen and Zamani, Hamed and Wu, Lingfei and Karypis,
		  George},
  title		= {The 3rd International Workshop on Interactive and Scalable
		  Information Retrieval Methods for eCommerce (ISIR-eCom
		  2024)},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3635724},
  doi		= {10.1145/3616855.3635724},
  abstract	= {Over the past few years, consumer behavior has shifted
		  from traditional in-store shopping to online shopping. For
		  example, eCommerce sales have grown from around 5% of total
		  US sales in 2012 to around 15.4% in year 2023. This rapid
		  growth of eCommerce has created new challenges and vital
		  new requirements for intelligent information retrieval
		  systems. Which lead to the primary motivations of this
		  workshop:(1) Since the pandemic hit, eCommerce became an
		  important part of people's routine and they started using
		  online shop- ping for smallest grocery items to big
		  electronics as well as cars. With such a large assortment
		  of products and millions of users, achieving higher
		  scalability without losing accuracy is a leading concern
		  for information retrieval systems for eCommerce.(2) The
		  diverse buyers make the relevance of the results highly
		  subjective, because relevance varies for different buyers.
		  The most suitable and intuitive solution to this problem is
		  to make the system interactive and provide correct
		  relevance for different users. Hence, interactive
		  information retrieval systems are becoming necessity in
		  eCommerce.(3) To handle sudden change in buyers' behavior,
		  industries adopted existing sub-optimal information
		  retrieval techniques for various eCommerce tasks.
		  Parallelly, they also started exploring/researching for
		  better solutions and in dire need of help from research
		  community.This workshop will provide a forum to discuss and
		  learn the latest trends for interactive and scalable
		  information retrieval approaches for eCommerce. It will
		  provide academic and industrial researchers a platform to
		  present their latest works, share research ideas, present
		  and discuss various challenges, and identify the areas
		  where further research is needed. It will foster the
		  development of a strong research community focused on
		  solving eCommerce-related information retrieval problems
		  that provide superior eCommerce experience to all users.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {1208–1209},
  numpages	= {2},
  keywords	= {ecommerce search, information retrieval, interactive
		  systems, large language models (llms) in ecommerce, natural
		  language processing (nlp) for ecommerce, ranking models,
		  recommender systems},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@Article{	  10.1145/3665244,
  author	= {Zhang, Jinyi and Su, Ke and Li, Haowei and Mao, Jiannan
		  and Tian, Ye and Wen, Feng and Guo, Chong and Matsumoto,
		  Tadahiro},
  title		= {Neural Machine Translation for Low-Resource Languages from
		  a Chinese-centric Perspective: A Survey},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {6},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3665244},
  doi		= {10.1145/3665244},
  abstract	= {Machine translation–the automatic transformation of one
		  natural language (source language) into another (target
		  language) through computational means–occupies a central
		  role in computational linguistics and stands as a
		  cornerstone of research within the field of Natural
		  Language Processing (NLP). In recent years, the prominence
		  of Neural Machine Translation (NMT) has grown
		  exponentially, offering an advanced framework for machine
		  translation research. It is noted for its superior
		  translation performance, especially when tackling the
		  challenges posed by low-resource language pairs that suffer
		  from a limited corpus of data resources. This article
		  offers an exhaustive exploration of the historical
		  trajectory and advancements in NMT, accompanied by an
		  analysis of the underlying foundational concepts. It
		  subsequently provides a concise demarcation of the unique
		  characteristics associated with low-resource languages and
		  presents a succinct review of pertinent translation models
		  and their applications, specifically within the context of
		  languages with low-resources. Moreover, this article delves
		  deeply into machine translation techniques, highlighting
		  approaches tailored for Chinese-centric low-resource
		  languages. Ultimately, it anticipates upcoming research
		  directions in the realm of low-resource language
		  translation.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jun,
  articleno	= {80},
  numpages	= {60},
  keywords	= {Low-resource languages, neural machine translation,
		  unsupervised learning, transfer learning, multilingual
		  translation, large language models, Chinese-centric
		  languages}
}

@InProceedings{	  10.1145/3651671.3651709,
  author	= {Shi, Yuning and Kimura, Masaomi},
  title		= {BERT-Based Models with Attention Mechanism and Lambda
		  Layer for Biomedical Named Entity Recognition},
  year		= {2024},
  isbn		= {9798400709234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3651671.3651709},
  doi		= {10.1145/3651671.3651709},
  abstract	= {Biomedical named entity recognition (NER) is a crucial
		  subtask in the field of information extraction within
		  natural language processing (NLP). Its primary objective is
		  to identify and classify entities in biomedical text,
		  playing a pivotal role in applications such as medical
		  information retrieval and biomedical knowledge discovery.
		  In this paper, we propose several enhanced versions of
		  BERT-BiLSTM-CRF and BERT-IDCNN-CRF by incorporating an
		  attention mechanism or lambda layer to improve entity
		  recognition accuracy. Specifically, we utilize the
		  attention mechanism to enable the model to learn
		  interrelationships among all words in the input sequence.
		  Additionally, we employ the lambda layer to enhance the
		  model's capacity for capturing semantic relationships
		  between words and considering word order. This integration
		  results in superior accuracy in entity recognition. We
		  evaluate our proposed methods using the i2b2 2010 dataset
		  and six additional biomedical datasets from the Biomedical
		  Language Understanding and Reasoning Benchmark (BLURB),
		  including JNLPBA, BC2GM, BC5CDR, AnatEM, BioNLP-CG, and
		  NCBI-disease. Experimental results demonstrate that our
		  proposed methods achieve higher accuracy than the original
		  methods, indicating superior capabilities in medical
		  knowledge extraction for our models.},
  booktitle	= {Proceedings of the 2024 16th International Conference on
		  Machine Learning and Computing},
  pages		= {536–544},
  numpages	= {9},
  keywords	= {Attention Mechanism, BERT-BiLSTM-CRF, BERT-IDCNN-CRF, Deep
		  Learning, Lambda Layer, Named Entity Recognition},
  location	= {Shenzhen, China},
  series	= {ICMLC '24}
}

@Article{	  10.1145/3655618,
  author	= {Zhang, Xinghua and Yu, Bowen and Cong, Xin and Su, Taoyu
		  and Li, Quangang and Liu, Tingwen and Xu, Hongbo},
  title		= {Cross-Domain NER under a Divide-and-Transfer Paradigm},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {5},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3655618},
  doi		= {10.1145/3655618},
  abstract	= {Cross-domain Named Entity Recognition (NER) transfers
		  knowledge learned from a rich-resource source domain to
		  improve the learning in a low-resource target domain. Most
		  existing works are designed based on the sequence labeling
		  framework, defining entity detection and type prediction as
		  a monolithic process. However, they typically ignore the
		  discrepant transferability of these two sub-tasks: the
		  former locating spans corresponding to entities is largely
		  domain-robust, whereas the latter owns distinct entity
		  types across domains. Combining them into an entangled
		  learning problem may contribute to the complexity of domain
		  transfer. In this work, we propose the novel
		  divide-and-transfer paradigm in which different sub-tasks
		  are learned using separate functional modules for
		  respective cross-domain transfer. To demonstrate the
		  effectiveness of divide-and-transfer, we concretely
		  implement two NER frameworks by applying this paradigm with
		  different cross-domain transfer strategies. Experimental
		  results on 10 different domain pairs show the notable
		  superiority of our proposed frameworks. Experimental
		  analyses indicate that significant advantages of the
		  divide-and-transfer paradigm over prior monolithic ones
		  originate from its better performance on low-resource data
		  and a much greater transferability. It gives us a new
		  insight into cross-domain NER. Our code is available on
		  GitHub.1},
  journal	= {ACM Trans. Inf. Syst.},
  month		= may,
  articleno	= {137},
  numpages	= {32},
  keywords	= {Named entity recognition, cross-domain transfer,
		  information extraction, knowledge acquisition, task
		  decomposition}
}

@Proceedings{	  10.1145/3685650,
  title		= {DocEng '24: Proceedings of the ACM Symposium on Document
		  Engineering 2024},
  year		= {2024},
  isbn		= {9798400711695},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {San Jose, CA, USA}
}

@InProceedings{	  10.1145/3664647.3681012,
  author	= {Zhao, Deji and Han, Donghong and Yuan, Ye and Ning, Bo and
		  Li, Mengxiang and He, Zhongjiang and Song, Shuangyong},
  title		= {AutoGraph: Enabling Visual Context via Graph Alignment in
		  Open Domain Multi-Modal Dialogue Generation},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681012},
  doi		= {10.1145/3664647.3681012},
  abstract	= {Open-domain multi-modal dialogue system heavily relies on
		  visual information to generate contextually relevant
		  responses. The existing open-domain multi-modal dialog
		  generation methods ignore the complementary relationship
		  between multiple modalities, and are difficult to integrate
		  with LLMs. To tackle these challenges, we introduce
		  AutoGraph, an innovative method for constructing visual
		  context graphs automatically. We aim to structure complex
		  information and seamlessly integrate it with large language
		  models (LLMs), aligning information from multiple
		  modalities at both semantic and structural levels.
		  Specifically, we fully connect the text graphs and scene
		  graphs, and then trim unnecessary edges via LLMs to
		  automatically construct a visual context graph. Next, we
		  design several graph sampling grammar for the first time to
		  convert graph structures into sequence which is suitable
		  for LLMs. Finally, we propose a two-stage fine-tuning
		  strategy to allow LLMs to understand graph sampling grammar
		  and generate responses. We validate our proposed method on
		  text-based LLMs, and visual-based LLMs, respectively.
		  Experimental results show that our proposed method achieves
		  state-of-the-art performance on multiple public datasets.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {2079–2088},
  numpages	= {10},
  keywords	= {dialogue generation, dialogue graph, multi-modal
		  alignment},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Article{	  10.1145/3615668,
  author	= {Hemberg, Erik and Turner, Matthew J. and Rutar, Nick and
		  O’reilly, Una-May},
  title		= {Enhancements to Threat, Vulnerability, and Mitigation
		  Knowledge for Cyber Analytics, Hunting, and Simulations},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {1},
  url		= {https://doi.org/10.1145/3615668},
  doi		= {10.1145/3615668},
  abstract	= {Cross-linked threat, vulnerability, and defensive
		  mitigation knowledge is critical in defending against
		  diverse and dynamic cyber threats. Cyber analysts consult
		  it by deductively or inductively creating a chain of
		  reasoning to identify a threat starting from indicators
		  they observe or vice versa. Cyber hunters use it
		  abductively to reason when hypothesizing specific threats.
		  Threat modelers use it to explore threat postures. We
		  aggregate five public sources of threat knowledge and three
		  public sources of knowledge that describe cyber defensive
		  mitigations, analytics, and engagements and which share
		  some unidirectional links between them. We unify the
		  sources into a graph, and in the graph, we make all
		  unidirectional cross-source links bidirectional. This
		  enhancement of the knowledge makes the questions that
		  analysts and automated systems formulate easier to answer.
		  We demonstrate this in the context of various cyber
		  analytic and hunting tasks as well as modeling and
		  simulations. Because the number of linked entries is very
		  sparse, to further increase the analytic utility of the
		  data, we use natural language processing and supervised
		  machine learning to identify new links. These two
		  contributions demonstrably increase the value of the
		  knowledge sources for cyber security activities.},
  journal	= {Digital Threats},
  month		= mar,
  articleno	= {8},
  numpages	= {33},
  keywords	= {Cyber security, threat hunting, machine learning, natural
		  language processing, information retrieval, reinforcement
		  learning, coevolutionary algorithm}
}

@InProceedings{	  10.1145/3627673.3679746,
  author	= {Wang, Yu and Lipka, Nedim and Zhang, Ruiyi and Siu, Alexa
		  and Zhao, Yuying and Ni, Bo and Wang, Xin and Rossi, Ryan
		  and Derr, Tyler},
  title		= {Topology-aware Retrieval Augmentation for Text
		  Generation},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679746},
  doi		= {10.1145/3627673.3679746},
  abstract	= {Retrieval-augmented Generation has been used to augment
		  Language Models by retrieving texts from external
		  databases. Since real-world texts are often connected in
		  the graph (e.g., papers in citation networks), we use these
		  relations to guide the retrieval process of RAG.
		  Concretely, we investigate proximity and role-based
		  relations, where the former considers topologically close
		  nodes and the latter considers structurally similar nodes.
		  We empirically verify their correlation to text relations,
		  which motivates us to propose the framework of
		  Topology-aware Retrieval-augmented Generation for text
		  generation, which consists of a retrieval module to
		  retrieve texts by their topological relations and an
		  aggregation module to compose retrieved texts into prompts
		  triggering LLMs for text generation. Extensive experiments
		  verify the effectiveness of this framework, signifying the
		  potential of equipping RAG with topological awareness.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2442–2452},
  numpages	= {11},
  keywords	= {graph structural relations, retrieval-augmented
		  generation},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3661996,
  author	= {Chen, Xiaocong and Wang, Siyu and McAuley, Julian and
		  Jannach, Dietmar and Yao, Lina},
  title		= {On the Opportunities and Challenges of Offline
		  Reinforcement Learning for Recommender Systems},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3661996},
  doi		= {10.1145/3661996},
  abstract	= {Reinforcement learning serves as a potent tool for
		  modeling dynamic user interests within recommender systems,
		  garnering increasing research attention of late. However, a
		  significant drawback persists: its poor data efficiency,
		  stemming from its interactive nature. The training of
		  reinforcement learning-based recommender systems demands
		  expensive online interactions to amass adequate
		  trajectories, essential for agents to learn user
		  preferences. This inefficiency renders reinforcement
		  learning-based recommender systems a formidable
		  undertaking, necessitating the exploration of potential
		  solutions. Recent strides in offline reinforcement learning
		  present a new perspective. Offline reinforcement learning
		  empowers agents to glean insights from offline datasets and
		  deploy learned policies in online settings. Given that
		  recommender systems possess extensive offline datasets, the
		  framework of offline reinforcement learning aligns
		  seamlessly. Despite being a burgeoning field, works
		  centered on recommender systems utilizing offline
		  reinforcement learning remain limited. This survey aims to
		  introduce and delve into offline reinforcement learning
		  within recommender systems, offering an inclusive review of
		  existing literature in this domain. Furthermore, we strive
		  to underscore prevalent challenges, opportunities, and
		  future pathways, poised to propel research in this evolving
		  field.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= aug,
  articleno	= {150},
  numpages	= {26},
  keywords	= {Offline reinforcement learning}
}

@Proceedings{	  10.1145/3665939,
  title		= {HILDA 24: Proceedings of the 2024 Workshop on
		  Human-In-the-Loop Data Analytics},
  year		= {2024},
  isbn		= {9798400706936},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Santiago, AA, Chile}
}

@InProceedings{	  10.1145/3652628.3652794,
  author	= {Zhu, Yanbing and Xu, Sitian and Liu, Boyang and Jia,
		  Yuntao},
  title		= {Optimization of Smart Healthcare Services and Development
		  Strategies Based on Large Language Models},
  year		= {2024},
  isbn		= {9798400708831},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652628.3652794},
  doi		= {10.1145/3652628.3652794},
  abstract	= {We aims to optimize online smart healthcare service
		  projects and provide a reference for the development
		  strategy of healthcare large language models. In terms of
		  optimizing service projects, the paper utilizes the U&amp;A
		  model to statistically analyze 27 commonly used digital
		  products in the smart healthcare market. Employing the Kano
		  model and empirical investigation, the service projects are
		  categorized into basic, premium, and personalized types.
		  Among these, basic service projects exhibit the highest
		  positive impact on user satisfaction. Subsequently,
		  building upon these research findings, the paper centers on
		  basic service projects to explore the development strategy
		  of large language models in smart healthcare. Concerning
		  development strategy, the paper collects a total of 15,073
		  data points and establishes 13 testing dimensions. Tests
		  are conducted on platforms such as chatGPT, chatDoctor, and
		  the open-source chatGLM from Tsinghua iFLYTEK AI Research.
		  Evaluation results indicate that general-purpose large
		  models perform well in overall scores, but their
		  performance in the healthcare domain is lower than that of
		  fine-tuned medical large models. Therefore, the paper
		  suggests that smart healthcare large models should start
		  with the widely recognized "basic smart healthcare service
		  projects." Leveraging the advantages of general-purpose
		  large models, incorporating domain expertise for fine-tuned
		  training is proposed to enhance model accuracy and meet the
		  usage demands of medical scenarios.},
  booktitle	= {Proceedings of the 4th International Conference on
		  Artificial Intelligence and Computer Engineering},
  pages		= {1004–1011},
  numpages	= {8},
  location	= {Dalian, China},
  series	= {ICAICE '23}
}

@InProceedings{	  10.1145/3660043.3660071,
  author	= {Jin, Guanghao and Zhao, Junhua and Wang, Yuqing and Wang,
		  Jieying and Du, Hui and Song, Qingzeng},
  title		= {Multi-level Teaching Text Classification based on the
		  Fusion of Deep Learning Models},
  year		= {2024},
  isbn		= {9798400716157},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3660043.3660071},
  doi		= {10.1145/3660043.3660071},
  abstract	= {Deep learning is widely used in text classification, which
		  can be help the collection of teaching text samples from
		  multiple datasets. On the other side, the variety of texts
		  causes the difficulty of the classification. To solve this
		  problem, we design a multi-level text classification system
		  that fuses multiple models to increase the accuracy of the
		  classification. In more details, we train the deep learning
		  models on some public datasets. Then, we organize these to
		  construct multi-level sets of models. On a text sample, we
		  firstly detect if the dataset classification is needed.
		  Then, we send this sample to the proper level of model set.
		  Finally, we continuously optimize the system based on the
		  collected samples. As the experimental results show, our
		  method can achieve higher accuracy than the existing
		  ones.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Information Education and Artificial Intelligence},
  pages		= {155–159},
  numpages	= {5},
  location	= {Xiamen, China},
  series	= {ICIEAI '23}
}

@InProceedings{	  10.1145/3643991.3644931,
  author	= {Islam, Md Anaytul and Asaduzzman, Muhammad and Wang,
		  Shaowei},
  title		= {On the Executability of R Markdown Files},
  year		= {2024},
  isbn		= {9798400705878},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643991.3644931},
  doi		= {10.1145/3643991.3644931},
  abstract	= {R Markdown files are examples of literate programming
		  documents that combine R code with results and
		  explanations. Such dynamic documents are designed to
		  execute easily and reproduce study results. However, little
		  is known about the executability of R Markdown files which
		  can cause frustration among its users who intend to reuse
		  the document. This paper presents a large-scale study on
		  the executability of R Markdown files collected from
		  GitHub. Results from our study show that a significant
		  number of R Markdown files (64.95%) are not executable,
		  even after our best efforts. To better understand the
		  challenges, we categorize the exceptions encountered while
		  executing the documents into different categories. Finally,
		  we develop a classifier to determine which Markdown files
		  are likely to be executable. Such a classifier can be
		  utilized by search engines in their ranking which helps
		  developers to find literate programming documents as
		  learning resources.},
  booktitle	= {Proceedings of the 21st International Conference on Mining
		  Software Repositories},
  pages		= {254–264},
  numpages	= {11},
  keywords	= {R Markdown, GitHub, executability, literate programming},
  location	= {Lisbon, Portugal},
  series	= {MSR '24}
}

@InProceedings{	  10.1145/3677779.3677824,
  author	= {Yao, Zhan‘ao and chen, Tingwei},
  title		= {A hop-based parallel graph attention network for relation
		  extraction},
  year		= {2024},
  isbn		= {9798400709760},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3677779.3677824},
  doi		= {10.1145/3677779.3677824},
  abstract	= {Using graph neural networks to model dependency grammar
		  information for relation extraction methods has become a
		  very common approach. However, during the updating process
		  of graph neural networks, they may be affected by the model
		  architecture, which can lead to excessive attention to
		  nearby nodes and affect the learning of distant words along
		  the path. We suggest a graph neural network solution for
		  relation extraction tasks by incorporating hop connections
		  to tackle this issue. Meanwhile, in order to apply the
		  dependency relationship of one hop to multiple hops, we
		  designed a dependency relationship embedding based on the
		  number of hops.The name is hop-based parallel graph
		  attention network(Hp-GAT). This method not only compensates
		  for the shortcomings of graph neural networks but also
		  takes into account the information on the hop distance
		  between words. We conducted experiments on the TACRED and
		  SEMEVAL datasets, with findings indicating that our model
		  outperformed the baseline dependency-driven graph neural
		  network model.},
  booktitle	= {Proceedings of the International Conference on Modeling,
		  Natural Language Processing and Machine Learning},
  pages		= {272–281},
  numpages	= {10},
  location	= {Xi'an, China},
  series	= {CMNM '24}
}

@InProceedings{	  10.1145/3631802.3631816,
  author	= {Malaise, Yoshi and Signer, Beat},
  title		= {Explorotron: An IDE Extension for Guided and Independent
		  Code Exploration and Learning (Discussion Paper)},
  year		= {2024},
  isbn		= {9798400716539},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3631802.3631816},
  doi		= {10.1145/3631802.3631816},
  abstract	= {We introduce the Explorotron Visual Studio Code extension
		  for guided and independent code exploration and learning.
		  Explorotron is a continuation of earlier work to explore
		  how we can enable small organisations with limited
		  resources to provide pedagogically sound learning
		  experiences in programming. We situate Explorotron in the
		  field of Computing Education Research&nbsp;(CER) and
		  envision it to initiate a discussion around different
		  topics, including how to balance the optimisation between
		  the researcher-student-teacher trifecta that is inherent in
		  CER, how to ethically and responsibly use large language
		  models&nbsp;(LLMs) in the independent learning and
		  exploration by students, and how to define better learning
		  sessions over coding content that students obtained on
		  their own. We further reflect on the question raised by
		  Begel and Ko whether technology should “structure
		  learning for learners” or whether learners should “be
		  taught how to structure their own independent learning”
		  outside of the classroom.},
  booktitle	= {Proceedings of the 23rd Koli Calling International
		  Conference on Computing Education Research},
  articleno	= {24},
  numpages	= {8},
  keywords	= {PRIMM, Programming Education, Study Lenses},
  location	= {Koli, Finland},
  series	= {Koli Calling '23}
}

@Proceedings{	  10.1145/3633637,
  title		= {ICCPR '23: Proceedings of the 2023 12th International
		  Conference on Computing and Pattern Recognition},
  year		= {2023},
  isbn		= {9798400707988},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Qingdao, China}
}

@Proceedings{	  10.1145/3638584,
  title		= {CSAI '23: Proceedings of the 2023 7th International
		  Conference on Computer Science and Artificial
		  Intelligence},
  year		= {2023},
  isbn		= {9798400708688},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@InProceedings{	  10.1145/3613904.3642855,
  author	= {Sivertsen, Christian and Salimbeni, Guido and L\o{}vlie,
		  Anders Sundnes and Benford, Steven David and Zhu, Jichen},
  title		= {Machine Learning Processes As Sources of Ambiguity:
		  Insights from AI Art},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642855},
  doi		= {10.1145/3613904.3642855},
  abstract	= {Ongoing efforts to turn Machine Learning (ML) into a
		  design material have encountered limited success. This
		  paper examines the burgeoning area of AI art to understand
		  how artists incorporate ML in their creative work. Drawing
		  upon related HCI theories, we investigate how artists
		  create ambiguity by analyzing nine AI artworks that use
		  computer vision and image synthesis. Our analysis shows
		  that, in addition to the established types of ambiguity,
		  artists worked closely with the ML process (dataset
		  curation, model training, and application) and developed
		  various techniques to evoke the ambiguity of processes. Our
		  finding indicates that the current conceptualization of ML
		  as a design material needs to reframe the ML process as
		  design elements, instead of technical details. Finally,
		  this paper offers reflections on commonly held assumptions
		  in HCI about ML uncertainty, dependability, and
		  explainability, and advocates to supplement the
		  artifact-centered design perspective of ML with a
		  process-centered one.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {165},
  numpages	= {14},
  keywords	= {ambiguity, art, artificial intelligence, computer vision,
		  generative art, machine learning},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@InProceedings{	  10.1145/3638584.3638614,
  author	= {Wen, Luhan and Zhou, Dongmei and Luo, Hao and Cheng,
		  Yongjian},
  title		= {A Novel Classification Model for Automatic Multi-Label ICD
		  Coding via BERT-LSTM},
  year		= {2024},
  isbn		= {9798400708688},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638584.3638614},
  doi		= {10.1145/3638584.3638614},
  abstract	= {Clinical notes are text documents created by physicians at
		  each patient visit to record details of diagnosis and
		  treatment, and are labeled using medical codes. However,
		  manually marking up these codes is time-consuming and
		  error-prone. To address this problem, we propose a new
		  multi-label classification method inspired by the
		  encoder-decoder structure that utilizes the BERT-LSTM
		  network structure to automatically assign ICD codes to
		  clinical texts. The model is able to accurately predict the
		  appropriate medical codes based on the content and
		  contextual information of the clinical text, improving
		  efficiency while reducing errors. By combining these two
		  powerful neural network models, we are able to better
		  handle the task of coding clinical notes. In comparative
		  experiments, the application results of the model are
		  better than some basic neural network architectures,
		  achieving 85.7% of AUC, 61.2% of precesion@5 and 56.5% of
		  Micro-F1. This result demonstrates the robustness of our
		  proposed method and the effectiveness automatic ICD coding
		  classification.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {1–7},
  numpages	= {7},
  keywords	= {BERT, Clinical note automatic coding, LSTM, Multi-label
		  classification},
  location	= {Beijing, China},
  series	= {CSAI '23}
}

@Proceedings{	  10.5555/3694718,
  title		= {JCDL '23: Proceedings of the 2023 ACM/IEEE Joint
		  Conference on Digital Libraries},
  year		= {2024},
  isbn		= {9798350399318},
  publisher	= {IEEE Press},
  location	= {Santa Fe, New Mexico, USA}
}

@Proceedings{	  10.1145/3671016,
  title		= {Internetware '24: Proceedings of the 15th Asia-Pacific
		  Symposium on Internetware},
  year		= {2024},
  isbn		= {9798400707056},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Macau, China}
}

@InProceedings{	  10.1145/3689236.3696049,
  author	= {Shi, Jing and Yan, Jing and Zhang, Hong and Liu, Ting},
  title		= {Intelligent Editing and Publishing System Based on Deep
		  Learning and Multi-Dimensional Cross-Encoding},
  year		= {2024},
  isbn		= {9798400718137},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689236.3696049},
  doi		= {10.1145/3689236.3696049},
  abstract	= {Abstract: With the rapid development of intelligent
		  technologies in recent years, to enhance the level of
		  intelligence in semantic modeling and automated processing
		  of editing and publishing systems, this paper designs an
		  intelligent editing and publishing system based on deep
		  learning and multidimensional cross-encoding. The system's
		  multidimensional cross-encoder performs semantic modeling
		  of user-input keywords and achieves precise control over
		  topics in the editing process through a weighted
		  concatenation attention mechanism. To further optimize
		  system performance, this study introduces a Generative
		  Adversarial Network (GAN) to optimize the semantic
		  representation of the generator. Experimental validation
		  shows that the system performs excellently on evaluation
		  metrics such as BLEU-3, BLEU-4, and Correlation,
		  particularly with significant improvements in the
		  consistency between keywords and text semantics. The
		  research results indicate that the application of this
		  system in intelligent editing and publishing processes
		  offers significant technical advantages, effectively
		  supporting topic planning and content proofreading in
		  intelligent publishing workflows and providing technical
		  support for the development of the intelligent publishing
		  field.},
  booktitle	= {Proceedings of the 2024 9th International Conference on
		  Cyber Security and Information Engineering},
  pages		= {923–930},
  numpages	= {8},
  keywords	= {Deep Learning, Generative Adversarial Network,
		  Intelligence, Intelligent Editing and Publishing,
		  Multi-Dimensional Cross-Encoding},
  location	= { },
  series	= {ICCSIE '24}
}

@Proceedings{	  10.1145/3649158,
  title		= {SACMAT 2024: Proceedings of the 29th ACM Symposium on
		  Access Control Models and Technologies},
  year		= {2024},
  isbn		= {9798400704918},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the 29th ACM
		  Symposium on Access Control Models and Technologies (SACMAT
		  2024). This year's symposium continues its tradition of
		  being the premier venue for presenting research results and
		  experience reports on cutting edge advances on access
		  control, including models, systems, applications, and
		  theory, while also embracing an expanded focus on the
		  general area of computer and information security and
		  privacy. The overarching goal of the symposium is to share
		  novel access control and computer security solutions that
		  fulfill the needs of emerging applications and
		  environments, and also to identify new directions for
		  future research and development. ACM SACMAT provides
		  researchers and also practitioners with a unique
		  opportunity to share their perspectives with others
		  interested in the various aspects of access control and
		  computer security.},
  location	= {San Antonio, TX, USA}
}

@Proceedings{	  10.1145/3640543,
  title		= {IUI '24: Proceedings of the 29th International Conference
		  on Intelligent User Interfaces},
  year		= {2024},
  isbn		= {9798400705083},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Greenville, SC, USA}
}

@Proceedings{	  10.1145/3665463,
  title		= {CHI PLAY Companion '24: Companion Proceedings of the 2024
		  Annual Symposium on Computer-Human Interaction in Play},
  year		= {2024},
  isbn		= {9798400706929},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Tampere, Finland}
}

@InProceedings{	  10.1145/3589335.3653009,
  author	= {Poria, Soujanya},
  title		= {Understanding, Leveraging, and Improving Large Language
		  Models},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3653009},
  doi		= {10.1145/3589335.3653009},
  abstract	= {The emergence of Large Language Models (LLMs) has marked a
		  substantial advancement in Natural Language Processing
		  (NLP), contributing significantly to enhanced task
		  performance both within and outside specific domains.
		  However, amidst these achievements, three key questions
		  remain unanswered: 1) The mechanism through which LLMs
		  accomplish their tasks and their limitations, 2)
		  Effectively harnessing the power of LLMs across diverse
		  domains, and 3) Strategies for enhancing the performance of
		  LLMs. This talk aims to delve into our research group's
		  endeavors to address these pivotal questions. Firstly, I
		  will outline our approach, which involves utilizing
		  ontology-guided prompt perturbations to unravel the primary
		  limitations of LLMs in solving mathematical problems.
		  Moving on to the second question, we will explore the
		  utilization of synthetic data generated by LLMs to bolster
		  challenging downstream tasks, particularly focusing on
		  structured prediction where LLMs face persistent
		  challenges. I will elaborate on our initiatives aimed at
		  improving LLMs by incorporating highly effective retrieval
		  strategies, specifically addressing the prevalent challenge
		  of hallucinations that often plagues contemporary LLMs.
		  Finally, I will present a technique on LLM realignment to
		  restore safety lost during fine-tuning.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1805},
  numpages	= {1},
  keywords	= {keynote},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3662739,
  title		= {MIDA '24: Proceedings of the 2024 International Conference
		  on Machine Intelligence and Digital Applications},
  year		= {2024},
  isbn		= {9798400718144},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Ningbo, China}
}

@InProceedings{	  10.1145/3625007.3630110,
  author	= {Ventrice, Laura and Di Caro, Luigi},
  title		= {Enriching Wikipedia Texts through Geographic Information
		  Extraction},
  year		= {2024},
  isbn		= {9798400704093},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625007.3630110},
  doi		= {10.1145/3625007.3630110},
  abstract	= {Geographic Information Extraction (GIE) involves the
		  extraction of geo-referenced information from a data
		  collection through steps of geoparsing and geocoding. The
		  former is a process that starts from a free textual
		  description of locations with the goal of identifying an
		  unambiguous location, such as specific geographic
		  coordinates expressed as latitude-longitude. Differently,
		  geocoding regards the easier task of translating an exact
		  and well-formatted location such as postal addresses. This
		  paper presents MAWI, i.e. a pipeline that starts from
		  generic texts about cities that first extracts geographic
		  information to automatically detect possible points of
		  interest, then generates textual snippets from their
		  contexts by means of Natural Language Processing (NLP)
		  techniques. The adopted methodology involves several
		  modules, ranging from publicly available geocoding systems
		  to NLP libraries for Named Entity Recognition and text
		  segmentation. The impact of the proposal includes multiple
		  tasks and applications, e.g. i) the enrichment of public
		  platforms of geographic data, ii) the detection of
		  geographic scopes in textual documents, iii) a geo-centric
		  exploration of locations in the tourism domain, and so
		  forth. In this contribution, we present an experimentation
		  of the system with 50 input Wikipedia pages referring
		  different cities, first demonstrating its effectiveness
		  with a running example, then evaluating its power to detect
		  and structure a highly-significant amount of novel
		  geo-referenced information with respect to what currently
		  encoded in Wikipedia. Data and code are publicly available
		  for future research at
		  https://anonymous.4open.science/r/PointOfInterest-8D80/.},
  booktitle	= {Proceedings of the 2023 IEEE/ACM International Conference
		  on Advances in Social Networks Analysis and Mining},
  pages		= {775–779},
  numpages	= {5},
  keywords	= {geographic information extraction, named entity
		  recognition, geoparsing, wikipedia enrichment},
  location	= {Kusadasi, Turkiye},
  series	= {ASONAM '23}
}

@Proceedings{	  10.1145/3652988,
  title		= {IVA '24: Proceedings of the 24th ACM International
		  Conference on Intelligent Virtual Agents},
  year		= {2024},
  isbn		= {9798400706257},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {GLASGOW, United Kingdom}
}

@Proceedings{	  10.1145/3658271,
  title		= {SBSI '24: Proceedings of the 20th Brazilian Symposium on
		  Information Systems},
  year		= {2024},
  isbn		= {9798400709968},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Juiz de Fora, Brazil}
}

@InProceedings{	  10.1145/3652583.3658890,
  author	= {Mai, Tai Tan and Tran, Quang-Linh and Tran, Ly-Duyen and
		  Ninh, Tu and Dang-Nguyen, Duc-Tien and Gurrin, Cathal},
  title		= {The First ACM Workshop on AI-Powered Question Answering
		  Systems for Multimedia},
  year		= {2024},
  isbn		= {9798400706196},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652583.3658890},
  doi		= {10.1145/3652583.3658890},
  abstract	= {The advent of large language models (LLMs) has energised
		  research in Question-Answering (QA) tasks, enabling
		  responses across varied domains like economics and
		  mathematics. Despite their capabilities, LLMs often lack
		  explainability due to their complex parameter embeddings.
		  Additionally, integrating multimedia data into QA systems
		  introduces challenges in processing and interpreting
		  diverse data types such as text, images, audio, and video.
		  This necessitates sophisticated algorithms for accurate
		  information retrieval across media while ensuring the
		  reliability of the data and responses remains a significant
		  challenge. The AIQAM workshop aims to bring together
		  researchers and practitioners to address these challenges
		  and enhance QA systems with multimedia data. The focus is
		  on promoting innovations that improve the accuracy,
		  explainability, and trustworthiness of QA systems,
		  contributing to the development of the field.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Multimedia Retrieval},
  pages		= {1328–1329},
  numpages	= {2},
  keywords	= {artificial intelligence, large language models,
		  multimedia, question answering systems},
  location	= {Phuket, Thailand},
  series	= {ICMR '24}
}

@InProceedings{	  10.1145/3644032.3644456,
  author	= {Canizares, Pablo C. and \'{A}vila, Daniel and Perez-Soler,
		  Sara and Guerra, Esther and De Lara, Juan},
  title		= {Coverage-based Strategies for the Automated Synthesis of
		  Test Scenarios for Conversational Agents},
  year		= {2024},
  isbn		= {9798400705885},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644032.3644456},
  doi		= {10.1145/3644032.3644456},
  abstract	= {Conversational agents - or chatbots - are increasingly
		  used as the user interface to many software services. While
		  open-domain chatbots like ChatGPT excel in their ability to
		  chat about any topic, task-oriented conversational agents
		  are designed to perform goal-oriented tasks (e.g., booking
		  or shopping) guided by a dialogue-based user interaction,
		  which is explicitly designed. Like any kind of software
		  system, task-oriented conversational agents need to be
		  properly tested to ensure their quality. For this purpose,
		  some tools permit defining and executing conversation test
		  cases. However, there are currently no established means to
		  assess the coverage of the design of a task-oriented agent
		  by a test suite, or mechanisms to automate quality test
		  case generation ensuring the agent coverage.To attack this
		  problem, we propose test coverage criteria for
		  task-oriented conversational agents, and define
		  coverage-based strategies to synthesise test scenarios,
		  some oriented to test case reduction. We provide an
		  implementation of the criteria and the strategies that is
		  independent of the agent development platform. Finally, we
		  report on their evaluation on open-source Dialogflow and
		  Rasa agents, and a comparison against a state-of-the-art
		  testing tool. The experiment shows benefits in terms of
		  test generation correctness, increased coverage and reduced
		  testing time.},
  booktitle	= {Proceedings of the 5th ACM/IEEE International Conference
		  on Automation of Software Test (AST 2024)},
  pages		= {23–33},
  numpages	= {11},
  keywords	= {testing, test suite generation, task-oriented
		  conversational agents},
  location	= {Lisbon, Portugal},
  series	= {AST '24}
}

@Proceedings{	  10.1145/3686081,
  title		= {ICDSM '24: Proceedings of the International Conference on
		  Decision Science &amp; Management},
  year		= {2024},
  isbn		= {9798400718151},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3649142,
  author	= {Chen, April and Rossi, Ryan A. and Park, Namyong and
		  Trivedi, Puja and Wang, Yu and Yu, Tong and Kim, Sungchul
		  and Dernoncourt, Franck and Ahmed, Nesreen K.},
  title		= {Fairness-Aware Graph Neural Networks: A Survey},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {6},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3649142},
  doi		= {10.1145/3649142},
  abstract	= {Graph Neural Networks (GNNs) have become increasingly
		  important due to their representational power and
		  state-of-the-art predictive performance on many fundamental
		  learning tasks. Despite this success, GNNs suffer from
		  fairness issues that arise as a result of the underlying
		  graph data and the fundamental aggregation mechanism that
		  lies at the heart of the large class of GNN models. In this
		  article, we examine and categorize fairness techniques for
		  improving the fairness of GNNs. We categorize these
		  techniques by whether they focus on improving fairness in
		  the pre-processing, in-processing (during training), or
		  post-processing phases. We discuss how such techniques can
		  be used together whenever appropriate and highlight the
		  advantages and intuition as well. We also introduce an
		  intuitive taxonomy for fairness evaluation metrics,
		  including graph-level fairness, neighborhood-level
		  fairness, embedding-level fairness, and prediction-level
		  fairness metrics. In addition, graph datasets that are
		  useful for benchmarking the fairness of GNN models are
		  summarized succinctly. Finally, we highlight key open
		  problems and challenges that remain to be addressed.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= apr,
  articleno	= {138},
  numpages	= {23},
  keywords	= {Fairness, Bias, Graph Neural Networks}
}

@InProceedings{	  10.1145/3664647.3680865,
  author	= {Zhang, Jiaxin and Wang, Yiqi and Yang, Xihong and Wang,
		  Siwei and Feng, Yu and Shi, Yu and Ren, Ruichao and Zhu, En
		  and Liu, Xinwang},
  title		= {Test-Time Training on Graphs with Large Language Models
		  (LLMs)},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3680865},
  doi		= {10.1145/3664647.3680865},
  abstract	= {Graph Neural Networks have demonstrated great success in
		  various fields of multimedia. However, the distribution
		  shift between the training and test data challenges the
		  effectiveness of GNNs. To mitigate this challenge,
		  Test-Time Training (TTT) has been proposed as a promising
		  approach. Traditional TTT methods require a demanding
		  unsupervised training strategy to capture the information
		  from test to benefit the main task. Inspired by the great
		  annotation ability of Large Language Models (LLMs) on
		  Text-Attributed Graphs (TAGs), we propose to enhance the
		  test-time training on graphs with LLMs as annotators. In
		  this paper, we design a novel Test-Time Training pipeline,
		  LLMTTT, which conducts the test-time adaptation under the
		  annotations by LLMs on a carefully-selected node set.
		  Specifically, LLMTTT introduces a hybrid active node
		  selection strategy that considers not only node diversity
		  and representativeness, but also prediction signals from
		  the pre-trained model. Given annotations from LLMs, a
		  two-stage training strategy is designed to tailor the
		  test-time model with the limited and noisy labels. A
		  theoretical analysis ensures the validity of our method and
		  extensive experiments demonstrate that the proposed LLMTTT
		  can achieve a significant performance improvement compared
		  to existing Out-of-Distribution (OOD) generalization
		  methods.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {2089–2098},
  numpages	= {10},
  keywords	= {graph neural networks, large language models, ood
		  generalization, test time training},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Proceedings{	  10.1145/3597503,
  title		= {ICSE '24: Proceedings of the IEEE/ACM 46th International
		  Conference on Software Engineering},
  year		= {2024},
  isbn		= {9798400702174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lisbon, Portugal}
}

@Article{	  10.1145/3657285,
  author	= {Huang, Jiani and Chen, Haihua and Yu, Fengchang and Lu,
		  Wei},
  title		= {From Detection to Application: Recent Advances in
		  Understanding Scientific Tables and Figures},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {10},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3657285},
  doi		= {10.1145/3657285},
  abstract	= {Tables&nbsp;and figures are usually used to present
		  information in a structured and visual way in scientific
		  documents. Understanding the tables and figures in
		  scientific documents is significant for a series of
		  downstream tasks, such as academic search, scientific
		  knowledge graphs, and so on. Existing studies mainly focus
		  on detecting figures and tables from scientific documents,
		  interpreting their semantics, and integrating them into
		  downstream tasks. However, a systematic and comprehensive
		  literature review on the mining and application of tables
		  and figures in academic papers is still missing. In this
		  article, we introduce the research framework and the whole
		  pipeline for understanding tables and figures, including
		  detection, structural analysis, interpretation, and
		  application. We deliver a thorough analysis of benchmark
		  datasets, recent techniques, and their pros and cons.
		  Additionally, a quantitative analysis of the effectiveness
		  of different models on popular benchmarks is presented. We
		  further outline several important applications that exploit
		  the semantics of scientific tables and figures. Finally, we
		  highlight the challenges and some potential directions for
		  future research. We believe this is the first comprehensive
		  survey in understanding scientific tables and figures that
		  covers the landscape from detection to application.},
  journal	= {ACM Comput. Surv.},
  month		= jun,
  articleno	= {261},
  numpages	= {39},
  keywords	= {Scientific documents, figure understanding, table
		  understanding}
}

@Article{	  10.1145/3637320,
  author	= {Hu, Jiaxiong and Guo, Jingya and Tang, Ningjing and Ma,
		  Xiaojuan and Yao, Yuan and Yang, Changyuan and Xu,
		  Yingqing},
  title		= {Designing the Conversational Agent: Asking Follow-up
		  Questions for Information Elicitation},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW1},
  url		= {https://doi.org/10.1145/3637320},
  doi		= {10.1145/3637320},
  abstract	= {Conversational Agents (CAs) can facilitate information
		  elicitation in various scenarios, such as semi-structured
		  interviews. Current CAs can ask predetermined questions but
		  lack skills for asking follow-up questions. Thus, we
		  designed three approaches for CAs to automatically ask
		  follow-up questions, i.e., follow-ups on concepts,
		  follow-ups on related concepts, and general follow-ups. To
		  investigate their effects, we conducted a user study (N=26)
		  in which a CA interviewer asked follow-up questions
		  generated by algorithms and crafted by human wizards. Our
		  results showed that the CA's follow-up questions were
		  readable and effective in information elicitation. The
		  follow-ups on concepts and related concepts achieved a
		  lower drop rate and better relevance, while the general
		  follow-ups elicited more informative responses. Further
		  qualitative analysis of the human-CA interview data
		  revealed algorithm drawbacks and identified follow-up
		  question techniques used by the human wizards. We provided
		  design implications for improving information elicitation
		  of future CAs based on the results.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= apr,
  articleno	= {43},
  numpages	= {30},
  keywords	= {conversational agent, conversational user interface,
		  follow-up question, information elicitation, interview}
}

@InProceedings{	  10.1145/3589334.3645649,
  author	= {Gong, Jiaying and Eldardiry, Hoda},
  title		= {Multi-Label Zero-Shot Product Attribute-Value Extraction},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645649},
  doi		= {10.1145/3589334.3645649},
  abstract	= {E-commerce platforms should provide detailed product
		  descriptions (attribute values) for effective product
		  search and recommendation. However, attribute value
		  information is typically not available for new products. To
		  predict unseen attribute values, large quantities of
		  labeled training data are needed to train a traditional
		  supervised learning model. Typically, it is difficult,
		  time-consuming, and costly to manually label large
		  quantities of new product profiles. In this paper, we
		  propose a novel method to efficiently and effectively
		  extract unseen attribute values from new products in the
		  absence of labeled data (zero-shot setting). We propose
		  HyperPAVE, a multi-label zero-shot attribute value
		  extraction model that leverages inductive inference in
		  heterogeneous hypergraphs. In particular, our proposed
		  technique constructs heterogeneous hypergraphs to capture
		  complex higher-order relations (i.e. user behavior
		  information) to learn more accurate feature representations
		  for graph nodes. Furthermore, our proposed HyperPAVE model
		  uses an inductive link prediction mechanism to infer future
		  connections between unseen nodes. This enables HyperPAVE to
		  identify new attribute values without the need for labeled
		  training data. We conduct extensive experiments with
		  ablation studies on different categories of the MAVE
		  dataset. The results demonstrate that our proposed
		  HyperPAVE model significantly outperforms existing
		  classification-based, generation-based large language
		  models for attribute value extraction in the zero-shot
		  setting.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2259–2270},
  numpages	= {12},
  keywords	= {attribute value extraction, heterogeneous hypergraph,
		  inductive link prediction, zero-shot learning},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3675888,
  title		= {IC3-2024: Proceedings of the 2024 Sixteenth International
		  Conference on Contemporary Computing},
  year		= {2024},
  isbn		= {9798400709722},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Noida, India}
}

@InProceedings{	  10.1145/3627673.3679793,
  author	= {Liu, Qi and He, Yongyi and Xu, Tong and Lian, Defu and
		  Liu, Che and Zheng, Zhi and Chen, Enhong},
  title		= {UniMEL: A Unified Framework for Multimodal Entity Linking
		  with Large Language Models},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679793},
  doi		= {10.1145/3627673.3679793},
  abstract	= {Multimodal Entity Linking (MEL) is a crucial task that
		  aims at linking ambiguous mentions within multimodal
		  contexts to the referent entities in a multimodal knowledge
		  base, such as Wikipedia. Existing methods focus heavily on
		  using complex mechanisms and extensive model tuning methods
		  to model the multimodal interaction on specific datasets.
		  However, these methods overcomplicate the MEL task and
		  overlook the visual semantic information, which makes them
		  costly and hard to scale. Moreover, these methods cannot
		  solve the issues like textual ambiguity, redundancy, and
		  noisy images, which severely degrade their performance.
		  Fortunately, the advent of Large Language Models (LLMs)
		  with robust capabilities in text understanding and
		  reasoning, particularly Multimodal Large Language Models
		  (MLLMs) that can process multimodal inputs, provides new
		  insights into addressing this challenge. However, how to
		  design a universally applicable LLMs-based MEL approach
		  remains a pressing challenge. To this end, we propose
		  UniMEL, a &lt;u&gt;uni&lt;/u&gt;fied framework which
		  establishes a new paradigm to process
		  &lt;u&gt;m&lt;/u&gt;ultimodal &lt;u&gt;e&lt;/u&gt;ntity
		  &lt;u&gt;l&lt;/u&gt;inking tasks using LLMs. In this
		  framework, we employ LLMs to augment the representation of
		  mentions and entities individually by integrating textual
		  and visual information and refining textual information.
		  Subsequently, we employ the embedding-based method for
		  retrieving and re-ranking candidate entities. Then, with
		  only ~0.26% of the model parameters fine-tuned, LLMs can
		  make the final selection from the candidate entities.
		  Extensive experiments on three public benchmark datasets
		  demonstrate that our solution achieves state-of-the-art
		  performance, and ablation studies verify the effectiveness
		  of all modules. Our code is available at
		  https://github.com/Javkonline/UniMEL.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {1909–1919},
  numpages	= {11},
  keywords	= {large language models, multimodal entity linking,
		  multimodal knowledge base},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3568164,
  author	= {Ahmed, Usman and Lin, Jerry Chun-Wei and Garcia Diaz,
		  Vicente},
  title		= {Automatically Temporal Labeled Data Generation Using
		  Positional Lexicon Expansion for Focus Time Estimation of
		  News Articles},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3568164},
  doi		= {10.1145/3568164},
  abstract	= {Many facts change over time, which is a fundamental aspect
		  of our physical environment. In the case of pandemic
		  articles, the user is not interested in the creation date
		  of the document but in the facts and the cause of the last
		  pandemic. Fake news can be better combated by having a
		  document with a temporal focus. Currently, neither the
		  sequence of events nor the temporal focus is considered
		  when obtaining news documents. Despite the limited number
		  of temporal aspects in the available datasets, it is
		  difficult to test and evaluate the temporal conclusions of
		  the model. The goal of this work is to develop a temporal
		  focus news article retrieval model based on co-training to
		  advance research in semi-supervised learning. A mapping of
		  the dataset is performed using (1) the evolving focus time
		  of news articles and (2) the semi-supervised method based
		  on coincidence contexts for learning low-dimensional
		  continuous vectors for learning neural contrast embedding
		  models generating focus time-based query in sequential news
		  articles to facilitate temporal understanding by learning
		  low-dimensional continuous vectors. A diverse dataset of
		  news articles is used to evaluate the effectiveness of the
		  proposed method. With semi-supervised learning and lexicon
		  expansion, the result of the developed model can achieve
		  89%. The method performed better than previous baselines
		  and traditional machine learning models with improvements
		  of 12.65% and 4.7%, respectively.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {64},
  numpages	= {20},
  keywords	= {Information retrieval, temporal information retrieval,
		  focus time, inverted pyramid, news retrieval}
}

@Proceedings{	  10.1145/3639233,
  title		= {NLPIR '23: Proceedings of the 2023 7th International
		  Conference on Natural Language Processing and Information
		  Retrieval},
  year		= {2023},
  isbn		= {9798400709227},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Seoul, Republic of Korea}
}

@Proceedings{	  10.1145/3643491,
  title		= {MAD '24: Proceedings of the 3rd ACM International Workshop
		  on Multimedia AI against Disinformation},
  year		= {2024},
  isbn		= {9798400705526},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Phuket, Thailand}
}

@InProceedings{	  10.1145/3640457.3688071,
  author	= {Irrera, Ornella and Lissandrini, Matteo and Dell'Aglio,
		  Daniele and Silvello, Gianmaria},
  title		= {Reproducibility and Analysis of Scientific Dataset
		  Recommendation Methods},
  year		= {2024},
  isbn		= {9798400705052},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640457.3688071},
  doi		= {10.1145/3640457.3688071},
  abstract	= {Datasets play a central role in scholarly communications.
		  However, scholarly graphs are often incomplete,
		  particularly due to the lack of connections between
		  publications and datasets. Therefore, the importance of
		  dataset recommendation—identifying relevant datasets for
		  a scientific paper, an author, or a textual query—is
		  increasing. Although various methods have been proposed for
		  this task, their reproducibility remains unexplored, making
		  it difficult to compare them with new approaches. We
		  reviewed current recommendation methods for scientific
		  datasets, focusing on the most recent and competitive
		  approaches, including an SVM-based model, a bi-encoder
		  retriever, a method leveraging co-authors and citation
		  network embeddings, and a heterogeneous variational graph
		  autoencoder. These approaches underwent a comprehensive
		  analysis under consistent experimental conditions. Our
		  reproducibility efforts show that three methods can be
		  reproduced, while the graph variational autoencoder is
		  challenging due to unavailable code and test datasets.
		  Hence, we re-implemented this method and performed a
		  component-based analysis to examine its strengths and
		  limitations. Furthermore, our study indicated that three
		  out of four considered methods produce subpar results when
		  applied to real-world data instead of specialized datasets
		  with ad-hoc features.},
  booktitle	= {Proceedings of the 18th ACM Conference on Recommender
		  Systems},
  pages		= {570–579},
  numpages	= {10},
  keywords	= {Dataset Recommendations, Recommender Systems,
		  Reproducibility},
  location	= {Bari, Italy},
  series	= {RecSys '24}
}

@InProceedings{	  10.1145/3626772.3657848,
  author	= {Zhai, ChengXiang},
  title		= {Large Language Models and Future of Information Retrieval:
		  Opportunities and Challenges},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657848},
  doi		= {10.1145/3626772.3657848},
  abstract	= {Recent years have seen great success of large language
		  models (LLMs) in performing many natural language
		  processing tasks with impressive performance, including
		  tasks that directly serve users such as question answering
		  and text summarization. They open up unprecedented
		  opportunities for transforming information retrieval (IR)
		  research and applications. However, concerns such as
		  halluciation undermine their trustworthiness, limiting
		  their actual utility when deployed in real-world
		  applications, especially high-stake applications where
		  trust is vital. How can we both exploit the strengths of
		  LLMs and mitigate any risk caused by their weaknesses when
		  applying LLMs to IR? What are the best opportunities for us
		  to apply LLMs to IR? What are the major challenges that we
		  will need to address in the future to fully exploit such
		  opportunities? Given the anticipated growth of LLMs, what
		  will future information retrieval systems look like? Will
		  LLMs eventually replace an IR system? In this perspective
		  paper, we examine these questions and provide provisional
		  answers to them. We argue that LLMs will not be able to
		  replace search engines, and future LLMs would need to learn
		  how to use a search engine so that they can interact with a
		  search engine on behalf of users. We conclude with a set of
		  promising future research directions in applying LLMs to
		  IR.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {481–490},
  numpages	= {10},
  keywords	= {conversational information access, information retrieval
		  models, intelligent agent, large language models, search
		  engines},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@InProceedings{	  10.1145/3639479.3639527,
  author	= {Wang, Chen and Xiong, Xiong and Wang, Linjie and Zheng,
		  Yifeng and Liu, Yunfei and Li, Shengyang},
  title		= {A Lexicon Enhanced Chinese Long Named Entity Recognition
		  Using Word-Aware Attention},
  year		= {2024},
  isbn		= {9798400709241},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639479.3639527},
  doi		= {10.1145/3639479.3639527},
  abstract	= {In recent years, due to the rapid growth of space science
		  and utilization research of China Manned Space Engineering,
		  a considerable amount of technical documents and web data
		  have been produced. Named entity recognition (NER) plays a
		  vital role in extracting valuable information from these
		  resources. However, the presence of numerous long named
		  entities, which consist of complex, specialized terms and
		  diverse phrase combinations, poses significant challenges
		  for existing methods in identifying them correctly. To
		  resolve this issue, we introduced two modules,
		  SkipWord-Lattice and Word-Aware Attention, to improve the
		  widely used lexical enhancement model in Chinese NER.
		  SkipWord-Lattice reduces the overlap, redundancy, and
		  confusion of word tokens, while Word-Aware Attention
		  enhances the semantic interaction between character tokens
		  and word tokens, improving the model’s comprehension of
		  word tokens. Collectively, these modules significantly
		  increase the model’s ability to identify long named
		  entities. Moreover, using relevant corpus data publicly
		  available in the field of space science and utilization of
		  China Manned Space Engineering, we built a Chinese NER
		  dataset, named SSUIE-NER, comprising rich entity types and
		  a substantial number of long named entities. Experimental
		  results indicate that, our method significantly improves
		  the recognition of long named entities, outperforming other
		  state-of-the-art (SOTA) approaches on SSUIE-NER and other
		  three benchmarks in Chinese NER.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Machine Learning and Natural Language Processing},
  pages		= {234–242},
  numpages	= {9},
  keywords	= {Chinese NER, Information extraction, Long named entity
		  recognition},
  location	= {Sanya, China},
  series	= {MLNLP '23}
}

@Proceedings{	  10.1145/3686169,
  title		= {HttF '24: Proceedings of the Halfway to the Future
		  Symposium},
  year		= {2024},
  isbn		= {9798400710421},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Santa Cruz, CA, USA}
}

@Proceedings{	  10.1145/3641825,
  title		= {VRST '24: Proceedings of the 30th ACM Symposium on Virtual
		  Reality Software and Technology},
  year		= {2024},
  isbn		= {9798400705359},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Trier, Germany}
}

@Article{	  10.1145/3652600,
  author	= {Chen, Junfan and Zhang, Richong and Jiang, Xiaohan and Hu,
		  Chunming},
  title		= {SPContrastNet: A Self-Paced Contrastive Learning Model for
		  Few-Shot Text Classification},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {5},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3652600},
  doi		= {10.1145/3652600},
  abstract	= {Meta-learning has recently promoted few-shot text
		  classification, which identifies target classes based on
		  information transferred from source classes through a
		  series of small tasks or episodes. Existing works
		  constructing their meta-learner on Prototypical Networks
		  need improvement in learning discriminative text
		  representations between similar classes that may lead to
		  conflicts in label prediction. The overfitting problems
		  caused by a few training instances need to be adequately
		  addressed. In addition, efficient episode sampling
		  procedures that could enhance few-shot training should be
		  utilized. To address the problems mentioned above, we first
		  present a contrastive learning framework that
		  simultaneously learns discriminative text representations
		  via supervised contrastive learning while mitigating the
		  overfitting problem via unsupervised contrastive
		  regularization, and then we build an efficient self-paced
		  episode sampling approach on top of it to include more
		  difficult episodes as training progresses. Empirical
		  results on eight few-shot text classification datasets show
		  that our model outperforms the current state-of-the-art
		  models. The extensive experimental analysis demonstrates
		  that our supervised contrastive representation learning and
		  unsupervised contrastive regularization techniques improve
		  the performance of few-shot text classification. The
		  episode-sampling analysis reveals that our self-paced
		  sampling strategy improves training efficiency.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= apr,
  articleno	= {130},
  numpages	= {25},
  keywords	= {Text classification, few-shot learning, contrastive
		  learning, self-paced learning}
}

@Proceedings{	  10.1145/3639476,
  title		= {ICSE-NIER'24: Proceedings of the 2024 ACM/IEEE 44th
		  International Conference on Software Engineering: New Ideas
		  and Emerging Results},
  year		= {2024},
  isbn		= {9798400705007},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lisbon, Portugal}
}

@InProceedings{	  10.1145/3637528.3671698,
  author	= {Wang, Chen and Fan, Ziwei and Yang, Liangwei and Yang,
		  Mingdai and Liu, Xiaolong and Liu, Zhiwei and Yu, Philip},
  title		= {Pre-Training with Transferable Attention for Addressing
		  Market Shifts in Cross-Market Sequential Recommendation},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671698},
  doi		= {10.1145/3637528.3671698},
  abstract	= {Cross-market recommendation (CMR) involves selling the
		  same set of items across multiple nations or regions within
		  a transfer learning framework. However, CMR's distinctive
		  characteristics, including limited data sharing due to
		  privacy policies, absence of user overlap, and a shared
		  item set between markets present challenges for traditional
		  recommendation methods. Moreover, CMR experiences market
		  shifts, leading to differences in item popularity and user
		  preferences among different markets. This study focuses on
		  cross-market sequential recommendation (CMSR) and proposes
		  the Cross-market Attention Transferring with Sequential
		  Recommendation (CAT-SR) framework to address these
		  challenges and market shifts. CAT-SR incorporates a
		  pre-training strategy emphasizing item-item correlation,
		  selective self-attention transferring for effective
		  transfer learning, and query and key adapters for
		  market-specific user preferences. Experimental results on
		  real-world cross-market datasets demonstrate the
		  superiority of CAT-SR, and ablation studies validate the
		  benefits of its components across different geographical
		  continents. CAT-SR offers a robust and adaptable solution
		  for cross-market sequential recommendation. The code is
		  available at https://github.com/ChenMetanoia/CATSR-KDD/.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {2970–2979},
  numpages	= {10},
  keywords	= {cross-market recommendation, pre-training, self-attention,
		  sequential recommendation},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Proceedings{	  10.1145/3653081,
  title		= {IoTAAI '23: Proceedings of the 2023 5th International
		  Conference on Internet of Things, Automation and Artificial
		  Intelligence},
  year		= {2023},
  isbn		= {9798400716485},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Nanchang, China}
}

@Proceedings{	  10.1145/3649921,
  title		= {FDG '24: Proceedings of the 19th International Conference
		  on the Foundations of Digital Games},
  year		= {2024},
  isbn		= {9798400709555},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Worcester, MA, USA}
}

@Proceedings{	  10.1145/3659211,
  title		= {BDEIM '23: Proceedings of the 2023 4th International
		  Conference on Big Data Economy and Information Management},
  year		= {2023},
  isbn		= {9798400716669},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Zhengzhou, China}
}

@Proceedings{	  10.1145/3650400,
  title		= {EITCE '23: Proceedings of the 2023 7th International
		  Conference on Electronic Information Technology and
		  Computer Engineering},
  year		= {2023},
  isbn		= {9798400708305},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@InProceedings{	  10.1145/3635059.3635104,
  author	= {Karanikolas, Nikitas and Manga, Eirini and Samaridi,
		  Nikoletta and Tousidou, Eleni and Vassilakopoulos,
		  Michael},
  title		= {Large Language Models versus Natural Language
		  Understanding and Generation},
  year		= {2024},
  isbn		= {9798400716263},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3635059.3635104},
  doi		= {10.1145/3635059.3635104},
  abstract	= {In recent years, the process humans adopt to learn a
		  foreign language has moved from the strict "Grammar
		  –Translation" method, which is based mainly on grammar
		  and syntax rules, to more innovative processes, resulting
		  to the more modern "Communicative approach". As its name
		  states, this approach focuses on the coherent communication
		  with native speakers and the cultivation of oral skills,
		  without taking into consideration, at least at the first
		  stages, the rules that govern the language. The same trend
		  seems to have been applied to the way machinery can be
		  "educated" to comprehend and reproduce the unfamiliar,
		  human language. The "rule based" Natural Language
		  Generation (NLG) and Natural Language Understanding (NLU)
		  algorithms, on one hand, and the "text based" Large
		  Language Models (LLMs), on the other, are two, analogous to
		  the two human foreign language learning processes, subareas
		  of Natural Language Processing (NLP). This paper presents
		  these two alternative approaches, LLMs (a technology having
		  surfaced as an influential catalyst of NLP, during last
		  years) on the one hand and NLG/NLU on the other,
		  highlighting their applications, their technologies, their
		  capabilities, their differences, their strengths and
		  weaknesses and the challenges they present, contributing to
		  a deeper comprehension of the evolving landscape of
		  Artificial Intelligence and human-computer communication.},
  booktitle	= {Proceedings of the 27th Pan-Hellenic Conference on
		  Progress in Computing and Informatics},
  pages		= {278–290},
  numpages	= {13},
  keywords	= {Large Language Models, Natural Language Generation,
		  Natural Language Processing, Natural Language
		  Understanding},
  location	= {Lamia, Greece},
  series	= {PCI '23}
}

@InProceedings{	  10.1145/3664647.3681215,
  author	= {Han, Weixiang and Cai, Chengjun and Guo, Yu and Peng,
		  Jialiang},
  title		= {ERL-MR: Harnessing the Power of Euler Feature
		  Representations for Balanced Multi-modal Learning},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681215},
  doi		= {10.1145/3664647.3681215},
  abstract	= {Multi-modal learning leverages data from diverse
		  perceptual media to obtain enriched representations,
		  thereby empowering machine learning models to complete more
		  complex tasks. However, recent research results indicate
		  that multi-modal learning still suffers from " modality
		  imbalance '': Certain modalities' contributions are
		  suppressed by dominant ones, consequently constraining the
		  overall performance enhancement of multimodal learning. To
		  tackle this issue, current approaches attempt to mitigate
		  modality competition in various ways, but their
		  effectiveness is still limited. To this end, we propose an
		  Euler Representation Learning-based Modality Rebalance
		  (ERL-MR) strategy, which reshapes the underlying
		  competitive relationships between modalities into mutually
		  reinforcing win-win situations while maintaining stable
		  feature optimization directions. Specifically, ERL-MR
		  employs Euler's formula to map original features to complex
		  space, constructing cooperatively enhanced non-redundant
		  features for each modality, which helps reverse the
		  situation of modality competition. Moreover, to counteract
		  the performance degradation resulting from optimization
		  drift among modalities, we propose a Multi-Modal
		  Constrained (MMC) loss based on cosine similarity of
		  complex feature phase and cross-entropy loss of individual
		  modalities, guiding the optimization direction of the
		  fusion network. Extensive experiments conducted on four
		  multi-modal multimedia datasets and two task-specific
		  multi-modal multimedia datasets demonstrate the superiority
		  of our ERL-MR strategy over state-of-the-art baselines,
		  achieving modality rebalancing and further performance
		  improvements.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {4591–4600},
  numpages	= {10},
  keywords	= {euler formula, modality imbalance, multi-modal constrained
		  loss, multi-modal learning},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Proceedings{	  10.1145/3639631,
  title		= {ACAI '23: Proceedings of the 2023 6th International
		  Conference on Algorithms, Computing and Artificial
		  Intelligence},
  year		= {2023},
  isbn		= {9798400709203},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sanya, China}
}

@InProceedings{	  10.1145/3691620.3694987,
  author	= {Gao, Xinyu and Xiong, Yun and Wang, Deze and Guan, Zhenhan
		  and Shi, Zejian and Wang, Haofen and Li, Shanshan},
  title		= {Preference-Guided Refactored Tuning for Retrieval
		  Augmented Code Generation},
  year		= {2024},
  isbn		= {9798400712487},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691620.3694987},
  doi		= {10.1145/3691620.3694987},
  abstract	= {Retrieval-augmented code generation utilizes Large
		  Language Models as the generator and significantly expands
		  their code generation capabilities by providing relevant
		  code, documentation, and more via the retriever. The
		  current approach suffers from two primary limitations: 1)
		  information redundancy. The indiscriminate inclusion of
		  redundant information can result in resource wastage and
		  may misguide generators, affecting their effectiveness and
		  efficiency. 2) preference gap. Due to different
		  optimization objectives, the retriever strives to procure
		  code with higher ground truth similarity, yet this effort
		  does not substantially benefit the generator. The retriever
		  and the generator may prefer different golden code, and
		  this gap in preference results in a suboptimal design.
		  Additionally, differences in parameterization knowledge
		  acquired during pre-training result in varying preferences
		  among different generators.To address these limitations, in
		  this paper, we propose RRG (Retrieve, Refactor, Generate),
		  a novel framework for effective and efficient code
		  generation. This framework introduces a code refactorer
		  module between the retriever and the generator to bridge
		  them. The refactoring process transforms the raw retrieved
		  code into a more concise, efficient, and model-friendly
		  version. It eliminates redundant information and noise,
		  reducing the input length. Consequently, the generator
		  receives higher-quality context, enabling it to produce
		  more accurate results with lower inference costs. We
		  conducted comprehensive experiments on multiple datasets.
		  In the experiments, we confirmed the existence of a
		  preference gap between the retriever and the generator, and
		  RRG effectively bridges this gap. Specifically, RRG
		  achieved significant performance improvements, with
		  increases of up to 28% on EM, 13% on BLEU, and 6.8% on
		  CodeBLEU.},
  booktitle	= {Proceedings of the 39th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {65–77},
  numpages	= {13},
  keywords	= {retrieval-augmented code generation, preference-guided
		  refactorer, deep reinforcement learning},
  location	= {Sacramento, CA, USA},
  series	= {ASE '24}
}

@Proceedings{	  10.1145/3665601,
  title		= {GUIDE-AI '24: Proceedings of the Conference on Governance,
		  Understanding and Integration of Data for Effective and
		  Responsible AI},
  year		= {2024},
  isbn		= {9798400706943},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Santiago, AA, Chile}
}

@Article{	  10.1145/3709007,
  author	= {Mishra, Sahil and Sudev, Ujjwal and Chakraborty, Tanmoy},
  title		= {FLAME: Self-Supervised Low-Resource Taxonomy Expansion
		  Using Large Language Models},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3709007},
  doi		= {10.1145/3709007},
  abstract	= {Taxonomies represent an arborescence hierarchical
		  structure that establishes relationships among entities to
		  convey knowledge within a specific domain. They find
		  utility in various real-world applications, such as
		  e-commerce search engines and recommendation systems.
		  Consequently, there arises a necessity to enhance these
		  taxonomies over time. However, manually curating taxonomies
		  with neoteric data presents challenges due to limitations
		  in available human resources and the exponential growth of
		  data. Therefore, it becomes imperative to develop automatic
		  taxonomy expansion methods. Traditional approaches
		  encounter difficulties stemming from limited resources,
		  primarily due to the small size of existing taxonomies.
		  This scarcity of training data often leads to overfitting.
		  In this paper, we propose FLAME (Fine-tuning LArge language
		  Models for taxonomy Expansion), a novel approach for
		  taxonomy expansion in low-resource environments (i.e.,
		  limited size of existing taxonomies, lack of robust
		  representation capabilities of pre-trained language models,
		  etc.) by harnessing the capabilities of large language
		  models (LLMs) that are trained on extensive real-world
		  knowledge. LLMs help compensate for the scarcity of
		  domain-specific knowledge. Specifically, FLAME leverages
		  prompting in few-shot settings to extract the inherent
		  knowledge within the LLMs, ascertaining the hypernym
		  entities within the taxonomy. Furthermore, it employs
		  reinforcement learning to fine-tune LLMs, resulting in more
		  accurate predictions. Experiments on four real-world
		  benchmark datasets demonstrate the effectiveness of FLAME
		  in real-world scenarios, achieving a remarkable improvement
		  of 12.8% in accuracy and 5.6% in Wu &amp; Palmer metric
		  over eleven baselines. Furthermore, we discuss the
		  strengths and weaknesses of FLAME through an extensive case
		  study, error analysis and ablation studies on the
		  benchmarks.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= dec,
  keywords	= {Taxonomy Expansion, Large Language Models, Self-supervised
		  Learning}
}

@Proceedings{	  10.1145/3686215,
  title		= {ICMI Companion '24: Companion Proceedings of the 26th
		  International Conference on Multimodal Interaction},
  year		= {2024},
  isbn		= {9798400704635},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {San Jose, Costa Rica}
}

@InProceedings{	  10.1145/3637528.3671603,
  author	= {Zheng, Da and Song, Xiang and Zhu, Qi and Zhang, Jian and
		  Vasiloudis, Theodore and Ma, Runjie and Zhang, Houyu and
		  Wang, Zichen and Adeshina, Soji and Nisa, Israt and
		  Mottini, Alejandro and Cui, Qingjun and Rangwala, Huzefa
		  and Zeng, Belinda and Faloutsos, Christos and Karypis,
		  George},
  title		= {GraphStorm: All-in-one Graph Machine Learning Framework
		  for Industry Applications},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671603},
  doi		= {10.1145/3637528.3671603},
  abstract	= {Graph machine learning (GML) is effective in many business
		  applications. However, making GML easy to use and
		  applicable to industry applications with massive datasets
		  remain challenging. We developed GraphStorm, which provides
		  an end-to-end solution for scalable graph construction,
		  graph model training and inference. GraphStorm has the
		  following desirable properties: (a) Easy to use: it can
		  perform graph construction and model training and inference
		  with just a single command; (b) Expert-friendly: GraphStorm
		  contains many advanced GML modeling techniques to handle
		  complex graph data and improve model performance; (c)
		  Scalable: every component in GraphStorm can operate on
		  graphs with billions of nodes and can scale model training
		  and inference to different hardware without changing any
		  code. GraphStorm has been used and deployed for over a
		  &lt;u&gt;dozen&lt;/u&gt; &lt;u&gt;billion-scale&lt;/u&gt;
		  industry applications after its release in May 2023. It is
		  open-sourced in Github:
		  https://github.com/awslabs/graphstorm.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {6356–6367},
  numpages	= {12},
  keywords	= {graph machine learning, industry scale},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@InProceedings{	  10.1145/3605098.3635888,
  author	= {Sourty, Rapha\"{e}l and Moreno, Jose G and Servant,
		  Fran\c{c}ois-Paul and Tamine, Lynda},
  title		= {Knowledge Base Grounded Pre-trained Language Models via
		  Distillation},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3635888},
  doi		= {10.1145/3605098.3635888},
  abstract	= {Knowledge bases are key resources in a wide range of
		  knowledge intensive applications. However, their
		  incompleteness inherently limits their use and gives rise
		  to the importance of their completion. To this end, an
		  open-world view has recently been held in the literature by
		  coupling the ability of knowledge bases to represent
		  factual knowledge, with the abilities of pre-trained
		  language models (PLMs) to capture high-level and contextual
		  linguistic knowledge from large-scale text corpora. In this
		  work, we propose a distillation framework for knowledge
		  base completion where PLMs leverage soft labels in the form
		  of entity and relations predictions provided by a knowledge
		  base embedding model, while keeping their power of entity
		  prediction over large-scale of texts. To better fit with
		  the task of knowledge completion, we extend the traditional
		  masked language modelling of PLMs toward predicting
		  entities and related entities in context. Experiments using
		  the fact classification and relation extraction tasks
		  within the standard KILT evaluation benchmark shows the
		  potential of our proposed approach.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {1617–1625},
  numpages	= {9},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@Article{	  10.1145/3687273.3687289,
  author	= {Campos, Ricardo and Jorge, Al\'{\i}pio M. and Jatowt, Adam
		  and Bhatia, Sumit and Litvak, Marina and Cordeiro, Jo\~{a}o
		  Paulo and Rocha, Concei\c{c}\~{a}o and Sousa, Hugo and
		  Mansouri, Behrooz},
  title		= {Report on the 7th International Workshop on Narrative
		  Extraction from Texts (Text2Story 2024) at ECIR 2024},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {58},
  number	= {1},
  issn		= {0163-5840},
  url		= {https://doi.org/10.1145/3687273.3687289},
  doi		= {10.1145/3687273.3687289},
  abstract	= {The Seventh International Workshop on Narrative Extraction
		  from Texts (Text2Story'24) was held on March 24th, 2024, in
		  conjunction with the 46th European Conference on
		  Information Retrieval (ECIR 2024) in Glasgow, Scotland.
		  Over the day, more than 50 attendees engaged in discussions
		  and presentations focused on recent advancements in
		  narrative representation, extraction, and generation. The
		  workshop featured two invited keynote addresses, fourteen
		  research paper presentations, and a poster session. The
		  workshop proceedings are available online.1Date: 24 March
		  2024.Website: https://text2story24.inesctec.pt/.},
  journal	= {SIGIR Forum},
  month		= aug,
  pages		= {1–11},
  numpages	= {11}
}

@Proceedings{	  10.1145/3661167,
  title		= {EASE '24: Proceedings of the 28th International Conference
		  on Evaluation and Assessment in Software Engineering},
  year		= {2024},
  isbn		= {9798400717017},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Salerno, Italy}
}

@Proceedings{	  10.1145/3627050,
  title		= {IoT '23: Proceedings of the 13th International Conference
		  on the Internet of Things},
  year		= {2023},
  isbn		= {9798400708541},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Nagoya, Japan}
}

@Proceedings{	  10.1145/3673805,
  title		= {ECCE '24: Proceedings of the European Conference on
		  Cognitive Ergonomics 2024},
  year		= {2024},
  isbn		= {9798400718243},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Paris, France}
}

@Proceedings{	  10.1145/3626246,
  title		= {SIGMOD/PODS '24: Companion of the 2024 International
		  Conference on Management of Data},
  year		= {2024},
  isbn		= {9798400704222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {On behalf of the SIGMOD 2024 organizing committee, it is
		  our distinct honor, as General Chairs, to welcome you to
		  the 2024 ACM International Conference on Management of Data
		  - SIGMOD 2024. We are thrilled to be hosting this
		  prestigious event for the very first time in Latin America,
		  and specifically in Santiago de Chile, a recognized leader
		  in data technology within the region. This marks a
		  significant milestone for the SIGMOD community, and we are
		  honored to have you join us for a fully in-person
		  experience in this vibrant and innovative city.},
  location	= {Santiago AA, Chile}
}

@InProceedings{	  10.1145/3644116.3644314,
  author	= {Wang, Yue and Zhang, Xi},
  title		= {Research on Named Entity Recognition for Chinese Medical
		  Case Reports},
  year		= {2024},
  isbn		= {9798400708138},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644116.3644314},
  doi		= {10.1145/3644116.3644314},
  abstract	= {In the domain of medical informatics, accurately
		  identifying entities within Electronic Health Records (EHR)
		  presents a significant challenge, especially within
		  specialized medical fields. This research delves deeply
		  into this challenge, underscoring the unique linguistic
		  complexities associated with Chinese medical texts. Unlike
		  English where words are delineated by spaces, Chinese
		  characters flow continuously without clear demarcations.
		  This intrinsic characteristic necessitates a departure from
		  traditional word-level Named Entity Recognition (NER)
		  approaches. To address this, our study adopts a
		  character-level method, ensuring more precise entity
		  identification. Utilizing the comprehensive Yidu Cloud
		  Structure 4K dataset, we juxtapose its original and
		  corrected versions, shedding light on discrepancies and
		  their implications. Central to our research is the
		  introduction of the BERT BiLSTM CRF model. This innovative
		  model, grounded in the data-intensive BERT framework, holds
		  promise in enhancing entity recognition within EHRs. Our
		  findings indicate an improved accuracy in named entity
		  recognition for Chinese medical texts using this model. By
		  harnessing the power of this model, we aspire to pave the
		  way for a new era of intelligent and automated medical
		  systems where data-driven insights can be seamlessly
		  integrated into clinical decision-making processes.},
  booktitle	= {Proceedings of the 2023 4th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {1165–1169},
  numpages	= {5},
  location	= {Chengdu, China},
  series	= {ISAIMS '23}
}

@InProceedings{	  10.1145/3626246.3653378,
  author	= {Pavlenko, Anna and Cahoon, Joyce and Zhu, Yiwen and Kroth,
		  Brian and Nelson, Michael and Carter, Andrew and Liao,
		  David and Wright, Travis and Camacho-Rodr\'{\i}guez,
		  Jes\'{u}s and Saur, Karla},
  title		= {Vertically Autoscaling Monolithic Applications with
		  CaaSPER: Scalable Container-as-a-Service Performance
		  Enhanced Resizing Algorithm for the Cloud},
  year		= {2024},
  isbn		= {9798400704222},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626246.3653378},
  doi		= {10.1145/3626246.3653378},
  abstract	= {Kubernetes has emerged as a prominent open-source platform
		  for managing cloud applications, including stateful
		  databases. These monolithic applications rely on vertical
		  scaling, adjusting CPU cores based on load fluctuations.
		  However, our analysis of Kubernetes-based
		  Database-as-a-Service (DBaaS) offerings at Microsoft
		  revealed that many customers consistently over-provision
		  resources for peak workloads, neglecting cost-saving
		  opportunities through resource scale-down. We found that
		  there is a gap in the ability of existing vertical
		  autoscaling tools to minimize resource slack and respond
		  promptly to throttling, leading to increased costs and
		  impacting crucial metrics such as throughput and
		  availability.To address this challenge, we propose CaaSPER,
		  a vertical autoscaling algorithm that blends reactive and
		  proactive strategies. By dynamically adjusting CPU
		  resources, CaaSPER minimizes resource slack, maintains
		  optimal CPU utilization, and reduces throttling.
		  Importantly, customers have the flexibility to prioritize
		  either cost savings or high performance based on their
		  preferences. Extensive testing demonstrates that CaaSPER
		  effectively reduces throttling and keeps CPU utilization
		  within target levels. CaaSPER is designed to be
		  application-agnostic and platform-agnostic, with potential
		  for extension to other applications requiring vertical
		  autoscaling.},
  booktitle	= {Companion of the 2024 International Conference on
		  Management of Data},
  pages		= {241–254},
  numpages	= {14},
  keywords	= {containers, kubernetes, resource optimization, vertical
		  auto-scaling},
  location	= {Santiago AA, Chile},
  series	= {SIGMOD/PODS '24}
}

@Article{	  10.1145/3637873,
  author	= {Razgallah, H\'{e}di and Vlachos, Michalis and Ajalloeian,
		  Ahmad and Liu, Ninghao and Schneider, Johannes and
		  Steinmann, Alexis},
  title		= {Using Neural and Graph Neural Recommender Systems to
		  Overcome Choice Overload: Evidence From a Music Education
		  Platform},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3637873},
  doi		= {10.1145/3637873},
  abstract	= {The application of recommendation technologies has been
		  crucial in the promotion of physical and digital content
		  across numerous global platforms such as Amazon, Apple, and
		  Netflix. Our study aims to investigate the advantages of
		  employing recommendation technologies on educational
		  platforms, with a particular focus on an educational
		  platform for learning and practicing music. Our research is
		  based on data from Tomplay, a music platform that offers
		  sheet music with professional audio recordings, enabling
		  users to discover and practice music content at varying
		  levels of difficulty. Through our analysis, we emphasize
		  the distinct interaction patterns on educational platforms
		  like Tomplay, which we compare with other commonly used
		  recommendation datasets. We find that interactions are
		  comparatively sparse on educational platforms, with users
		  often focusing on specific content as they learn, rather
		  than interacting with a broader range of material.
		  Therefore, our primary goal is to address the issue of data
		  sparsity. We achieve this through entity resolution
		  principles and propose a neural network (NN)-based
		  recommendation model. Further, we improve this model by
		  utilizing graph neural networks (GNNs), which provide
		  superior predictive accuracy compared to NNs. Notably, our
		  study demonstrates that GNNs are highly effective even for
		  users with little or no historical preferences (cold-start
		  problem). Our cold-start experiments also provide valuable
		  insights into an independent issue, namely, the number of
		  historical interactions needed by a recommendation model to
		  gain a comprehensive understanding of a user. Our findings
		  demonstrate that a platform acquires a solid knowledge of a
		  user’s general preferences and characteristics with 50
		  past interactions. Overall, our study makes significant
		  contributions to information systems research on business
		  analytics and prescriptive analytics. Moreover, our
		  framework and evaluation results offer implications for
		  various stakeholders, including online educational
		  institutions, education policymakers, and learning platform
		  users.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= feb,
  articleno	= {92},
  numpages	= {26},
  keywords	= {Neural networks, digital education, embeddings, entity
		  resolution}
}

@InProceedings{	  10.1145/3627673.3680111,
  author	= {Chen, Huaming and Zhuang, Jun and Yao, Yu and Jin, Wei and
		  Wang, Haohan and Xie, Yong and Chi, Chi-Hung and Choo,
		  Kim-Kwang Raymond},
  title		= {Trustworthy and Responsible AI for Information and
		  Knowledge Management System},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680111},
  doi		= {10.1145/3627673.3680111},
  abstract	= {The way research and business manage and utilize knowledge
		  is undergoing a significant transformation, driven by
		  Artificial Intelligence (AI). Deep learning and machine
		  learning are emerging as powerful tools for optimizing
		  knowledge management systems, leading to more informed and
		  productive development. AI offers unique solutions for
		  organizations struggling with information overload and
		  inefficient knowledge transfer. These AI models can
		  significantly improve data management and utilization.
		  Imagine an AI-powered system that streamlines onboarding
		  processes, provides precise answers to various queries, and
		  even captures the valuable tacit knowledge (implicit skills
		  and expertise) often residing within individuals. AI
		  bridges the gap between explicit knowledge (easily
		  documented information) and tacit knowledge, fostering a
		  more comprehensive and accessible knowledge base. However,
		  such AI systems solicit trustworthy and responsible
		  approaches to mitigate potential misuse and malfunction. In
		  this workshop, we aim to gather researchers and engineers
		  from academia and industry to discuss the latest advances
		  in trustworthy and responsible AI solutions for information
		  and knowledge management systems.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {5574–5576},
  numpages	= {3},
  keywords	= {information retrieval, knowledge management, trustworthy
		  AI},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@InProceedings{	  10.1145/3639631.3639644,
  author	= {Jiang, Yueqi and Sun, Xiao and Wang, Jiamin},
  title		= {EmoDamp: A Model for Speaker Emotion Inference in
		  Dialogues with Bidirectional Emotion Damping Mechanism},
  year		= {2024},
  isbn		= {9798400709203},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3639631.3639644},
  doi		= {10.1145/3639631.3639644},
  abstract	= {Emotion analysis in conversation has been a popular
		  research topic in the natural language processing field.
		  While much of the existing research has focused on emotion
		  recognition in conversation, the emotion inference task in
		  conversation is more challenging due to its specific
		  prerequisites. In this paper, we propose EmoDamp, a novel
		  model for the conversational emotion inference task. By
		  referring to psychological theories, a bidirectional
		  emotion damping network(BED Net) is designed to model the
		  emotion generation mechanism. Furthermore, external
		  commonsense knowledge is also used to supplement the
		  prediction results to make them more consistent with
		  authentic logic. Finally, a conditional random field (CRF)
		  is used to capture the potential constraints of the emotion
		  transfer process in conversation and simulate emotion
		  contagion. Experimental results on two benchmark datasets
		  demonstrate that EmoDamp outperforms the baseline model,
		  achieving improved results on each emotion category and
		  showcasing its effectiveness.},
  booktitle	= {Proceedings of the 2023 6th International Conference on
		  Algorithms, Computing and Artificial Intelligence},
  pages		= {74–81},
  numpages	= {8},
  keywords	= {Affective computing, Emotion Inference, Human–computer
		  interaction},
  location	= {Sanya, China},
  series	= {ACAI '23}
}

@Proceedings{	  10.1145/3685767,
  title		= {CTCNet '24: Proceedings of the 2024 Asia Pacific
		  Conference on Computing Technologies, Communications and
		  Networking},
  year		= {2024},
  isbn		= {9798400709609},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Chengdu, China}
}

@InProceedings{	  10.1145/3634737.3645000,
  author	= {Kumarasinghe, Udesh and Lekssays, Ahmed and Sencar, Husrev
		  Taha and Boughorbel, Sabri and Elvitigala, Charitha and
		  Nakov, Preslav},
  title		= {Semantic Ranking for Automated Adversarial Technique
		  Annotation in Security Text},
  year		= {2024},
  isbn		= {9798400704826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3634737.3645000},
  doi		= {10.1145/3634737.3645000},
  abstract	= {We introduce a novel approach for mapping attack behaviors
		  described in threat analysis reports to entries in an
		  adversarial techniques knowledge base. Our method leverages
		  a multi-stage ranking architecture to efficiently rank the
		  most related techniques based on their semantic relevance
		  to the input text. Each ranker in our pipeline uses a
		  distinct design for text representation. To enhance
		  relevance modeling, we leverage pretrained language models,
		  which we fine-tune for the technique annotation task. While
		  generic large language models are not yet capable of fully
		  addressing this challenge, we obtain very promising
		  results. We achieve a recall rate improvement of +35%
		  compared to the previous state-of-the-art results. We
		  further create new public benchmark datasets for training
		  and validating methods in this domain, which we release to
		  the research community aiming to promote future research in
		  this important direction.},
  booktitle	= {Proceedings of the 19th ACM Asia Conference on Computer
		  and Communications Security},
  pages		= {49–62},
  numpages	= {14},
  keywords	= {threat intelligence, TTP annotation, text ranking, text
		  attribution},
  location	= {Singapore, Singapore},
  series	= {ASIA CCS '24}
}

@InProceedings{	  10.1145/3638584.3638615,
  author	= {Wang, Na and Zhang, Rongqiang and Wu, Hengyang},
  title		= {Cultural Heritage Triple Information Extraction Based on
		  Span Pointer Network},
  year		= {2024},
  isbn		= {9798400708688},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638584.3638615},
  doi		= {10.1145/3638584.3638615},
  abstract	= {Knowledge in the field of cultural heritage has the
		  characteristics of long data, large entity span, high
		  semantic complexity and nesting among some entities, which
		  increase the difficulty of knowledge extraction in the
		  field of cultural heritage. In order to effectively solve
		  the problem of knowledge extraction in the field of
		  cultural heritage, and to solve the problem of error
		  accumulation and entity redundancy in the pipeline method
		  in the relationship extraction task, a relationship joint
		  extraction model based on span pointer network is proposed
		  to extract the triple information in the field of cultural
		  heritage. In addition, in order to improve the extraction
		  performance and generalization ability of the model, based
		  on the model of span pointer network, ERNIE pre-training
		  model which is more suitable for Chinese tasks is used as
		  the coding layer and AdamW is used as the optimizer. The
		  final experimental results show that this method has
		  superior performance in solving the problem of knowledge
		  extraction in the field of cultural heritage.},
  booktitle	= {Proceedings of the 2023 7th International Conference on
		  Computer Science and Artificial Intelligence},
  pages		= {381–387},
  numpages	= {7},
  keywords	= {AdamW, Cultural Heritage, ERNIE, Relation Extraction, Span
		  Pointer Network},
  location	= {Beijing, China},
  series	= {CSAI '23}
}

@InProceedings{	  10.1145/3644116.3644319,
  author	= {Han, Zongwang and Lin, Shaofu and Huang, Zhisheng and Guo,
		  Chaohui},
  title		= {Named Entity Recognition for Long COVID Biomedical
		  Literature by Using Bert-BiLSTM-IDCNN-ATT-CRF Approach},
  year		= {2024},
  isbn		= {9798400708138},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3644116.3644319},
  doi		= {10.1145/3644116.3644319},
  abstract	= {In recent years, with the exploration of pathological
		  mechanisms and treatments of Long COVID, there has been a
		  dramatic increase in related scientific publications.
		  Effective extraction of key information from these texts is
		  of great importance for public health and research
		  progress. In the Long COVID context, Named Entity
		  Recognition (NER) can be used to identify disease names as
		  well as symptoms, which can help to analyze the sequelae
		  caused by COVID-19 and its relationship with other
		  diseases. Distinguished from molecular biomedical text
		  mining, which focuses on the identification of entities
		  such as genes, proteins, and chemistries and their
		  relationships, Long COVID text mining faces problems such
		  as the lack of publicly labeled datasets and the heavy
		  workload of manual annotation. Moreover, due to the strong
		  domain characteristics of Long COVID relevant named
		  entities, models and methods that have achieved great
		  performance in the generic domain will have significantly
		  degraded named entity recognition performance on this
		  domain. Based on the above problems, we constructed a Long
		  COVID literature abstract NER dataset (LNER) and proposed a
		  Long COVID biomedical literature NER model
		  Bert-BiLSTM-IDCNN-ATT-CRF (BBIAC). First, the
		  BERT-BiLSTM-CRF model is constructed on the LNER dataset.
		  Then, the inflated convolutional neural network (IDCNN) is
		  added between the BiLSTM and the CRF layers to obtain the
		  local features in the text sequences. Finally, feature
		  enhancement is performed by fusing the features of global
		  and local information using the attention mechanism. The
		  experimental results show that the method proposed in this
		  paper for Long COVID literature can accurately extract the
		  characteristic information of Long COVID symptoms and
		  diseases, and has better performance compared to other
		  baseline models.},
  booktitle	= {Proceedings of the 2023 4th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {1200–1205},
  numpages	= {6},
  location	= {Chengdu, China},
  series	= {ISAIMS '23}
}

@Article{	  10.1145/3641278,
  author	= {Li, Jiuyi and Liu, Junpeng and Ma, Jianjun and Yang, Wei
		  and Huang, Degen},
  title		= {Boundary-Aware Abstractive Summarization with
		  Entity-Augmented Attention for Enhancing Faithfulness},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {4},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3641278},
  doi		= {10.1145/3641278},
  abstract	= {With the successful application of deep learning, document
		  summarization systems can produce more readable results.
		  However, abstractive summarization still suffers from
		  unfaithful outputs and factual errors, especially in named
		  entities. Current approaches tend to employ external
		  knowledge to improve model performance while neglecting the
		  boundary information and the semantics of the entities. In
		  this article, we propose an entity-augmented method (EAM)
		  to encourage the model to make full use of the entity
		  boundary information and pay more attention to the critical
		  entities. Experimental results on three Chinese and English
		  summarization datasets show that our method outperforms
		  several strong baselines and achieves state-of-the-art
		  performance on the CLTS dataset. Our method can also
		  improve the faithfulness of the summary and generalize well
		  to different pre-trained language models. Moreover, we
		  propose a method to evaluate the integrity of generated
		  entities. Besides, we adapt the data augmentation method in
		  the FactCC model according to the difference between
		  Chinese and English in grammar and train a new evaluation
		  model for factual consistency evaluation in Chinese
		  summarization.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= apr,
  articleno	= {53},
  numpages	= {18},
  keywords	= {Abstractive text summarization, factual consistency,
		  entity-augmented}
}

@Proceedings{	  10.1145/3677525,
  title		= {GoodIT '24: Proceedings of the 2024 International
		  Conference on Information Technology for Social Good},
  year		= {2024},
  isbn		= {9798400710940},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bremen, Germany}
}

@InProceedings{	  10.1145/3632754.3633480,
  author	= {Paul, Soumen and Majumdar, Srijoni and Bandyopadhyay, Ayan
		  and Dave, Bhargav and Chattopadhyay, Samiran and Das,
		  Partha and Clough, Paul D and Majumder, Prasenjit},
  title		= {Efficiency of Large Language Models to scale up Ground
		  Truth: Overview of the IRSE Track at Forum for Information
		  Retrieval 2023},
  year		= {2024},
  isbn		= {9798400716324},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3632754.3633480},
  doi		= {10.1145/3632754.3633480},
  abstract	= {The Software Engineering Information Retrieval (IRSE)
		  track aims to devise solutions for the automated evaluation
		  of code comments within a machine learning framework, with
		  labels generated by both humans and large language models.
		  Within this track, there is a binary classification task:
		  discerning comments as either useful or not useful. The
		  dataset includes 9,048 pairs of code comments and
		  surrounding code snippets drawn from open-source C-based
		  projects on GitHub and an additional dataset generated by
		  teams employing large language models. In total, 17 teams
		  representing various universities and software companies
		  have contributed 56 experiments. These experiments were
		  assessed through quantitative metrics, primarily the
		  F1-Score, and qualitative evaluations based on the features
		  developed, the supervised learning models employed, and
		  their respective hyperparameters. It is worth noting that
		  labels generated by large language models introduce bias
		  into the prediction model but lead to less over-fitted
		  results.},
  booktitle	= {Proceedings of the 15th Annual Meeting of the Forum for
		  Information Retrieval Evaluation},
  pages		= {16–18},
  numpages	= {3},
  keywords	= {Abstract syntax tree, Bert, GPT-2, Neural networks,
		  Stanford POS Tagging},
  location	= {Panjim, India},
  series	= {FIRE '23}
}

@InProceedings{	  10.1145/3589335.3651945,
  author	= {Ayoub, Michael Antonios Kruse and Su, Zhan and Li,
		  Qiuchi},
  title		= {A Case Study of Enhancing Sparse Retrieval using LLMs},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651945},
  doi		= {10.1145/3589335.3651945},
  abstract	= {While dense retrieval methods have made significant
		  advancements, sparse retrieval techniques continue to offer
		  advantages in terms of interpretability and
		  generalizability. However, query-document term mismatch in
		  sparse retrieval persists, rendering it infeasible for many
		  practical applications. Recent research has shown that
		  Large Language Models (LLMs) hold relevant information that
		  can enhance sparse retrieval through the application of
		  prompt engineering. In this paper, we build upon this
		  concept to explore various strategies employing LLMs for
		  information retrieval purposes. Specifically, we utilize
		  LLMs to enhance sparse retrieval by query rewriting and
		  query expansion. In query rewriting, the original query is
		  refined by creating several new queries. For query
		  expansion, LLMs are employed to generate extra terms,
		  thereby enriching the original query. We conduct
		  experiments on a range of well-known information retrieval
		  datasets, including MSMARCO-passage, TREC2019, TREC2020,
		  Natural Questions, SCIFACT. The experiments show that LLMs
		  can be beneficial for sparse methods since the added
		  information provided by the LLMs can help diminish the
		  discrepancy between the term frequencies of the important
		  terms in a query and the relevant document. In certain
		  domains, we demonstrate that the effectiveness of LLMs is
		  constrained, indicating that they may not consistently
		  perform optimally, which will be explored in future
		  research.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1609–1615},
  numpages	= {7},
  keywords	= {information retrieval, large language models, query
		  expansion, query writing},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3664476,
  title		= {ARES '24: Proceedings of the 19th International Conference
		  on Availability, Reliability and Security},
  year		= {2024},
  isbn		= {9798400717185},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Vienna, Austria}
}

@Article{	  10.1145/3656168,
  author	= {Wang, Quan and Mao, Zhendong and Gao, Jie and Zhang,
		  Yongdong},
  title		= {Document-level Relation Extraction with Progressive
		  Self-distillation},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {6},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3656168},
  doi		= {10.1145/3656168},
  abstract	= {Document-level relation extraction (RE) aims to
		  simultaneously predict relations (including no-relation
		  cases denoted as NA) between all entity pairs in a
		  document. It is typically formulated as a relation
		  classification task with entities pre-detected in advance
		  and solved by a hard-label training regime, which, however,
		  neglects the divergence of the NA class and the
		  correlations among other classes. This article introduces
		  progressive self-distillation (PSD), a new training regime
		  that employs online, self-knowledge distillation (KD) to
		  produce and incorporate soft labels for document-level
		  RE.The key idea of PSD is to gradually soften hard labels
		  using past predictions from an RE model itself, which are
		  adjusted adaptively as training proceeds. As such, PSD has
		  to learn only one RE model within a single training pass,
		  requiring no extra computation or annotation to pretrain
		  another high-capacity teacher. PSD is conceptually simple,
		  easy to implement, and generally applicable to various RE
		  models to further improve their performance, without
		  introducing additional parameters or significantly
		  increasing training overheads into the models. It is also a
		  general framework that can be flexibly extended to
		  distilling various types of knowledge, rather than being
		  restricted to soft labels themselves. Extensive experiments
		  on four benchmarking datasets verify the effectiveness and
		  generality of the proposed approach. The code is available
		  at},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jun,
  articleno	= {143},
  numpages	= {34},
  keywords	= {Document-level relation extraction, soft-label training
		  regime, online knowledge distillation, self-knowledge
		  distillation}
}

@InProceedings{	  10.1145/3701571.3701606,
  author	= {S. Jha, Sanjiv and Ghielmini, Nicol\`{o} and Garcia,
		  Kimberly and Mayer, Simon},
  title		= {The Spectrum of Proactive Functioning in Digital
		  Companions},
  year		= {2024},
  isbn		= {9798400712838},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701571.3701606},
  doi		= {10.1145/3701571.3701606},
  abstract	= {The future of proactive Digital Companions (DCs)—smart
		  agents capable of assisting and protecting their
		  users—lies in their ability to collaborate effectively
		  with users, learn their preferences, and adjust their
		  behavior according to the user’s current state and their
		  environment. To achieve this, DCs must carefully strike a
		  balance between acting autonomously, while keeping users
		  informed to minimize inconveniences, thereby enhancing user
		  acceptance. In this article, we conduct a user survey to
		  enrich the architecture of proactive personal DCs that
		  explores the trade-off between full autonomous functioning
		  and ensuring sufficient user control in various critical
		  and non-critical scenarios. Our findings indicate that most
		  of the participants lean towards having control over the
		  actions of DCs, and will actively collaborate with those
		  systems in everyday situations, in which decisions are not
		  urgent. However, participants would not mind yielding their
		  control to DCs in time-sensitive or urgent scenarios.
		  Furthermore, in our survey, participants highlighted the
		  importance of explaining such actions well. Given the
		  results from our survey, we implemented a system prototype
		  with the enriched architecture of an explainable and
		  unobtrusive proactive DC for a smart home environment. The
		  actions of this DC are not always fully autonomous and
		  follow our survey findings.},
  booktitle	= {Proceedings of the International Conference on Mobile and
		  Ubiquitous Multimedia},
  pages		= {331–337},
  numpages	= {7},
  keywords	= {Scene Understanding, Proactive, Video, Digital Assistants,
		  Digital Companion},
  location	= { },
  series	= {MUM '24}
}

@InProceedings{	  10.1145/3675094.3679000,
  author	= {Fiori, Michele and Civitarese, Gabriele and Bettini,
		  Claudio},
  title		= {Using Large Language Models to Compare Explainable Models
		  for Smart Home Human Activity Recognition},
  year		= {2024},
  isbn		= {9798400710582},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3675094.3679000},
  doi		= {10.1145/3675094.3679000},
  abstract	= {Recognizing daily activities with unobtrusive sensors in
		  smart environments enables various healthcare applications.
		  Monitoring how subjects perform activities at home and
		  their changes over time can reveal early symptoms of health
		  issues, such as cognitive decline. Most approaches in this
		  field use deep learning models, which are often seen as
		  black boxes mapping sensor data to activities. However,
		  non-expert users like clinicians need to trust and
		  understand these models' outputs. Thus, eXplainable AI
		  (XAI) methods for Human Activity Recognition have emerged
		  to provide intuitive natural language explanations from
		  these models. Different XAI methods generate different
		  explanations, and their effectiveness is typically
		  evaluated through user surveys, that are often challenging
		  in terms of costs and fairness. This paper proposes an
		  automatic evaluation method using Large Language Models
		  (LLMs) to identify, in a pool of candidates, the best XAI
		  approach for non-expert users. Our preliminary results
		  suggest that LLM evaluation aligns with user surveys.},
  booktitle	= {Companion of the 2024 on ACM International Joint
		  Conference on Pervasive and Ubiquitous Computing},
  pages		= {881–884},
  numpages	= {4},
  keywords	= {evaluation, human activity recognition, llms, xai},
  location	= {Melbourne VIC, Australia},
  series	= {UbiComp '24}
}

@InProceedings{	  10.1145/3627673.3679544,
  author	= {Du, Kelvin and Mao, Rui and Xing, Frank and Cambria,
		  Erik},
  title		= {Explainable Stock Price Movement Prediction using
		  Contrastive Learning},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679544},
  doi		= {10.1145/3627673.3679544},
  abstract	= {Predicting stock price movements is a high-stakes task
		  that demands explainability for human decision-makers. A
		  key shortcoming in current methods is treating
		  sub-predictions independently, without learning from
		  accumulated experiences. We propose a novel triplet network
		  for contrastive learning to enhance the explainability of
		  stock movement prediction by considering instances of
		  "integrated textual information and quantitative
		  indicators". We refer to the target past-l-day tweet-price
		  time series as the "anchor instance". Each anchor instance
		  is paired with a "positive instance" characterized by
		  highly correlated return trends yet significant differences
		  across the entire feature space, and a "negative instance"
		  that exhibits similar return trends along with high
		  proximity in the feature space. The model is designed with
		  the objective of (1) minimizing the cross entropy loss
		  between input logits and target, (2) minimizing the
		  distance between the anchor instances and positive
		  instances, and (3) maximizing the distance between the
		  anchor instances and negative instances. Our framework's
		  effectiveness is demonstrated through extensive testing,
		  showing superior performance on stock prediction
		  benchmarks.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {529–537},
  numpages	= {9},
  keywords	= {AI, NLP, contrastive learning, explainability, stock
		  price},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Article{	  10.1145/3652891,
  author	= {Ge, Yingqiang and Liu, Shuchang and Fu, Zuohui and Tan,
		  Juntao and Li, Zelong and Xu, Shuyuan and Li, Yunqi and
		  Xian, Yikun and Zhang, Yongfeng},
  title		= {A Survey on Trustworthy Recommender Systems},
  year		= {2024},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {2},
  url		= {https://doi.org/10.1145/3652891},
  doi		= {10.1145/3652891},
  abstract	= {Recommender systems (RS), serving at the forefront of
		  Human-centered AI, are widely deployed in almost every
		  corner of the web and facilitate the human decision-making
		  process. However, despite their enormous capabilities and
		  potential, RS may also lead to undesired effects on users,
		  items, producers, platforms, or even the society at large,
		  such as compromised user trust due to non-transparency,
		  unfair treatment of different consumers, or producers,
		  privacy concerns due to extensive use of user’s private
		  data for personalization, just to name a few. All of these
		  create an urgent need for Trustworthy Recommender Systems
		  (TRS) so as to mitigate or avoid such adverse impacts and
		  risks. In this survey, we will introduce techniques related
		  to trustworthy recommendation, including but not limited to
		  explainable recommendation, fairness in recommendation,
		  privacy-aware recommendation, robustness in recommendation,
		  user-controllable recommendation, as well as the
		  relationship between these different perspectives in terms
		  of trustworthy recommendation. Through this survey, we hope
		  to deliver readers with a comprehensive view of the
		  research area and raise attention to the community about
		  the importance, existing research achievements, and future
		  research directions on trustworthy recommendation.},
  journal	= {ACM Trans. Recomm. Syst.},
  month		= nov,
  articleno	= {13},
  numpages	= {68},
  keywords	= {Recommender Systems, Trustworthiness, Explainability,
		  Fairness, Privacy, Robustness, Controllability}
}

@InProceedings{	  10.1145/3643916.3644412,
  author	= {Pepe, Federica and Nardone, Vittoria and Mastropaolo,
		  Antonio and Bavota, Gabriele and Canfora, Gerardo and Di
		  Penta, Massimiliano},
  title		= {How do Hugging Face Models Document Datasets, Bias, and
		  Licenses? An Empirical Study},
  year		= {2024},
  isbn		= {9798400705861},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643916.3644412},
  doi		= {10.1145/3643916.3644412},
  abstract	= {Pre-trained Machine Learning (ML) models help to create
		  ML-intensive systems without having to spend conspicuous
		  resources on training a new model from the ground up.
		  However, the lack of transparency for such models could
		  lead to undesired consequences in terms of bias, fairness,
		  trustworthiness of the underlying data, and, potentially
		  even legal implications. Taking as a case study the
		  transformer models hosted by Hugging Face, a popular hub
		  for pre-trained ML models, this paper empirically
		  investigates the transparency of pre-trained transformer
		  models. We look at the extent to which model descriptions
		  (i) specify the datasets being used for their pre-training,
		  (ii) discuss their possible training bias, (iii) declare
		  their license, and whether projects using such models take
		  these licenses into account. Results indicate that
		  pre-trained models still have a limited exposure of their
		  training datasets, possible biases, and adopted licenses.
		  Also, we found several cases of possible licensing
		  violations by client projects. Our findings motivate
		  further research to improve the transparency of ML models,
		  which may result in the definition, generation, and
		  adoption of Artificial Intelligence Bills of Materials.},
  booktitle	= {Proceedings of the 32nd IEEE/ACM International Conference
		  on Program Comprehension},
  pages		= {370–381},
  numpages	= {12},
  keywords	= {ML-intensive systems, pre-trained models, transparency,
		  bias, and fairness, deep learning, empirical study},
  location	= {Lisbon, Portugal},
  series	= {ICPC '24}
}

@Proceedings{	  10.1145/3690407,
  title		= {CAIBDA '24: Proceedings of the 2024 4th International
		  Conference on Artificial Intelligence, Big Data and
		  Algorithms},
  year		= {2024},
  isbn		= {9798400710247},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3649921.3650001,
  author	= {Zhou, Hongwei and Zhu, Jichen and Mateas, Michael and
		  Wardrip-Fruin, Noah},
  title		= {The Eyes, the Hands and the Brain: What can Text-to-Image
		  Models Offer for Game Design and Visual Creativity?},
  year		= {2024},
  isbn		= {9798400709555},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3649921.3650001},
  doi		= {10.1145/3649921.3650001},
  abstract	= {Text-to-image models such as DALL-E, Stable Diffusion, and
		  Midjourney have seen a boom in development and adoption in
		  both commercial and hobbyist spaces. This paper is a
		  theoretical analysis aimed at informing the development of
		  games that help improve critical literacy around
		  text-to-image models. It asks: what assumptions and
		  perspectives do text-to-image models have on visual
		  creativity, and how do we bring that out through games? We
		  propose a theory to differentiate between seeing an image
		  through the expression of color, shapes and lines, and
		  seeing an image through the recognition of concepts and
		  ideas. These two ways of seeing are two different ways of
		  orienting the player/user to their visual creativity. While
		  traditional painting mechanics emphasize the former,
		  text-to-image interfaces emphasize the latter. We deploy
		  this perspective to study games with traditional painting
		  interactions and games with text-to-image interactions.
		  This paper hopes to contribute to design both broadly for
		  games about visual creativity, and narrowly for gameplay
		  with text-to-image models — specifically, how the latter
		  fosters a different type of visual creativity than
		  traditional painting interactions.},
  booktitle	= {Proceedings of the 19th International Conference on the
		  Foundations of Digital Games},
  articleno	= {23},
  numpages	= {13},
  keywords	= {Game Design, Painting, Stable Diffusion, Text-to-Image,
		  Visual Creativity},
  location	= {Worcester, MA, USA},
  series	= {FDG '24}
}

@Article{	  10.1145/3686902,
  author	= {Do, Hyo Jin and Brachman, Michelle and Dugan, Casey and
		  Johnson, James M. and Lauer, Julia and Rai, Priyanshu and
		  Pan, Qian},
  title		= {Grounding with Structure: Exploring Design Variations of
		  Grounded Human-AI Collaboration in a Natural Language
		  Interface},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW2},
  url		= {https://doi.org/10.1145/3686902},
  doi		= {10.1145/3686902},
  abstract	= {Selecting an effective utterance among countless
		  possibilities that match a user's intention poses a
		  challenge when using natural language interfaces. To
		  address the challenge, we leveraged the principle of least
		  collaborative effort in communication grounding theory and
		  designed three grounded conversational interactions: 1) a
		  grounding interface allows users to start with a
		  provisional input and then invite a conversational agent to
		  complete their input, 2) a multiple grounding interface
		  presents multiple inputs for the user to select from, and
		  3) a structured grounding interface guides users to write
		  inputs in a structure best understood by the system. We
		  compared our three grounding interfaces to an ungrounded
		  control interface in a crowdsourced study (N=80) using a
		  natural language system that generates small programs. We
		  found that the grounding interfaces reduced cognitive load
		  and improved task performance. The structured grounding
		  interface further reduced speaker change costs and improved
		  technology acceptance, without sacrificing the perception
		  of control. We discuss the implications of designing
		  grounded conversational interactions in natural language
		  systems.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= nov,
  articleno	= {363},
  numpages	= {27},
  keywords	= {conversational grounding, conversational user interface,
		  natural language interface}
}

@Proceedings{	  10.1145/3630138,
  title		= {PCCNT '23: Proceedings of the 2023 International
		  Conference on Power, Communication, Computing and
		  Networking Technologies},
  year		= {2023},
  isbn		= {9781450399951},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Wuhan, China}
}

@Proceedings{	  10.1145/3673277,
  title		= {CNSCT '24: Proceedings of the 2024 3rd International
		  Conference on Cryptography, Network Security and
		  Communication Technology},
  year		= {2024},
  isbn		= {9798400716959},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Harbin, China}
}

@Proceedings{	  10.1145/3654522,
  title		= {ICIIT '24: Proceedings of the 2024 9th International
		  Conference on Intelligent Information Technology},
  year		= {2024},
  isbn		= {9798400716713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Ho Chi Minh City, Vietnam}
}

@InProceedings{	  10.1145/3657054.3657077,
  author	= {Maratsi, Maria Ioanna and Ahmed, Umair and Alexopoulos,
		  Charalampos and Charalabidis, Yannis and Polini, Andrea},
  title		= {Towards Cross-Domain Linking of Data: A Semantic Mapping
		  of Cultural Heritage Ontologies},
  year		= {2024},
  isbn		= {9798400709883},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3657054.3657077},
  doi		= {10.1145/3657054.3657077},
  abstract	= {The Linked Open Vocabularies (LOV) registry, designed with
		  the Linked Data principles at core, provides an environment
		  suitable for research which targets domain-specific, but
		  also potentially reusable, information representation. The
		  main purpose of this study is to follow the recommendations
		  pertaining to the utilisation of LOV as a basis for
		  experimentation in order to examine how information within
		  the Cultural Heritage (CH) domain can be improved in terms
		  of reusability and interoperability. The present lack of
		  cross-domain knowledge transfer forms the motivation behind
		  this study, with the aim of facilitating the transition
		  from conventional, domain-specific knowledge representation
		  to reusable and semantically interoperable information. The
		  methodology of this study involves the manual semantic
		  mapping of elements from 12 vocabularies in the LOV
		  registry, reinforced by a small-scale experiment using
		  contemporary large language models (LLMs), particularly
		  GPT, for a preliminary assessment of the mapping process.
		  The findings revealed several key aspects to consider
		  regarding the alignment of semantically adjacent vocabulary
		  elements in the CH domain and beyond, emphasising the
		  potential unveiled by linking domain-focused schemata to
		  standardised, established ones while preserving the
		  conceptual hierarchies inherent to each individual
		  knowledge domain. The contribution of this research
		  pertains to the vision of linking data across different
		  domains by initiating the alignment among representation
		  schemata in CH, with the ultimate aim to expand beyond the
		  boundaries of the in-word knowledge domain, while employing
		  combinatory methodological approaches of technological
		  means and human expertise to facilitate this process.},
  booktitle	= {Proceedings of the 25th Annual International Conference on
		  Digital Government Research},
  pages		= {165–176},
  numpages	= {12},
  location	= {Taipei, Taiwan},
  series	= {dg.o '24}
}

@Article{	  10.1145/3698236,
  author	= {Yin Hong, Kung and Han, Lifeng and Batista-Navarro, Riza
		  and Nenadic, Goran},
  title		= {CantonMT: Investigating Back-Translation and Model-Switch
		  Mechanisms for Cantonese-English Neural Machine
		  Translation},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3698236},
  doi		= {10.1145/3698236},
  abstract	= {This paper investigates the development and evaluation of
		  machine translation models from Cantonese to English (and
		  backward), where we propose a novel approach to tackle
		  low-resource language translations. Despite recent
		  improvements in Neural Machine Translation (NMT) models
		  with Transformer-based architectures, Cantonese, a language
		  with over 80 million native speakers, has below-par
		  State-of-the-art commercial translation models due to a
		  lack of resources. The main objectives of the study are to
		  develop a model that can effectively translate Cantonese to
		  English and evaluate it against state-of-the-art commercial
		  models. To achieve this, a new parallel corpus has been
		  created by combining different available corpora online
		  with preprocessing and cleaning. In addition, a monolingual
		  Cantonese dataset has been created through web scraping to
		  aid the synthetic parallel corpus generation. Following the
		  data collection process, several approaches, including
		  fine-tuning models, back-translation, and model switch,
		  have been used. The translation quality of models has been
		  evaluated with multiple quality metrics, including
		  lexicon-based metrics (SacreBLEU and hLEPOR) and
		  embedding-space metrics (COMET and BERTscore). Based on the
		  automatic metrics, the best model is selected and compared
		  against the 2 best commercial translators using a new human
		  evaluation framework HOPES. The best model proposed in this
		  investigation (NLLB-mBART) with model switch mechanisms has
		  reached comparable and even better automatic evaluation
		  scores against State-of-the-art commercial models (Bing and
		  Baidu Translators), with a SacreBLEU score of 16.8 on our
		  test set. Furthermore, an open-source web application has
		  been developed to allow users to translate between
		  Cantonese and English, with the different trained models
		  available for effective comparisons between models from
		  this investigation and users. CantonMT is available at
		  https://github.com/kenrickkung/CantoneseTranslation},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= oct,
  keywords	= {Neural Machine Translation, Cantonese-English Translation,
		  Low Resource MT, Data Augmentation, Model Switch
		  Mechanism}
}

@Article{	  10.1109/taslp.2024.3426287,
  author	= {Xiao, Yinlong and Ji, Zongcheng and Li, Jianqiang and Han,
		  Mei},
  title		= {MVT: Chinese NER Using Multi-View Transformer},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3426287},
  doi		= {10.1109/TASLP.2024.3426287},
  abstract	= {Integrating lexical knowledge in Chinese named entity
		  recognition (NER) has been proven effective. Among the
		  existing methods, Flat-LAttice Transformer (FLAT) has
		  achieved great success in both performance and efficiency.
		  FLAT performs lexical enhancement for each sentence by
		  constructing a flat lattice (i.e., a sequence of tokens
		  including the characters in a sentence and the matched
		  words in a lexicon) and calculating self-attention with a
		  fully-connected structure. However, the different
		  interactions between tokens, which can bring different
		  aspects of semantic information for Chinese NER, cannot be
		  well captured by self-attention with a fully-connected
		  structure. In this paper, we propose a novel Multi-View
		  Transformer (MVT) to effectively capture the different
		  interactions between tokens. We first define four views to
		  capture four different token interaction structures. We
		  then construct a view-aware visible matrix for each view
		  according to the corresponding structure and introduce a
		  view-aware dot-product attention for each view to limit the
		  attention scope by incorporating the corresponding visible
		  matrix. Finally, we design three different MVT variants to
		  fuse the multi-view features at different levels of the
		  Transformer architecture. Experimental results conducted on
		  four public Chinese NER datasets show the effectiveness of
		  the proposed method. Specifically, on the most challenging
		  dataset Weibo, which is in an informal text style, MVT
		  outperforms FLAT in F1 score by 2.56%, and when combined
		  with BERT, MVT outperforms FLAT in F1 score by 3.03%.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= jul,
  pages		= {3656–3668},
  numpages	= {13}
}

@InProceedings{	  10.1145/3664647.3680924,
  author	= {Lin, Mingkai and Li, Wenzhong and Hong, Xiaobin and Lu,
		  Sanglu},
  title		= {Scalable Multi-Source Pre-training for Graph Neural
		  Networks},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3680924},
  doi		= {10.1145/3664647.3680924},
  abstract	= {Graph Neural Networks (GNNs) have proven effective in
		  various scenarios. A key strategy involves pre-training
		  existing graphs to extract knowledge that can be
		  transferred to improve performance on downstream tasks,
		  reducing the need for extensive labeled data. However,
		  previous works commonly assumed that pre-training and
		  fine-tuning occur in the same or closely related domains. A
		  limitation is that for each individual graph without
		  accessible pre-training data, a GNN must be trained from
		  scratch, imposing high training overhead and hindering the
		  ability of generalization. In this paper, we address the
		  GNN multi-domain pre-training problem, which intends to
		  pre-train a transferable GNN model from heterogeneous
		  multi-source graph domains and then apply it in an unseen
		  one with minor fine-tuning costs. To this end, we propose a
		  scaLA ble Multi-source Pre-training (LAMP) method. For
		  pre-training, LAMP presents a graph dual-distillation
		  approach to distill massive knowledge from various graph
		  domains to form synthetic homogeneous graphs.
		  Simultaneously, high-level meta-knowledge from the
		  synthetic graphs is extracted to train the GNN model, whose
		  capability can be adjusted according to target graph
		  contexts through a co-training modulation architecture. For
		  fine-tuning, LAMP respectively aligns the target graph
		  distribution, graph context, and graph task with the
		  pretext so that the downstream task in the unseen domain
		  can be reshaped to leverage the transferable knowledge
		  efficiently. Extensive experiments on four different graph
		  domain datasets show the superiority of LAMP.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {1292–1301},
  numpages	= {10},
  keywords	= {gnns, multi-source pre-training, unseen domain
		  fine-tuning},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@Proceedings{	  10.1145/3658852,
  title		= {MOCO '24: Proceedings of the 9th International Conference
		  on Movement and Computing},
  year		= {2024},
  isbn		= {9798400709944},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Utrecht, Netherlands}
}

@InProceedings{	  10.1145/3648188.3678216,
  author	= {Brooker, Sam},
  title		= {Computer, Enhance! Augmentation, Ideation, Hypertext},
  year		= {2024},
  isbn		= {9798400705953},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3648188.3678216},
  doi		= {10.1145/3648188.3678216},
  abstract	= {By ‘augmenting human intellect’ we mean increasing the
		  capability of a man to approach a complex problem or
		  situation, to gain comprehension to suit his particular
		  needs, and to derive solutions to problems.” So wrote
		  Douglas Engelbart in 1962, initiating a narrative of
		  augmentation that runs through hypertext's history. To
		  augment is to amplify something: our ideas, our activities,
		  our thoughts. This paper explores hypertext's relationship
		  with computers as intentional machines, its relationship
		  with artificial and human intelligence, and what role it
		  can play in negotiating between the two.},
  booktitle	= {Proceedings of the 35th ACM Conference on Hypertext and
		  Social Media},
  pages		= {193–196},
  numpages	= {4},
  location	= {Poznan, Poland},
  series	= {HT '24}
}

@Article{	  10.1145/3626234,
  author	= {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon
		  and Zowghi, Didar and Jacquet, Aurelie},
  title		= {Responsible AI Pattern Catalogue: A Collection of Best
		  Practices for AI Governance and Engineering},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {7},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3626234},
  doi		= {10.1145/3626234},
  abstract	= {Responsible Artificial Intelligence (RAI) is widely
		  considered as one of the greatest scientific challenges of
		  our time and is key to increase the adoption of Artificial
		  Intelligence (AI). Recently, a number of AI ethics
		  principles frameworks have been published. However, without
		  further guidance on best practices, practitioners are left
		  with nothing much beyond truisms. In addition, significant
		  efforts have been placed at algorithm level rather than
		  system level, mainly focusing on a subset of
		  mathematics-amenable ethical principles, such as fairness.
		  Nevertheless, ethical issues can arise at any step of the
		  development lifecycle, cutting across many AI and non-AI
		  components of systems beyond AI algorithms and models. To
		  operationalize RAI from a system perspective, in this
		  article, we present an RAI Pattern Catalogue based on the
		  results of a multivocal literature review. Rather than
		  staying at the principle or algorithm level, we focus on
		  patterns that AI system stakeholders can undertake in
		  practice to ensure that the developed AI systems are
		  responsible throughout the entire governance and
		  engineering lifecycle. The RAI Pattern Catalogue classifies
		  the patterns into three groups: multi-level governance
		  patterns, trustworthy process patterns, and RAI-by-design
		  product patterns. These patterns provide systematic and
		  actionable guidance for stakeholders to implement RAI.},
  journal	= {ACM Comput. Surv.},
  month		= apr,
  articleno	= {173},
  numpages	= {35},
  keywords	= {Responsible AI, ethical AI, trustworthy AI, AI governance,
		  AI engineering, MLOps, software engineering, software
		  architecture, pattern, best practice}
}

@Article{	  10.14778/3659437.3659448,
  author	= {Deng, Yuhao and Chai, Chengliang and Cao, Lei and Yuan,
		  Qin and Chen, Siyuan and Yu, Yanrui and Sun, Zhaoze and
		  Wang, Junyi and Li, Jiajun and Cao, Ziqi and Jin, Kaisen
		  and Zhang, Chi and Jiang, Yuqing and Zhang, Yuanfang and
		  Wang, Yuping and Yuan, Ye and Wang, Guoren and Tang, Nan},
  title		= {LakeBench: A Benchmark for Discovering Joinable and
		  Unionable Tables in Data Lakes},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {8},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3659437.3659448},
  doi		= {10.14778/3659437.3659448},
  abstract	= {Discovering tables from poorly maintained data lakes is a
		  significant challenge in data management. Two key tasks are
		  identifying joinable and unionable tables, crucial for data
		  integration, analysis, and machine learning. However,
		  there's a lack of a comprehensive benchmark for evaluating
		  existing methods. To address this, we introduce LakeBench,
		  a large-scale table discovery benchmark. It evaluates
		  effectiveness, efficiency, and scalability of table join
		  &amp; union search methods. With over 16 million real
		  tables, LakeBench is 1,600X larger than existing datasets
		  and 100X larger in storage size. It includes synthesized
		  and real queries with ground truth, totaling more than 10
		  thousand queries - 10X more than used in any existing
		  evaluation. We spent over 7,500 human hours labeling these
		  queries and constructing diverse query categories for
		  thorough evaluation. Our benchmark thoroughly evaluates
		  state-of-the-art table discovery methods, providing
		  insights into their performance and highlighting research
		  opportunities.},
  journal	= {Proc. VLDB Endow.},
  month		= apr,
  pages		= {1925–1938},
  numpages	= {14}
}

@Article{	  10.1145/3647639,
  author	= {Lien, Yen-Chieh and Zamani, Hamed and Croft, Bruce},
  title		= {Generalized Weak Supervision for Neural Information
		  Retrieval},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {5},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3647639},
  doi		= {10.1145/3647639},
  abstract	= {Neural ranking models (NRMs) have demonstrated effective
		  performance in several information retrieval (IR) tasks.
		  However, training NRMs often requires large-scale training
		  data, which is difficult and expensive to obtain. To
		  address this issue, one can train NRMs via weak
		  supervision, where a large dataset is automatically
		  generated using an existing ranking model (called the weak
		  labeler) for training NRMs. Weakly supervised NRMs can
		  generalize from the observed data and significantly
		  outperform the weak labeler. This paper generalizes this
		  idea through an iterative re-labeling process,
		  demonstrating that weakly supervised models can iteratively
		  play the role of weak labeler and significantly improve
		  ranking performance without using manually labeled data.
		  The proposed Generalized Weak Supervision (GWS) solution is
		  generic and orthogonal to the ranking model architecture.
		  This paper offers four implementations of GWS:
		  self-labeling, cross-labeling, joint cross- and
		  self-labeling, and greedy multi-labeling. GWS also benefits
		  from a query importance weighting mechanism based on query
		  performance prediction methods to reduce noise in the
		  generated training data. We further draw a theoretical
		  connection between self-labeling and
		  Expectation-Maximization. Our experiments on four retrieval
		  benchmarks suggest that our implementations of GWS lead to
		  substantial improvements compared to weak supervision if
		  the weak labeler is sufficiently reliable.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= apr,
  articleno	= {121},
  numpages	= {26},
  keywords	= {Weak supervision, distant supervision, neural ranking
		  models, zero-shot learning, unsupervised learning}
}

@Article{	  10.1145/3639563,
  author	= {Razniewski, Simon and Arnaout, Hiba and Ghosh, Shrestha
		  and Suchanek, Fabian},
  title		= {Completeness, Recall, and Negation in Open-world Knowledge
		  Bases: A Survey},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3639563},
  doi		= {10.1145/3639563},
  abstract	= {General-purpose knowledge bases (KBs) are a cornerstone of
		  knowledge-centric AI. Many of them are constructed
		  pragmatically from web sources and are thus far from
		  complete. This poses challenges for the consumption as well
		  as the curation of their content. While several surveys
		  target the problem of completing incomplete KBs, the first
		  problem is arguably to know whether and where the KB is
		  incomplete in the first place, and to which degree. In this
		  survey, we discuss how knowledge about completeness,
		  recall, and negation in KBs can be expressed, extracted,
		  and inferred. We cover (i) the logical foundations of
		  knowledge representation and querying under partial
		  closed-world semantics; (ii) the estimation of this
		  information via statistical patterns; (iii) the extraction
		  of information about recall from KBs and text; (iv) the
		  identification of interesting negative statements; and (v)
		  relaxed notions of relative recall. This survey is targeted
		  at two types of audiences: (1) practitioners who are
		  interested in tracking KB quality, focusing extraction
		  efforts, and building quality-aware downstream
		  applications; and (2) data management, knowledge base, and
		  semantic web researchers who wish to understand the
		  state-of-the-art of knowledge bases beyond the open-world
		  assumption. Consequently, our survey presents both
		  fundamental methodologies and the results that they have
		  produced, and gives practice-oriented recommendations on
		  how to choose between different approaches for a problem at
		  hand.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {150},
  numpages	= {42},
  keywords	= {Knowledge bases, data completeness}
}

@Proceedings{	  10.1145/3626203,
  title		= {PEARC '24: Practice and Experience in Advanced Research
		  Computing 2024: Human Powered Computing},
  year		= {2024},
  isbn		= {9798400704192},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Providence, RI, USA}
}

@Proceedings{	  10.1145/3674225,
  title		= {PEAI '24: Proceedings of the 2024 International Conference
		  on Power Electronics and Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400716638},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@Proceedings{	  10.1145/3637494,
  title		= {CECCT '23: Proceedings of the 2023 International
		  Conference on Electronics, Computers and Communication
		  Technology},
  year		= {2023},
  isbn		= {9798400716300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guilin, China}
}

@InProceedings{	  10.1145/3643916.3644413,
  author	= {Dhaouadi, Mouna and Oakes, Bentley James and Famelis,
		  Michalis},
  title		= {Rationale Dataset and Analysis for the Commit Messages of
		  the Linux Kernel Out-of-Memory Killer},
  year		= {2024},
  isbn		= {9798400705861},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3643916.3644413},
  doi		= {10.1145/3643916.3644413},
  abstract	= {Code commit messages can contain useful information on why
		  a developer has made a change. However, the presence and
		  structure of rationale in real-world code commit messages
		  is not well studied. Here, we detail the creation of a
		  labelled dataset to analyze the code commit messages of the
		  Linux Kernel Out-Of-Memory Killer component. We study
		  aspects of rationale information, such as presence,
		  temporal evolution, and structure. We find that 98.9% of
		  commits in our dataset contain sentences with rationale
		  information, and that experienced developers report
		  rationale in about 60% of the sentences in their commits.
		  We report on the challenges we faced and provide examples
		  for our labelling.},
  booktitle	= {Proceedings of the 32nd IEEE/ACM International Conference
		  on Program Comprehension},
  pages		= {415–425},
  numpages	= {11},
  keywords	= {developer rationale, dataset, Linux kernel, commit
		  messages},
  location	= {Lisbon, Portugal},
  series	= {ICPC '24}
}

@Proceedings{	  10.1145/3640771,
  title		= {ISCAI '23: Proceedings of the 2023 2nd International
		  Symposium on Computing and Artificial Intelligence},
  year		= {2023},
  isbn		= {9798400708954},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shanghai, China}
}

@InProceedings{	  10.1145/3664647.3681389,
  author	= {Zhao, Shengwei and Xu, Linhai and Liu, Yuying and Du,
		  Shaoyi},
  title		= {Multi-grained Correspondence Learning of Audio-language
		  Models for Few-shot Audio Recognition},
  year		= {2024},
  isbn		= {9798400706868},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664647.3681389},
  doi		= {10.1145/3664647.3681389},
  abstract	= {Large-scale pre-trained audio-language models excel in
		  general multi-modal representation, facilitating their
		  adaptation to downstream audio recognition tasks in a
		  data-efficient manner. However, existing few-shot audio
		  recognition methods based on audio-language models
		  primarily focus on learning coarse-grained correlations,
		  which are not sufficient to capture the intricate matching
		  patterns between the multi-level information of audio and
		  the diverse characteristics of category concepts. To
		  address this gap, we propose multi-grained correspondence
		  learning for bootstrapping audio-language models to improve
		  audio recognition with few training samples. This approach
		  leverages generative models to enrich multi-modal
		  representation learning, mining the multi-level information
		  of audio alongside the diverse characteristics of category
		  concepts. Multi-grained matching patterns are then
		  established through multi-grained key-value cache and
		  multi-grained cross-modal contrast, enhancing the alignment
		  between audio and category concepts. Additionally, we
		  incorporate optimal transport to tackle temporal
		  misalignment and semantic intersection issues in
		  fine-grained correspondence learning, enabling flexible
		  fine-grained matching. Our method achieves state-of-the-art
		  results on multiple benchmark datasets for few-shot audio
		  recognition, with comprehensive ablation experiments
		  validating its effectiveness.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Multimedia},
  pages		= {9244–9252},
  numpages	= {9},
  keywords	= {audio-language models, few-shot audio recognition,
		  multi-grained correspondence learning, optimal transport},
  location	= {Melbourne VIC, Australia},
  series	= {MM '24}
}

@InProceedings{	  10.1145/3616855.3635822,
  author	= {Kaiser, Magdalena and Saha Roy, Rishiraj and Weikum,
		  Gerhard},
  title		= {Robust Training for Conversational Question Answering
		  Models with Reinforced Reformulation Generation},
  year		= {2024},
  isbn		= {9798400703713},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616855.3635822},
  doi		= {10.1145/3616855.3635822},
  abstract	= {Models for conversational question answering (ConvQA) over
		  knowledge graphs (KGs) are usually trained and tested on
		  benchmarks of gold QA pairs. This implies that training is
		  limited to surface forms seen in the respective datasets,
		  and evaluation is on a small set of held-out questions.
		  Through our proposed framework REIGN, we take several steps
		  to remedy this restricted learning setup. First, we
		  systematically generate reformulations of training
		  questions to increase robustness of models to surface form
		  variations. This is a particularly challenging problem,
		  given the incomplete nature of such questions. Second, we
		  guide ConvQA models towards higher performance by feeding
		  it only those reformulations that help improve their
		  answering quality, using deep reinforcement learning.
		  Third, we demonstrate the viability of training major model
		  components on one benchmark and applying them zero-shot to
		  another. Finally, for a rigorous evaluation of robustness
		  for trained models, we use and release large numbers of
		  diverse reformulations generated by prompting ChatGPT for
		  benchmark test sets (resulting in 20x increase in sizes).
		  Our findings show that ConvQA models with robust training
		  via reformulations significantly outperform those with
		  standard training from gold QA pairs only.},
  booktitle	= {Proceedings of the 17th ACM International Conference on
		  Web Search and Data Mining},
  pages		= {322–331},
  numpages	= {10},
  keywords	= {conversations, knowledge graphs, question answering},
  location	= {Merida, Mexico},
  series	= {WSDM '24}
}

@Article{	  10.1145/3651313,
  author	= {Denisenko, Natalia and Zhang, Youzhi and Pulice, Chiara
		  and Bhattasali, Shohini and Jajodia, Sushil and Resnik,
		  Philip and Subrahmanian, V.S.},
  title		= {A Psycholinguistics-inspired Method to Counter IP Theft
		  Using Fake Documents},
  year		= {2024},
  issue_date	= {June 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {2},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3651313},
  doi		= {10.1145/3651313},
  abstract	= {Intellectual property (IP) theft is a growing problem. We
		  build on prior work to deter IP theft by generating n fake
		  versions of a technical document so a thief has to expend
		  time and effort in identifying the correct document. Our
		  new SbFAKE framework proposes, for the first time, a novel
		  combination of language processing, optimization, and the
		  psycholinguistic concept of surprisal to generate a set of
		  such fakes. We start by combining psycholinguistic-based
		  surprisal scores and optimization to generate two bilevel
		  surprisal optimization problems (an Explicit one and a
		  simpler Implicit one) whose solutions correspond directly
		  to the desired set of fakes. As bilevel problems are
		  usually hard to solve, we then show that these two bilevel
		  surprisal optimization problems can each be reduced to
		  equivalent surprisal-based linear programs. We performed
		  detailed parameter tuning experiments and identified the
		  best parameters for each of these algorithms. We then
		  tested these two variants of SbFAKE (with their best
		  parameter settings) against the best performing prior work
		  in the field. Our experiments show that SbFAKE is able to
		  more effectively generate convincing fakes than past work.
		  In addition, we show that replacing words in an original
		  document with words having similar surprisal scores
		  generates greater levels of deception.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= jun,
  articleno	= {7},
  numpages	= {25},
  keywords	= {AI for security, fake document generation}
}

@Article{	  10.1145/3689906,
  author	= {Shukla, Shiv Shankar Prasad and Singh, Maheshwari Prasad},
  title		= {Stacked Classification Approach using Optimized Hybrid
		  Deep Learning Model for Early Prediction of Behaviour
		  Changes on Social Media},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {11},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3689906},
  doi		= {10.1145/3689906},
  abstract	= {Detecting signs of suicidal thoughts on social media is
		  paramount for preventing suicides, given the platforms'
		  role as primary outlets for emotional expression.
		  Traditional embedding techniques focus solely on semantic
		  analysis and lack the sentiment analysis essential for
		  capturing emotions. This limitation poses challenges in
		  developing high-accuracy models. Additionally, previous
		  studies often rely on a single dataset, further
		  constraining their effectiveness. To overcome these
		  challenges, this study proposes an innovative approach that
		  integrates embedding techniques such as BERT, which offers
		  semantic and syntactic analysis of the posts, with
		  sentiment analysis provided by VADER scores extracted from
		  the VADER sentiment analysis tool. The identified features
		  are then input into the proposed optimised hybrid deep
		  learning model, specifically the Bi-GRU and Attention
		  incorporated with Stacked or Stacking Classifier (Decision
		  Tree, Random Forest, Gradient Boost, as the base classifier
		  and XGBoost as meta classifier), which undergoes
		  optimisation using the grid search technique to enhance
		  detection capabilities. In evaluations, the model achieved
		  an impressive accuracy and F1-score of 98% on the Reddit
		  dataset and 97% on the twitter (formally known as X)
		  dataset. The research evaluates the efficacy of several
		  machine learning models, encompassing Decision Trees,
		  Random Forests, Gradient Boosting, and XGBoost. Moreover,
		  it examines sophisticated models like LSTM with Attention,
		  Bi-LSTM with Attention, and Bi-GRU with Attention,
		  augmented with word embeddings such as BERT, MUSE, and
		  fastText, alongside the fusion of sentiment VADER score.
		  These results emphasise the promise of a holistic strategy
		  that combines advanced feature embedding techniques with
		  semantic features, showcasing a notably efficient detection
		  of suicidal ideation on social media.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= nov,
  articleno	= {159},
  numpages	= {22},
  keywords	= {Bi-GRU, Attention, Stacked Classifier, XGBoost, Random
		  Forest, Natural Language Processing, Suicidal Ideation}
}

@InProceedings{	  10.1145/3589335.3641300,
  author	= {Ding, Yujuan and Fan, Wenqi and Huang, Xiao and Li, Qing},
  title		= {Large Language Models for Graph Learning},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3641300},
  doi		= {10.1145/3589335.3641300},
  abstract	= {Graphs are widely applied to encode entities with various
		  relations in web applications such as social media and
		  recommender systems. Meanwhile, graph learning-based
		  technologies, such as graph neural networks, are demanding
		  to support the analysis, understanding, and usage of the
		  data in graph structures. Recently, the boom of language
		  foundation models, especially Large Language Models (LLMs),
		  has advanced several main research areas in artificial
		  intelligence, such as natural language processing, graph
		  mining, and recommender systems. The synergy between LLMs
		  and graph learning holds great potential to prompt the
		  research in both areas. For example, LLMs can facilitate
		  existing graph learning models by providing high-quality
		  textual features for entities and edges, or enhancing the
		  graph data with encoded knowledge and information. It may
		  also innovate with novel problem formulations on
		  graph-related tasks. Due to the research significance as
		  well as the potential, the convergent area of LLMs and
		  graph learning has attracted considerable research
		  attention. Therefore, we propose to hold the workshop Large
		  Language Models for Graph Learning at WWW'24, in order to
		  provide a venue to gather researchers in academia and
		  practitioners in the industry to present the recent
		  progress on relevant topics and exchange their critical
		  insights.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1643–1646},
  numpages	= {4},
  keywords	= {fine-tuning, graph learning, in-context learning, large
		  language models, pre-training, prompting},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3656766,
  title		= {ICBAR '23: Proceedings of the 2023 3rd International
		  Conference on Big Data, Artificial Intelligence and Risk
		  Management},
  year		= {2023},
  isbn		= {9798400716478},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Chengdu, China}
}

@Article{	  10.1145/3686981,
  author	= {Zhu, Mengxiao and Wang, Xin and Wang, Xiantao and Chen,
		  Zihang and Huang, Wei},
  title		= {Application of Prompt Learning Models in Identifying the
		  Collaborative Problem Solving Skills in an Online Task},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW2},
  url		= {https://doi.org/10.1145/3686981},
  doi		= {10.1145/3686981},
  abstract	= {Collaborative problem solving (CPS) competence is
		  considered one of the essential 21st-century skills. To
		  facilitate the assessment and learning of CPS competence,
		  researchers have proposed a series of frameworks to
		  conceptualize CPS and explored ways to make sense of the
		  complex processes involved in collaborative problem
		  solving. However, encoding explicit behaviors into
		  subskills within the frameworks of CPS skills is still a
		  challenging task. Traditional studies have relied on manual
		  coding to decipher behavioral data for CPS, but such coding
		  methods can be very time-consuming and cannot support
		  real-time analyses. Scholars have begun to explore
		  approaches for constructing automatic coding models.
		  Nevertheless, the existing models built using machine
		  learning or deep learning techniques depend on a large
		  amount of training data and have relatively low accuracy.
		  To address these problems, this paper proposes a
		  prompt-based learning pre-trained model. The model can
		  achieve high performance even with limited training data.
		  In this study, three experiments were conducted, and the
		  results showed that our model not only produced the highest
		  accuracy, macro F1 score, and kappa values on large
		  training sets, but also performed the best on small
		  training sets of the CPS behavioral data. The application
		  of the proposed prompt-based learning pre-trained model
		  contributes to the CPS skills coding task and can also be
		  used for other CSCW coding tasks to replace manual
		  coding.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= nov,
  articleno	= {442},
  numpages	= {23},
  keywords	= {automatic coding, collaborative problem solving, natural
		  language processing, prompt-based learning}
}

@Proceedings{	  10.1145/3661455,
  title		= {PDC '24: Proceedings of the Participatory Design
		  Conference 2024: Exploratory Papers and Workshops - Volume
		  2},
  year		= {2024},
  isbn		= {9798400706547},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  location	= {Sibu, Malaysia}
}

@Proceedings{	  10.1145/3674029,
  title		= {ICMLT '24: Proceedings of the 2024 9th International
		  Conference on Machine Learning Technologies},
  year		= {2024},
  isbn		= {9798400716379},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Oslo, Norway}
}

@InProceedings{	  10.1145/3627673.3679662,
  author	= {Balsebre, Pasquale and Huang, Weiming and Cong, Gao and
		  Li, Yi},
  title		= {City Foundation Models for Learning General Purpose
		  Representations from OpenStreetMap},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679662},
  doi		= {10.1145/3627673.3679662},
  abstract	= {Pre-trained Foundation Models (PFMs) have ushered in a
		  paradigm-shift in AI, due to their ability to learn
		  general-purpose representations that can be readily
		  employed in downstream tasks. While PFMs have been
		  successfully adopted in various fields such as NLP and
		  Computer Vision, their capacity in handling geospatial data
		  remains limited. This can be attributed to the intrinsic
		  heterogeneity of such data, which encompasses different
		  types, including points, segments and regions, as well as
		  multiple information modalities. The proliferation of
		  Volunteered Geographic Information initiatives, like
		  OpenStreetMap, unveils a promising opportunity to bridge
		  this gap. In this paper, we present CityFM, a
		  self-supervised framework to train a foundation model
		  within a selected geographical area. CityFM relies solely
		  on open data from OSM, and produces multimodal
		  representations, incorporating spatial, visual, and textual
		  information. We analyse the entity representations
		  generated by our foundation models from a qualitative
		  perspective, and conduct experiments on road, building, and
		  region-level downstream tasks. In all the experiments,
		  CityFM achieves performance superior to, or on par with,
		  application-specific algorithms.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {87–97},
  numpages	= {11},
  keywords	= {contrastive learning, foundation models, geospatial data},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Proceedings{	  10.1145/3643915,
  title		= {SEAMS '24: Proceedings of the 19th International Symposium
		  on Software Engineering for Adaptive and Self-Managing
		  Systems},
  year		= {2024},
  isbn		= {9798400705854},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lisbon, AA, Portugal}
}

@InProceedings{	  10.1145/3652620.3688197,
  author	= {Silva Mercado, Jonathan},
  title		= {AI Assisted Domain Modeling Explainability and
		  Traceability},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688197},
  doi		= {10.1145/3652620.3688197},
  abstract	= {Domain Models are abstract representations of selected
		  elements in a domain that is created in a collaborative
		  process between domain and modeler experts. The
		  participants share domain knowledge to conceptualize and
		  reason about the elements that will create the domain
		  models. Through this exchange, a comprehensive and accurate
		  representation of the domain is achieved, ensuring that the
		  model captures the relevant aspects and relationships in
		  the domain. Research in Artificial Intelligence (AI) has
		  explored various methods to assist in the creation of
		  domain models from text using Natural Language Processing
		  (NLP) and Machine Learning (ML). Recent advancements with
		  Large Language Models (LLMs) have shown that it is possible
		  to create domain models using prompting techniques;
		  however, the generated domain models contain errors and
		  remain constrained by the performance of the LLM
		  used.Despite the impressive capabilities of LLMs to create
		  domain models, it is evident that it does not address the
		  needs of domain and modelers experts that participate in
		  the creation of domain models. Every AI technique has its
		  advantages and limitations that must be integrated with
		  human feedback in a collaboration process. Therefore, we
		  propose an approach that incorporates human-AI
		  collaboration supported by AI assistants that follows a
		  dialogue approach to understand the users needs and purpose
		  to suggest relevant models. Our proposal combines symbolic
		  and subsymbolic AI techniques with explainability and
		  traceability of the decisions that assist to create domain
		  models that are relevant for the users.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {130–135},
  numpages	= {6},
  keywords	= {domain modeling, large language models, uncertainty,
		  explainability, traceability},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3679431.3679514,
  author	= {Gao, Xiang and Zhou, Yuanchao},
  title		= {Exploring Computational Visual Interfaces for Artificial
		  Intelligence Language Modeling User Experience among
		  College Students: A Rooted Theoretical Approach},
  year		= {2024},
  isbn		= {9798400709951},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3679431.3679514},
  doi		= {10.1145/3679431.3679514},
  abstract	= {Artificial Intelligence (AI) is an emerging technology
		  with the aim of developing intelligent applications that
		  have broad applications in various fields such as
		  healthcare, education, and design. AI design research is
		  presently in an exploratory phase, yet its influence on
		  computer vision interfaces for subjective user experience
		  is becoming increasingly significant. Focused on Chinese
		  university students, the research delves into AI user
		  experience, emphasizing NLP, HCI, and SU. Data was
		  collected via surveys and interviews, with deep learning
		  techniques aiding data processing. A substantial volume of
		  user data was gathered through user surveys and in-depth
		  interviews, with deep learning techniques employed for data
		  preprocessing and feature extraction. Results show a
		  preference for personalized services and data mining in
		  interface design, while technical features and operational
		  fluency are priorities in programming. Enhancing HCI design
		  can improve operational efficiency and meet individual
		  needs, bolstered by clear visual interfaces and HCI
		  technologies, thus enhancing overall user experience
		  quality and effectiveness.},
  booktitle	= {Proceedings of the 2024 3rd International Symposium on
		  Control Engineering and Robotics},
  pages		= {516–522},
  numpages	= {7},
  location	= {Changsha, China},
  series	= {ISCER '24}
}

@Proceedings{	  10.1145/3655693,
  title		= {EICC '24: Proceedings of the 2024 European
		  Interdisciplinary Cybersecurity Conference},
  year		= {2024},
  isbn		= {9798400716515},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xanthi, Greece}
}

@Proceedings{	  10.1145/3691016,
  title		= {IPICE '24: Proceedings of the 2024 International
		  Conference on Image Processing, Intelligent Control and
		  Computer Engineering},
  year		= {2024},
  isbn		= {9798400710285},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Qingdao, China}
}

@Proceedings{	  10.1145/3672406,
  title		= {IMXw '24: Proceedings of the 2024 ACM International
		  Conference on Interactive Media Experiences Workshops},
  year		= {2024},
  isbn		= {9798400717949},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Stockholm, Sweden}
}

@Article{	  10.1109/taslp.2024.3497586,
  author	= {Ma, Hao and Peng, Zhiyuan and Li, Xu and Shao, Mingjie and
		  Wu, Xixin and Liu, Ju},
  title		= {CLAPSep: Leveraging Contrastive Pre-Trained Model for
		  Multi-Modal Query-Conditioned Target Sound Extraction},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3497586},
  doi		= {10.1109/TASLP.2024.3497586},
  abstract	= {Universal sound separation (USS) aims to extract arbitrary
		  types of sounds from real-world recordings. This can be
		  achieved by language-queried target sound extraction (TSE),
		  which typically consists of two components: a query network
		  that converts user queries into conditional embeddings, and
		  a separation network that extracts the target sound
		  accordingly. Existing methods commonly train models from
		  scratch. As a consequence, substantial data and
		  computational resources are required to make the randomly
		  initialized model comprehend sound events and perform
		  separation accordingly. In this paper, we propose to
		  integrate pre-trained models into TSE models to address the
		  above issue. To be specific, we tailor and adapt the
		  powerful contrastive language-audio pre-trained model
		  (CLAP) for USS, denoted as CLAPSep. CLAPSep also accepts
		  flexible user inputs, taking both positive and negative
		  user prompts of uni- and/or multi-modalities for target
		  sound extraction. These key features of CLAPSep can not
		  only enhance the extraction performance but also improve
		  the versatility of its application. We provide extensive
		  experiments on 5 diverse datasets to demonstrate the
		  superior performance and zero- and few-shot
		  generalizability of our proposed CLAPSep with fast training
		  convergence, surpassing previous methods by a significant
		  margin. Full codes and some audio examples are released for
		  reproduction and evaluation.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= nov,
  pages		= {4945–4960},
  numpages	= {16}
}

@Article{	  10.1145/3700890,
  author	= {Jannach, Dietmar and Zanker, Markus},
  title		= {A Survey on Intent-aware Recommender Systems},
  year		= {2024},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {2},
  url		= {https://doi.org/10.1145/3700890},
  doi		= {10.1145/3700890},
  abstract	= {Many modern online services feature personalized
		  recommendations. A central challenge when providing such
		  recommendations is that the reason why an individual user
		  accesses the service may change from visit to visit or even
		  during an ongoing usage session. To be effective, a
		  recommender system should therefore aim to take the
		  users’ probable intent of using the service at a certain
		  point in time into account. In recent years, researchers
		  have thus started to address this challenge by
		  incorporating intent-awareness into recommender systems.
		  Correspondingly, a number of technical approaches were put
		  forward, including diversification techniques, intent
		  prediction models, or latent intent modeling approaches. In
		  this article, we survey and categorize existing approaches
		  to building the next generation of Intent-Aware Recommender
		  Systems (IARS). Based on an analysis of current evaluation
		  practices, we outline open gaps and possible future
		  directions in this area, which in particular include the
		  consideration of additional interaction signals and
		  contextual information to further improve the effectiveness
		  of such systems.},
  journal	= {ACM Trans. Recomm. Syst.},
  month		= dec,
  articleno	= {23},
  numpages	= {32},
  keywords	= {Recommender systems, intent-awareness, survey}
}

@InProceedings{	  10.1145/3678717.3691305,
  author	= {Yankov, Dragomir and Karatzoglou, Antonios and Zhang,
		  Chiqun and Evans, Mike and Dhifallah, Oussama and Sabau,
		  Florin and Najafabadi, Maryam Mousaarab and Predovic,
		  Goran},
  title		= {Routing As a Relevance System},
  year		= {2024},
  isbn		= {9798400711077},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3678717.3691305},
  doi		= {10.1145/3678717.3691305},
  abstract	= {Searching for directions is one of the most used features
		  of map applications. This paper shares our vision on how
		  Direction Services will change, with LLM-based chat
		  assistants rapidly becoming an integral part of the
		  underlying path search mechanism. We anticipate an influx
		  of more complex, conversational route planning sessions,
		  where users colloquially describe route-related preferences
		  as if they were talking to their personal chauffeur. We
		  envision future systems able to support asks like "avoid
		  the East River tunnel", "take the bridge", or "find me a
		  scenic route around the lake, oh and by the way, I'm
		  driving the EV today". At present, popular map search
		  engines fail in even simple, yet very natural preferences,
		  such as 'take me from A to B via road C'. The reason is
		  mainly twofold, inadequate query understanding and lack of
		  mechanisms in routing to satisfy this type of preferences.
		  The here proposed solution is a novel treatment of routing,
		  one which casts it into an end-to-end 2-layer relevance
		  framework. The framework is capable of performing query
		  understanding for route queries with complex preferences
		  and intents. It treats routes as richly annotated documents
		  and the routing engine, in addition to performing
		  optimization, acts (1) as a retriever of route documents
		  that match the user intent and (2) as a ranker that ranks
		  route candidates not just by a simple time-distance cost
		  model, but by inferring the importance of many variables,
		  some derived from explicitly stated preferences and others
		  identified as relevant through data-driven methodology.},
  booktitle	= {Proceedings of the 32nd ACM International Conference on
		  Advances in Geographic Information Systems},
  pages		= {613–616},
  numpages	= {4},
  keywords	= {Optimal Path, Query Understanding, Ranking, Retrieval,
		  Routing},
  location	= {Atlanta, GA, USA},
  series	= {SIGSPATIAL '24}
}

@InProceedings{	  10.1145/3641343.3641422,
  author	= {Wang, Meilin},
  title		= {Design of Automatic Translation System for English for
		  Special Purpose in Agriculture Based on Neural Machine
		  Translation},
  year		= {2024},
  isbn		= {9798400716775},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3641343.3641422},
  doi		= {10.1145/3641343.3641422},
  abstract	= {Agricultural terms have some unique characteristics, which
		  make them need special treatment in machine translation.
		  Agriculture is a highly specialized field, with a large
		  number of specialized terms and concepts, which are not
		  common in general texts. Neural Machine Translation (NMT)
		  technology has made remarkable progress in recent years,
		  and has been successful in various fields. The purpose of
		  this thesis is to design and develop an automatic English
		  translation system for special purposes in agricultural
		  field based on NMT, so as to meet the translation needs of
		  professional knowledge in agricultural field. Based on the
		  embedded environment, the system software is designed. By
		  using the sample training mechanism of deep learning
		  algorithm and combining the characteristics of agricultural
		  terminology translation, the system interaction and
		  translation data are trained separately. By redesigning the
		  interaction hardware, the hardware structure of the
		  translation system is completely defined. The practical
		  application test results with the comparison system show
		  that the translation system designed by the deep learning
		  algorithm has the characteristics of high efficiency, high
		  translation accuracy and good stability in the interactive
		  translation of agricultural terms.},
  booktitle	= {Proceedings of the 3rd International Conference on
		  Electronic Information Technology and Smart Agriculture},
  pages		= {390–394},
  numpages	= {5},
  keywords	= {Agriculture, Automatic Translation System, Neural Machine
		  Translation},
  location	= {Sanya, China},
  series	= {ICEITSA '23}
}

@InProceedings{	  10.1145/3658644.3690321,
  author	= {Zhan, Yuxia and Meng, Yan and Zhou, Lu and Xiong, Yichang
		  and Zhang, Xiaokuan and Ma, Lichuan and Chen, Guoxing and
		  Pei, Qingqi and Zhu, Haojin},
  title		= {VPVet: Vetting Privacy Policies of Virtual Reality Apps},
  year		= {2024},
  isbn		= {9798400706363},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3658644.3690321},
  doi		= {10.1145/3658644.3690321},
  abstract	= {Virtual reality (VR) apps can harvest a wider range of
		  user data than web/mobile apps running on personal
		  computers or smartphones. Existing law and privacy
		  regulations emphasize that VR developers should inform
		  users of what data are collected/used/shared (CUS) through
		  privacy policies. However, privacy policies in the VR
		  ecosystem are still in their early stages, and many
		  developers fail to write appropriate privacy policies that
		  comply with regulations and meet user expectations. In this
		  paper, we propose VPVet to automatically vet privacy policy
		  compliance issues for VR apps. VPVet first analyzes the
		  availability and completeness of a VR privacy policy and
		  then refines its analysis based on three key criteria:
		  granularity, minimization, and consistency of CUS
		  statements. Our study establishes the first and currently
		  largest VR privacy policy dataset named VRPP, consisting of
		  privacy policies of 11,923 different VR apps from 10
		  mainstream platforms. Our vetting results reveal severe
		  privacy issues within the VR ecosystem, including the
		  limited availability and poor quality of privacy policies,
		  along with their coarse granularity, lack of adaptation to
		  VR traits and the inconsistency between CUS statements in
		  privacy policies and their actual behaviors. We open-source
		  VPVet system along with our findings at repository
		  https://github.com/kalamoo/PPAudit, aiming to raise
		  awareness within the VR community and pave the way for
		  further research in this field.},
  booktitle	= {Proceedings of the 2024 on ACM SIGSAC Conference on
		  Computer and Communications Security},
  pages		= {1746–1760},
  numpages	= {15},
  keywords	= {privacy policy analysis, vetting, virtual reality
		  applications},
  location	= {Salt Lake City, UT, USA},
  series	= {CCS '24}
}

@Proceedings{	  10.1145/3603273,
  title		= {AAIA '23: Proceedings of the 2023 International Conference
		  on Advances in Artificial Intelligence and Applications},
  year		= {2023},
  isbn		= {9798400708268},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Wuhan, China}
}

@Proceedings{	  10.1145/3676581,
  title		= {CCCAI '24: Proceedings of the 2024 2nd International
		  Conference on Communications, Computing and Artificial
		  Intelligence},
  year		= {2024},
  isbn		= {9798400716898},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Jeju, Republic of Korea}
}

@Proceedings{	  10.1145/3661725,
  title		= {CMLDS '24: Proceedings of the International Conference on
		  Computing, Machine Learning and Data Science},
  year		= {2024},
  isbn		= {9798400716393},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Singapore, Singapore}
}

@Proceedings{	  10.1145/3631908,
  title		= {ICACS '23: Proceedings of the 7th International Conference
		  on Algorithms, Computing and Systems},
  year		= {2023},
  isbn		= {9798400709098},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Larissa, Greece}
}

@Article{	  10.14778/3659437.3659461,
  author	= {Kayali, Moe and Lykov, Anton and Fountalis, Ilias and
		  Vasiloglou, Nikolaos and Olteanu, Dan and Suciu, Dan},
  title		= {Chorus: Foundation Models for Unified Data Discovery and
		  Exploration},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {VLDB Endowment},
  volume	= {17},
  number	= {8},
  issn		= {2150-8097},
  url		= {https://doi.org/10.14778/3659437.3659461},
  doi		= {10.14778/3659437.3659461},
  abstract	= {We apply foundation models to data discovery and
		  exploration tasks. Foundation models are large language
		  models (LLMS) that show promising performance on a range of
		  diverse tasks unrelated to their training. We show that
		  these models are highly applicable to the data discovery
		  and data exploration domain. When carefully used, they have
		  superior capability on three representative tasks:
		  table-class detection, column-type annotation and
		  join-column prediction. On all three tasks, we show that a
		  foundation-model-based approach outperforms the
		  task-specific models and so the state of the art. Further,
		  our approach often surpasses human-expert task performance.
		  We investigate the fundamental characteristics of this
		  approach including generalizability to several foundation
		  models and the impact of non-determinism on the outputs.
		  All in all, this suggests a future direction in which
		  disparate data management tasks can be unified under
		  foundation models.},
  journal	= {Proc. VLDB Endow.},
  month		= apr,
  pages		= {2104–2114},
  numpages	= {11}
}

@Article{	  10.1145/3643885,
  author	= {Wan, Qizhi and Wan, Changxuan and Xiao, Keli and Xiong,
		  Hui and Liu, Dexi and Liu, Xiping and Hu, Rong},
  title		= {Token-Event-Role Structure-Based Multi-Channel
		  Document-Level Event Extraction},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {42},
  number	= {4},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3643885},
  doi		= {10.1145/3643885},
  abstract	= {Document-level event extraction is a long-standing
		  challenging information retrieval problem involving a
		  sequence of sub-tasks: entity extraction, event type
		  judgment, and event type-specific multi-event extraction.
		  However, addressing the problem as multiple learning tasks
		  leads to increased model complexity. Also, existing methods
		  insufficiently utilize the correlation of entities crossing
		  different events, resulting in limited event extraction
		  performance. This article introduces a novel framework for
		  document-level event extraction, incorporating a new data
		  structure called token-event-role and a multi-channel
		  argument role prediction module. The proposed data
		  structure enables our model to uncover the primary role of
		  tokens in multiple events, facilitating a more
		  comprehensive understanding of event relationships. By
		  leveraging the multi-channel prediction module, we
		  transform entity and multi-event extraction into a single
		  task of predicting token–event pairs, thereby reducing
		  the overall parameter size and enhancing model efficiency.
		  The results demonstrate that our approach outperforms the
		  state-of-the-art method by 9.5 percentage points in terms
		  of the F1 score, highlighting its superior performance in
		  event extraction. Furthermore, an ablation study confirms
		  the significant value of the proposed data structure in
		  improving event extraction tasks, further validating its
		  importance in enhancing the overall performance of the
		  framework.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= mar,
  articleno	= {104},
  numpages	= {27},
  keywords	= {Document-level event extraction, token-event-role data
		  structure, joint learning, multi-channel, neural network}
}

@Proceedings{	  10.1145/3644116,
  title		= {ISAIMS '23: Proceedings of the 2023 4th International
		  Symposium on Artificial Intelligence for Medicine Science},
  year		= {2023},
  isbn		= {9798400708138},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Chengdu, China}
}

@Proceedings{	  10.1145/3674805,
  title		= {ESEM '24: Proceedings of the 18th ACM/IEEE International
		  Symposium on Empirical Software Engineering and
		  Measurement},
  year		= {2024},
  isbn		= {9798400710476},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Barcelona, Spain}
}

@Proceedings{	  10.1145/3689236,
  title		= {ICCSIE '24: Proceedings of the 2024 9th International
		  Conference on Cyber Security and Information Engineering},
  year		= {2024},
  isbn		= {9798400718137},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1109/taslp.2024.3490373,
  author	= {Li, Ren and Xiao, Qiao and Yang, Jianxi and Zhang, Luyi
		  and Chen, Yu},
  title		= {MRC-PASCL: A Few-Shot Machine Reading Comprehension
		  Approach via Post-Training and Answer Span-Oriented
		  Contrastive Learning},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3490373},
  doi		= {10.1109/TASLP.2024.3490373},
  abstract	= {The rapid development of pre-trained language models
		  (PLMs) has significantly enhanced the performance of
		  machine reading comprehension (MRC). Nevertheless, the
		  traditional fine-tuning approaches necessitate extensive
		  labeled data. MRC remains a challenging task in the
		  few-shot settings or low-resource scenarios. This study
		  proposes a novel few-shot MRC approach via post-training
		  and answer span-oriented contrastive learning, termed
		  MRC-PASCL. Specifically, in the post-training module, a
		  novel noun-entity-aware data selection and generation
		  strategy is proposed according to characteristics of MRC
		  task and data, focusing on masking nouns and named entities
		  in the context. In terms of fine-tuning, the proposed
		  answer span-oriented contrastive learning manner selects
		  spans around the golden answers as negative examples, and
		  performs multi-task learning together with the standard MRC
		  answer prediction task. Experimental results show that
		  MRC-PASCL outperforms the PLMs-based baseline models and
		  the 7B and 13B large language models (LLMs) cross most MRQA
		  2019 datasets. Further analyses show that our approach
		  achieves better inference efficiency with lower
		  computational resource requirement. The analysis results
		  also indicate that the proposed method can better adapt to
		  the domain-specific scenarios.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= oct,
  pages		= {4838–4849},
  numpages	= {12}
}

@InProceedings{	  10.1145/3695080.3695170,
  author	= {Zhao, Yu and Ding, Zhaoyun and Wang, Fei and Zou, Longyin
		  and Nian, Aixin},
  title		= {Integrating Entities in Text Summarization: A Review},
  year		= {2024},
  isbn		= {9798400710223},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3695080.3695170},
  doi		= {10.1145/3695080.3695170},
  abstract	= {Advancements in pretrained and large language models have
		  significantly propelled the development of text
		  summarization in recent years, making the generation of
		  fluent and readable text possible. However, issues related
		  to factuality, faithfulness, and controllability have
		  limited the application of this technology in specific
		  contexts such as news, medical, and reviews, hindering its
		  adoption. To address these challenges, the scholarly
		  attention has increasingly focused on more effective and
		  feasible summarization methods, particularly emphasizing
		  the enhancement effects of entities in text summarization.
		  Although a growing number of studies have integrated
		  entities as key elements within summarization systems,
		  there is still a lack of comprehensive reviews on the
		  methods for integrating entities into text summarization
		  systems, analyzing the challenges faced and anticipating
		  future research directions. Therefore, we have
		  systematically organized and thoroughly examined recent
		  studies in this area. We categorize various entity
		  integration techniques according to the stages of the text
		  summarization process, conduct an in-depth analysis and
		  comprehensive evaluation of these techniques at each stage.
		  Based on our findings, we identify the current effects and
		  limitations of research on the integration of entity
		  analysis with text summarization and discuss potential
		  future research directions.},
  booktitle	= {Proceedings of the 2024 International Conference on Cloud
		  Computing and Big Data},
  pages		= {521–528},
  numpages	= {8},
  location	= {Dali, China},
  series	= {ICCBD '24}
}

@Proceedings{	  10.1145/3646547,
  title		= {IMC '24: Proceedings of the 2024 ACM on Internet
		  Measurement Conference},
  year		= {2024},
  isbn		= {9798400705922},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to ACM IMC 2024 in Madrid!We are thrilled to host
		  you in the vibrant city of Madrid, a historical and
		  cultural hub, for this year's ACM Internet Measurement
		  Conference. As we gather in the iconic Espacio
		  Telef\'{o}nica to discuss the latest advances in network
		  measurement, we continue to face evolving challenges and
		  opportunities in this dynamic research field. IMC 2024
		  maintains a strong focus on reproducibility and data
		  sharing, aiming to foster transparency and collaboration
		  within our community.This year, we are privileged to have
		  an exceptional program curated by our Program Chairs, Dave
		  Levin (University of Maryland) and Cristel Pelsser (KU
		  Louvain). Their hard work, along with contributions from
		  all members of the organizing committee, ensures a
		  high-quality program with 55 insightful paper
		  presentations, 45 posters, and discussions, covering topics
		  from Cellular and wireless networks to Security &amp;
		  Privacy. The technical program is enriched by an opening
		  keynote from Prof. Alan Mislove and several social and
		  cultural activities, including a guided tour of the
		  renowned art collection at the Thyssen-Bornemisza Museum.
		  Together, these activities will not only advance our
		  understanding of network performance, structure, and
		  behavior but also foster new collaborations among community
		  members.We are particularly proud of our efforts to promote
		  diversity and inclusivity at IMC 2024, offering student
		  travel grants, diversity grants, and expanding outreach to
		  members of our global community, particularly in developing
		  countries. These initiatives aim to ensure a more diverse,
		  inclusive, and representative group of participants,
		  enriching the conference with varied perspectives.},
  location	= {Madrid, Spain}
}

@Article{	  10.1145/3688398,
  author	= {Gabbolini, Giovanni and Bridge, Derek},
  title		= {Surveying More Than Two Decades of Music Information
		  Retrieval Research on Playlists},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {15},
  number	= {6},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3688398},
  doi		= {10.1145/3688398},
  abstract	= {In this article, we present an extensive survey of music
		  information retrieval (MIR) research into music playlists.
		  Our survey spans more than 20 years, and includes around
		  300 papers about playlists, with over 70 supporting
		  sources. It is the first survey that is self-contained in
		  the sense that it combines all the different MIR research
		  into playlists. It embraces topics such as algorithms for
		  automatic generation, for automatic continuation, for
		  assisting with manual generation, for tagging and for
		  captioning. It looks at manually constructed playlists,
		  both those that are constructed for and by individuals and
		  those constructed in collaboration with others. It covers
		  ground-breaking research into enhancing playlists by
		  cross-fading consecutive songs and by interleaving
		  consecutive songs with speech, similar to what happens on a
		  radio show. Most significantly, it is the first survey that
		  can fully incorporate the paradigm shift that has taken
		  place in the way people consume recorded music: the shift
		  from physical media to music streaming. This has wrought
		  profound changes in the size of music collections available
		  to listeners and thus the algorithms that support the
		  construction, curation and presentation of playlists and
		  the methods adopted by users when they also construct,
		  curate and listen to playlists.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= nov,
  articleno	= {114},
  numpages	= {68},
  keywords	= {playlists, music, information retrieval, recommender
		  systems}
}

@InProceedings{	  10.1145/3627673.3679599,
  author	= {Xi, Yunjia and Liu, Weiwen and Lin, Jianghao and Chen, Bo
		  and Tang, Ruiming and Zhang, Weinan and Yu, Yong},
  title		= {MemoCRS: Memory-enhanced Sequential Conversational
		  Recommender Systems with Large Language Models},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3679599},
  doi		= {10.1145/3627673.3679599},
  abstract	= {Conversational recommender systems (CRSs) aim to capture
		  user preferences and provide personalized recommendations
		  through multi-round natural language dialogues. However,
		  most existing CRS models mainly focus on dialogue
		  comprehension and preferences mining from the current
		  dialogue session, overlooking user preferences in
		  historical dialogue sessions. The preferences embedded in
		  historical sessions and the current session exhibit
		  continuity and sequentiality, and we refer to such CRSs as
		  sequential CRSs. In this work, we leverage memory-enhanced
		  LLMs to model the preference continuity, addressing two key
		  issues: (1) redundancy and noise in historical dialogue
		  sessions, and (2) the cold-start users problem. Thus, we
		  propose a &lt;u&gt;Memo&lt;/u&gt;ry-enhanced
		  &lt;u&gt;C&lt;/u&gt;onversational
		  &lt;u&gt;R&lt;/u&gt;ecommender &lt;u&gt;S&lt;/u&gt;ystem
		  Framework with Large Language Models (dubbed MemoCRS),
		  consisting of user-specific memory and general memory.
		  User-specific memory is tailored to each user's interests
		  and uses an entity-based memory bank to refine preferences
		  and retrieve relevant memory, thereby reducing the
		  redundancy and noise of historical sessions. The general
		  memory, encapsulating collaborative knowledge and reasoning
		  guidelines, can provide shared knowledge for users,
		  especially cold-start users. With the above memory, LLMs
		  are empowered to deliver more precise and tailored
		  recommendations for each user. Extensive experiments on
		  Chinese and English datasets demonstrate MemoCRS's
		  effectiveness.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {2585–2595},
  numpages	= {11},
  keywords	= {conversational recommendation, large language models,
		  memory},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Proceedings{	  10.1145/3638884,
  title		= {ICCIP '23: Proceedings of the 2023 9th International
		  Conference on Communication and Information Processing},
  year		= {2023},
  isbn		= {9798400708909},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lingshui, China}
}

@Proceedings{	  10.1145/3685088,
  title		= {ICSCIS '24: Proceedings of the 2024 International
		  Conference on Smart City and Information System},
  year		= {2024},
  isbn		= {9798400710155},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Kuala Lumpur, Malaysia}
}

@InProceedings{	  10.1145/3663529.3663861,
  author	= {Goel, Drishti and Husain, Fiza and Singh, Aditya and
		  Ghosh, Supriyo and Parayil, Anjaly and Bansal, Chetan and
		  Zhang, Xuchao and Rajmohan, Saravan},
  title		= {X-Lifecycle Learning for Cloud Incident Management using
		  LLMs},
  year		= {2024},
  isbn		= {9798400706585},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3663529.3663861},
  doi		= {10.1145/3663529.3663861},
  abstract	= {Incident management for large cloud services is a complex
		  and tedious process that requires a significant amount of
		  manual effort from on-call engineers (OCEs). OCEs typically
		  leverage data from different stages of the software
		  development lifecycle [SDLC] (e.g., codes, configuration,
		  monitor data, service properties, service dependencies,
		  trouble-shooting documents, etc.) to generate insights for
		  detection, root cause analysis and mitigation of incidents.
		  Recent advancements in large language models [LLMs] (e.g.,
		  ChatGPT, GPT-4, Gemini) have created opportunities to
		  automatically generate contextual recommendations for the
		  OCEs, assisting them in quickly identifying and mitigating
		  critical issues. However, existing research typically takes
		  a silo-ed view of solving a certain task in incident
		  management by leveraging data from a single stage of the
		  SDLC. In this paper, we demonstrate that augmenting
		  additional contextual data from different stages of the
		  SDLC improves the performance of two critically important
		  and practically challenging tasks: (1) automatically
		  generating root cause recommendations for dependency
		  failure related incidents, and (2) identifying the ontology
		  of service monitors used for automatically detecting
		  incidents. By leveraging a dataset of 353 incidents and 260
		  monitors from Microsoft, we demonstrate that augmenting
		  contextual information from different stages of the SDLC
		  improves the performance over state-of-the-art methods.},
  booktitle	= {Companion Proceedings of the 32nd ACM International
		  Conference on the Foundations of Software Engineering},
  pages		= {417–428},
  numpages	= {12},
  keywords	= {Cloud Services, Large language models, Monitor management,
		  Reliability, Root-cause analysis},
  location	= {Porto de Galinhas, Brazil},
  series	= {FSE 2024}
}

@Proceedings{	  10.1145/3638550,
  title		= {HotMobile '24: Proceedings of the 25th International
		  Workshop on Mobile Computing Systems and Applications},
  year		= {2024},
  isbn		= {9798400704970},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {San Diego, CA, USA}
}

@Article{	  10.1109/taslp.2024.3378108,
  author	= {Xiang, Chunli and Zhang, Junchi and Zhou, Jun and Li, Fei
		  and Teng, Chong and Ji, Donghong},
  title		= {Phrase-Aware Financial Sentiment Analysis Based on
		  Constituent Syntax},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3378108},
  doi		= {10.1109/TASLP.2024.3378108},
  abstract	= {Financial sentiment analysis is a fine-grained sentiment
		  analysis task that needs to predict the sentiment value
		  toward a given target entity. Recently, dependency-based
		  graph neural networks have been introduced for target-based
		  sentiment analysis. However, financial sentiment analysis
		  with implicit sentiment expression is more challenging than
		  target-based explicit sentiment analysis, requiring a deep
		  understanding of the complex association between the
		  sentiment clue in context and the target entity. In
		  previous work related to financial sentiment analysis, most
		  methods focused on learning the simple word-to-word
		  relations between the contextual words and the target
		  entity based on the dependency tree of the sentence,
		  ignoring the exploitation of span-boundary information and
		  phrase-level syntactic knowledge with regard to the target
		  entity. In this paper, we perform financial implicit
		  sentiment analysis by taking phrases as basic semantic
		  units and proposing a graph attention network
		  (&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$PhraseGAT$&lt;/tex-math&gt;&lt;/inline-formula&gt;)
		  based on the constituent tree to leverage the phrase
		  syntactic knowledge. To enhance the information flow
		  between the nodes in the graph, we construct a
		  heterogeneous graph based on the constituent tree and
		  encode higher-order neighbor information. In addition, we
		  introduce a multi-edge-type graph attention network
		  (&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$MET$&lt;/tex-math&gt;&lt;/inline-formula&gt;-&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$GAT$&lt;/tex-math&gt;&lt;/inline-formula&gt;)
		  to take full consideration of syntax and semantic
		  interactions for the final prediction on the sentiment
		  value of the target entity. Our proposed approach achieves
		  85.56% and 84.37% cosine similarity on public benchmark
		  HEADLINE and MICROBLOG datasets, outperforms several strong
		  baselines and achieves new state-of-the-art performance,
		  verifying its effectiveness.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= mar,
  pages		= {1994–2005},
  numpages	= {12}
}

@Article{	  10.1145/3661485,
  author	= {Huang, Yinqiu and Gao, Min and Shu, Kai and Lin, Chenghua
		  and Wang, Jia and Zhou, Wei},
  title		= {EML: Emotion-Aware Meta Learning for Cross-Event False
		  Information Detection},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {8},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3661485},
  doi		= {10.1145/3661485},
  abstract	= {Modern social media’s development has dramatically
		  changed how people obtain information. However, the wide
		  dissemination of various false information has severe
		  detrimental effects. Accordingly, many deep learning-based
		  methods have been proposed to detect false information and
		  achieve promising results. However, these methods are
		  unsuitable for new events due to the extremely limited
		  labeled data and their discrepant data distribution to
		  existing events. Domain adaptation methods have been
		  proposed to mitigate these problems. However, their
		  performance is suboptimal because they are not sensitive to
		  new events due to they aim to align the domain information
		  between existing events, and they hardly capture the
		  fine-grained difference between real and fake claims by
		  only using semantic information. Therefore, we propose a
		  novel Emotion-aware Meta Learning (EML) approach for
		  cross-event false information early detection, which deeply
		  integrates emotions in meta learning to find
		  event-sensitive initialization parameters that quickly
		  adapt to new events. EML is non-trivial and faces three
		  challenges: (1) How to effectively model semantic and
		  emotional features to capture fine-grained differences? (2)
		  How to reduce the impact of noise in meta learning based on
		  semantic and emotional features? (3) How to detect the
		  false information in a zero-shot detection scenario, i.e.,
		  no labeled data for new events? To tackle these challenges,
		  firstly, we construct the emotion-aware meta tasks by
		  selecting claims with similar and opposite emotions to the
		  target claim other than usually used random sampling.
		  Secondly, we propose a task weighting method and
		  event-adaptation meta tasks to further improve the
		  model’s robustness and generalization ability for
		  detecting new events. Finally, we propose a weak label
		  annotation method to extend EML to zero-shot detection
		  according to the calculated labels’ confidence. Extensive
		  experiments on real-world datasets show that the EML
		  achieves superior performances on false information
		  detection for new events.},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= jul,
  articleno	= {185},
  numpages	= {25},
  keywords	= {False information detection, Meta learning, Emotional
		  feature extraction}
}

@Article{	  10.1145/3676959,
  author	= {Jiang, Yanjie and Liu, Hui and Cheung, Shing Chi and
		  Zhang, Lu},
  title		= {Shortening Overlong Method Names with Abbreviations},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {8},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3676959},
  doi		= {10.1145/3676959},
  abstract	= {Methods should be named to summarize their
		  responsibilities meaningfully. When a method has a
		  non-trivial responsibility, it may require a naming using
		  multiple words. However, overlong method names are
		  susceptible to typos and reduced readability (e.g.,
		  displaying a statement partially in standard screen width
		  or splitting it into multiple lines). Programming naming
		  conventions commonly adopt a maximal length (in characters)
		  for identifiers. In practice, developers may not
		  necessarily find a meaningful name that follows such naming
		  conventions when coding a non-trivial method. This article
		  presents the first automated technique (called
		  NameCompressor) to shorten overlong method names. Our
		  inspiration is that many lengthy words/phrases in an
		  overlong method name have known and unambiguous
		  abbreviations. The use of these abbreviations for method
		  names is common. To shorten an overlong method name,
		  NameCompressor employs three compression techniques, i.e.,
		  context-aware compression, probability-based compression,
		  and machine learning-based compression, to find appropriate
		  abbreviations for the words/phrases in the method name. We
		  evaluate NameCompressor on a dataset of 700 overlong method
		  names. It correctly generates 613 short names identical to
		  those specified by the developers of these methods.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= nov,
  articleno	= {205},
  numpages	= {24},
  keywords	= {Method name, abbreviation, identifier, code quality}
}

@Article{	  10.1145/3687300,
  author	= {Al-Sada, Bader and Sadighian, Alireza and Oligeri,
		  Gabriele},
  title		= {MITRE ATT&amp;CK: State of the Art and Way Forward},
  year		= {2024},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {1},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3687300},
  doi		= {10.1145/3687300},
  abstract	= {MITRE ATT&amp;CK is a comprehensive framework of adversary
		  tactics, techniques, and procedures based on real-world
		  observations. It has been used as a foundation for threat
		  modeling in different sectors, such as government,
		  academia, and industry. To the best of our knowledge, no
		  previous work has been devoted to the comprehensive
		  collection, study, and investigation of the current state
		  of the art leveraging the MITRE ATT&amp;CK framework. We
		  select and inspect more than 50 major research
		  contributions, while conducting a detailed analysis of
		  their methodology and objectives in relation to the MITRE
		  ATT&amp;CK framework. We provide a categorization of the
		  identified papers according to different criteria such as
		  use cases, application scenarios, adopted methodologies,
		  and the use of additional data. Finally, we discuss open
		  issues and future research directions involving not only
		  the MITRE ATT&amp;CK framework but also the fields of
		  threat analysis, threat modeling, and in general
		  cyber-threat intelligence.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {12},
  numpages	= {37},
  keywords	= {MITRE ATT&amp;CK framework, cyber-threat intelligence,
		  security risk analysis}
}

@Article{	  10.1145/3708532,
  author	= {H\"{a}m\"{a}l\"{a}inen, Joonas and Das, Teerath and
		  Mikkonen, Tommi},
  title		= {A Systematic Literature Review of Multi-Label Learning in
		  Software Engineering},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3708532},
  doi		= {10.1145/3708532},
  abstract	= {In this paper, we provide the first systematic literature
		  review of the intersection of two research areas,
		  Multi-Label Learning (MLL) and Software Engineering (SE).
		  We refer to this intersection as MLL4SE. In recent years,
		  MLL problems have increased in many applications and
		  research areas because real-world datasets often have a
		  multi-label nature. For multi-label data, simplifying the
		  assumption of traditional classification approaches that an
		  instance can only be associated with one class only leads
		  to worse accuracy. Thus, a better match of methods and
		  assumptions about the data is required. We identified 50
		  primary studies in our systematic literature review in the
		  MLL4SE domain. Based on this review, we identified six main
		  SE application domains where MLL has been applied. These
		  domains include Software Requirement Engineering, Issue
		  Tracking and Management, Community and Knowledge
		  Management, API Usage and Management, Code Quality and
		  Maintenance, and Mobile Application Development. We
		  summarized the methods used and the data nature of the
		  MLL4SE applications. Moreover, we separately provide
		  taxonomies of future work directions from machine learning
		  and software engineering perspectives. In general, we
		  highlight current trends, research gaps, and
		  shortcomings.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= dec,
  keywords	= {Machine Learning, Multi-Label Learning, Software
		  Engineering, Systematic Literature Review, Software
		  Development Life Cycle (SDLC) Activities}
}

@Article{	  10.1145/3631430,
  author	= {Rahman, Wasifur and Abdelkader, Abdelrahman and Lee,
		  Sangwu and Yang, Phillip and Islam, Md Saiful and Adnan,
		  Tariq and Hasan, Masum and Wagner, Ellen and Park, Sooyong
		  and Dorsey, E. Ray and Schwartz, Catherine and Jaffe, Karen
		  and Hoque, Ehsan},
  title		= {A User-Centered Framework to Empower People with
		  Parkinson's Disease},
  year		= {2024},
  issue_date	= {December 2023},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {7},
  number	= {4},
  url		= {https://doi.org/10.1145/3631430},
  doi		= {10.1145/3631430},
  abstract	= {We present a user-centric validation of a teleneurology
		  platform, assessing its effectiveness in conveying
		  screening information, facilitating user queries, and
		  offering resources to enhance user empowerment. This
		  validation process is implemented in the setting of
		  Parkinson's disease (PD), in collaboration with a neurology
		  department of a major medical center in the USA. Our
		  intention is that with this platform, anyone globally with
		  a webcam and microphone-equipped computer can carry out a
		  series of speech, motor, and facial mimicry tasks. Our
		  validation method demonstrates to users a mock PD risk
		  assessment and provides access to relevant resources,
		  including a chatbot driven by GPT, locations of local
		  neurologists, and actionable and scientifically-backed PD
		  prevention and management recommendations. We share
		  findings from 91 participants (48 with PD, 43 without)
		  aimed at evaluating the user experience and collecting
		  feedback. Our framework was rated positively by 80.85%
		  (standard deviation ± 8.92%) of the participants, and it
		  achieved an above-average 70.42 (standard deviation ±
		  13.85) System-Usability-Scale (SUS) score. We also
		  conducted a thematic analysis of open-ended feedback to
		  further inform our future work. When given the option to
		  ask any questions to the chatbot, participants typically
		  asked for information about neurologists, screening
		  results, and the community support group. We also provide a
		  roadmap of how the knowledge generated in this paper can be
		  generalized to screening frameworks for other diseases
		  through designing appropriate recording environments,
		  appropriate tasks, and tailored user-interfaces.},
  journal	= {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month		= jan,
  articleno	= {175},
  numpages	= {29},
  keywords	= {End-to-end framework, Framework Evaluation, Parkinson's
		  Disease}
}

@Proceedings{	  10.1145/3700906,
  title		= {IPMLP '24: Proceedings of the International Conference on
		  Image Processing, Machine Learning and Pattern
		  Recognition},
  year		= {2024},
  isbn		= {9798400707032},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3605098.3636026,
  author	= {Jamil, Hasan and Krawetz, Stephen and Gow, Alexander},
  title		= {Knowledge Synthesis using Large Language Models for a
		  Computational Biology Workflow Ecosystem},
  year		= {2024},
  isbn		= {9798400702433},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3605098.3636026},
  doi		= {10.1145/3605098.3636026},
  abstract	= {An understanding of the molecular basis of musculoskeletal
		  pain is necessary for the development of therapeutics,
		  their management, and possible personalization.
		  One-in-three Americans use OTC pain killers, and one tenth
		  use prescription drugs to manage pain. The CDC also
		  estimates that about 20% Americans suffer from chronic
		  pain. As the experience of acute or chronic pain varies due
		  to individual genetics and physiology, it is imperative
		  that researchers continue to find novel therapeutics to
		  treat or manage symptoms. In this paper, our goal is to
		  develop a seed knowledgebase computational platform, called
		  BioNursery, that will allow biologists to computationally
		  hypothesize, define and test molecular mechanisms
		  underlying pain. In our knowledge ecosystem, we accumulate
		  curated information from users about the relationships
		  among biological databases, analysis tools, and database
		  contents to generate biological analyses modules, called
		  π-graphs, or process graphs. We propose a mapping function
		  from a natural language description of a hypothesized
		  molecular model to a computational workflow for testing in
		  BioNursery. We use a crowd computing feedback and curation
		  system, called Explorer, to improve proposed computational
		  models for molecular mechanism discovery, and growing the
		  knowledge ecosystem. Since the pain knowledge ecosystem
		  does not yet exist, we validate our approach over a similar
		  application in fertility research.},
  booktitle	= {Proceedings of the 39th ACM/SIGAPP Symposium on Applied
		  Computing},
  pages		= {523–530},
  numpages	= {8},
  keywords	= {knowledge ecosystem, crowdsourcing, query reformulation},
  location	= {Avila, Spain},
  series	= {SAC '24}
}

@InProceedings{	  10.1145/3613905.3651035,
  author	= {Kang, Hyeonsu B and Lin, David Chuan-En and Martelaro,
		  Nikolas and Kittur, Aniket and Chen, Yan-Ying and Hong,
		  Matthew K.},
  title		= {BioSpark: An End-to-End Generative System for
		  Biological-Analogical Inspirations and Ideation},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3651035},
  doi		= {10.1145/3613905.3651035},
  abstract	= {Nature often inspires solutions for complex engineering
		  problems, but it is challenging for designers to discover
		  relevant analogies and synthesize from them. Here, we
		  present an end-to-end system, BioSpark, that generates
		  biological-analogical mechanisms and provides an
		  interactive interface for comprehension and ideation. From
		  a small seed set of expert-curated mechanisms, BioSpark’s
		  pipeline iteratively expands them by constructing and
		  traversing organism taxonomies, aiming to overcome both
		  data sparsity in expert curation and limited conceptual
		  diversity in purely automated analogy generation. The
		  interface helps designers recognize and understand relevant
		  analogs to design problems using four interaction features.
		  We conduct an exploratory study with design students to
		  showcase how BioSpark facilitated analogical transfer of
		  ideas but was limited in conveying active ingredients, the
		  core abstraction underpinning how mechanisms work. We
		  discuss this limitation and other implications such as
		  generative hallucination that could facilitate shifts in
		  human exploration of new design spaces.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {61},
  numpages	= {13},
  keywords	= {Analogies, Design Creativity, Diversity-enhanced
		  Generation, Ideation, Large Language Models, Nature},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@Proceedings{	  10.1145/3672758,
  title		= {CAICE '24: Proceedings of the 3rd International Conference
		  on Computer, Artificial Intelligence and Control
		  Engineering},
  year		= {2024},
  isbn		= {9798400716942},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xi' an, China}
}

@Proceedings{	  10.1145/3658644,
  title		= {CCS '24: Proceedings of the 2024 on ACM SIGSAC Conference
		  on Computer and Communications Security},
  year		= {2024},
  isbn		= {9798400706363},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is with great enthusiasm that we, on behalf of the
		  Organizing Committee, invite you to join us for the 31st
		  ACM SIGSAC Conference on Computer and Communications
		  Security (CCS), a premier security and privacy conference
		  where researchers, practitioners, and educators come
		  together to present, learn, and debate research,
		  innovation, and trends in the field of Computer and
		  Communications Security and Privacy.This year, we are proud
		  to introduce our conference theme to be "Inclusion,
		  Mentorship, Community." These three pillars reflect our
		  collective commitment to fostering a vibrant, supportive,
		  and forwardthinking environment within the CCS community.
		  Particularly, we host our inaugural Doctoral Symposium,
		  which offers PhD students a unique platform to receive
		  timely, constructive feedback on their dissertation
		  research from leading experts in our community.
		  Additionally, our first-ever Diversity, Equity, and
		  Inclusion (DEI) Workshop is designed to cultivate a culture
		  that embraces diversity and champions equity in our field.
		  Moreover, understanding the importance of guidance and
		  support, we have organized panels focusing on Student
		  Mentoring, Faculty Mentoring, and Public Service. These
		  panels are designed to facilitate mentorship connections,
		  share valuable experiences, and encourage service that
		  extends the impact of our work beyond academia. These new
		  initiatives are also opportunities to strengthen the bonds
		  within our CCS community.Regarding the main conference,
		  this year's main conference is our largest ever, featuring
		  328 paper presentations that showcase the latest research
		  and developments in our field. We are also honored to have
		  two distinguished keynote speakers: Dr. Dan Boneh and Dr.
		  Gene Tsudik, who will share their invaluable insights and
		  perspectives on pressing topics in security and privacy.
		  Additionally, 18 specialized workshops will take place on
		  the pre-conference and post-conference days, providing
		  platforms for focused discussions and collaborations on
		  numerous specialized topics.},
  location	= {Salt Lake City, UT, USA}
}

@Article{	  10.1145/3623381,
  author	= {Li, Miaoran and Peng, Baolin and Gao, Jianfeng and Zhang,
		  Zhu},
  title		= {OPERA: Harmonizing Task-Oriented Dialogs and Information
		  Seeking Experience},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {4},
  issn		= {1559-1131},
  url		= {https://doi.org/10.1145/3623381},
  doi		= {10.1145/3623381},
  abstract	= {Existing studies in conversational AI mostly treat
		  task-oriented dialog (TOD) and question answering (QA) as
		  separate tasks. Towards the goal of constructing a
		  conversational agent that can complete user tasks and
		  support information seeking, it is important to develop a
		  system that can handle both TOD and QA with access to
		  various external knowledge sources. In this work, we
		  propose a new task, Open-Book TOD (OB-TOD), which combines
		  TOD with QA and expands the external knowledge sources to
		  include both explicit sources (e.g., the web) and implicit
		  sources (e.g., pre-trained language models). We create a
		  new dataset OB-MultiWOZ, where we enrich TOD sessions with
		  QA-like information-seeking experience grounded on external
		  knowledge. We propose a unified model OPERA (Open-book
		  End-to-end Task-oriented Dialog) which can appropriately
		  access explicit and implicit external knowledge to tackle
		  the OB-TOD task. Experimental results show that OPERA
		  outperforms closed-book baselines, highlighting the value
		  of both types of knowledge.1},
  journal	= {ACM Trans. Web},
  month		= oct,
  articleno	= {45},
  numpages	= {27},
  keywords	= {Web search, task-oriented dialog systems, language
		  models}
}

@Proceedings{	  10.1145/3650212,
  title		= {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT
		  International Symposium on Software Testing and Analysis},
  year		= {2024},
  isbn		= {9798400706127},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 33rd edition of the International Symposium
		  on Software Testing and Analysis, ISSTA 2024, held on
		  September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is
		  co-located with ECOOP and MPLR 2024. ISSTA brings together
		  academics, industrial researchers, and practitioners from
		  all over the world working on testing and analyzing
		  software systems.},
  location	= {Vienna, Austria}
}

@Proceedings{	  10.1145/3625468,
  title		= {MMSys '24: Proceedings of the 15th ACM Multimedia Systems
		  Conference},
  year		= {2024},
  isbn		= {9798400704123},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Dear MMSys 2024 Participants,On behalf of the organizers,
		  we are very pleased to welcome you to the 15th ACM
		  Multimedia Systems Conference, taking place for the first
		  time in Italy, in the city of Bari.MMSys is a premier
		  conference dedicated to the exciting and multidisciplinary
		  field of multimedia, with a specific focus on its systems
		  and applications. The conference provides a platform for
		  researchers from both academia and industry to share their
		  latest findings in the multimedia systems research area.
		  Many international researchers, practitioners, engineers,
		  and students from academia, industry, standardization
		  bodies, and government agencies join the MMSys conference
		  each year.},
  location	= {Bari, Italy}
}

@Article{	  10.1145/3630025,
  author	= {Bianchini, Devis and Bono, Carlo and Campi, Alessandro and
		  Cappiello, Cinzia and Ceri, Stefano and De Luzi, Francesca
		  and Mecella, Massimo and Pernici, Barbara and Plebani,
		  Pierluigi},
  title		= {Challenges in AI-supported Process Analysis in the Italian
		  Judicial System: what After Digitalization?},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {5},
  number	= {1},
  url		= {https://doi.org/10.1145/3630025},
  doi		= {10.1145/3630025},
  abstract	= {In this commentary article, we outline research challenges
		  and possible directions for the potential applications of
		  AI in the judicial domain by specifically considering
		  process analysis in the Italian context. Applying AI to
		  process analysis poses several challenges, including
		  information extraction from legacy information systems and
		  analysis of legal documents, process modeling with a
		  particular emphasis on temporal analysis, real-time process
		  monitoring, conformance and compliance checking, predictive
		  techniques for accurate predictions, and analysis of
		  judges’ workload. Solutions to these challenges include
		  methods and tools for data identification and collection,
		  innovative approaches to process modeling, reactive
		  techniques for real-time monitoring, conformance checking
		  with explainability, language models adapted to specific
		  domains, and the identification of suitable indicators for
		  the analysis of case handling efficiency and case
		  classification.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= mar,
  articleno	= {10},
  numpages	= {10},
  keywords	= {Process analysis, process improvement, domain-specific
		  text analysis, temporal analysis}
}

@Article{	  10.1145/3665252.3665262,
  author	= {Doan, AnHai},
  title		= {Technical Perspective: Unicorn: A Unified Multi-Tasking
		  Matching Model},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {53},
  number	= {1},
  issn		= {0163-5808},
  url		= {https://doi.org/10.1145/3665252.3665262},
  doi		= {10.1145/3665252.3665262},
  abstract	= {Data integration has been a long-standing challenge for
		  data management. It has recently received significant
		  attention due to at least three main reasons. First, many
		  data science projects require integrating data from
		  disparate sources before analysis can be carried out to
		  extract insights. Second, many organizations want to build
		  knowledge graphs, such as Customer 360s, Product 360s, and
		  Supplier 360s, which capture all available information
		  about the customers, products, and suppliers of an
		  organization. Building such knowledge graphs often requires
		  integrating data from multiple sources. Finally, there is
		  also an increasing need to integrate a massive amount of
		  data to create training data for AI models, such as large
		  language models.},
  journal	= {SIGMOD Rec.},
  month		= may,
  pages		= {43},
  numpages	= {1}
}

@InProceedings{	  10.1145/3589334.3645512,
  author	= {Kang, SeongKu and Agarwal, Shivam and Jin, Bowen and Lee,
		  Dongha and Yu, Hwanjo and Han, Jiawei},
  title		= {Improving Retrieval in Theme-specific Applications using a
		  Corpus Topical Taxonomy},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645512},
  doi		= {10.1145/3589334.3645512},
  abstract	= {Document retrieval has greatly benefited from the
		  advancements of large-scale pre-trained language models
		  (PLMs). However, their effectiveness is often limited in
		  theme-specific applications for specialized areas or
		  industries, due to unique terminologies, incomplete
		  contexts of user queries, and specialized search intents.
		  To capture the theme-specific information and improve
		  retrieval, we propose to use a corpus topical taxonomy,
		  which outlines the latent topic structure of the corpus
		  while reflecting user-interested aspects. We introduce
		  ToTER (Topical Taxonomy Enhanced Retrieval) framework,
		  which identifies the central topics of queries and
		  documents with the guidance of the taxonomy, and exploits
		  their topical relatedness to supplement missing contexts.
		  As a plug-and-play framework, ToTER can be flexibly
		  employed to enhance various PLM-based retrievers. Through
		  extensive quantitative, ablative, and exploratory
		  experiments on two real-world datasets, we ascertain the
		  benefits of using topical taxonomy for retrieval in
		  theme-specific applications and demonstrate the
		  effectiveness of ToTER.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {1497–1508},
  numpages	= {12},
  keywords	= {document retrieval, theme-specific application, topical
		  taxonomy},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3644523,
  title		= {ICCSMT '23: Proceedings of the 2023 4th International
		  Conference on Computer Science and Management Technology},
  year		= {2023},
  isbn		= {9798400709517},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xi'an, China}
}

@InProceedings{	  10.1145/3637528.3671992,
  author	= {Xiao, Congxi and Zhou, Jingbo and Xiao, Yixiong and Huang,
		  Jizhou and Xiong, Hui},
  title		= {ReFound: Crafting a Foundation Model for Urban Region
		  Understanding upon Language and Visual Foundations},
  year		= {2024},
  isbn		= {9798400704901},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3637528.3671992},
  doi		= {10.1145/3637528.3671992},
  abstract	= {Understanding urban regional characteristics is pivotal in
		  driving critical insights for urban planning and
		  management. We have witnessed the successful application of
		  pre-trained Foundation Models (FMs) in generating universal
		  representations for various downstream tasks. However,
		  applying this principle to the geospatial domain remains
		  challenging, primarily due to the difficulty of gathering
		  extensive data for developing a dedicated urban foundation
		  model. Though there have been some attempts to empower the
		  existing FMs with urban data, most of them focus on
		  single-modality FMs without considering the multi-modality
		  nature of urban region understanding tasks. To address this
		  gap, we introduce ReFound - a novel framework for
		  &lt;u&gt;Re&lt;/u&gt;-training a
		  &lt;u&gt;Found&lt;/u&gt;ation model for urban region
		  understanding, harnessing the strengths of both language
		  and visual FMs. In this framework, we first invent a
		  Mixture-of-Geospatial-Expert (MoGE) Transformer, to
		  effectively integrate the embedding of multi-source
		  geospatial data. Building on this, ReFound is enhanced by
		  jointly distilling knowledge from language, visual, and
		  visual-language FMs respectively, thus augmenting its
		  generalization capabilities. Meanwhile, we design a masked
		  geospatial data modeling approach alongside a cross-modal
		  spatial alignment mechanism, to enhance the spatial
		  knowledge of ReFound derived from geospatial data.
		  Extensive experiments conducted on six real-world datasets
		  over three urban region understanding tasks demonstrate the
		  superior performance of our framework.},
  booktitle	= {Proceedings of the 30th ACM SIGKDD Conference on Knowledge
		  Discovery and Data Mining},
  pages		= {3527–3538},
  numpages	= {12},
  keywords	= {foundation model, multimodal data, urban region
		  understanding},
  location	= {Barcelona, Spain},
  series	= {KDD '24}
}

@Article{	  10.1145/3675781,
  author	= {Pham, Quoc-Hung and Le, Huu-Loi and Dang Nhat, Minh and
		  Tran T., Khang and Tran-Tien, Manh and Dang, Viet-Hung and
		  Vu, Huy-The and Nguyen, Minh-Tien and Phan, Xuan-Hieu},
  title		= {Towards Vietnamese Question and Answer Generation: An
		  Empirical Study},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {9},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3675781},
  doi		= {10.1145/3675781},
  abstract	= {Question-answer generation (QAG) is a challenging task
		  that generates both questions and answers from a given
		  input paragraph context. The QAG task has recently achieved
		  promising results thanks to the appearance of large
		  pre-trained language models, yet, QAG models are mainly
		  implemented in common languages, e.g., English. There still
		  remains a gap in domain and language adaptation of these
		  QAG models to low-resource languages such as Vietnamese. To
		  address the gap, this article presents a large-scale and
		  systematic study of QAG in Vietnamese. To do that, we first
		  implement several QAG models by using the common
		  fine-tuning techniques based on powerful pre-trained
		  language models. We next introduce a set of instructions
		  designed for the QAG task. These instructions are used to
		  fine-tuned the pre-trained language and large language
		  models. Extensive experimental results of both automatic
		  and human evaluation on five benchmark machine reading
		  comprehension datasets show two important points. First,
		  the instruction-tuning method has the potential to enhance
		  the performance of QAG models. Second, large language
		  models trained in English need more data for fine-tuning to
		  work well on the downstream QAG tasks of low-resource
		  languages. We also provide a prototype system to
		  demonstrate how our QAG models actually work. The code for
		  fine-tuning QAG models and instructions are also made
		  available.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  articleno	= {132},
  numpages	= {28},
  keywords	= {Natural language processing, question and answer
		  generation, large pre-trained language models, instruction
		  fine-tuning, BARTPho, ViT5, LlaMa2}
}

@InProceedings{	  10.1145/3686215.3689201,
  author	= {Porfirio, Rui Pedro and Santos, Pedro Albuquerque and
		  Madeira, Rui Neves},
  title		= {Enhancing Digital Agriculture with XAI: Case Studies on
		  Tabular Data and Future Directions},
  year		= {2024},
  isbn		= {9798400704635},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3686215.3689201},
  doi		= {10.1145/3686215.3689201},
  abstract	= {Given the pivotal role of agriculture in ensuring food
		  security, fostering economic stability, and addressing
		  environmental sustainability, the sector has increasingly
		  embraced smart farming solutions to respond to recent
		  climate and societal challenges, such as rising water and
		  food demands. These solutions provide actionable insights
		  crucial for decision-making, enabling farm stakeholders to
		  optimize resources, improve yields, and mitigate risks.
		  However, the complexity of the predictive models often
		  associated with this type of solutions results in a lack of
		  transparency, hindering trust and adoption. To respond to
		  such challenges, this paper explores the application of
		  explainable AI (XAI) techniques to agriculture tabular
		  data. Specifically, we focus on two case studies: wheat
		  yield prediction and grapes produced for wine purposes
		  yield prediction. Through these case studies, we propose
		  initial contributions on how XAI techniques can be applied
		  in the context of agriculture and how generated
		  explanations can be adapted to the users’ level of
		  expertise. Finally, as part of ongoing and future research
		  directions, we introduce AgriUXE (Agricultural eXperience
		  Enhanced through eXplainability), a novel user-centered
		  digital platform designed to augment the explainability of
		  multimodal data and machine learning model predictions for
		  sustainable smart farming solutions. By providing
		  transparent, data-driven decisions and generating
		  user-adaptive explanations, AgriUXE aims to support the
		  optimization of the user experience within these
		  solutions.},
  booktitle	= {Companion Proceedings of the 26th International Conference
		  on Multimodal Interaction},
  pages		= {211–217},
  numpages	= {7},
  keywords	= {Digital Agriculture, Explainable AI, Human-Computer
		  Interaction, Machine Learning, Multimodal Systems},
  location	= {San Jose, Costa Rica},
  series	= {ICMI Companion '24}
}

@InProceedings{	  10.1145/3691620.3695019,
  author	= {Zhao, Jiuang and Yang, Zitian and Zhang, Li and Lian,
		  Xiaoli and Yang, Donghao and Tan, Xin},
  title		= {DRMiner: Extracting Latent Design Rationale from Jira
		  Issue Logs},
  year		= {2024},
  isbn		= {9798400712487},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3691620.3695019},
  doi		= {10.1145/3691620.3695019},
  abstract	= {Software architectures are usually meticulously designed
		  to address multiple quality concerns and support long-term
		  maintenance. However, there may be a lack of motivation for
		  developers to document design rationales (i.e., the design
		  alternatives and the underlying arguments for making or
		  rejecting decisions) when they will not gain immediate
		  benefit, resulting in a lack of standard capture of these
		  rationales. With the turnover of developers, the
		  architecture inevitably becomes eroded. This issue has
		  motivated a number of studies to extract design knowledge
		  from open-source communities in recent years.
		  Unfortunately, none of the existing research has
		  successfully extracted solutions alone with their
		  corresponding arguments due to challenges such as the
		  intricate semantics of online discussions and the lack of
		  benchmarks for design rationale extraction.In this paper,
		  we propose a novel approach, named DRMiner, to
		  automatically mine latent design rationales from
		  developers' live discussion in open-source community (i.e.,
		  issue logs in Jira). To better identify solutions and their
		  relevant arguments, DRMiner skillfully decomposes the
		  problem into multiple text classification tasks and tackles
		  them using prompt tuning of large language models (LLMs)
		  and specific heuristic features. To evaluate DRMiner, we
		  acquire issue logs from Cassandra, Flink, and Solr
		  repositories in Jira and form a dataset for design
		  rationale mining. Experimental results show that DRMiner
		  outperforms all baselines and achieves F1 improvements of
		  24%, 22%, and 20% for mining design rationales, solutions,
		  and arguments, respectively, compared to the best baseline.
		  Furthermore, we investigate the usefulness of the design
		  rationales mined by DRMiner for automated program repair
		  (APR) and find that advanced LLMs, when prompted with these
		  extracted rationales, generate
		  10\texttimes{}-18\texttimes{} more full-match patches and
		  achieve a 10%-13% gain in CodeBLEU scores.},
  booktitle	= {Proceedings of the 39th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {468–480},
  numpages	= {13},
  keywords	= {design rationale, issue logs, design discussion, design
		  recovery, program maintenance},
  location	= {Sacramento, CA, USA},
  series	= {ASE '24}
}

@InProceedings{	  10.1145/3673791.3698415,
  author	= {Soudani, Heydar and Kanoulas, Evangelos and Hasibi,
		  Faegheh},
  title		= {Fine Tuning vs. Retrieval Augmented Generation for Less
		  Popular Knowledge},
  year		= {2024},
  isbn		= {9798400707247},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3673791.3698415},
  doi		= {10.1145/3673791.3698415},
  abstract	= {Language Models (LMs) memorize a vast amount of factual
		  knowledge, exhibiting strong performance across diverse
		  tasks and domains. However, it has been observed that the
		  performance diminishes when dealing with less-popular or
		  low-frequency concepts and entities, for example in domain
		  specific applications. The two prominent approaches to
		  enhance the performance of LMs on low-frequent topics are:
		  Retrieval Augmented Generation (RAG) and fine-tuning (FT)
		  over synthetic data. This paper explores and evaluates the
		  impact of RAG and FT on customizing LMs in handling
		  low-frequency entities on question answering tasks. We
		  conduct extensive experiments on twelve LMs of varying size
		  and type and different FT methods, data augmentation, and
		  retrieval models. Our findings indicate that while FT
		  boosts the performance across entities of varying
		  popularity, RAG surpasses FT by a large margin particularly
		  for least popular factual knowledge. Additionally, the
		  success of both RAG and FT approaches is amplified by
		  improving retrieval and data augmentation techniques. Fine
		  tuning, while beneficial for small LMs, requires extensive
		  resources. To address this issue, we propose the new
		  Stimulus RAG approach that surpasses the effectiveness of
		  fine tuning based approaches, thereby eliminating the need
		  for the costly data augmentation and fine tuning step for
		  enriching LMs with less popular factual knowledge.},
  booktitle	= {Proceedings of the 2024 Annual International ACM SIGIR
		  Conference on Research and Development in Information
		  Retrieval in the Asia Pacific Region},
  pages		= {12–22},
  numpages	= {11},
  keywords	= {data augmentation, fine tuning, retrieval augmented
		  generation},
  location	= {Tokyo, Japan},
  series	= {SIGIR-AP 2024}
}

@Proceedings{	  10.1145/3705618,
  title		= {DECS '24: Proceedings of the 2024 International Conference
		  on Digital Economy and Computer Science},
  year		= {2024},
  isbn		= {9798400711855},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3651169,
  author	= {Wu, Yaxiong and Macdonald, Craig and Ounis, Iadh},
  title		= {Personalised Multi-modal Interactive Recommendation with
		  Hierarchical State Representations},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {3},
  url		= {https://doi.org/10.1145/3651169},
  doi		= {10.1145/3651169},
  abstract	= {Multi-modal interactive recommender systems (MMIRS) can
		  effectively guide users towards their desired items through
		  multi-turn interactions by leveraging the users’
		  real-time feedback (in the form of natural-language
		  critiques) on previously recommended items (such as images
		  of fashion products). In this scenario, the users’
		  preferences can be expressed by both the users’ past
		  interests from their historical interactions and their
		  current needs from the real-time interactions. However, it
		  is typically challenging to make satisfactory personalised
		  recommendations across multi-turn interactions due to the
		  difficulty in balancing the users’ past interests and the
		  current needs for generating the users’ state (i.e.,
		  current preferences) representations over time. However,
		  hierarchical reinforcement learning has been successfully
		  applied in various fields by decomposing a complex task
		  into a hierarchy of more easily addressed subtasks. In this
		  journal article, we propose a novel personalised
		  multi-modal interactive recommendation model (PMMIR) using
		  hierarchical reinforcement learning to more effectively
		  incorporate the users’ preferences from both their past
		  and real-time interactions. In particular, PMMIR decomposes
		  the personalised interactive recommendation process into a
		  sequence of two subtasks with hierarchical state
		  representations: a first subtask where a history encoder
		  learns the users’ past interests with the hidden states
		  of history for providing personalised initial
		  recommendations and a second subtask where a state tracker
		  estimates the current needs with the real-time estimated
		  states for updating the subsequent recommendations. The
		  history encoder and the state tracker are jointly optimised
		  with a single objective by maximising the users’ future
		  satisfaction with the recommendations. Following previous
		  work, we train and evaluate our PMMIR model using a user
		  simulator that can generate natural-language critiques
		  about the recommendations as a surrogate for real human
		  users. Experiments conducted on two derived fashion
		  datasets from two well-known public datasets demonstrate
		  that our proposed PMMIR model yields significant
		  improvements in comparison to the existing state-of-the-art
		  baseline models. The datasets and code are publicly
		  available at:},
  journal	= {ACM Trans. Recomm. Syst.},
  month		= jun,
  articleno	= {21},
  numpages	= {25},
  keywords	= {Interactive recommendation, multi-modal, personalisation,
		  reinforcement learning}
}

@InProceedings{	  10.1145/3646547.3689015,
  author	= {Huang, Ziyuan and Tang, Jiaming and Karir, Manish and Liu,
		  Mingyan and Sarabi, Armin},
  title		= {Analyzing Corporate Privacy Policies using AI Chatbots},
  year		= {2024},
  isbn		= {9798400705922},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3646547.3689015},
  doi		= {10.1145/3646547.3689015},
  abstract	= {In this paper, we present and evaluate an automated
		  pipeline for the large-scale analysis of corporate privacy
		  policies. Organizations usually develop their privacy
		  policies in isolation to best balance their business needs,
		  user rights, as well as regulatory requirements. A
		  wide-ranging and structured analysis of corporate privacy
		  policies is essential to facilitate a deeper understanding
		  of how organizations have balanced competing requirements.
		  Our approach consists of a web crawler that can navigate to
		  and scrape content from web pages that contain privacy
		  policies, and a set of AI chatbot task prompts to process
		  and extract structured/labeled annotations from the raw
		  data. The analysis includes the types of collected user
		  data, the purposes for which data is collected and
		  processed, data retention and protection practices, and
		  user rights and choices. Our validation shows that our
		  annotations are highly accurate and consistent. We use this
		  architecture to gather data on the privacy policies of
		  companies in the Russell 3000 index, resulting in hundreds
		  of thousands of annotations across all categories. Analysis
		  of the resulting data allows us to obtain unique insights
		  into the state of the privacy policy ecosystem as a
		  whole.},
  booktitle	= {Proceedings of the 2024 ACM on Internet Measurement
		  Conference},
  pages		= {505–515},
  numpages	= {11},
  keywords	= {ai chatbots, large language models, privacy policies, text
		  annotation, web crawling},
  location	= {Madrid, Spain},
  series	= {IMC '24}
}

@Article{	  10.1145/3708521,
  author	= {Li, Rui and Liu, Huai and Poon, Pak-Lok and Towey, Dave
		  and Sun, Chang-Ai and Zheng, Zheng and Zhou, Zhi Quan and
		  Chen, Tsong Yueh},
  title		= {Metamorphic Relation Generation: State of the Art and
		  Research Directions},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3708521},
  doi		= {10.1145/3708521},
  abstract	= {Metamorphic testing has become one mainstream technique to
		  address the notorious oracle problem in software testing,
		  thanks to its great successes in revealing real-life bugs
		  in a wide variety of software systems. Metamorphic
		  relations, the core component of metamorphic testing, have
		  continuously attracted research interests from both
		  academia and industry. In the last decade, a rapidly
		  increasing number of studies have been conducted to
		  systematically generate metamorphic relations from various
		  sources and for different application domains. In this
		  article, based on the systematic review on the state of the
		  art for metamorphic relations’ generation, we summarize
		  and highlight visions for further advancing the theory and
		  techniques for identifying and constructing metamorphic
		  relations, and discuss promising research directions in
		  related areas.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= dec,
  keywords	= {Metamorphic testing, Metamorphic relation, Metamorphic
		  relation generation}
}

@InProceedings{	  10.1145/3626772.3657882,
  author	= {Roy, Soumyadeep and Khatua, Aparup and Ghoochani, Fatemeh
		  and Hadler, Uwe and Nejdl, Wolfgang and Ganguly, Niloy},
  title		= {Beyond Accuracy: Investigating Error Types in GPT-4
		  Responses to USMLE Questions},
  year		= {2024},
  isbn		= {9798400704314},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3626772.3657882},
  doi		= {10.1145/3626772.3657882},
  abstract	= {GPT-4 demonstrates high accuracy in medical QA tasks,
		  leading with an accuracy of 86.70%, followed by Med-PaLM 2
		  at 86.50%. However, around 14% of errors remain.
		  Additionally, current works use GPT-4 to only predict the
		  correct option without providing any explanation and thus
		  do not provide any insight into the thinking process and
		  reasoning used by GPT-4 or other LLMs. Therefore, we
		  introduce a new domain-specific error taxonomy derived from
		  collaboration with medical students. Our GPT-4 USMLE Error
		  (G4UE) dataset comprises 4153 GPT-4 correct responses and
		  919 incorrect responses to the United States Medical
		  Licensing Examination (USMLE) respectively. These responses
		  are quite long (258 words on average), containing detailed
		  explanations from GPT-4 justifying the selected option. We
		  then launch a large-scale annotation study using the Potato
		  annotation platform and recruit 44 medical experts through
		  Prolific, a well-known crowdsourcing platform. We annotated
		  300 out of these 919 incorrect data points at a granular
		  level for different classes and created a multi-label span
		  to identify the reasons behind the error. In our annotated
		  dataset, a substantial portion of GPT-4's incorrect
		  responses is categorized as a "Reasonable response by
		  GPT-4," by annotators. This sheds light on the challenge of
		  discerning explanations that may lead to incorrect options,
		  even among trained medical professionals. We also provide
		  medical concepts and medical semantic predications
		  extracted using the SemRep tool for every data point. We
		  believe that it will aid in evaluating the ability of LLMs
		  to answer complex medical questions. We make the resources
		  available at
		  https://github.com/roysoumya/usmle-gpt4-error-taxonomy.},
  booktitle	= {Proceedings of the 47th International ACM SIGIR Conference
		  on Research and Development in Information Retrieval},
  pages		= {1073–1082},
  numpages	= {10},
  keywords	= {gpt-4, medical qa, multi-label dataset, usmle error
		  taxonomy},
  location	= {Washington DC, USA},
  series	= {SIGIR '24}
}

@Proceedings{	  10.1145/3625223,
  title		= {RSP '23: Proceedings of the 34th International Workshop on
		  Rapid System Prototyping},
  year		= {2023},
  isbn		= {9798400704109},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hamburg, Germany}
}

@InProceedings{	  10.1145/3658644.3670377,
  author	= {Li, Shuai and Yang, Zhemin and Nan, Yuhong and Yu, Shutian
		  and Zhu, Qirui and Yang, Min},
  title		= {Are We Getting Well-informed? An In-depth Study of Runtime
		  Privacy Notice Practice in Mobile Apps},
  year		= {2024},
  isbn		= {9798400706363},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3658644.3670377},
  doi		= {10.1145/3658644.3670377},
  abstract	= {Under the General Data Protection Regulation (GDPR),
		  mobile app developers are required to inform users of
		  necessary information at the time when user data is
		  collected (called users' "Right-to-be-Informed"). This is
		  typically done by app developers via providing runtime
		  privacy notices (RPNs for short). However, given the
		  heterogeneous privacy data types and data access patterns
		  in modern apps, it is not clear to what extent apps (app
		  developers) effectively fulfill this compliance requirement
		  in practice.In this paper, we perform the first systematic
		  study of current RPN practices in mobile apps. Our research
		  endeavors to comprehend (1) the ecosystem of RPN, (2)
		  potential gaps between legal requirements and RPN
		  practices, and (3) the underlying reasons for such gaps. To
		  achieve this, we design an automated pipeline - RENO that
		  can effectively identify, extract, and analyze RPN at a
		  large scale. With the help of RENO, we investigated 4,656
		  mobile apps selected from 19 European Union countries. Our
		  analysis reveals a number of interesting findings. For
		  example, 77.10% of user data collection behaviors lack
		  RPNs. Among those provided RPNs, 86.35% of them have no
		  more than three required notice elements when GDPR requires
		  seven. In addition, to further understand the reasons
		  behind such gaps, we perform a notification campaign and
		  ask for feedback from the app developers. Indeed, the
		  collected responses highlighted several critical reasons.
		  For instance, a substantial proportion of app developers
		  regard RPN as an optional complement to their privacy
		  policies as RPNs are not strictly enforced by app stores.
		  Our study shows the pressing need for better transparency
		  in user data collection delivered by RPN.},
  booktitle	= {Proceedings of the 2024 on ACM SIGSAC Conference on
		  Computer and Communications Security},
  pages		= {1581–1595},
  numpages	= {15},
  keywords	= {GDPR compliance, mobile application, right to be informed,
		  runtime privacy notice},
  location	= {Salt Lake City, UT, USA},
  series	= {CCS '24}
}

@Proceedings{	  10.1145/3702879,
  title		= {IoTCCT '24: Proceedings of the 2024 2nd International
		  Conference on Internet of Things and Cloud Computing
		  Technology},
  year		= {2024},
  isbn		= {9798400710148},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3631802,
  title		= {Koli Calling '23: Proceedings of the 23rd Koli Calling
		  International Conference on Computing Education Research},
  year		= {2023},
  isbn		= {9798400716539},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Koli, Finland}
}

@Proceedings{	  10.1145/3616901,
  title		= {FAIML '23: Proceedings of the 2023 International
		  Conference on Frontiers of Artificial Intelligence and
		  Machine Learning},
  year		= {2023},
  isbn		= {9798400707544},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@Proceedings{	  10.1145/3663741,
  title		= {BiDEDE '24: Proceedings of the International Workshop on
		  Big Data in Emergent Distributed Environments},
  year		= {2024},
  isbn		= {9798400706790},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Santiago, AA, Chile}
}

@Proceedings{	  10.1145/3650215,
  title		= {ICMLCA '23: Proceedings of the 2023 4th International
		  Conference on Machine Learning and Computer Application},
  year		= {2023},
  isbn		= {9798400709449},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hangzhou, China}
}

@Proceedings{	  10.1145/3643651,
  title		= {IWSPA '24: Proceedings of the 10th ACM International
		  Workshop on Security and Privacy Analytics},
  year		= {2024},
  isbn		= {9798400705564},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the 2024 ACM
		  International Workshop on Security and Privacy Analytics -
		  IWSPA 2024. This year's workshop is the tenth in the series
		  and co-hosted with the Fourteenth ACM Annual Conference on
		  Data and Applications Security and Privacy (CODASPY
		  2024).IWSPA addresses important research topics associated
		  with the application of data analytics tools and techniques
		  (including statistical, machine/deep learning, data mining,
		  and natural language processing) to challenges that arise
		  with security and privacy preservation. IWSPA provides a
		  forum for the interaction between researchers in these
		  areas, identifying and pursuing new topics that arise in
		  the intersection between the fields of Artificial
		  Intelligence and Cybersecurity.},
  location	= {Porto, Portugal}
}

@Proceedings{	  10.1145/3673971,
  title		= {ICMHI '24: Proceedings of the 2024 8th International
		  Conference on Medical and Health Informatics},
  year		= {2024},
  isbn		= {9798400716874},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Yokohama, Japan}
}

@Proceedings{	  10.1145/3674658,
  title		= {ICBBT '24: Proceedings of the 2024 16th International
		  Conference on Bioinformatics and Biomedical Technology},
  year		= {2024},
  isbn		= {9798400717666},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3685650.3685660,
  author	= {Opitz, Dominik and Hamm, Andreas and El Baff, Roxanne and
		  Korte, Jasper and Hecking, Tobias},
  title		= {Graph Detective: A User Interface for Intuitive Graph
		  Exploration Through Visualized Queries},
  year		= {2024},
  isbn		= {9798400711695},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3685650.3685660},
  doi		= {10.1145/3685650.3685660},
  abstract	= {Graph databases are used across several domains due to the
		  intuitive structure of graphs. They are well-suited for
		  storing document collections together with their
		  interlinkages through metadata and annotations. Yet,
		  querying such graphs requires database experts' involvement
		  for query formulation, reducing accessibility to
		  nonexperts. To address this issue, we present Graph
		  Detective, a web interface that provides an intuitive entry
		  point for graph data exploration, where users can create
		  queries visually with little effort, eliminating the need
		  for expertise in query writing. After processing, the
		  resulting query output (a graph) is then rendered in an
		  interactive 3D visualization. This visualization allows the
		  analysis of structural traits of the resulting graph data,
		  exploiting the documents and metadata interlinkage. Our
		  user evaluation revealed that even individuals
		  inexperienced with graph databases or graph data, in
		  general, could satisfactorily access the graph data through
		  our interface. Furthermore, experienced participants
		  commented that our interface was more efficient than
		  writing explicit queries in graph database query language.
		  Interested users can find the code openly accessible on
		  GitHub1.},
  booktitle	= {Proceedings of the ACM Symposium on Document Engineering
		  2024},
  articleno	= {6},
  numpages	= {9},
  keywords	= {Graph Data, Graph Database, Graph Retrieval Interface,
		  Query Generation, Visual Querying, Web Application},
  location	= {San Jose, CA, USA},
  series	= {DocEng '24}
}

@Proceedings{	  10.1145/3660853,
  title		= {AICCONF '24: Proceedings of the Cognitive Models and
		  Artificial Intelligence Conference},
  year		= {2024},
  isbn		= {9798400716928},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {undefinedstanbul, Turkiye}
}

@Proceedings{	  10.1145/3680528,
  title		= {SA '24: SIGGRAPH Asia 2024 Conference Papers},
  year		= {2024},
  isbn		= {9798400711312},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3589334.3645631,
  author	= {Zhao, Rui and Zhao, Jun},
  title		= {Perennial Semantic Data Terms of Use for Decentralized
		  Web},
  year		= {2024},
  isbn		= {9798400701719},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589334.3645631},
  doi		= {10.1145/3589334.3645631},
  abstract	= {In today's digital landscape, the Web has become
		  increasingly centralized, raising concerns about user
		  privacy violations. Decentralized Web architectures, such
		  as Solid, offer a promising solution by empowering users
		  with better control over their data in their personal
		  'Pods'. However, a significant challenge remains: users
		  must navigate numerous applications to decide which
		  application can be trusted with access to their data Pods.
		  This often involves reading lengthy and complex Terms of
		  Use agreements, a process that users often find daunting or
		  simply ignore. This compromises user autonomy and impedes
		  detection of data misuse. We propose a novel formal
		  description of Data Terms of Use (DToU), along with a DToU
		  reasoner. Users and applications specify their own parts of
		  the DToU policy with local knowledge, covering permissions,
		  requirements, prohibitions and obligations. Automated
		  reasoning verifies compliance, and also derives policies
		  for output data. This constitutes a "perennial'' DToU
		  language, where the policy authoring only occurs once, and
		  we can conduct ongoing automated checks across users,
		  applications and activity cycles. Our solution is built on
		  Turtle, Notation 3 and RDF Surfaces, for the language and
		  the reasoning engine. It ensures seamless integration with
		  other semantic tools for enhanced interoperability. We have
		  successfully integrated this language into the Solid
		  framework, and conducted performance benchmark. We believe
		  this work demonstrates a practicality of a perennial DToU
		  language and the potential of a paradigm shift to how users
		  interact with data and applications in a decentralized Web,
		  offering both improved privacy and usability.},
  booktitle	= {Proceedings of the ACM Web Conference 2024},
  pages		= {2238–2249},
  numpages	= {12},
  keywords	= {automated reasoning, data terms of use, decentralized web,
		  formal modelling, notation 3, usage control},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Proceedings{	  10.1145/3665689,
  title		= {BIC '24: Proceedings of the 2024 4th International
		  Conference on Bioinformatics and Intelligent Computing},
  year		= {2024},
  isbn		= {9798400716645},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@InProceedings{	  10.1145/3625549.3658830,
  author	= {Huang, Shaohan and Luan, Zhongzhi},
  title		= {Semantic-Aware Log Understanding and Analysis},
  year		= {2024},
  isbn		= {9798400704130},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3625549.3658830},
  doi		= {10.1145/3625549.3658830},
  abstract	= {The exponential growth in system complexity and the
		  corresponding surge in log data volume necessitate advanced
		  log analysis techniques for efficient system management and
		  anomaly detection. Traditional log understanding and
		  analysis methods often fail to capture the rich semantic
		  context inherent in log messages, leading to suboptimal
		  monitoring and diagnostic capabilities. This paper aims to
		  bridge the semantic gap by integrating cutting-edge
		  semantic technologies into the log analysis pipeline. We
		  leverage natural language processing, information
		  retrieval, and large language models to enrich log data
		  with semantic information, facilitating a deeper
		  understanding of log messages. Our methodology enhances
		  anomaly detection accuracy by utilizing hierarchical
		  contextual information and pre-training technology, and
		  refining log-based QA processes by log retrieval and log
		  reader. Preliminary results demonstrate a significant
		  improvement in identifying and diagnosing system anomalies,
		  as well as in the automated answering log questions. This
		  research not only presents a breakthrough in log data
		  analysis but also sets the stage for future advancements in
		  intelligent system monitoring and proactive fault
		  resolution. Through this semantic-aware approach, we
		  envision a new paradigm in log analysis that transcends
		  traditional machine learning methods, offering a more
		  robust and intuitive understanding of system behaviors and
		  states.},
  booktitle	= {Proceedings of the 33rd International Symposium on
		  High-Performance Parallel and Distributed Computing},
  pages		= {413–416},
  numpages	= {4},
  keywords	= {semantic-aware analysis, log understanding, natural
		  language processing, anomaly detection, log parsing},
  location	= {Pisa, Italy},
  series	= {HPDC '24}
}

@Article{	  10.1145/3675759,
  author	= {Mussa, Omar and Rana, Omer and Goossens, Benoit and Orozco
		  Ter wengel, Pablo and Perera, Charith},
  title		= {ForestQB: Enhancing Linked Data Exploration through
		  Graphical and Conversational UIs Integration},
  year		= {2024},
  issue_date	= {September 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  number	= {3},
  url		= {https://doi.org/10.1145/3675759},
  doi		= {10.1145/3675759},
  abstract	= {This article introduces the Forest Query Builder
		  (ForestQB), an innovative toolkit designed to enhance the
		  exploration and application of observational Linked Data
		  (LD) within the field of wildlife research and
		  conservation. Addressing the challenges faced by
		  non-experts in navigating Resource Description Framework
		  (RDF) triplestores and executing SPARQL queries, ForestQB
		  employs a novel integrated approach. This approach combines
		  a graphical user interface (GUI) with a conversational user
		  interface (CUI), thereby greatly simplifying the process of
		  query formulation and making observational LD accessible to
		  users without expertise in RDF or SPARQL. Developed through
		  insights derived from a comprehensive ethnographic study
		  involving wildlife researchers, ForestQB is specifically
		  designed to improve the accessibility of SPARQL endpoints
		  and facilitate the exploration of observational LD in
		  wildlife research contexts. To evaluate the effectiveness
		  of our approach, we conducted a user experiment. The
		  results of this evaluation affirm that ForestQB is not only
		  efficient and user-friendly but also plays a crucial role
		  in eliminating barriers for users, facilitating the
		  effective use of observational LD in wildlife conservation
		  and extending its benefits to wider domains. (GitHub Link:
		  github.com/i3omar/ForestQB).},
  journal	= {ACM J. Comput. Sustain. Soc.},
  month		= sep,
  articleno	= {32},
  numpages	= {33},
  keywords	= {Linked data, SPARQL, RDF, query builder, visual queryin}
}

@InProceedings{	  10.1145/3689936.3694692,
  author	= {\r{A}str\"{o}m, Alexander},
  title		= {Revisiting Automotive Threat Analysis by Leveraging the
		  Elements of the Threat Landscape},
  year		= {2024},
  isbn		= {9798400712326},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3689936.3694692},
  doi		= {10.1145/3689936.3694692},
  abstract	= {The automotive industry faces significant challenges due
		  to rapid advancements in areas such as increased
		  connectivity, automated driving, and electrification. These
		  advancements, coupled with an evolving standard and
		  regulatory landscape, place substantial demands on product
		  development, particularly in the realm of cybersecurity.
		  Features that enhance connectivity introduce various
		  external interfaces, leading to numerous attack vectors and
		  potential vulnerabilities in vehicle-connected devices at
		  unprecedented levels. Additionally, the dynamic nature of
		  the threat landscape complicates manufacturers' efforts to
		  maintain adequate security and stay up-to-date. Two major
		  pain points are the extensive resources required and the
		  time-consuming nature of cybersecurity activities. In this
		  paper, we propose a methodology to modularize cybersecurity
		  activities and the corresponding work products related to
		  the threat landscape. These modules are designed to be used
		  as reusable elements in activities mandated by standards
		  such as ISO/SAE 21434.},
  booktitle	= {Proceedings of the 2024 Cyber Security in CarS Workshop},
  pages		= {1–12},
  numpages	= {12},
  keywords	= {automotive, cybersecurity, threat landscape, threat
		  modelling, threat profiling},
  location	= {Salt Lake City, UT, USA},
  series	= {CSCS '24}
}

@Article{	  10.1109/tcbb.2024.3427381,
  author	= {Taha, Kamal},
  title		= {Employing Machine Learning Techniques to Detect Protein
		  Function: A Survey, Experimental, and Empirical
		  Evaluations},
  year		= {2024},
  issue_date	= {Nov.-Dec. 2024},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {21},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2024.3427381},
  doi		= {10.1109/TCBB.2024.3427381},
  abstract	= {This review article delves deeply into the various machine
		  learning (ML) methods and algorithms employed in discerning
		  protein functions. Each method discussed is assessed for
		  its efficacy, limitations, potential improvements, and
		  future prospects. We present an innovative hierarchical
		  classification system that arranges algorithms into
		  intricate categories and unique techniques. This taxonomy
		  is based on a tri-level hierarchy, starting with the
		  methodology category and narrowing down to specific
		  techniques. Such a framework allows for a structured and
		  comprehensive classification of algorithms, assisting
		  researchers in understanding the interrelationships among
		  diverse algorithms and techniques. The study incorporates
		  both empirical and experimental evaluations to
		  differentiate between the techniques. The empirical
		  evaluation ranks the techniques based on four criteria. The
		  experimental assessments rank: (1) individual techniques
		  under the same methodology sub-category, (2) different
		  sub-categories within the same category, and (3) the broad
		  categories themselves. Integrating the innovative
		  methodological classification, empirical findings, and
		  experimental assessments, the article offers a well-rounded
		  understanding of ML strategies in protein function
		  identification. The paper also explores techniques for
		  multi-task and multi-label detection of protein functions,
		  in addition to focusing on single-task methods. Moreover,
		  the paper sheds light on the future avenues of ML in
		  protein function determination.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jul,
  pages		= {1965–1986},
  numpages	= {22}
}

@Proceedings{	  10.1145/3660395,
  title		= {AIBDF '23: Proceedings of the 2023 3rd Guangdong-Hong
		  Kong-Macao Greater Bay Area Artificial Intelligence and Big
		  Data Forum},
  year		= {2023},
  isbn		= {9798400716362},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Guangzhou, China}
}

@Proceedings{	  10.1145/3644713,
  title		= {ICFNDS '23: Proceedings of the 7th International
		  Conference on Future Networks and Distributed Systems},
  year		= {2023},
  isbn		= {9798400709036},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dubai, United Arab Emirates}
}

@Proceedings{	  10.1145/3686424,
  title		= {EDCS '24: Proceedings of the 2024 Guangdong-Hong
		  Kong-Macao Greater Bay Area International Conference on
		  Education Digitalization and Computer Science},
  year		= {2024},
  isbn		= {9798400710360},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shenzhen, China}
}

@Article{	  10.1145/3690381,
  author	= {Zhang, Xiaoyu and Shi, Shaoyun and Li, Yishan and Ma,
		  Weizhi and Sun, Peijie and Zhang, Min},
  title		= {Feature-Enhanced Neural Collaborative Reasoning for
		  Explainable Recommendation},
  year		= {2024},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {1},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3690381},
  doi		= {10.1145/3690381},
  abstract	= {Providing reasonable explanations for a specific
		  suggestion given by the recommender can help users trust
		  the system more. As logic rule-based inference is concise,
		  transparent, and aligned with human cognition, it can be
		  adopted to improve the interpretability of recommendation
		  models. Previous work that interprets user preference with
		  logic rules merely focuses on the construction of rules
		  while neglecting the usage of feature embeddings. This
		  limits the model in capturing implicit relationships
		  between features. In this article, we aim to improve both
		  the effectiveness and explainability of recommendation
		  models by simultaneously representing logic rules and
		  feature embeddings. We propose a novel model-intrinsic
		  explainable recommendation method named Feature-Enhanced
		  Neural Collaborative Reasoning (FENCR). The model
		  automatically extracts representative logic rules from
		  massive possibilities in a data-driven way. In addition, we
		  utilize feature interaction-based neural modules to
		  represent logic operators on embeddings. Experiments on two
		  large public datasets show our model outperforms
		  state-of-the-art neural logical recommendation models.
		  Further case analyses demonstrate that FENCR can derive
		  reasonable rules, indicating its high robustness and
		  expandability.1},
  journal	= {ACM Trans. Inf. Syst.},
  month		= nov,
  articleno	= {7},
  numpages	= {33},
  keywords	= {collaborative reasoning, explainable recommendation, rule
		  learning}
}

@Article{	  10.1145/3649886,
  author	= {Zhang, Guangping and Li, Dongsheng and Gu, Hansu and Lu,
		  Tun and Gu, Ning},
  title		= {Heterogeneous Graph Neural Network with Personalized and
		  Adaptive Diversity for News Recommendation},
  year		= {2024},
  issue_date	= {August 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {3},
  issn		= {1559-1131},
  url		= {https://doi.org/10.1145/3649886},
  doi		= {10.1145/3649886},
  abstract	= {The emergence of online media has facilitated the
		  dissemination of news, but has also introduced the problem
		  of information overload. To address this issue, providing
		  users with accurate and diverse news recommendations has
		  become increasingly important. News possesses rich and
		  heterogeneous content, and the factors that attract users
		  to news reading are varied. Consequently, accurate news
		  recommendation requires modeling of both the heterogeneous
		  content of news and the heterogeneous user-news
		  relationships. Furthermore, users’ news consumption is
		  highly dynamic, which is reflected in the differences in
		  topic concentration among different users and in the
		  real-time changes in user interests. To this end, we
		  propose a Heterogeneous Graph Neural Network with
		  Personalized and Adaptive Diversity for News Recommendation
		  (DivHGNN). DivHGNN first represents the heterogeneous
		  content of news and the heterogeneous user-news
		  relationships as an attributed heterogeneous graph. Then,
		  through a heterogeneous node content adapter, it models the
		  heterogeneous node attributes into aligned and fused node
		  representations. With the proposed attributed heterogeneous
		  graph neural network, DivHGNN integrates the heterogeneous
		  relationships to enhance node representation for accurate
		  news recommendations. We also discuss relation pruning,
		  model deployment, and cold-start issues to further improve
		  model efficiency. In terms of diversity, DivHGNN
		  simultaneously models the variance of nodes through
		  variational representation learning for providing
		  personalized diversity. Additionally, a time-continuous
		  exponentially decaying distribution cache is proposed to
		  model the temporal dynamics of user real-time interests for
		  providing adaptive diversity. Extensive experiments on
		  real-world news datasets demonstrate the effectiveness of
		  the proposed method.},
  journal	= {ACM Trans. Web},
  month		= may,
  articleno	= {34},
  numpages	= {33},
  keywords	= {News recommendation, graph neural network, heterogeneous
		  information network, recommendation diversity}
}

@InProceedings{	  10.1145/3651671.3651759,
  author	= {Yang, Shuling and Chen, Hanzhu and Fang, Binbin},
  title		= {QuDial: A Quadruple-driven Dialogue System for Real Estate
		  Consulting Services},
  year		= {2024},
  isbn		= {9798400709234},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3651671.3651759},
  doi		= {10.1145/3651671.3651759},
  abstract	= {Establishing connections with users is a critical task for
		  brokers in real estate consulting dialogues and can be
		  assisted by a task-oriented dialogue system. Existing
		  task-oriented systems typically follow a pipeline structure
		  with four subtasks, achieving great success in many
		  domains. However, these systems confront expensive
		  development costs in real estate domain, as they require
		  multiple subtasks and multiple types of data labels. To
		  tackle this problem, we introduce a novel semantic frame of
		  quadruples {(action, subject, predicate, object)}, to build
		  a simplified system with fewer subtasks and lower data
		  costs than existing methods. Based on this, we propose
		  QuDial, a simple yet effective quadruple-driven dialogue
		  system that consists of two subtasks: Natural Language
		  Understanding (NLU) and Dialogue Policy Learning (DPL). In
		  NLU, we parse each dialogue into a quadruple sequence,
		  which captures the evolution of dialogue contents. Then, in
		  DPL, we encode the historical quadruple sequence to predict
		  the next quadruple, which provides the main response
		  content for brokers. Experiments on real estate business
		  domain demonstrate that QuDial effectively improves the
		  response predictions with its two simple subtasks.},
  booktitle	= {Proceedings of the 2024 16th International Conference on
		  Machine Learning and Computing},
  pages		= {609–615},
  numpages	= {7},
  location	= {Shenzhen, China},
  series	= {ICMLC '24}
}

@InProceedings{	  10.1145/3627673.3680009,
  author	= {Agrawal, Sanjay and Merugu, Srujana and Sembium, Vivek},
  title		= {Boosting Entity Recognition by leveraging Cross-task
		  Domain Models for Weak Supervision},
  year		= {2024},
  isbn		= {9798400704369},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3627673.3680009},
  doi		= {10.1145/3627673.3680009},
  abstract	= {Entity Recognition (ER) is a common natural language
		  processing task encountered in a number of real-world
		  applications. For common domains and named entities such as
		  places and organisations, there exists sufficient high
		  quality annotated data and foundational models such as T5
		  and GPT-3.5 also provide highly accurate predictions.
		  However, for niche domains such as e-commerce and medicine
		  with specialized entity types, there is a paucity of
		  labeled data since manual labeling of tokens is often
		  time-consuming and expensive, which makes entity
		  recognition challenging for such domains. Recent works such
		  as NEEDLE [48] propose hybrid solutions to efficiently
		  combine a small amount of strongly labeled
		  (human-annotated) with a large amount of weakly labeled
		  (distant supervision) data to yield superior performance
		  relative to supervised training. The extensive noise in the
		  weakly labeled data, however, remains a challenge. In this
		  paper, we propose WeSDoM (Weak Supervision with Domain
		  Models), which leverages pretrained encoder models from the
		  same domain but different tasks to create domain ontologies
		  that can enable the creation of less noisy weakly labeled
		  data. Experiments on internal e-commerce and public
		  biomedical NER datasets demonstrate that WeSDoM outperforms
		  existing SOTA baselines by a significant margin. We achieve
		  new SOTA F1 scores on two popular Biomedical NER datasets,
		  BC5CDR-chem 94.27, BC5CDR-disease 91.23.},
  booktitle	= {Proceedings of the 33rd ACM International Conference on
		  Information and Knowledge Management},
  pages		= {4324–4331},
  numpages	= {8},
  keywords	= {cross-task domain encoder, entity recognition, ontologies,
		  weak supervision},
  location	= {Boise, ID, USA},
  series	= {CIKM '24}
}

@Proceedings{	  10.1145/3640824,
  title		= {CCEAI '24: Proceedings of the 2024 8th International
		  Conference on Control Engineering and Artificial
		  Intelligence},
  year		= {2024},
  isbn		= {9798400707971},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shanghai, China}
}

@InProceedings{	  10.1145/3665689.3665768,
  author	= {Deng, Qiwen and Han, Yuexia and Sun, Jianfei},
  title		= {A Joint Framework for Predicting Disease-Gene Interactions
		  Based on Pre-trained Models and Graph Attention Networks},
  year		= {2024},
  isbn		= {9798400716645},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665689.3665768},
  doi		= {10.1145/3665689.3665768},
  abstract	= {The study of disease-gene interactions is crucial in
		  biomedical research. Identifying genes associated with
		  diseases can provide critical insights into disease
		  mechanisms, facilitate early diagnosis, and contribute to
		  the development of targeted therapies. In this paper, we
		  propose a novel framework for predicting disease-gene
		  interactions called the PRGAT-DG, which utilizes
		  pre-trained language models and graph attention networks to
		  extract semantic and graph structure features respectively.
		  Moreover, we introduce residual structure to alleviate the
		  problem of excessive smoothing. Experimental results on a
		  dataset released by Stanford University demonstrate the
		  remarkable predictive accuracy of our framework, showcasing
		  its superiority compared to other existing methods. This
		  research holds significant implications for advancing our
		  understanding of disease-gene interaction mechanisms and
		  accelerating the development of relevant therapeutics.},
  booktitle	= {Proceedings of the 2024 4th International Conference on
		  Bioinformatics and Intelligent Computing},
  pages		= {474–478},
  numpages	= {5},
  location	= {Beijing, China},
  series	= {BIC '24}
}

@Proceedings{	  10.1145/3629296,
  title		= {ICETC '23: Proceedings of the 15th International
		  Conference on Education Technology and Computers},
  year		= {2023},
  isbn		= {9798400709111},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Barcelona, Spain}
}

@Article{	  10.1145/3678470,
  author	= {Swaileh A. Alzaidi, Muhammad and Alshammari, Alya and
		  Almanea, Manar and Al-khawaja, Haneen A. and Al Sultan,
		  Hanan and Alotaibi, Shoayee and Almukadi, Wafa},
  title		= {A Text-Inception-Based Natural Language Processing Model
		  for Sentiment Analysis of Drug Experiences},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3678470},
  doi		= {10.1145/3678470},
  abstract	= {The study of sentiment in Natural Language Processing
		  (NLP) is among the most successful research areas because
		  of the availability of millions of user opinions online
		  since the turn of the century. The economic, political, and
		  medical fields are just some of the many that have
		  benefited from studies of sentiment research. While
		  numerous studies have examined more mainstream topics like
		  consumer electronics, movies, and restaurants, relatively
		  few have examined health and medical concerns. Considerable
		  insight into where to direct efforts to improve public
		  health might be gained by a study of how people feel about
		  healthcare as a whole and of individual drug experiences in
		  particular. When it comes to medicine, automatic analysis
		  of online user evaluations paves the way for sifting
		  through massive amounts of user feedback to find
		  information regarding medications' efficacy and side
		  effects that might be used to enhance pharmacovigilance
		  programs. Simple rules-based methods have given way to more
		  complex machine learning approaches like deep learning,
		  which is developing as a technology for many natural
		  language processing jobs. The opensource datasets have been
		  analyzed with models that use word embeddings and term
		  frequency-inverse document frequency (TF-IDF). A
		  feature-enhanced text-inception model for sentiment
		  classification was presented to work in tandem with this
		  approach. The model first employed a cutting-edge
		  text-inception module to glean useful shallow features from
		  the text. K-MaxPooling was subsequently employed to reduce
		  the dimensionality of its shallow and deep includes as well
		  as enhance the generalization of characteristics, and a
		  deep feature extraction module was formed using the
		  bidirectional gated recurrent unit (Bi-GRU) and the capsule
		  neural network to comprehend the text's semantic data. By
		  combining traditional methods with cutting-edge artificial
		  intelligence techniques, this hybrid approach can
		  revolutionize public health initiatives, decision-making,
		  and pharmacovigilance in the healthcare industry. This
		  model achieved an exceptional accuracy rate of 99%,
		  underscoring its effectiveness in sentiment classification
		  and demonstrating its potential to significantly contribute
		  to advancing healthcare and medical research.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= aug,
  keywords	= {Natural Language Processing (NLP), User Opinions,
		  Healthcare, Medical Sentiment, Public Health, Deep
		  learning}
}

@Proceedings{	  10.1145/3635059,
  title		= {PCI '23: Proceedings of the 27th Pan-Hellenic Conference
		  on Progress in Computing and Informatics},
  year		= {2023},
  isbn		= {9798400716263},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Lamia, Greece}
}

@InProceedings{	  10.1145/3638884.3638979,
  author	= {Wu, Yu and Miao, Lin and Li, Han},
  title		= {Attribute Value Extraction in Weapon Domain Based on
		  Bi-LSTM and Attention},
  year		= {2024},
  isbn		= {9798400708909},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3638884.3638979},
  doi		= {10.1145/3638884.3638979},
  abstract	= {Aiming at the problem that the traditional extraction
		  method caused by the diversification of weapon attributes
		  has a large amount of work to construct the label of weapon
		  attributes, in this paper, we propose a weapon attribute
		  value extraction method based on bidirectional long-term
		  and short-term memory network (Bi-LSTM) and attention
		  mechanism. The method first uses the Bi-LSTM model to
		  extract the features of the input text and attribute names.
		  Then, the attention mechanism focuses on the relations
		  between words and attributes in the sentence. Afterward,
		  the global BIO tag marks the position of the attribute
		  values in the sentence. In this way, the method can reduce
		  the workload during the corpus preparation period to
		  improve the generalization ability of the model so that it
		  can extract different weapon attribute data. Compared with
		  Bi-LSTM, Bi-LSTM_CRF, and OpenTag from the experimental
		  results, the F1 values of the proposed model on the weapon
		  domain attribute dataset are increased by about 6.9%, 5.7%,
		  and 2.5%, respectively.},
  booktitle	= {Proceedings of the 2023 9th International Conference on
		  Communication and Information Processing},
  pages		= {603–610},
  numpages	= {8},
  keywords	= {Attribute Value Extraction, Information Extraction,
		  Knowledge Base, Natural Language Processing},
  location	= {Lingshui, China},
  series	= {ICCIP '23}
}

@Article{	  10.1613/jair.1.15407,
  author	= {Castagna, Federico and K\"{o}kciyan, Nadin and Sassoon,
		  Isabel and Parsons, Simon and Sklar, Elizabeth},
  title		= {Computational Argumentation-based Chatbots: A Survey},
  year		= {2024},
  issue_date	= {Sep 2024},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {80},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.15407},
  doi		= {10.1613/jair.1.15407},
  abstract	= {Chatbots are conversational software applications designed
		  to interact dialectically with users for a plethora of
		  different purposes. Surprisingly, these colloquial agents
		  have only recently been coupled with computational models
		  of arguments (i.e. computational argumentation), whose aim
		  is to formalise, in a machine-readable format, the ordinary
		  exchange of information that characterises human
		  communications. Chatbots may employ argumentation with
		  different degrees and in a variety of manners. The present
		  survey sifts through the literature to review papers
		  concerning this kind of argumentation-based bot, drawing
		  conclusions about the benefits and drawbacks that this
		  approach entails in comparison with standard chatbots,
		  while also envisaging possible future development and
		  integration with the Transformer-based architecture and
		  state-of-the-art Large Language models.},
  journal	= {J. Artif. Int. Res.},
  month		= sep,
  numpages	= {40}
}

@Proceedings{	  10.1145/3696230,
  title		= {ICDTE '24: Proceedings of the 2024 8th International
		  Conference on Digital Technology in Education (ICDTE)},
  year		= {2024},
  isbn		= {9798400717574},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1109/jcdl52503.2021.00021,
  author	= {Iana, Andreea and Paulheim, Heiko},
  title		= {GraphConfRec: A Graph Neural Network-Based Conference
		  Recommender System},
  year		= {2024},
  isbn		= {9781665417709},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL52503.2021.00021},
  doi		= {10.1109/JCDL52503.2021.00021},
  abstract	= {In today's academic publishing model, especially in
		  Computer Science, conferences commonly constitute the main
		  platforms for releasing the latest peer-reviewed
		  advancements in their respective fields. However, choosing
		  a suitable academic venue for publishing one's research can
		  represent a challenging task considering the plethora of
		  available conferences, particularly for those at the start
		  of their academic careers, or for those seeking to publish
		  outside of their usual domain. In this paper, we propose
		  GraphConfRec, a conference recommender system which
		  combines SciGraph and graph neural networks, to infer
		  suggestions based not only on title and abstract, but also
		  on co-authorship and citation relationships. GraphConfRec
		  achieves a recall@10 of up to 0.580 and a MAP of up to
		  0.336 with a graph attention network-based recommendation
		  model. A user study with 25 subjects supports the positive
		  results.},
  booktitle	= {Proceedings of the 2021 ACM/IEEE Joint Conference on
		  Digital Libraries},
  pages		= {90–99},
  numpages	= {10},
  keywords	= {recommender system, graph neural network, SciGraph,
		  scientific publications},
  location	= {Virtual Event},
  series	= {JCDL '21}
}

@Article{	  10.1145/3637321,
  author	= {Peng, Zhenhui and Chen, Qiaoyi and Shen, Zhiyu and Ma,
		  Xiaojuan and Oulasvirta, Antti},
  title		= {DesignQuizzer: A Community-Powered Conversational Agent
		  for Learning Visual Design},
  year		= {2024},
  issue_date	= {April 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {CSCW1},
  url		= {https://doi.org/10.1145/3637321},
  doi		= {10.1145/3637321},
  abstract	= {Online design communities, where members exchange
		  free-form views on others' designs, offer a space for
		  beginners to learn visual design. However, the content of
		  these communities is often unorganized for learners,
		  containing many redundancies and irrelevant comments. In
		  this paper, we propose a computational approach for
		  leveraging online design communities to run a
		  conversational agent that assists informal learning of
		  visual elements (e.g., color and space). Our method
		  extracts critiques, suggestions, and rationales on visual
		  elements from comments. We present DesignQuizzer, which
		  asks questions about visual design in UI examples and
		  provides structured comment summaries. Two user studies
		  demonstrate the engagement and usefulness of DesignQuizzer
		  compared with the baseline (reading
		  reddit.com/r/UI_design). We also showcase how effectively
		  novices can apply what they learn with DesignQuizzer in a
		  design critique task and a visual design task. We discuss
		  how to use our approach with other communities and offer
		  design considerations for community-powered learning
		  support tools.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= apr,
  articleno	= {44},
  numpages	= {40},
  keywords	= {comment processing, informal learning, online communities,
		  visual design}
}

@Proceedings{	  10.1145/3623509,
  title		= {TEI '24: Proceedings of the Eighteenth International
		  Conference on Tangible, Embedded, and Embodied
		  Interaction},
  year		= {2024},
  isbn		= {9798400704024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Cork, Ireland}
}

@Proceedings{	  10.1145/3669754,
  title		= {ICCAI '24: Proceedings of the 2024 10th International
		  Conference on Computing and Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400717055},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bali Island, Indonesia}
}

@InProceedings{	  10.1145/3589335.3651898,
  author	= {Chen, Zefeng and Gan, Wensheng and Sun, Jiayi and Wu,
		  Jiayang and Yu, Philip S.},
  title		= {Open Metaverse: Issues, Evolution, and Future},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651898},
  doi		= {10.1145/3589335.3651898},
  abstract	= {With the content evolution on the web and the Internet,
		  there is a need for cyberspace that can be used to work,
		  live, and play in digital worlds regardless of geography.
		  The Metaverse provides the possibility of the future
		  Internet, representing a future trend for the web and the
		  Internet. In the future, the Metaverse is a dataspace where
		  the real and the virtual are combined instead of a virtual
		  space. In this paper, we have a comprehensive survey of the
		  compelling Metaverse, including issues of the Metaverse and
		  Metaverse's evolution and future. We hope this survey can
		  provide some helpful prospects and insightful directions
		  for the Metaverse.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1351–1360},
  numpages	= {10},
  keywords	= {dataspace, digital world, evolution, internet, issues,
		  metaverse},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1109/ase56229.2023.00150,
  author	= {Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai,
		  Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
  title		= {LiSum: Open Source Software License Summarization with
		  Multi-Task Learning},
  year		= {2024},
  isbn		= {9798350329964},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/ASE56229.2023.00150},
  doi		= {10.1109/ASE56229.2023.00150},
  abstract	= {Open source software (OSS) licenses regulate the
		  conditions under which users can reuse, modify, and
		  distribute the software legally. However, there exist
		  various OSS licenses in the community, written in a formal
		  language, which are typically long and complicated to
		  understand. In this paper, we conducted a 661-participants
		  online survey to investigate the perspectives and practices
		  of developers towards OSS licenses. The user study revealed
		  an indeed need for an automated tool to facilitate license
		  understanding. Motivated by the user study and the fast
		  growth of licenses in the community, we propose the first
		  study towards automated license summarization.
		  Specifically, we released the first high quality text
		  summarization dataset and designed two tasks, i.e., license
		  text summarization (LTS), aiming at generating a relatively
		  short summary for an arbitrary license, and license term
		  classification (LTC), focusing on the attitude inference
		  towards a predefined set of key license terms (e.g.,
		  Distribute). Aiming at the two tasks, we present LiSum, a
		  multi-task learning method to help developers overcome the
		  obstacles of understanding OSS licenses. Comprehensive
		  experiments demonstrated that the proposed jointly training
		  objective boosted the performance on both tasks, surpassing
		  state-of-the-art baselines with gains of at least 5 points
		  w.r.t. F1 scores of four summarization metrics and
		  achieving 95.13% micro average F1 score for classification
		  simultaneously. We released all the datasets, the
		  replication package, and the questionnaires for the
		  community.},
  booktitle	= {Proceedings of the 38th IEEE/ACM International Conference
		  on Automated Software Engineering},
  pages		= {787–799},
  numpages	= {13},
  keywords	= {open source software licenses, multi-task learning,
		  license comprehension},
  location	= {Echternach, Luxembourg},
  series	= {ASE '23}
}

@Proceedings{	  10.1145/3641584,
  title		= {AIPR '23: Proceedings of the 2023 6th International
		  Conference on Artificial Intelligence and Pattern
		  Recognition},
  year		= {2023},
  isbn		= {9798400707674},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@Proceedings{	  10.1145/3638530,
  title		= {GECCO '24 Companion: Proceedings of the Genetic and
		  Evolutionary Computation Conference Companion},
  year		= {2024},
  isbn		= {9798400704956},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Melbourne, VIC, Australia}
}

@InProceedings{	  10.1145/3589335.3651464,
  author	= {Huang, Run and Chattopadhyay, Souti},
  title		= {A Tale of Two Communities: Exploring Academic References
		  on Stack Overflow},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3651464},
  doi		= {10.1145/3589335.3651464},
  abstract	= {Stack Overflow is widely recognized by software
		  practitioners as the go-to resource for addressing
		  technical issues and sharing practical solutions. While not
		  typically seen as a scholarly forum, users on Stack
		  Overflow commonly refer to academic sources in their
		  discussions. Yet, little is known about these referenced
		  academic works and how they intersect the needs and
		  interests of the Stack Overflow community. To bridge this
		  gap, we conducted an exploratory large-scale study on the
		  landscape of academic references in Stack Overflow. Our
		  findings reveal that Stack Overflow communities with
		  different domains of interest engage with academic
		  literature at varying frequencies and speeds. The
		  contradicting patterns suggest that some disciplines may
		  have diverged in their interests and development
		  trajectories from the corresponding practitioner community.
		  Finally, we discuss the potential of Stack Overflow in
		  gauging the real-world relevance of academic research.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {855–858},
  numpages	= {4},
  keywords	= {citation analysis, industry impact, stack overflow},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1109/tcbb.2024.3426491,
  author	= {Kumar, Vikash and Deepak, Akshay and Ranjan, Ashish and
		  Prakash, Aravind},
  title		= {&lt;italic&gt;Bi-SeqCNN:&lt;/italic&gt; A Novel
		  Light-Weight Bi-Directional CNN Architecture for Protein
		  Function Prediction},
  year		= {2024},
  issue_date	= {Nov.-Dec. 2024},
  publisher	= {IEEE Computer Society Press},
  address	= {Washington, DC, USA},
  volume	= {21},
  number	= {6},
  issn		= {1545-5963},
  url		= {https://doi.org/10.1109/TCBB.2024.3426491},
  doi		= {10.1109/TCBB.2024.3426491},
  abstract	= {Deep learning approaches, such as convolution neural
		  networks (CNNs) and deep recurrent neural networks (RNNs),
		  have been the backbone for predicting protein function,
		  with promising state-of-the-art (SOTA) results. RNNs with
		  an in-built ability (i) focus on past information, (ii)
		  collect both &lt;italic&gt;short-and-long&lt;/italic&gt;
		  range dependency information, and (iii) bi-directional
		  processing offers a strong sequential processing mechanism.
		  CNNs, however, are confined to focusing on
		  &lt;italic&gt;short-term&lt;/italic&gt; information from
		  both the past and the future, although they offer
		  parallelism. Therefore, a novel
		  &lt;italic&gt;bi-directional CNN&lt;/italic&gt; that
		  strictly complies with the sequential processing mechanism
		  of RNNs is introduced and is used for developing a protein
		  function prediction framework, Bi-SeqCNN. This is a
		  sub-sequence-based framework. Further,
		  Bi-SeqCNN&lt;inline-formula&gt;&lt;tex-math
		  notation="LaTeX"&gt;$^+$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic
		  xlink:href="kumar-ieq1-3426491.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;
		  is an ensemble approach to better the prediction results.
		  To our knowledge, this is the first time
		  &lt;italic&gt;bi-directional CNNs&lt;/italic&gt; are
		  employed for general temporal data analysis and not just
		  for protein sequences. The proposed architecture produces
		  improvements up to +5.5% over contemporary SOTA methods on
		  three benchmark protein sequence datasets. Moreover, it is
		  substantially lighter and attain these results with
		  (0.50–0.70 times) fewer parameters than the SOTA
		  methods.},
  journal	= {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  month		= jul,
  pages		= {1922–1933},
  numpages	= {12}
}

@Proceedings{	  10.1145/3661904,
  title		= {ICETT '24: Proceedings of the 2024 10th International
		  Conference on Education and Training Technologies},
  year		= {2024},
  isbn		= {9798400717895},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Macau, China}
}

@InProceedings{	  10.1145/3616901.3616924,
  author	= {Kong, Xiangyi},
  title		= {Balanced Data Augmentation for Dialogue State Tracking},
  year		= {2024},
  isbn		= {9798400707544},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3616901.3616924},
  doi		= {10.1145/3616901.3616924},
  abstract	= {Data augmentation methods for dialogue state tracking
		  (DST) have made progress on producing more data based on
		  internal or external datasets. However, they fall short in
		  the balancing the generated dataset. We propose a balanced
		  data augmentation method for DST to improve the quality of
		  generated dataset. Our approach has two steps: (1) balance
		  method in selecting slots and values: modify the
		  possibility of selecting each slot and value when adding or
		  replacing slots; (2) balance method for under-generation
		  and over-generation: generate or delete cases based on
		  their overall frequency of slots. We apply our method to
		  MultiWOZ and the dataset generated by our method is more
		  balance than the original and COCO augmented data. Then we
		  evaluate a strong DST model trained on our generated
		  training set. The performance of this DST model improves
		  5.93% (from 55.04% to 60.97%) joint goal accuracy than
		  original model, and also surpasses the performance of the
		  model trained on the data generated by COCO. This
		  demonstrates the advantages and potential of our approach
		  to be incorporated into data augmentation for DST.},
  booktitle	= {Proceedings of the 2023 International Conference on
		  Frontiers of Artificial Intelligence and Machine Learning},
  pages		= {98–103},
  numpages	= {6},
  location	= {Beijing, China},
  series	= {FAIML '23}
}

@Proceedings{	  10.1145/3638985,
  title		= {ICIT '23: Proceedings of the 2023 11th International
		  Conference on Information Technology: IoT and Smart City},
  year		= {2023},
  isbn		= {9798400709043},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Kyoto, Japan}
}

@Proceedings{	  10.1109/3686225,
  title		= {JCDL '21: Proceedings of the 2021 ACM/IEEE Joint
		  Conference on Digital Libraries},
  year		= {2021},
  isbn		= {9781665417709},
  publisher	= {IEEE Press},
  location	= {Virtual Event}
}

@Article{	  10.1145/3656341,
  author	= {Sun, Weisong and Fang, Chunrong and Ge, Yifei and Hu,
		  Yuling and Chen, Yuchen and Zhang, Quanjun and Ge, Xiuting
		  and Liu, Yang and Chen, Zhenyu},
  title		= {A Survey of Source Code Search: A 3-Dimensional
		  Perspective},
  year		= {2024},
  issue_date	= {July 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {6},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3656341},
  doi		= {10.1145/3656341},
  abstract	= {(Source) code search is widely concerned by software
		  engineering researchers because it can improve the
		  productivity and quality of software development. Given a
		  functionality requirement usually described in a natural
		  language sentence, a code search system can retrieve code
		  snippets that satisfy the requirement from a large-scale
		  code corpus, e.g., GitHub. To realize effective and
		  efficient code search, many techniques have been proposed
		  successively. These techniques improve code search
		  performance mainly by optimizing three core components,
		  including query understanding component, code understanding
		  component, and query-code matching component. In this
		  article, we provide a 3-dimensional perspective survey for
		  code search. Specifically, we categorize existing code
		  search studies into query-end optimization techniques,
		  code-end optimization techniques, and match-end
		  optimization techniques according to the specific
		  components they optimize. These optimization techniques are
		  proposed to enhance the performance of specific components,
		  and thus the overall performance of code search.
		  Considering that each end can be optimized independently
		  and contributes to the code search performance, we treat
		  each end as a dimension. Therefore, this survey is
		  3-dimensional in nature, and it provides a comprehensive
		  summary of each dimension in detail. To understand the
		  research trends of the three dimensions in existing code
		  search studies, we systematically review 68 relevant
		  literatures. Different from existing code search surveys
		  that only focus on the query end or code end or introduce
		  various aspects shallowly (including codebase, evaluation
		  metrics, modeling technique, etc.), our survey provides a
		  more nuanced analysis and review of the evolution and
		  development of the underlying techniques used in the three
		  ends. Based on a systematic review and summary of existing
		  work, we outline several open challenges and opportunities
		  at the three ends that remain to be addressed in future
		  work.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jun,
  articleno	= {166},
  numpages	= {51},
  keywords	= {Source code search, deep learning, query-end optimization,
		  code-end optimization, match-end optimization}
}

@Article{	  10.1145/3604612,
  author	= {Dave, Nakul R. and Mehta, Mayuri A. and Kotecha, Ketan},
  title		= {A Systematic Review of Stemmers of Indian and Non-Indian
		  Vernacular Languages},
  year		= {2024},
  issue_date	= {January 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3604612},
  doi		= {10.1145/3604612},
  abstract	= {The stemming process is crucial and significant in the
		  pre-processing step of natural language processing. The
		  stemmer oversees the stemming process. It facilitates the
		  extraction of morphological variants of a root or base word
		  from the provided word. Over the period, several stemmers
		  for various vernacular languages have been proposed.
		  However, very few research studies have comprehensively
		  investigated these available stemmers. This article makes
		  multifold contributions. First, we discuss the various
		  stemmers of 15 Indian and 17 non-Indian languages
		  describing their key points, benefits, and drawbacks. All
		  the Indian languages for which stemmers have been built are
		  covered in this study. For the non-Indian languages,
		  stemmers of commonly spoken languages have been covered.
		  Second, we present a language-wise comparative analysis of
		  stemmers based on our identified parameters. Third, we
		  discuss the wordnets and dictionaries available for
		  different languages. Fourth, we provide details of the
		  datasets available for various languages. Fifth, we also
		  provide challenges in existing stemmers and future
		  directions for future researchers. The study presented in
		  this article reveals that significant research has been
		  carried out for the stemmers of influential languages such
		  as English, Arabic, and Urdu. On the other hand, languages
		  with d resources, such as Farsi, Polish, Odia, Amharic, and
		  others, have received the least attention for research.
		  Moreover, rigorous analysis reveals that most of the
		  stemmers suffer from over-stemming errors. With a complete
		  catalogue of available stemmers, this study aims at
		  assisting the researchers and professionals working in the
		  areas such as information retrieval, semantic annotation,
		  word meaning disambiguation, and ontology learning.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  articleno	= {18},
  numpages	= {51},
  keywords	= {Natural Language Processing (NLP), stemming, rule-based
		  stemmer, dictionary-based stemmer, hybrid stemmer,
		  over-stemming error, under-stemming error}
}

@Article{	  10.1145/3582261,
  author	= {Wei, Kaiwen and Jin, Li and Zhang, Zequn and Guo, Zhi and
		  Li, Xiaoyu and Liu, Qing and Feng, Weimiao},
  title		= {More Than Syntaxes: Investigating Semantics to Zero-shot
		  Cross-lingual Relation Extraction and Event Argument Role
		  Labelling},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {23},
  number	= {5},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3582261},
  doi		= {10.1145/3582261},
  abstract	= {Syntactic dependency structures are commonly utilized as
		  language-agnostic features to solve the word order
		  difference issues in zero-shot cross-lingual relation and
		  event extraction tasks. However, while sentences in
		  multiple forms can be employed to express the same meaning,
		  the syntactic structure may vary considerably in specific
		  scenarios. To fix this problem, we find semantics are
		  rarely considered, which could provide a more consistent
		  semantic analysis of sentences and be served as another
		  bridge between different languages. Therefore, in this
		  article, we introduce Syntax and Semantic Driven Network
		  (SSDN) to equip syntax and semantic knowledge across
		  languages simultaneously. Specifically,
		  predicate–argument structures from semantic role
		  labelling are explicitly incorporated into word
		  representations. Then, a semantic-aware relational graph
		  convolutional network and a transformer-based encoder are
		  utilized to model both semantic dependency and syntactic
		  dependency structures, respectively. Finally, a fusion
		  module is introduced to integrate output representations
		  adaptively. We conduct experiments on the widely used
		  Automatic Content Extraction 2005 English, Chinese, and
		  Arabic datasets. The evaluation results demonstrate that
		  the proposed method achieves the state-of-the-art
		  performance. Further study also indicates SSDN could
		  produce robust representations that facilitate the transfer
		  operations across languages.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= may,
  articleno	= {61},
  numpages	= {21},
  keywords	= {Cross-lingual relation and event extraction, zero-resource
		  transfer, semantic parsing, relational graph convolutional
		  network}
}

@Proceedings{	  10.1145/3686812,
  title		= {ICCMS '24: Proceedings of the 2024 16th International
		  Conference on Computer Modeling and Simulation},
  year		= {2024},
  isbn		= {9798400717215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dalian, China}
}

@Proceedings{	  10.1145/3678890,
  title		= {RAID '24: Proceedings of the 27th International Symposium
		  on Research in Attacks, Intrusions and Defenses},
  year		= {2024},
  isbn		= {9798400709593},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Padua, Italy}
}

@Proceedings{	  10.1145/3666015,
  title		= {ICSSP '24: Proceedings of the 2024 International
		  Conference on Software and Systems Processes},
  year		= {2024},
  isbn		= {9798400709913},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {M\, Germany}
}

@InProceedings{	  10.1145/3702038.3702113,
  author	= {Braga, Dieinison Jack Freire and Silva, Marisa Carmo da
		  and Lima, Raul de Ara\'{u}jo and Barbosa, Gabriel Diniz
		  Junqueira and Barbosa, Simone Diniz Junqueira},
  title		= {VisStoryMaker: supporting non-expert analysts in visually
		  exploring datasets and communicating insights with visual
		  annotations and data stories},
  year		= {2024},
  isbn		= {9798400712241},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3702038.3702113},
  doi		= {10.1145/3702038.3702113},
  abstract	= {Due to data production and availability growth,
		  professionals in several disciplines have been facing an
		  increasing need to explore and understand data, obtain
		  insights, and communicate them effectively. Many
		  visualization systems have been developed commercially and
		  within the research community to support non-expert
		  analysts. We can consider at least three challenges these
		  tools aim to face: support the selection of appropriate
		  visualizations and decide on the visual mappings, extract
		  and communicate factual information from the
		  visualizations, and use visualizations in data-rich
		  narratives. In response to these challenges, we developed
		  VisStoryMaker, a visualization tool that supports both
		  exploration and communication about data. To aid users in
		  exploring and understanding data, VisStoryMaker recommends
		  visualizations through system-generated questions and data
		  facts. To support communicating about data, the system
		  recommends visual annotations of data facts and provides a
		  story-building module, allowing analysts to use the
		  generated charts and facts as a blueprint for a data story.
		  We have conducted empirical studies to compare
		  VisStoryMaker’s features with existing applications:
		  chart recommendations with Voyager&nbsp;2, storytelling
		  construction with Flourish, and data facts and chart
		  annotations with Tableau. Our findings indicate that the
		  system-generated questions and data facts supported
		  non-expert analysts in exploratory analysis. They perceived
		  visual data facts annotations as useful and supported them
		  in raising hypotheses about the data, understanding data,
		  and leading to insights, thus enhancing data analysis.
		  Participants perceived the visual annotations and
		  StoryMaker as helpful in organizing the system-generated
		  pieces of information and incorporating them into
		  comprehensive narratives and presentations.},
  booktitle	= {Proceedings of the XXIII Brazilian Symposium on Human
		  Factors in Computing Systems},
  articleno	= {74},
  numpages	= {15},
  keywords	= {Visualization recommendations, Exploratory questions, Data
		  facts, Visual annotations, Data story, Visual data
		  exploration, Visual data communication},
  location	= { },
  series	= {IHC '24}
}

@Article{	  10.1109/taslp.2024.3485547,
  author	= {Ma, Jun-Yu and Gu, Jia-Chen and Ling, Zhen-Hua and Liu,
		  Quan and Liu, Cong and Hu, Guoping},
  title		= {Syntax-Augmented Hierarchical Interactive Encoder for
		  Zero-Shot Cross-Lingual Information Extraction},
  year		= {2024},
  issue_date	= {2024},
  publisher	= {IEEE Press},
  volume	= {32},
  issn		= {2329-9290},
  url		= {https://doi.org/10.1109/TASLP.2024.3485547},
  doi		= {10.1109/TASLP.2024.3485547},
  abstract	= {Zero-shot cross-lingual information extraction (IE) aims
		  at constructing an IE model for some low-resource target
		  languages, given annotations exclusively in some
		  rich-resource languages. Recent studies have shown
		  language-universal features can bridge the gap between
		  languages. However, prior work has neither explored the
		  potential of establishing interactions between
		  language-universal features and contextual representations
		  nor incorporated features that can effectively model
		  constituent span attributes and relationships between
		  multiple spans. In this study, a
		  &lt;bold&gt;s&lt;/bold&gt;yntax-augmented
		  &lt;bold&gt;h&lt;/bold&gt;ierarchical
		  &lt;bold&gt;in&lt;/bold&gt;teractive
		  &lt;bold&gt;e&lt;/bold&gt;ncoder (SHINE) is proposed to
		  transfer cross-lingual IE knowledge. The proposed encoder
		  is capable of interactively capturing complementary
		  information between features and contextual information, to
		  derive language-agnostic representations for various
		  cross-lingual IE tasks. Concretely, a multi-level
		  interaction network is designed to hierarchically interact
		  the complementary information to strengthen domain
		  adaptability. Besides, in addition to the well-studied
		  word-level syntax features of part-of-speech and dependency
		  relation, a new span-level syntax feature of constituency
		  structure is introduced to model the constituent span
		  information which is crucial for IE. Experiments across
		  seven languages on three IE tasks and four benchmarks
		  verify the effectiveness and generalization ability of the
		  proposed method.},
  journal	= {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  month		= oct,
  pages		= {4795–4809},
  numpages	= {15}
}

@Proceedings{	  10.1145/3691422,
  title		= {ICEME '24: Proceedings of the 2024 15th International
		  Conference on E-business, Management and Economics},
  year		= {2024},
  isbn		= {9798400717260},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3631085,
  title		= {SBGames '23: Proceedings of the 22nd Brazilian Symposium
		  on Games and Digital Entertainment},
  year		= {2023},
  isbn		= {9798400716270},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rio Grande (RS), Brazil}
}

@Proceedings{	  10.1145/3703847,
  title		= {SHWID '24: Proceedings of the 2024 International
		  Conference on Smart Healthcare and Wearable Intelligent
		  Devices},
  year		= {2024},
  isbn		= {9798400709746},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3698062,
  title		= {WSSE '24: Proceedings of the 2024 The 6th World Symposium
		  on Software Engineering (WSSE)},
  year		= {2024},
  isbn		= {9798400717086},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3697467,
  title		= {IoTML '24: Proceedings of the 2024 4th International
		  Conference on Internet of Things and Machine Learning},
  year		= {2024},
  isbn		= {9798400710353},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3597503.3639185,
  author	= {Ferrara, Carmine and Casillo, Francesco and Gravino,
		  Carmine and De Lucia, Andrea and Palomba, Fabio},
  title		= {ReFAIR: Toward a Context-Aware Recommender for Fairness
		  Requirements Engineering},
  year		= {2024},
  isbn		= {9798400702174},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3597503.3639185},
  doi		= {10.1145/3597503.3639185},
  abstract	= {Machine learning (ML) is increasingly being used as a key
		  component of most software systems, yet serious concerns
		  have been raised about the fairness of ML predictions.
		  Researchers have been proposing novel methods to support
		  the development of fair machine learning solutions.
		  Nonetheless, most of them can only be used in late
		  development stages, e.g., during model training, while
		  there is a lack of methods that may provide practitioners
		  with early fairness analytics enabling the treatment of
		  fairness throughout the development lifecycle. This paper
		  proposes ReFair, a novel context-aware requirements
		  engineering framework that allows to classify sensitive
		  features from User Stories. By exploiting natural language
		  processing and word embedding techniques, our framework
		  first identifies both the use case domain and the machine
		  learning task to be performed in the system being
		  developed; afterward, it recommends which are the
		  context-specific sensitive features to be considered during
		  the implementation. We assess the capabilities of ReFair by
		  experimenting it against a synthetic dataset---which we
		  built as part of our research---composed of 12,401 User
		  Stories related to 34 application domains. Our findings
		  showcase the high accuracy of ReFair, other than
		  highlighting its current limitations.},
  booktitle	= {Proceedings of the IEEE/ACM 46th International Conference
		  on Software Engineering},
  articleno	= {213},
  numpages	= {12},
  keywords	= {software fairness, machine learning, requirements
		  engineering},
  location	= {Lisbon, Portugal},
  series	= {ICSE '24}
}

@InProceedings{	  10.1109/jcdl52503.2021.00060,
  author	= {Gunaratne, Chathika and Walker, Vickie and Rooney, Andrew
		  and Patton, Robert and Wolfe, Mary and Schmitt, Charles},
  title		= {Weak Supervision for Scientific Document Relevance Tagging
		  Drahomira Herrmannova},
  year		= {2024},
  isbn		= {9781665417709},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/JCDL52503.2021.00060},
  doi		= {10.1109/JCDL52503.2021.00060},
  abstract	= {Developing training data for predicting the relevance of
		  research articles to scientific concepts is a
		  resource-intensive process, and existing datasets are only
		  available for limited subject domains. In this work, we
		  investigate the possibility of weakly supervised data
		  generation for developing relevance models. We approach
		  this by generating document, query, and label triples in an
		  automated manner and by using this data to create a
		  training set for a classification model. Published
		  documents were sampled from an open access repository, and
		  the concepts appearing in these documents were used as
		  queries. We use the location of occurrence of each query
		  concept within a document to determine the relevance label.
		  We find that a classification model trained on this
		  synthetic data can learn to tag documents according to
		  their relevance to a query surprisingly well, providing an
		  11% f-score improvement over a model trained on ground
		  truth data.},
  booktitle	= {Proceedings of the 2021 ACM/IEEE Joint Conference on
		  Digital Libraries},
  pages		= {338–339},
  numpages	= {2},
  location	= {Virtual Event},
  series	= {JCDL '21}
}

@Article{	  10.1145/3638243,
  author	= {Oakes, Bentley James and Famelis, Michalis and Sahraoui,
		  Houari},
  title		= {Building Domain-Specific Machine Learning Workflows: A
		  Conceptual Framework for the State of the Practice},
  year		= {2024},
  issue_date	= {May 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {33},
  number	= {4},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3638243},
  doi		= {10.1145/3638243},
  abstract	= {Domain experts are increasingly employing machine learning
		  to solve their domain-specific problems. This article
		  presents to software engineering researchers the six key
		  challenges that a domain expert faces in addressing their
		  problem with a computational workflow, and the underlying
		  executable implementation. These challenges arise out of
		  our conceptual framework which presents the “route” of
		  transformations that a domain expert may choose to take
		  while developing their solution.To ground our conceptual
		  framework in the state of the practice, this article
		  discusses a selection of available textual and graphical
		  workflow systems and their support for the transformations
		  described in our framework. Example studies from the
		  literature in various domains are also examined to
		  highlight the tools used by the domain experts as well as a
		  classification of the domain specificity and machine
		  learning usage of their problem, workflow, and
		  implementation.The state of the practice informs our
		  discussion of the six key challenges, where we identify
		  which challenges and transformations are not sufficiently
		  addressed by available tools. We also suggest possible
		  research directions for software engineering researchers to
		  increase the automation of these tools and disseminate
		  best-practice techniques between software engineering and
		  various scientific domains.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= apr,
  articleno	= {91},
  numpages	= {50},
  keywords	= {Computational workflow, workflow composition, domain
		  experts, machine learning, machine learning pipelines,
		  software engineering framework}
}

@Proceedings{	  10.1145/3654823,
  title		= {CACML '24: Proceedings of the 2024 3rd Asia Conference on
		  Algorithms, Computing and Machine Learning},
  year		= {2024},
  isbn		= {9798400716416},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shanghai, China}
}

@Proceedings{	  10.1145/3702038,
  title		= {IHC '24: Proceedings of the XXIII Brazilian Symposium on
		  Human Factors in Computing Systems},
  year		= {2024},
  isbn		= {9798400712241},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3698393,
  author	= {Javed, Yousra and Sajid, Ayesha},
  title		= {A Systematic Review of Privacy Policy Literature},
  year		= {2024},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3698393},
  doi		= {10.1145/3698393},
  abstract	= {An organization’s privacy policy states how it collects,
		  stores, processes, and shares its users’ personal
		  information. The growing number of data protection laws and
		  regulations, as well as the numerous sectors where the
		  organizations are collecting user information, has led to
		  the investigation of privacy policies with regards to their
		  accessibility, readability, completeness, comparison with
		  organization’s actual data practices, use of machine
		  learning/natural language processing for automated
		  analysis, and comprehension/perception/concerns of
		  end-users via summarization/visualization tools and user
		  studies. However, there is limited work on systematically
		  reviewing the existing research on this topic. We address
		  this gap by conducting a systematic review of the existing
		  privacy policy literature. To this end, we compiled and
		  analyzed 202 papers (published till 31st December, 2023)
		  that investigated privacy policies. Our work advances the
		  field of privacy policies by summarizing the analysis
		  techniques that have been used to study them, the data
		  protection laws/regulations explored, and the sectors to
		  which these policies pertain. We provide actionable
		  insights for organizations to achieve better end-user
		  privacy.},
  journal	= {ACM Comput. Surv.},
  month		= nov,
  articleno	= {45},
  numpages	= {43},
  keywords	= {Privacy policy, systematic literature review, survey, data
		  protection, personal information}
}

@Proceedings{	  10.1145/3685651,
  title		= {eSAAM '24: Proceedings of the 4th Eclipse Security, AI,
		  Architecture and Modelling Conference on Data Space},
  year		= {2024},
  isbn		= {9798400709845},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Mainz, Germany}
}

@InProceedings{	  10.1145/3613905.3650810,
  author	= {Chakraborti, Mahasweta and Bonagiri, Sailendra Akash and
		  Virg\"{u}ez-Ruiz, Santiago and Frey, Seth},
  title		= {NLP4Gov: A Comprehensive Library for Computational Policy
		  Analysis},
  year		= {2024},
  isbn		= {9798400703317},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613905.3650810},
  doi		= {10.1145/3613905.3650810},
  abstract	= {Formal rules and policies are fundamental in formally
		  specifying a social system: its operation, boundaries,
		  processes, and even ontology. Recent scholarship has
		  highlighted the role of formal policy in collective
		  knowledge creation, game communities, the production of
		  digital public goods, and national social media governance.
		  Researchers have shown interest in how online communities
		  convene tenable self-governance mechanisms to regulate
		  member activities and distribute rights and privileges by
		  designating responsibilities, roles, and hierarchies. We
		  present NLP4Gov, an interactive kit to train and aid
		  scholars and practitioners alike in computational policy
		  analysis. The library explores and integrates methods and
		  capabilities from computational linguistics and NLP to
		  generate semantic and symbolic representations of community
		  policies from text records. Versatile, documented, and
		  accessible, NLP4Gov provides granular and comparative views
		  into institutional structures and interactions, along with
		  other information extraction capabilities for downstream
		  analysis.},
  booktitle	= {Extended Abstracts of the CHI Conference on Human Factors
		  in Computing Systems},
  articleno	= {248},
  numpages	= {8},
  keywords	= {Collective Action, OSS Governance, Online Communities,
		  Open Source Software, Peer Production, Policy Analysis},
  location	= {Honolulu, HI, USA},
  series	= {CHI EA '24}
}

@Proceedings{	  10.1145/3620666,
  title		= {ASPLOS '24: Proceedings of the 29th ACM International
		  Conference on Architectural Support for Programming
		  Languages and Operating Systems, Volume 3},
  year		= {2024},
  isbn		= {9798400703867},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  abstract	= {Welcome to the third volume of ASPLOS'24: the 29th ACM
		  International Conference on Architectural Support for
		  Programming Languages and Operating Systems. This document
		  is mostly dedicated to the 2024 fall cycle but also
		  provides some statistics summarizing all three cycles.We
		  introduced several notable changes to ASPLOS this year,
		  most of which were discussed in our previous messages from
		  program chairs in Volume 1 and 2, including: (1)
		  significantly increasing the program committee size to over
		  220 members (more than twice the size of last year); (2)
		  foregoing synchronous program committee (PC) meetings and
		  instead making all decisions online; (3) overhauling the
		  review assignment process; (4) developing an automated
		  submission format violation identifier script that
		  uncovers, e.g., disallowed vertical space manipulations
		  that "squeeze" space; (5) introducing the new ASPLOS role
		  of Program Vice Chairs to cope with the increased number of
		  submissions and the added load caused by foregoing
		  synchronous program committee; and (6) characterizing a
		  systematic problem that ASPLOS is facing in reviewing
		  quantum computing submissions, describing how we addressed
		  it and highlighting how we believe that it should be
		  handled in the future.Assuming readers have read our
		  previous messages, here, we will only describe differences
		  between the current cycle and the previous ones. These
		  include: (1) Finally unifying submission and acceptance
		  paper formatting instructions (forgoing the `jpaper' class)
		  to rid authors of accepted papers from the need to
		  reformat; (2) Describing the methodology we employed to
		  select best papers, which we believe ensures quality and
		  hope will persist; and (3) Reporting the ethical incidents
		  we encountered and how we handled them. In the final,
		  fourth volume, when the outcome of the ASPLOS'24 fall major
		  revisions will become known, we plan to conduct a broader
		  analysis of all the data we have gathered throughout the
		  year.Following are some key statistics of the fall cycle:
		  340 submissions were finalized (43% more than last year's
		  fall count and 17% less than our summer cycle) of which 111
		  are related to accelerators/FPGAs/GPUs, 105 to machine
		  learning, 54 to security, 50 to datacenter/cloud and 50 to
		  storage/memory; 183 (54%) submissions were promoted to the
		  second review round; 39 (11.5%) papers were accepted (of
		  which 19 were awarded artifact evaluation badges); 33
		  (9.7%) submissions were allowed to submit major revisions
		  and are currently under review (these will be addressed in
		  the fourth volume of ASPLOS'24 and will be presented in
		  ASPLOS'25 if accepted); 1,368 reviews were uploaded; and
		  4,949 comments were generated during online discussions, of
		  which 4,070 were dedicated to the submissions that made it
		  to the second review round.This year, in the submission
		  form, we asked authors to specify which of the three ASPLOS
		  research areas are related to their submitted work.
		  Analyzing this data revealed that 80%, 39%, and 29% of the
		  submissions are categorized by their authors as related to
		  architecture, operating systems, and programming languages,
		  respectively, generating the highest difference we have
		  observed across the cycles between architecture and the
		  other two. About 46% of the fall submissions are
		  "interdisciplinary," namely, were associated with two or
		  more of the three areas.Overall, throughout all the
		  ASPLOS'24 cycles, we received 922 submissions, constituting
		  a 1.54x increase compared to last year. Our reviewers
		  submitted a total of 3,634 reviews containing more than 2.6
		  million words, and we also generated 12,655 online comments
		  consisting of nearly 1.2 million words. As planned, PC
		  members submitted an average of 15.7 reviews and a median
		  of 15, and external review committee (ERC) members
		  submitted an average of 4.7 and a median of 5.We accepted
		  170 papers thus far, written by 1100 authors, leading to an
		  18.4% acceptance rate, with the aforementioned 33 major
		  revisions still under review. Assuming that the revision
		  acceptance rate will be similar to that of previous cycles,
		  we estimate that ASPLOS'24 will accept nearly 200 (!)
		  papers, namely, 21%–22% of the submissions.The ASPLOS'24
		  program consists of 193 papers: the 170 papers we accepted
		  thus far and, in addition, 23 major revisions from the fall
		  cycle of ASPLOS'23, which were re-reviewed and accepted.
		  The full details are available in the PDF of the front
		  matter.},
  location	= {La Jolla, CA, USA}
}

@Proceedings{	  10.1145/3690931,
  title		= {AIAHPC '24: Proceedings of the 2024 4th International
		  Conference on Artificial Intelligence, Automation and High
		  Performance Computing},
  year		= {2024},
  isbn		= {9798400710049},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Zhuhai, China}
}

@Article{	  10.1145/3702234,
  author	= {Fang, Chen and Wang, Yidong and Song, Yunze and Long,
		  Qingqing and Lu, Wang and Chen, Linghui and Feng, Guihai
		  and Zhou, Yuanchun and Li, Xin},
  title		= {How do Large Language Models understand Genes and Cells},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3702234},
  doi		= {10.1145/3702234},
  abstract	= {Researching genes and their interactions is crucial for
		  deciphering the fundamental laws of cellular activity,
		  advancing disease treatment, drug discovery, and more.
		  Large language Models (LLMs), with their profound text
		  comprehension and generation capabilities, have made
		  significant strides across various natural science fields.
		  However, their application in cell biology remains limited
		  and a systematic evaluation of their performance is
		  lacking. To address this gap, in this paper, we select
		  seven mainstream LLMs and evaluate their performance across
		  nine gene-related problem scenarios. Our findings indicate
		  that LLMs possess a certain level of understanding of genes
		  and cells, but still lag behind domain-specific models in
		  comprehending transcriptional expression profiles.
		  Moreover, we have improved the current method of textual
		  representation of cells, enhancing the LLMs’ ability to
		  tackle cell annotation tasks. We encourage cell biology
		  researchers to leverage LLMs for problem-solving while
		  being mindful of the associated challenges. We release our
		  code and data at .},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= oct,
  keywords	= {large language models, cell biology, gene gene
		  interaction, cell annotation}
}

@InProceedings{	  10.1145/3652620.3688223,
  author	= {Hallak, Yara and Blouin, Dominique and Pautet, Laurent and
		  Saab, Layale and Laborie, Baptiste and Mittal, Rakshit},
  title		= {Model Management at Renault Virtual Simulation Team: State
		  of Practice, Challenges and Research Directions},
  year		= {2024},
  isbn		= {9798400706226},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3652620.3688223},
  doi		= {10.1145/3652620.3688223},
  abstract	= {In the automotive industry, new systems are being
		  developed to enhance vehicle safety and driver convenience.
		  These systems are increasingly complex to build and
		  maintain. To develop these systems Renault makes intensive
		  use of simulation and must deal with thousands of models.
		  This huge number of models must be well managed. To manage
		  these models, Renault has developed the SysML-based Model
		  Identity Card (MIC), used with a Model-Based Simulation
		  (MBSi) approach. However, despite this first solution,
		  managing simulation models remains a difficult task.In this
		  paper, we describe the current simulation model management
		  approaches used at Renault, and their shortcomings and
		  challenges in the modelling and simulation of complex
		  automotive systems. We use Advanced Driver Assistance
		  System (ADAS) and its Automatic Emergency Breaking (AEB)
		  sub-system as examples to illustrate the utilization of the
		  MIC and demonstrate current practices. From these examples,
		  we derive main challenges faced by the virtual simulation
		  team and propose research directions to solve them, based
		  on state of the art methodologies for simulation models'
		  validation and verification management.},
  booktitle	= {Proceedings of the ACM/IEEE 27th International Conference
		  on Model Driven Engineering Languages and Systems},
  pages		= {1005–1014},
  numpages	= {10},
  keywords	= {model management, model-based systems engineering, model
		  identity card, model-based simulation, advanced driver
		  assistance system},
  location	= {Linz, Austria},
  series	= {MODELS Companion '24}
}

@InProceedings{	  10.1145/3666015.3666016,
  author	= {Boudjemila, Chahrazed and Dagnat, Fabien and
		  Mart\'{\i}nez, Salvador},
  title		= {Maintaining Security Consistency During System Development
		  with Security-Oriented Model Federation},
  year		= {2024},
  isbn		= {9798400709913},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3666015.3666016},
  doi		= {10.1145/3666015.3666016},
  abstract	= {Multi-modeling is an approach within the MDE realm that
		  promotes the development of complex systems by decomposing
		  them in sets of heterogeneous models. These models are
		  defined using different modeling languages and constructed
		  using diverse tools. They represent different but often
		  interdependent views. However, the models of a system are
		  far from being static. They change to accommodate new
		  requirements, functionality improvements, bug fixes, and
		  other evolution events. These changes represent a challenge
		  w.r.t. consistency. This is especially true in
		  security-critical scenarios. Indeed, security information
		  is often integrated within the systems models so that
		  security requirements are met following what is called
		  "security-by-design". In such scenarios, the security
		  concern of the systems models must remain consistent across
		  changes so that security properties continue to hold. In
		  order to tackle this problem, we propose a methodology to
		  enhance the (multi)model-based design phase of a system
		  development process. It comprises the creation of a
		  security federation in which security dependencies between
		  the different models are reified and equipped with security
		  rules expressing security consistency requirements. Then,
		  whenever a model is changed, the security rules are
		  evaluated to monitor the consistency of security across the
		  system models. We evaluate the capabilities of this
		  methodology by a prototype implementation and its
		  application to different use cases.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Software and Systems Processes},
  pages		= {66–76},
  numpages	= {11},
  keywords	= {Model-driven engineering, model evolution., model
		  federation, security by design},
  location	= {M\, Germany},
  series	= {ICSSP '24}
}

@Proceedings{	  10.1145/3643664,
  title		= {WSESE '24: Proceedings of the 1st IEEE/ACM International
		  Workshop on Methodological Issues with Empirical Studies in
		  Software Engineering},
  year		= {2024},
  isbn		= {9798400705670},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {WSESE 2024 was a one-day event held on April 16, 2024, in
		  Lisbon, Portugal. The theme of the workshop was
		  "Methodological Issues with Empirical Studies in Software
		  Engineering". The primary goal was to gain a better
		  understanding of the adoption of the empirical paradigm in
		  SE. Specifically, our focus was on identifying, discussing
		  and finding solutions for the issues in the empirical
		  methods currently employed. The workshop provided an
		  opportunity for researchers and practitioners to discuss
		  current methodological challenges and explore ways to
		  address them.},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3632971,
  title		= {JCRAI '23: Proceedings of the 2023 International Joint
		  Conference on Robotics and Artificial Intelligence},
  year		= {2023},
  isbn		= {9798400707704},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Shanghai, China}
}

@Proceedings{	  10.1145/3663976,
  title		= {CVIPPR '24: Proceedings of the 2024 2nd Asia Conference on
		  Computer Vision, Image Processing and Pattern Recognition},
  year		= {2024},
  isbn		= {9798400716607},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Xiamen, China}
}

@Proceedings{	  10.1145/3664934,
  title		= {ICIEI '24: Proceedings of the 2024 9th International
		  Conference on Information and Education Innovations},
  year		= {2024},
  isbn		= {9798400716409},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Verbania, Italy}
}

@Proceedings{	  10.1145/3610977,
  title		= {HRI '24: Proceedings of the 2024 ACM/IEEE International
		  Conference on Human-Robot Interaction},
  year		= {2024},
  isbn		= {9798400703225},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome one and all to the 19th Annual ACM/IEEE
		  International Conference on Human-Robot Interaction
		  (HRI)!We are so pleased to re-welcome the HRI community to
		  Boulder, Colorado, where HRI 2021 would have been held, had
		  the COVID pandemic not interfered. Following up on the
		  successful in-person conference held last year in Sweden,
		  this year's theme is "HRI in the Real World," and focuses
		  on advances that aim to bring human-robot interaction out
		  of the lab and into everyday life.One aspect of this that
		  we are very excited about is the introduction of a robot
		  challenge to the conference activities, where teams from
		  around the world will showcase their research and
		  development via actual, interactive robots in the "real
		  world" of an academic conference. It is our hope that this
		  feature will grow and develop over the coming years into a
		  staple of the HRI conference.This year's HRI conference saw
		  an impressive surge in global interest, with 352 full paper
		  submissions from around the world, marking a significant
		  40% increase compared to the previous year. These papers
		  were categorized under relevant thematic subcommittees and
		  underwent a double-blind review process, a rebuttal phase,
		  and selective shepherding by the HRI program committee.
		  From this process, 87 outstanding papers (24.7%) were
		  chosen for full presentation at the conference. Reflecting
		  our joint sponsorship with IEEE and ACM, all accepted
		  papers will be accessible in the ACM Digital Library and
		  IEEE Xplore.},
  location	= {Boulder, CO, USA}
}

@Proceedings{	  10.1145/3676288,
  title		= {SSDBM '24: Proceedings of the 36th International
		  Conference on Scientific and Statistical Database
		  Management},
  year		= {2024},
  isbn		= {9798400710209},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Rennes, France}
}

@Article{	  10.1145/3661484,
  author	= {Kr\"{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu,
		  Chenguang and Chechik, Marsha and Berger, Thorsten and
		  Rubin, Julia},
  title		= {A Meta-Study of Software-Change Intentions},
  year		= {2024},
  issue_date	= {December 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {56},
  number	= {12},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3661484},
  doi		= {10.1145/3661484},
  abstract	= {Every software system undergoes changes, for example, to
		  add new features, fix bugs, or refactor code. The
		  importance of understanding software changes has been
		  widely recognized, resulting in various techniques and
		  studies, for instance, on change-impact analysis or
		  classifying developers’ activities. Since changes are
		  triggered by developers’ intentions—something they plan
		  or want to change in the system—many researchers have
		  studied intentions behind changes. While there appears to
		  be a consensus among software-engineering researchers and
		  practitioners that knowing the intentions behind software
		  changes is important, it is not clear how developers can
		  actually benefit from this knowledge. In fact, there is no
		  consolidated, recent overview of the state of the art on
		  software-change intentions (SCIs) and their relevance for
		  software engineering. We present a meta-study of 122
		  publications, which we used to derive a categorization of
		  SCIs and to discuss motivations, evidence, and techniques
		  relating to SCIs. Unfortunately, we found that individual
		  pieces of research are often disconnected from each other,
		  because a common understanding is missing. Similarly, some
		  publications showcase the potential of knowing SCIs, but
		  more substantial research to understand the practical
		  benefits of knowing SCIs is needed. Our contributions can
		  help researchers and practitioners improve their
		  understanding of SCIs and how SCIs can aid software
		  engineering tasks.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {300},
  numpages	= {41},
  keywords	= {Intentions, software evolution, change management, version
		  control}
}

@Proceedings{	  10.1145/3670013,
  title		= {IC4E '24: Proceedings of the 2024 15th International
		  Conference on E-Education, E-Business, E-Management and
		  E-Learning},
  year		= {2024},
  isbn		= {9798400717062},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Fukuoka-shi, Japan}
}

@Proceedings{	  10.5555/3635637,
  title		= {AAMAS '24: Proceedings of the 23rd International
		  Conference on Autonomous Agents and Multiagent Systems},
  year		= {2024},
  isbn		= {9798400704864},
  publisher	= {International Foundation for Autonomous Agents and
		  Multiagent Systems},
  address	= {Richland, SC},
  abstract	= {Welcome to AAMAS-2024, the 23th edition of the
		  International Conference on Autonomous Agents and
		  Multiagent Systems!AAMAS is the largest and most
		  influential conference in the area of agents and multiagent
		  systems, bringing together researchers and practitioners in
		  all areas of agent technology and providing an
		  internationally renowned high-profile forum for publishing
		  and finding out about the latest developments in the field.
		  AAMAS is the flagship conference of the non-profit
		  International Foundation for Autonomous Agents and
		  Multiagent Systems (IFAAMAS).After two attempts to hold
		  AAMAS in New Zealand for the first time, which were forced
		  online by the COVID19 pandemic, we are happy that the 2024
		  edition finally comes to Auckland, New Zealand. Previous
		  editions were held in Bologna (2002), Melbourne (2003), New
		  York (2004), Utrecht (2005), Hakodate (2006), Honolulu
		  (2007), Estoril (2008), Budapest (2009), Toronto (2010),
		  Taipei (2011), Valencia (2012), Saint Paul (2013), Paris
		  (2014), Istanbul (2015), Singapore (2016), Sao Paulo
		  (2017), Stockholm (2018), Montreal (2019), Auckland/online
		  (2020), London/online (2021), Auckland/online (2022), and
		  London (2023).},
  location	= {Auckland, New Zealand}
}

@InProceedings{	  10.1145/3589335.3641302,
  author	= {Ragab, Mohamed and Savateeve, Yury and Wang, Wenjie and
		  Moosaei, Reza and Tiropanis, Thanassis and Poulovassilis,
		  Alexandra and Chapman, Adriane and Oliver, Helen and
		  Roussos, George},
  title		= {The 1st Workshop on Decentralised Search and
		  Recommendation},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3641302},
  doi		= {10.1145/3589335.3641302},
  abstract	= {The DESERE Workshop, our First Workshop on Decentralised
		  Search and Recommendation, offers a platform for
		  researchers to explore and share innovative ideas on
		  decentralised web services, mainly focusing on three major
		  topics: (i) societal impact of decentralised systems: their
		  effects on privacy, policy, and regulation; (ii) decen-
		  tralising applications: algorithmic and performance
		  challenges that arise from decentralisation; and (iii)
		  infrastructure to support de- centralised systems and
		  services: peer-to-peer networks, routing, and performance
		  evaluation tools.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1705–1708},
  numpages	= {4},
  keywords	= {decentralised web, distributed search, distributed
		  systems, network algorithms, recommendation systems},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@InProceedings{	  10.1145/3665601.3669844,
  author	= {Huang, Zezhou},
  title		= {Disambiguate Entity Matching using Large Language Models
		  through Relation Discovery},
  year		= {2024},
  isbn		= {9798400706943},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3665601.3669844},
  doi		= {10.1145/3665601.3669844},
  abstract	= {Entity matching is a critical problem in data integration,
		  central to tasks like fuzzy joins for tuple enrichment.
		  Traditional approaches have focused on overcoming fuzzy
		  term representations through methods such as edit distance,
		  Jaccard similarity, and more recently, embeddings and deep
		  neural networks, including advancements from large language
		  models (LLMs) like GPT. However, when integrating with
		  external databases, the core challenge in entity matching
		  extends beyond term fuzziness to the ambiguity in defining
		  what constitutes a "match". This is because external
		  databases contain tuples with varying levels of detail and
		  granularity among entities, and an "exact match" in
		  traditional entity matching rarely happens. As a result,
		  understanding how entities are related and the potential
		  nuances is critical, especially for high-stake tasks for
		  responsible AI. In this work, we study a case problem of
		  entity matching for ESG reporting. We propose a novel
		  approach that shifts focus from purely identifying semantic
		  similarities to understanding and defining the "relations"
		  between entities for resolving ambiguities in matching,
		  with a human-in-the-loop process to make the final
		  decision. By pre-defining a set of relations relevant to
		  the task at hand, our method allows analysts to navigate
		  the spectrum of similarity more effectively, from exact
		  matches to conceptually related entities, and responsibly
		  perform downstream tasks.},
  booktitle	= {Proceedings of the Conference on Governance, Understanding
		  and Integration of Data for Effective and Responsible AI},
  pages		= {36–39},
  numpages	= {4},
  keywords	= {Data Integration, Entity Matching, Large Language Models},
  location	= {Santiago, AA, Chile},
  series	= {GUIDE-AI '24}
}

@Proceedings{	  10.1145/3603166,
  title		= {UCC '23: Proceedings of the IEEE/ACM 16th International
		  Conference on Utility and Cloud Computing},
  year		= {2023},
  isbn		= {9798400702341},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The IEEE/ACM International Conference on Utility and Cloud
		  Computing (UCC) is a premier annual conference series
		  aiming to provide a platform for researchers from both
		  academia and industry to present new discoveries in the
		  broad area of Cloud and Edge utility computing and
		  applications.},
  location	= {Taormina (Messina), Italy}
}

@Article{	  10.1145/3709351,
  author	= {Wang, Shenao and Li, Yuekang and Wang, Kailong and Liu, Yi
		  and Li, Hui and Liu, Yang and Wang, Haoyu},
  title		= {MiniScope: Automated UI Exploration and Privacy
		  Inconsistency Detection of MiniApps via Two-phase Iterative
		  Hybrid Analysis},
  year		= {2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3709351},
  doi		= {10.1145/3709351},
  abstract	= {The advent of MiniApps, operating within larger SuperApps,
		  has revolutionized user experiences by offering a wide
		  range of services without the need for individual app
		  downloads. However, this convenience has raised significant
		  privacy concerns, as these MiniApps often require access to
		  sensitive data, potentially leading to privacy violations.
		  Despite existing privacy regulations and platform
		  guidelines, there is a lack of effective mechanisms to
		  safeguard user privacy fully. To address this critical gap,
		  we introduce MiniScope, a novel two-phase hybrid analysis
		  approach, specifically designed for the MiniApp
		  environment. This approach overcomes the limitations of
		  existing static analysis techniques by incorporating UI
		  transition states analysis, cross-package callback control
		  flow resolution, and automated iterative UI exploration.
		  This allows for a comprehensive understanding of
		  MiniApps’ privacy practices, addressing the unique
		  challenges of sub-package loading and event-driven
		  callbacks. Our empirical evaluation of over 120K MiniApps
		  using MiniScope demonstrates its effectiveness in
		  identifying privacy inconsistencies. The results reveal
		  significant issues, with 5.7% of MiniApps over-collecting
		  private data and 33.4% overclaiming data collection. We
		  have responsibly disclosed our findings to 2,282
		  developers, receiving 44 acknowledgments. These findings
		  emphasize the urgent need for more precise privacy
		  monitoring systems and highlight the responsibility of
		  SuperApp operators to enforce stricter privacy measures.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= dec,
  keywords	= {MiniApps, Privacy Compliance, Hybrid Analysis}
}

@Proceedings{	  10.1145/3705677,
  title		= {CITCE '24: Proceedings of the 4th International Conference
		  on Computer, Internet of Things and Control Engineering},
  year		= {2024},
  isbn		= {9798400711848},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3661638,
  title		= {AISNS '23: Proceedings of the 2023 International
		  Conference on Artificial Intelligence, Systems and Network
		  Security},
  year		= {2023},
  isbn		= {9798400716966},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Mianyang, China}
}

@Proceedings{	  10.1145/3625549,
  title		= {HPDC '24: Proceedings of the 33rd International Symposium
		  on High-Performance Parallel and Distributed Computing},
  year		= {2024},
  isbn		= {9798400704130},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {HPDC is the premier annual conference for presenting the
		  latest research on the design, implementation, evaluation,
		  and use of parallel and distributed systems for high-end
		  computing. HPDC provides research contributions in all
		  aspects of parallel and distributed computing such as
		  resilience, AI-based systems and applications, data
		  compression, serverless computing, software systems,
		  workflows, performance modeling, hardware accelerators,
		  scientific computing, resource management, security aspects
		  and many others. The scientific contribution of the
		  conference lays its groundwork for the significant endeavor
		  required to implement actual systems and applications,
		  along with the priceless knowledge acquired through active
		  measurement and experimentation in real-world use cases.},
  location	= {Pisa, Italy}
}

@Proceedings{	  10.1145/3638380,
  title		= {OzCHI '23: Proceedings of the 35th Australian
		  Computer-Human Interaction Conference},
  year		= {2023},
  isbn		= {9798400717079},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Wellington, New Zealand}
}

@Proceedings{	  10.1145/3644479,
  title		= {EBIMCS '23: Proceedings of the 2023 6th International
		  Conference on E-Business, Information Management and
		  Computer Science},
  year		= {2023},
  isbn		= {9798400709333},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hong Kong, Hong Kong}
}

@Proceedings{	  10.1145/3640792,
  title		= {AutomotiveUI '24: Proceedings of the 16th International
		  Conference on Automotive User Interfaces and Interactive
		  Vehicular Applications},
  year		= {2024},
  isbn		= {9798400705106},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Stanford, CA, USA}
}

@Proceedings{	  10.1145/3648536,
  title		= {TAHRI '24: Proceedings of the 2024 International Symposium
		  on Technological Advances in Human-Robot Interaction},
  year		= {2024},
  isbn		= {9798400716614},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Boulder, CO, USA}
}

@Proceedings{	  10.1145/3696952,
  title		= {ICIIP '24: Proceedings of the 2024 9th International
		  Conference on Intelligent Information Processing},
  year		= {2024},
  isbn		= {9798400718076},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3589335.3641296,
  author	= {Todorov, Konstantin and Fafalios, Pavlos and Dietze,
		  Stefan and Dimitrov, Dimitar},
  title		= {Beyond Facts: 4th International Workshop on Computational
		  Methods for Online Discourse Analysis},
  year		= {2024},
  isbn		= {9798400701726},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3589335.3641296},
  doi		= {10.1145/3589335.3641296},
  abstract	= {Expressing opinions and interacting with others on the Web
		  has led to the production of an abundance of online
		  discourse data, such as claims and viewpoints on
		  controversial topics, their sources and contexts (events,
		  entities). This data constitutes a valuable source of
		  insights for studies into misinformation spread, bias
		  reinforcement, echo chambers or political agenda setting.
		  Computational methods, mostly from the field of NLP, have
		  emerged that tackle a wide range of tasks in this context,
		  including argument and opinion mining, claim detection,
		  checkworthiness detection, stance detection or fact
		  verification. However, computational models require robust
		  definitions of classes and concepts under investigation.
		  Thus, these computational tasks require a strong
		  interdisciplinary and epistemological foundation,
		  specifically with respect to the underlying definitions of
		  key concepts such as claims, arguments, stances,
		  check-worthiness or veracity. This requires a highly
		  interdisciplinary approach combining expertise from fields
		  such as communication studies, computational linguistics
		  and computer science. As opposed to facts, claims are
		  inherently more complex. Their interpretation strongly
		  depends on the context and a variety of intentional or
		  unintended meanings, where terminology and conceptual
		  understandings strongly diverge across communities. From a
		  computational perspective, in order to address this
		  complexity, the synergy of multiple approaches, coming both
		  from symbolic (knowledge representation) and statistical AI
		  seem to be promising to tackle such challenges. This
		  workshop aims at strengthening the relations between these
		  communities, providing a forum for shared works on the
		  modeling, extraction and analysis of discourse on the Web.
		  It will address the need for a shared understanding and
		  structured knowledge about discourse data in order to
		  enable machine-interpretation, discoverability and reuse,
		  in support of scientific or journalistic studies into the
		  analysis of societal debates on the Web. Beyond research
		  into information and knowledge extraction, data
		  consolidation and modeling for knowledge graphs building,
		  the workshop targets communities focusing on the analysis
		  of online discourse, relying on methods from machine
		  learning, natural language processing, large language
		  models and Web data mining.},
  booktitle	= {Companion Proceedings of the ACM Web Conference 2024},
  pages		= {1418–1421},
  numpages	= {4},
  keywords	= {computational fact-checking, computational journalism,
		  intent detection, knowledge graphs, language models, mis-
		  and dis-information, online discourse analysis, social web
		  mining, stance and viewpoint discovery},
  location	= {Singapore, Singapore},
  series	= {WWW '24}
}

@Article{	  10.1145/3696206,
  author	= {Cao, Chengtai and Zhou, Fan and Dai, Yurou and Wang,
		  Jianping and Zhang, Kunpeng},
  title		= {A Survey of Mix-based Data Augmentation: Taxonomy,
		  Methods, Applications, and Explainability},
  year		= {2024},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {2},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3696206},
  doi		= {10.1145/3696206},
  abstract	= {Data augmentation (DA) is indispensable in modern machine
		  learning and deep neural networks. The basic idea of DA is
		  to construct new training data to improve the model’s
		  generalization by adding slightly disturbed versions of
		  existing data or synthesizing new data. This survey
		  comprehensively reviews a crucial subset of DA techniques,
		  namely Mix-based Data Augmentation (MixDA), which generates
		  novel samples by combining multiple examples. In contrast
		  to traditional DA approaches that operate on single samples
		  or entire datasets, MixDA stands out due to its
		  effectiveness, simplicity, computational efficiency,
		  theoretical foundation, and broad applicability. We begin
		  by introducing a novel taxonomy that categorizes MixDA into
		  Mixup-based, Cutmix-based, and mixture approaches based on
		  a hierarchical perspective of the data mixing operation.
		  Subsequently, we provide an in-depth review of various
		  MixDA techniques, focusing on their underlying motivations.
		  Owing to its versatility, MixDA has penetrated a wide range
		  of applications, which we also thoroughly investigate in
		  this survey. Moreover, we delve into the underlying
		  mechanisms of MixDA’s effectiveness by examining its
		  impact on model generalization and calibration while
		  providing insights into the model’s behavior by analyzing
		  the inherent properties of MixDA. Finally, we recapitulate
		  the critical findings and fundamental challenges of current
		  MixDA studies while outlining the potential directions for
		  future works. Different from previous related surveys that
		  focus on DA approaches in specific domains (e.g., computer
		  vision and natural language processing) or only review a
		  limited subset of MixDA studies, we are the first to
		  provide a systematical survey of MixDA, covering its
		  taxonomy, methodology, application, and explainability.
		  Furthermore, we provide promising directions for
		  researchers interested in this exciting area.},
  journal	= {ACM Comput. Surv.},
  month		= oct,
  articleno	= {37},
  numpages	= {38},
  keywords	= {Data augmentation, regularization, generalization, machine
		  learning, deep learning}
}

@Proceedings{	  10.1145/3670085,
  title		= {ICMAI '24: Proceedings of the 2024 9th International
		  Conference on Mathematics and Artificial Intelligence},
  year		= {2024},
  isbn		= {9798400717284},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@Proceedings{	  10.1145/3665065,
  title		= {ISMSI '24: Proceedings of the 2024 8th International
		  Conference on Intelligent Systems, Metaheuristics &amp;
		  Swarm Intelligence},
  year		= {2024},
  isbn		= {9798400717291},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Singapore, Singapore}
}

@Article{	  10.1145/3689430,
  author	= {Zeng, Ruihong and Fang, Jinyuan and Liu, Siwei and Meng,
		  Zaiqiao and Liang, Shangsong},
  title		= {Enhancing Graph Neural Networks via Memorized Global
		  Information},
  year		= {2024},
  issue_date	= {November 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {18},
  number	= {4},
  issn		= {1559-1131},
  url		= {https://doi.org/10.1145/3689430},
  doi		= {10.1145/3689430},
  abstract	= {Graph neural networks (GNNs) have gained significant
		  attention for their impressive results on different
		  graph-based tasks. The essential mechanism of GNNs is the
		  message-passing framework, whereby node representations are
		  aggregated from local neighborhoods. Recently,
		  Transformer-based GNNs have been introduced to learn the
		  long-range dependencies, enhancing performance. However,
		  their quadratic computational complexity, due to the
		  attention computation, has constrained their applicability
		  on large-scale graphs. To address this issue, we propose
		  MGIGNN (Memorized Global Information Graph Neural Network),
		  an innovative approach that leverages memorized global
		  information to enhance existing GNNs in both transductive
		  and inductive scenarios. Specifically, MGIGNN captures
		  long-range dependencies by identifying and incorporating
		  global similar nodes, which are defined as nodes exhibiting
		  similar features, structural patterns and label information
		  within a graph. To alleviate the computational overhead
		  associated with computing embeddings for all nodes, we
		  introduce an external memory module to facilitate the
		  retrieval of embeddings and optimize performance on large
		  graphs. To enhance the memory-efficiency, MGIGNN
		  selectively retrieves global similar nodes from a small set
		  of candidate nodes. These candidate nodes are selected from
		  the training nodes based on a sparse node selection
		  distribution with a Dirichlet prior. This selecting
		  approach not only reduces the memory size required but also
		  ensures efficient utilization of computational resources.
		  Through comprehensive experiments conducted on ten
		  widely-used and real-world datasets, including seven
		  homogeneous datasets and three heterogeneous datasets, we
		  demonstrate that our&nbsp;MGIGNN can generally improve the
		  performance of existing GNNs on node classification tasks
		  under both inductive and transductive settings.},
  journal	= {ACM Trans. Web},
  month		= oct,
  articleno	= {50},
  numpages	= {34},
  keywords	= {Network embedding, graph neural network, memorized global
		  information}
}

@Proceedings{	  10.1145/3633624,
  title		= {BDSIC '23: Proceedings of the 2023 5th International
		  Conference on Big-data Service and Intelligent
		  Computation},
  year		= {2023},
  isbn		= {9798400708923},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Singapore, Singapore}
}

@Proceedings{	  10.1145/3628034,
  title		= {EuroPLoP '23: Proceedings of the 28th European Conference
		  on Pattern Languages of Programs},
  year		= {2023},
  isbn		= {9798400700408},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Irsee, Germany}
}

@Proceedings{	  10.5555/3643142,
  title		= {WSC '23: Proceedings of the Winter Simulation Conference},
  year		= {2023},
  isbn		= {9798350369663},
  publisher	= {IEEE Press},
  location	= {San Antonio, Texas, USA}
}

@InProceedings{	  10.1145/3653946.3653961,
  author	= {Jiang, Yingdi and Yao, Jiarui and Li, Fangfei and Zhang,
		  Yan},
  title		= {Research on Engineering Management Question-answering
		  System in the Communication Industry Based on Large
		  Language Models and Knowledge Graphs},
  year		= {2024},
  isbn		= {9798400716553},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3653946.3653961},
  doi		= {10.1145/3653946.3653961},
  abstract	= {In the engineering management of the communication
		  industry, there are many issues, including low efficiency
		  in information acquisition and limitations in the level of
		  intelligence.Large language models, with their powerful
		  text comprehension and generation capabilities, offer new
		  perspectives for the development of this field.This study
		  constructed a question-answering system using a combined
		  approach of large language models and text knowledge bases.
		  The system dynamically leverages abundant external
		  knowledge and enhances the model's reasoning ability and
		  interpretability through knowledge graphs. In response to
		  five categories of issues in engineering management,
		  experiments and in-depth analysis revealed that although
		  large language models may lack granularity in addressing
		  some complex problems, the question-answering system
		  overall achieved intelligent assistance, improving the
		  efficiency of collaborative engineering management.},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Machine Vision and Applications},
  pages		= {100–105},
  numpages	= {6},
  keywords	= {Engineering management, Keywords • Large language
		  models, Knowledge graphs, Question-answering},
  location	= {Singapore, Singapore},
  series	= {ICMVA '24}
}

@Proceedings{	  10.1145/3634737,
  title		= {ASIA CCS '24: Proceedings of the 19th ACM Asia Conference
		  on Computer and Communications Security},
  year		= {2024},
  isbn		= {9798400704826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to ACM AsiaCCS
		  2024, the 19th ACM Asia Conference on Computer and
		  Communications Security. AsiaCCS 2024 takes place in
		  Singapore from 1 July to 5 July.},
  location	= {Singapore, Singapore}
}

@Article{	  10.1145/3655032.3655035,
  author	= {Freedman, Richard G.},
  title		= {2025 EAAI Mentored Undergraduate Research Challenge:
		  Playing Word Association Games},
  year		= {2024},
  issue_date	= {March 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {10},
  number	= {1},
  url		= {https://doi.org/10.1145/3655032.3655035},
  doi		= {10.1145/3655032.3655035},
  abstract	= {The topic for EAAI 2025's Mentored Undergraduate Research
		  Challenge is PlayingWord Association Games. What does that
		  mean? Where are the applications? How can you get started?
		  We break down the topic, discuss applications, and explore
		  project ideas in this column.},
  journal	= {AI Matters},
  month		= may,
  pages		= {16–25},
  numpages	= {10}
}

@Proceedings{	  10.1145/3647444,
  title		= {ICIMMI '23: Proceedings of the 5th International
		  Conference on Information Management &amp; Machine
		  Intelligence},
  year		= {2023},
  isbn		= {9798400709418},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Jaipur, India}
}

@Proceedings{	  10.1145/3626232,
  title		= {CODASPY '24: Proceedings of the Fourteenth ACM Conference
		  on Data and Application Security and Privacy},
  year		= {2024},
  isbn		= {9798400704215},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the fourteenth
		  edition of the ACM Conference on Data and Application
		  Security and Privacy (CODASPY 2024), for the first time
		  held outside United States of America. This conference
		  series has been founded to foster novel and exciting
		  research in the data and application security and privacy
		  arena and to help generate new directions for further
		  research and development. The initial concept was
		  established by the two co-founders, Elisa Bertino and Ravi
		  Sandhu, and sharpened by subsequent discussions with
		  several fellow data security and privacy researchers. Their
		  enthusiastic encouragement persuaded the co-founders to
		  move ahead with the always daunting task of creating a
		  high-quality conference. CODASPY has become a leading forum
		  for presentation of research results and experience reports
		  on hardware and software security. The conference gives
		  researchers and practitioners a unique opportunity to share
		  their perspectives with others interested in the various
		  aspects of data and applications security and privacy.},
  location	= {Porto, Portugal}
}

@Proceedings{	  10.1145/3620665,
  title		= {ASPLOS '24: Proceedings of the 29th ACM International
		  Conference on Architectural Support for Programming
		  Languages and Operating Systems, Volume 2},
  year		= {2024},
  isbn		= {9798400703850},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {2},
  abstract	= {Welcome to the second volume of ASPLOS'24: the 29th ACM
		  International Conference on Architectural Support for
		  Programming Languages and Operating Systems. This document
		  is dedicated to the 2024 summer review cycle.We introduced
		  several notable changes to ASPLOS this year, many of which
		  were discussed in the previous message from program chairs
		  in Volume 1. Here, to avoid repetition, we assume that
		  readers have already read the latter message and will only
		  describe differences between the current cycle and the
		  previous one. These include: (1) developing and utilizing
		  an automated format violation identifier script focused on
		  uncovering disallowed vertical space manipulations that
		  "squeeze" space; (2) incorporating authors-declared
		  best-matching topics into our review assignment process;
		  (3) introducing the new ASPLOS role of Program Vice Chairs
		  to cope with the increased number of submissions and the
		  added load caused by foregoing synchronous program
		  committee (PC) meetings, which necessitated additional
		  managerial involvement in online dissensions; and (4)
		  characterizing a systematic problem that ASPLOS is facing
		  in reviewing quantum computing submissions, describing how
		  we addressed it, and highlighting how we believe that it
		  should be handled in the future.Key statistics of the
		  ASPLOS'24 summer cycle include: 409 submissions were
		  finalized (about 1.5x more than last year's summer count
		  and nearly 2.4x more than our spring cycle), with 107
		  related to accelerators/FPGAs/GPUs, 97 to machine learning,
		  88 to storage/memory, 80 to security, and 69 to
		  datacenter/cloud; 179 (44%) submissions were promoted to
		  the second review round; 54 (13.2%) papers were accepted
		  (with 20 awarded one or more artifact evaluation badges);
		  33 (8.1%) submissions were allowed to submit major
		  revisions, of which 27 were subsequently accepted during
		  the fall cycle (with 13 awarded one or more artifact
		  evaluation badges); 1,499 reviews were uploaded; and 5,557
		  comments were generated during online discussions.Analyzing
		  the per-submission most-related broader areas of research,
		  which we asked authors to associate with their work in the
		  submission form, revealed that 71%, 47%, and 28% of the
		  submissions are categorized by their authors as related to
		  architecture, operating systems, and programming languages,
		  respectively, with about 45% being "interdisciplinary"
		  submissions (associated with more than one area). The full
		  details are available in the PDF of the front matter.},
  location	= {La Jolla, CA, USA}
}

@Proceedings{	  10.1145/3657242,
  title		= {Interacci\'{o}n '24: Proceedings of the XXIV International
		  Conference on Human Computer Interaction},
  year		= {2024},
  isbn		= {9798400717871},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {A Coru\~{n}a, Spain}
}

@Article{	  10.1145/3706057,
  author	= {Jayasundara, Sakuna Harinda and Gamagedara Arachchilage,
		  Nalin Asanka and Russello, Giovanni},
  title		= {SoK: Access Control Policy Generation from High-level
		  Natural Language Requirements},
  year		= {2024},
  issue_date	= {April 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {4},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3706057},
  doi		= {10.1145/3706057},
  abstract	= {Administrator-centered access control failures can cause
		  data breaches, putting organizations at risk of financial
		  loss and reputation damage. Existing graphical policy
		  configuration tools and automated policy generation
		  frameworks attempt to help administrators configure and
		  generate access control policies by avoiding such failures.
		  However, graphical policy configuration tools are prone to
		  human errors, making them unusable. On the other hand,
		  automated policy generation frameworks are prone to
		  erroneous predictions, making them unreliable. Therefore,
		  to find ways to improve their usability and reliability, we
		  conducted a Systematic Literature Review analyzing 49
		  publications. The thematic analysis of the publications
		  revealed that graphical policy configuration tools are
		  developed to write and visualize policies manually.
		  Moreover, automated policy generation frameworks are
		  developed using machine learning (ML) and natural language
		  processing (NLP) techniques to automatically generate
		  access control policies from high-level requirement
		  specifications. Despite their utility in the access control
		  domain, limitations of these tools, such as the lack of
		  flexibility, and limitations of frameworks, such as the
		  lack of domain adaptation, negatively affect their
		  usability and reliability, respectively. Our study offers
		  recommendations to address these limitations through
		  real-world applications and recent advancements in the NLP
		  domain, paving the way for future research.},
  journal	= {ACM Comput. Surv.},
  month		= dec,
  articleno	= {102},
  numpages	= {37},
  keywords	= {Access control, policy engineering, system administrator,
		  user interfaces, frameworks, usability, reliability}
}

@Proceedings{	  10.1145/3629104,
  title		= {DEBS '24: Proceedings of the 18th ACM International
		  Conference on Distributed and Event-based Systems},
  year		= {2024},
  isbn		= {9798400704437},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {We welcome you to the 18th ACM International Conference on
		  Distributed and Event-Based Systems (DEBS) 2024, hosted as
		  an in-person event at the Institut National des Sciences
		  Appliqu\'{e}es (INSA) Lyon.The history of DEBS spans over
		  20 years of scientific progress, beginning as a workshop
		  and evolving into a conference 17 years ago. The
		  conference's goals have evolved over time, but its primary
		  objective - to provide a dedicated forum for the
		  dissemination of high-quality, original, and impactful
		  research on distributed systems and event-based computing -
		  has remained unchanged. Alongside scientific contributions,
		  the conference has always featured the discussion of
		  practical insights and the reporting of experiences
		  relevant to the industrial sector.},
  location	= {Villeurbanne, France}
}

@InProceedings{	  10.1145/3613904.3642542,
  author	= {Mildner, Thomas and Cooney, Orla and Meck, Anna-Maria and
		  Bartl, Marion and Savino, Gian-Luca and Doyle, Philip R and
		  Garaialde, Diego and Clark, Leigh and Sloan, John and
		  Wenig, Nina and Malaka, Rainer and Niess, Jasmin},
  title		= {Listening to the Voices: Describing Ethical Caveats of
		  Conversational User Interfaces According to Experts and
		  Frequent Users},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642542},
  doi		= {10.1145/3613904.3642542},
  abstract	= {Advances in natural language processing and understanding
		  have led to a rapid growth in the popularity of
		  conversational user interfaces (CUIs). While CUIs introduce
		  novel benefits, they also yield risks that may exploit
		  people’s trust. Although research looking at unethical
		  design deployed through graphical user interfaces (GUIs)
		  established a thorough understanding of so-called dark
		  patterns, there is a need to continue this discourse within
		  the CUI community to understand potentially problematic
		  interactions. Addressing this gap, we interviewed 27
		  participants from three cohorts: researchers,
		  practitioners, and frequent users of CUIs. Applying
		  thematic analysis, we construct five themes reflecting each
		  cohort’s insights about ethical design challenges and
		  introduce the CUI Expectation Cycle, bridging system
		  capabilities and user expectations while considering each
		  theme’s ethical caveats. This research aims to inform
		  future development of CUIs to consider ethical constraints
		  while adopting a human-centred approach.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {307},
  numpages	= {18},
  keywords	= {CUI, chatbots, conversational agents, conversational user
		  interfaces, dark patterns, deceptive design patterns,
		  ethical design, thematic analysis, voice agents},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@Proceedings{	  10.1145/3644033,
  title		= {FormaliSE '24: Proceedings of the 2024 IEEE/ACM 12th
		  International Conference on Formal Methods in Software
		  Engineering (FormaliSE)},
  year		= {2024},
  isbn		= {9798400705892},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Historically, formal methods academic research and
		  practical software development have had limited mutual
		  interactions—except possibly in specialized domains such
		  as safety-critical software. In recent times, the outlook
		  has considerably improved: on the one hand, formal methods
		  research has delivered more flexible techniques and tools
		  that can support various aspects of the software
		  development process—from user requirements elicitation,
		  to design, implementation, verification and validation, as
		  well as the creation of documentation. On the other hand,
		  software engineering has developed a growing interest in
		  rigorous techniques applied at scale.This evolution, and
		  the desire to further improve it, motivated the creation of
		  FormaliSE: a well-established annual conference whose main
		  goal is to promote work at the intersection of the formal
		  methods and software engineering communities, providing a
		  venue to exchange ideas, experiences, techniques, and
		  results. The collaboration between these two communities
		  can be mutually beneficial by fostering the creation of
		  formal methods that are practically useful and by helping
		  develop higher-quality software.},
  location	= {Lisbon, Portugal}
}

@Proceedings{	  10.1145/3638067,
  title		= {IHC '23: Proceedings of the XXII Brazilian Symposium on
		  Human Factors in Computing Systems},
  year		= {2023},
  isbn		= {9798400717154},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Macei\'{o}, Brazil}
}

@InProceedings{	  10.1145/3664476.3664523,
  author	= {Ruman, \'{A}d\'{a}m and Dra\v{s}ar, Martin and Sadlek,
		  Luk\'{a}\v{s} and Yang, Shanchieh Jay and Celeda, Pavel},
  title		= {Adversary Tactic Driven Scenario and Terrain Generation
		  with Partial Infrastructure Specification},
  year		= {2024},
  isbn		= {9798400717185},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3664476.3664523},
  doi		= {10.1145/3664476.3664523},
  abstract	= {Diverse, accurate, and up-to-date training environments
		  are essential for training cybersecurity experts and
		  autonomous systems. However, preparation of their content
		  is time-consuming and requires experts to provide detailed
		  specifications. In this paper, we explore the challenges of
		  automated generation of the content (composed of scenarios
		  and terrains) for these environments. We propose new models
		  to represent the cybersecurity domain and associated action
		  spaces. These models are used to create sound and complex
		  training content based on partial specifications provided
		  by users. We compare the results with a real-world complex
		  malware campaign to assess the realism of the synthesized
		  content. To further evaluate the correctness and
		  variability of the results, we utilize the kill-chain
		  attack graph generation for the generated training content
		  to asses the internal correspondence of its key components.
		  Our results demonstrate that the proposed approach can
		  create complex training content similar to advanced attack
		  campaigns, which passes evaluation for soundness and
		  practicality. Our proposed approach and its implementation
		  significantly contribute to the state of the art, enabling
		  novel approaches to cybersecurity training and autonomous
		  system development.},
  booktitle	= {Proceedings of the 19th International Conference on
		  Availability, Reliability and Security},
  articleno	= {33},
  numpages	= {11},
  keywords	= {adversary framework, attack scenario generation, cyber
		  terrain generation, cybersecurity model},
  location	= {Vienna, Austria},
  series	= {ARES '24}
}

@Proceedings{	  10.1145/3653644,
  title		= {FAIML '24: Proceedings of the 2024 3rd International
		  Conference on Frontiers of Artificial Intelligence and
		  Machine Learning},
  year		= {2024},
  isbn		= {9798400709777},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Yichang, China}
}

@Article{	  10.1145/3673226,
  author	= {Uhrmacher, Adelinde M and Frazier, Peter and H\"{a}hnle,
		  Reiner and Kl\"{u}gl, Franziska and Lorig, Fabian and
		  Lud\"{a}scher, Bertram and Nenzi, Laura and Ruiz-Martin,
		  Cristina and Rumpe, Bernhard and Szabo, Claudia and Wainer,
		  Gabriel and Wilsdorf, Pia},
  title		= {Context, Composition, Automation, and Communication: The
		  C2AC Roadmap for Modeling and Simulation},
  year		= {2024},
  issue_date	= {October 2024},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {34},
  number	= {4},
  issn		= {1049-3301},
  url		= {https://doi.org/10.1145/3673226},
  doi		= {10.1145/3673226},
  abstract	= {Simulation has become, in many application areas, a sine
		  qua non. Most recently, COVID-19 has underlined the
		  importance of simulation studies and limitations in current
		  practices and methods. We identify four goals of
		  methodological work for addressing these limitations. The
		  first is to provide better support for capturing,
		  representing, and evaluating the context of simulation
		  studies, including research questions, assumptions,
		  requirements, and activities contributing to a simulation
		  study. In addition, the composition of simulation models
		  and other simulation studies’ products must be supported
		  beyond syntactical coherence, including aspects of
		  semantics and purpose, enabling their effective reuse. A
		  higher degree of automating simulation studies will
		  contribute to more systematic, standardized simulation
		  studies and their efficiency. Finally, it is essential to
		  invest increased effort into effectively communicating
		  results and the processes involved in simulation studies to
		  enable their use in research and decision making. These
		  goals are not pursued independently of each other, but they
		  will benefit from and sometimes even rely on advances in
		  other sub-fields. In this article, we explore the basis and
		  interdependencies evident in current research and practice
		  and delineate future research directions based on these
		  considerations.},
  journal	= {ACM Trans. Model. Comput. Simul.},
  month		= aug,
  articleno	= {23},
  numpages	= {51},
  keywords	= {Modeling, simulation, state of the art, open challenges,
		  reuse, composition, communication, reproducibility,
		  automation, intelligent modeling and simulation lifecycle}
}

@InProceedings{	  10.1145/3613904.3642206,
  author	= {Gould, Sandy J. J.},
  title		= {Stochastic Machine Witnesses at Work: Today's Critiques of
		  Taylorism are Inadequate for Workplace Surveillance
		  Epistemologies of the Future},
  year		= {2024},
  isbn		= {9798400703300},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3613904.3642206},
  doi		= {10.1145/3613904.3642206},
  abstract	= {I argue that epistemologies of workplace surveillance are
		  shifting in fundamental ways, and so critiques must shift
		  accordingly. I begin the paper by relating Scientific
		  Management to Human-Centred Computing’s ways of knowing
		  through a study of ‘metaverse’ virtual reality
		  workplaces. From this, I develop two observations. The
		  first is that today’s workplace measurement science does
		  not resemble the science that Taylor developed for
		  Scientific Management. Contemporary workplace science is
		  more passive, more intermediated and less controlled. The
		  second observation is that new forms of workplace
		  measurement challenge the norms of empirical science.
		  Instead of having credentialed human witnesses observe
		  phenomena and agree facts about them, we instead make
		  outsourced, uncredentialed stochastic machine witnesses
		  responsible for producing facts about work. With these
		  observations in mind, I assert that critiques of workplace
		  surveillance still framed by Taylorism will not be fit for
		  interrogating workplace surveillance practices of the
		  future.},
  booktitle	= {Proceedings of the 2024 CHI Conference on Human Factors in
		  Computing Systems},
  articleno	= {578},
  numpages	= {12},
  keywords	= {Metaverse, Neo-Taylorism, Scientific Management,
		  Taylorism, Ubiquitous Computing, Work Measurement,
		  Workplace Surveillance},
  location	= {Honolulu, HI, USA},
  series	= {CHI '24}
}

@Proceedings{	  10.1145/3641343,
  title		= {ICEITSA '23: Proceedings of the 3rd International
		  Conference on Electronic Information Technology and Smart
		  Agriculture},
  year		= {2023},
  isbn		= {9798400716775},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Sanya, China}
}

@Proceedings{	  10.1145/3689936,
  title		= {CSCS '24: Proceedings of the 2024 Cyber Security in CarS
		  Workshop},
  year		= {2024},
  isbn		= {9798400712326},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to the CSCS '24 -
		  1st Cyber Security in CarS Workshop. CSCS '24 aims to
		  address current issues in the rapidly advancing field of
		  automotive cybersecurity. The aim is to bring together
		  academia and industry to address cybersecurity problems in
		  the automotive domain. The CSCS '24 provides a forum for
		  deliberating on the most recent advancements, exchanging
		  current research contributions, and encouraging networking
		  and collaboration to devise novel solutions. The CSCS
		  workshop builds upon the foundation laid by the "ACM
		  Computer Science in Cars Symposium" (CSCS symposium) and
		  advances the field of automotive cybersecurity.},
  location	= {Salt Lake City, UT, USA}
}

@Proceedings{	  10.1145/3677182,
  title		= {ASENS '24: Proceedings of the International Conference on
		  Algorithms, Software Engineering, and Network Security},
  year		= {2024},
  isbn		= {9798400709784},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Nanchang, China}
}

@InProceedings{	  10.1145/3640543.3645208,
  author	= {Bendeck, Alexander and Bromley, Dennis and Setlur, Vidya},
  title		= {SlopeSeeker: A Search Tool for Exploring a Dataset of
		  Quantifiable Trends},
  year		= {2024},
  isbn		= {9798400705083},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3640543.3645208},
  doi		= {10.1145/3640543.3645208},
  abstract	= {Natural language and search interfaces intuitively
		  facilitate data exploration and provide visualization
		  responses to diverse analytical queries based on the
		  underlying datasets. However, these interfaces often fail
		  to interpret more complex analytical intents, such as
		  discerning subtleties and quantifiable differences between
		  terms like “bump’’ and “spike’’ in the context
		  of COVID cases, for example. We address this gap by
		  extending the capabilities of a data exploration search
		  interface for interpreting semantic concepts in time series
		  trends. We first create a comprehensive dataset of semantic
		  concepts by mapping quantifiable univariate data trends
		  such as slope and angle to crowdsourced, semantically
		  meaningful trend labels. The dataset contains quantifiable
		  properties that capture the slope-scalar effect of semantic
		  modifiers like “sharply” and “gradually,” as well
		  as multi-line trends (e.g., “peak,” “valley”). We
		  demonstrate the utility of this dataset in SlopeSeeker, a
		  tool that supports natural language querying of
		  quantifiable trends, such as “show me stocks that tanked
		  in 2010.” The tool incorporates novel scoring and ranking
		  techniques based on semantic relevance and visual
		  prominence to present relevant trend chart responses
		  containing these semantic trend concepts. In addition,
		  SlopeSeeker provides a faceted search interface for users
		  to navigate a semantic hierarchy of concepts from general
		  trends (e.g., “increase’’) to more specific ones
		  (e.g., “sharp increase’’). A preliminary user
		  evaluation of the tool demonstrates that the search
		  interface supports greater expressivity of queries
		  containing concepts that describe data trends. We identify
		  potential future directions for leveraging our publicly
		  available quantitative semantics dataset in other data
		  domains and for novel visual analytics interfaces.},
  booktitle	= {Proceedings of the 29th International Conference on
		  Intelligent User Interfaces},
  pages		= {817–836},
  numpages	= {20},
  keywords	= {Semantics, quantifiable metadata, search, trends, visual
		  analysis.},
  location	= {Greenville, SC, USA},
  series	= {IUI '24}
}

@Proceedings{	  10.1145/3640912,
  title		= {CNML '23: Proceedings of the 2023 International Conference
		  on Communication Network and Machine Learning},
  year		= {2023},
  isbn		= {9798400716683},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Zhengzhou, China}
}

@Proceedings{	  10.1145/3697090,
  title		= {LADC '24: Proceedings of the 13th Latin-American Symposium
		  on Dependable and Secure Computing},
  year		= {2024},
  isbn		= {9798400717406},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3659995,
  title		= {FlexScience'24: Proceedings of the 14th Workshop on AI and
		  Scientific Computing at Scale using Flexible Computing
		  Infrastructures},
  year		= {2024},
  isbn		= {9798400706424},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Pisa, Italy}
}

@Proceedings{	  10.1145/3689094,
  title		= {SUMAC '24: Proceedings of the 6th workshop on the
		  analySis, Understanding and proMotion of heritAge
		  Contents},
  year		= {2024},
  isbn		= {9798400712050},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {It is our great pleasure to welcome you to SUMAC 2024, the
		  6th edition of the ACM workshop on analySis, Understanding
		  and proMotion of heritAge Contents. The workshop focuses on
		  analyzing, processing and valorizing all types of data
		  related to cultural heritage, including tangible and
		  intangible heritage. As stated by UNESCO, cultural heritage
		  provides societies with a wealth of resources inherited
		  from the past, created in the present for the benefit of
		  future generations. The massive digitization of historical
		  analogue resources and production of born-digital documents
		  provide us with large volumes of varied multimedia heritage
		  data (images, maps, text, video, 3D objects, multi-sensor
		  data, etc.), which represent a rich heritage that can be
		  exploited in a wide variety of fields, from research in
		  social sciences and computational humanities to land use
		  and territorial policies, including urban modeling, digital
		  simulation, archaeology, tourism, education, culture
		  preservation, creative media and entertainment. In terms of
		  research in computer science, artificial intelligence and
		  digital humanities, they address challenging problems
		  related to the diversity, specificity or volume of the
		  media, the veracity of the data, and different user needs
		  with respect to engaging with this rich material and the
		  extraction of value out of the data. These challenges are
		  reflected in the corresponding sub-fields of machine
		  learning, signal processing, multi-modal techniques and
		  human-machine interaction, with special focus on:Analysis
		  of historical data,Content understanding and pattern
		  recognition,Linking and recommendation of multi-modal
		  digital heritage,Human-machine interaction for big data
		  analysis and visualization,Generative modeling of cultural
		  heritage.},
  location	= {Melbourne VIC, Australia}
}

@Proceedings{	  10.1145/3647817,
  title		= {ICBBS '23: Proceedings of the 2023 12th International
		  Conference on Bioinformatics and Biomedical Science},
  year		= {2023},
  isbn		= {9798400716140},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Singapore, Singapore}
}

@Proceedings{	  10.1145/3643833,
  title		= {WiSec '24: Proceedings of the 17th ACM Conference on
		  Security and Privacy in Wireless and Mobile Networks},
  year		= {2024},
  isbn		= {9798400705823},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {Welcome to the 2024 ACM Conference on Security and Privacy
		  in Wireless and Mobile Networks (ACM WiSec)!Now in its 17th
		  year, WiSec continues to be the premier venue for research
		  on all aspects of security and privacy in wireless and
		  mobile networks, their systems, and their applications. We
		  are hosted by the Korea Institute of Information Security
		  &amp; Cryptology, located in the city center of Seoul,
		  Korea - a city known for its dynamic mix of 600-year-old
		  palaces and the contemporary urban landscape characterized
		  by towering skyscrapers.We begin our exciting three-day
		  main conference program on May 27th with single-track
		  technical paper sessions, a poster and demo session, two
		  excellent keynotes from telecommunication security expert
		  Prof. Jean-Pierre Seifert (TU Berlin) and wireless security
		  expert Mathy Vanhoef (KU Leuven), and a panel on wireless
		  security and AI. Three invited talks named "Vision Talk"
		  discuss the future of wireless and mobile security issues.
		  The WiseML Workshop follows the main program on May 30th.
		  We invite participants to attend the exciting paper
		  presentations and keynotes, interact with the presenters
		  during the Q&amp;A sessions after each talk, network during
		  the coffee breaks and lunches each day, and socialize
		  during the banquet dinner.},
  location	= {Seoul, Republic of Korea}
}

@Proceedings{	  10.1145/3679409,
  title		= {ISCER '24: Proceedings of the 2024 3rd International
		  Symposium on Control Engineering and Robotics},
  year		= {2024},
  isbn		= {9798400709951},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Changsha, China}
}

@Proceedings{	  10.1145/3651781,
  title		= {ICSCA '24: Proceedings of the 2024 13th International
		  Conference on Software and Computer Applications},
  year		= {2024},
  isbn		= {9798400708329},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Bali Island, Indonesia}
}

@Proceedings{	  10.1145/3611315,
  title		= {NANOARCH '23: Proceedings of the 18th ACM International
		  Symposium on Nanoscale Architectures},
  year		= {2023},
  isbn		= {9798400703256},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Dresden, Germany}
}

@Proceedings{	  10.1145/3695652,
  title		= {IMMS '24: Proceedings of the 2024 7th International
		  Conference on Information Management and Management
		  Science},
  year		= {2024},
  isbn		= {9798400716997},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Beijing, China}
}

@Proceedings{	  10.1145/3661814,
  title		= {LICS '24: Proceedings of the 39th Annual ACM/IEEE
		  Symposium on Logic in Computer Science},
  year		= {2024},
  isbn		= {9798400706608},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {This volume contains the proceedings of the 39th Annual
		  ACM/IEEE Symposium on Logic in Computer Science (LICS
		  2024).},
  location	= {Tallinn, Estonia}
}

@Proceedings{	  10.1145/3655532,
  title		= {ICRSA '23: Proceedings of the 2023 6th International
		  Conference on Robot Systems and Applications},
  year		= {2023},
  isbn		= {9798400708039},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Wuhan, China}
}

@Proceedings{	  10.1145/3688459,
  title		= {EuroUSEC '24: Proceedings of the 2024 European Symposium
		  on Usable Security},
  year		= {2024},
  isbn		= {9798400717963},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3653946,
  title		= {ICMVA '24: Proceedings of the 2024 7th International
		  Conference on Machine Vision and Applications},
  year		= {2024},
  isbn		= {9798400716553},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Singapore, Singapore}
}

@Proceedings{	  10.1145/3640115,
  title		= {ICITEE '23: Proceedings of the 6th International
		  Conference on Information Technologies and Electrical
		  Engineering},
  year		= {2023},
  isbn		= {9798400708299},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Changde, Hunan, China}
}

@Article{	  10.1145/3711682,
  author	= {Li, Tong and Long, Qingyue and Chai, Haoye and Zhang,
		  Shiyuan and Jiang, Fenyu and Liu, Haoqiang and Huang,
		  Wenzhen and Jin, Depeng and Li, Yong},
  title		= {Generative AI Empowered Network Digital Twins:
		  Architecture, Technologies, and Applications},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3711682},
  doi		= {10.1145/3711682},
  abstract	= {The rapid advancement of mobile networks highlights the
		  limitations of traditional network planning and
		  optimization methods, particularly in modeling, evaluation,
		  and application. Network Digital Twins, which simulate
		  networks in the digital domain for evaluation, offer a
		  solution to these challenges. This concept is further
		  enhanced by generative AI technology, which promises more
		  efficient and accurate AI-driven data generation for
		  network simulation and optimization. This survey provides
		  insights into generative AI-empowered network digital
		  twins. We begin by outlining the architecture of a network
		  digital twin, which encompasses both digital and physical
		  domains. This architecture involves four key steps: data
		  processing and network monitoring, digital replication and
		  network simulation, designing and training network
		  optimizers, Sim2Real, and network control. Next, we
		  systematically discuss the related studies in each step and
		  make a detailed taxonomy of the problem studied, the
		  methods used, and the key designs leveraged. Each step is
		  examined with a focus on the role of generative AI, from
		  estimating missing data and simulating network behaviors to
		  designing control strategies and bridging the gap between
		  digital and physical domains. Finally, we discuss the open
		  issues and challenges of generative AI-based network
		  digital twins.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {157},
  numpages	= {43},
  keywords	= {Generative AI, digital twins, mobile networks, network
		  monitoring, network simulation, network operation}
}

@Article{	  10.1145/3689372,
  author	= {Jaidka, Kokil and Chen, Tsuhan and Chesterman, Simon and
		  Hsu, Wynne and Kan, Min-Yen and Kankanhalli, Mohan and Lee,
		  Mong Li and Seres, Gyula and Sim, Terence and Taeihagh,
		  Araz and Tung, Anthony and Xiao, Xiaokui and Yue, Audrey},
  title		= {Misinformation, Disinformation, and Generative AI:
		  Implications for Perception and Policy},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {1},
  url		= {https://doi.org/10.1145/3689372},
  doi		= {10.1145/3689372},
  abstract	= {The emergence of generative artificial intelligence
		  (GenAI) has exacerbated the challenges of misinformation,
		  disinformation, and mal-information (MDM) within digital
		  ecosystems. These multi-faceted challenges demand a
		  re-evaluation of the digital information lifecycle and a
		  deep understanding of its social impact. An
		  interdisciplinary strategy integrating insights from
		  technology, social sciences, and policy analysis is crucial
		  to address these issues effectively. This article
		  introduces a three-tiered framework to scrutinize the
		  lifecycle of GenAI-driven content from creation to
		  consumption, emphasizing the consumer perspective. We
		  examine the dynamics of consumer behavior that drive
		  interactions with MDM, pinpoints vulnerabilities in the
		  information dissemination process, and advocates for
		  adaptive, evidence-based policies. Our interdisciplinary
		  methodology aims to bolster information integrity and
		  fortify public trust, equipping digital societies to manage
		  the complexities of GenAI and proactively address the
		  evolving challenges of digital misinformation. We conclude
		  by discussing how GenAI can be leveraged to combat MDM,
		  thereby creating a reflective cycle of technological
		  advancement and mitigation.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= feb,
  articleno	= {11},
  numpages	= {15},
  keywords	= {Misinformation, disinformation, trust, resilience,
		  generative AI, social media}
}

@InProceedings{	  10.1145/3704289.3704300,
  author	= {Xiong, Xiao-Gang and Zeng, Meng-Ting},
  title		= {Research hotspots and path evolution of generative AI
		  development--A Bibliometric Analysis Based on CiteSpace},
  year		= {2025},
  isbn		= {9798400716980},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704289.3704300},
  doi		= {10.1145/3704289.3704300},
  booktitle	= {Proceedings of the 2024 7th International Conference on
		  Big Data and Education},
  pages		= {22–28},
  numpages	= {7},
  keywords	= {Citespace, bibliometrics, generative AI},
  location	= { },
  series	= {ICBDE '24}
}

@Article{	  10.1145/3713080,
  author	= {Li, Xiangyang and Chen, Bo and Hou, Lu and TANG, Ruiming},
  title		= {CTRL: Connect Collaborative and Language Model for CTR
		  Prediction},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3713080},
  doi		= {10.1145/3713080},
  abstract	= {Traditional click-through rate (CTR) prediction models
		  convert the tabular data into one-hot vectors and leverage
		  the collaborative relations among features for inferring
		  the user’s preference over items. This modeling paradigm
		  discards essential semantic information. Though some works
		  like P5 and KAR have explored the potential of using
		  Pre-trained Language Models (PLMs) to extract semantic
		  signals for CTR prediction, they are computationally
		  expensive and suffer from low efficiency. Besides, the
		  beneficial collaborative relations are not considered,
		  hindering the recommendation performance. To solve these
		  problems, in this paper, we propose a novel framework CTRL,
		  which is industrial-friendly and model-agnostic with
		  superior inference efficiency. Specifically, the original
		  tabular data is first converted into textual data. Both
		  tabular data and converted textual data are regarded as two
		  different modalities and are separately fed into the
		  collaborative CTR model and pre-trained language model. A
		  cross-modal knowledge alignment procedure is performed to
		  fine-grained align and integrate the collaborative and
		  semantic signals, and the lightweight collaborative model
		  can be deployed online for efficient serving after
		  fine-tuned with supervised signals. Experimental results on
		  three public datasets show that CTRL outperforms the
		  state-of-the-art (SOTA) CTR models significantly. Moreover,
		  we further verify its effectiveness on a large-scale
		  industrial recommender system.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Recomm. Syst.},
  month		= feb,
  keywords	= {Click-Through Rate Prediction, Language Model, Recommender
		  System}
}

@InProceedings{	  10.1145/3707292.3707356,
  author	= {Zhao, Pei and Zhang, Longxing and Zhao, Jiawen},
  title		= {Complete the exploration of low-resource knowledge graph
		  completion based on large model technology},
  year		= {2025},
  isbn		= {9798400707308},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3707292.3707356},
  doi		= {10.1145/3707292.3707356},
  abstract	= {In the construction and application of knowledge graph, it
		  is a realistic research problem to complete the knowledge
		  in the low resource field. Traditional methods that rely on
		  manual annotation and rules are not only costly, but also
		  have limitations in coverage and scalability. To solve this
		  problem, this paper proposes a large model technique,
		  combining fine-tuning and knowledge transfer strategies.
		  Firstly, to improve the ability of fine-tuning of the large
		  model, the rich knowledge learned by the large model in the
		  high resource field to assist the completion of the low
		  resource knowledge graph through knowledge transfer
		  technology to make up for the shortage of direct
		  extraction. The experimental results show that this method
		  can effectively improve the completion rate and accuracy of
		  the knowledge graph, especially in the completion of entity
		  relations and attribute filling. Furthermore, we explore
		  the impact of different fine-tuning strategies and
		  knowledge transfer methods on the completion effect,
		  providing experimental empirical and theoretical support
		  for future studies on similar issues.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Artificial Intelligence and Intelligent Information
		  Processing},
  pages		= {140–145},
  numpages	= {6},
  keywords	= {Fine-tuning, Knowledge graph, Large model technology, Low
		  resource, Transfer learning},
  location	= { },
  series	= {AIIIP '24}
}

@Article{	  10.1145/3714429,
  author	= {Zhang, Hongbin and Wang, Tao and Wang, Zhuowei and Lin,
		  Nankai and Chen, Chong and Cheng, Lianglun},
  title		= {A GPT-assisted Multi-Granularity Contrastive Learning
		  approach for Knowledge Graph Entity Typing},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3714429},
  doi		= {10.1145/3714429},
  abstract	= {Knowledge graph entity typing (KGET) is an efficient way
		  to infer possible missing types for entities, which has
		  become a key instrument to enhance the construction of
		  knowledge graphs (KGs). Existing models to KGET have mainly
		  focused on a single granularity information such as
		  distinct entity information, but other granularity
		  information including entity-to-type-clusters, the same
		  cluster and interaction information have not been fully
		  explored, resulting in inferring incorrect types in KGs. To
		  address this, we propose a GPT-assisted Multi-Granularity
		  Contrastive Learning (GMGCL) approach to acquire
		  entity-to-type-clusters, entity, type-cluster and relation
		  information by GPT-assisted entity-to-type-clusters
		  clustering, entity-based, cluster-based and relation-based
		  contrastive learning, respectively. Our approach is
		  evaluated on FB15kET and YAGO43kET datasets, outperforming
		  other baselines and obtaining a 1.35% average improvement
		  at least on MRR.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= jan,
  keywords	= {Knowledge Graph Entity Typing, Generative Pre-trained
		  Transformer, Contrastive Learning, Knowledge
		  Representation}
}

@Article{	  10.1145/3696662,
  author	= {Peng, Yingtao and Gao, Chen and Zhang, Yu and Dan,
		  Tangpeng and Du, Xiaoyi and Luo, Hengliang and Li, Yong and
		  Meng, Xiaofeng},
  title		= {Denoising Alignment with Large Language Model for
		  Recommendation},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3696662},
  doi		= {10.1145/3696662},
  abstract	= {The mainstream approach of GNN-based recommendation
		  aggregates high-order ID information associated with the
		  node in the user-item graph. The aggregation pattern using
		  ID as signal has two disadvantages: lack of textual
		  semantics and the impact of interaction noise. These
		  disadvantages pose a threat to effectively learn user
		  preferences, especially in capturing intricate user-item
		  semantic relationships. Although large language models
		  (LLMs) allow the integration of rich textual information
		  into recommenders and have had groundbreaking applications
		  in recommender systems, current works need to bridge the
		  gap between different representation spaces. This is
		  because LLM-based methods align the representations of
		  GNN-based models only by using text embedding of LLM,
		  leading to unsatisfactory results. To address this
		  challenge, we propose a denoising alignment framework with
		  LLMs for GNN-based recommenders (DALR), which aims to align
		  structural representation with textual representation and
		  mitigate the effects of noise. Specifically, we propose a
		  modeling framework that integrates the representation of
		  graph structure with textual information from LLMs to
		  capture intricate user-item interactions. We also suggest
		  an alignment paradigm to enhance representation performance
		  by aligning semantic signals from LLMs and structural
		  features from GNN models. Additionally, we introduce a
		  contrastive learning scheme to relieve the impact of noise
		  and improve model performance. Extensive experiments on
		  public datasets demonstrate that our model consistently
		  outperforms the state-of-the-art methods. DALR achieves
		  improvements ranging from 2.82% to 12.20% in Recall@5 and
		  from 1.04% to 3.48% in NDCG@5 compared to the strongest
		  baseline model, using the Steam dataset as an example.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {32},
  numpages	= {35},
  keywords	= {Recommender system, graph neural network, large language
		  models, contrastive learning}
}

@InProceedings{	  10.1145/3708036.3708272,
  author	= {Yang, Guangyuan and Xie, Quanying and Chen, Lei},
  title		= {A Scientometrics Analysis and Visualization of Large
		  Language Model in China's Library},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708272},
  doi		= {10.1145/3708036.3708272},
  abstract	= {Large Language Model has been researched in the field of
		  library from the following aspects:space reproduction,
		  service reform, library construction and so on. In order to
		  clarify the current research situation of Large Language
		  Model's application research in the field of library, and
		  provide some reference for the further development of
		  research fields related to Large Language Model empowering
		  library in the future. This paper utilizes two methods of
		  scientometrics and data visualization to analyze and study
		  the journal papers on the application of Large Language
		  Model in the field of Chinese libraries from the aspects of
		  the degree of academic focus, the way of creating academic
		  achievements and research topics of academic achievements,
		  and puts forward the research practice of strengthening the
		  application of Large Language Model in library from the
		  aspects of ’Strengthen the practical research of Large
		  Language Model empowering Chinese library’ and ‘Broaden
		  the field of research related to Large Language Model
		  empowering Chinese library’, in order to promote the
		  all-round development of Large Language Model in the field
		  of library.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {1403–1407},
  numpages	= {5},
  keywords	= {Chinese libraries, Data Visualization, Large Language
		  Model, Library Service, Scientometrics},
  location	= { },
  series	= {ICCSMT '24}
}

@InProceedings{	  10.1145/3706890.3706997,
  author	= {Nan, Beier and Gu, Jinguang and Qiu, Chen and Wu,
		  Jingyun},
  title		= {Construction and application of medical history knowledge
		  graph based on UIE model fine-tuning},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706997},
  doi		= {10.1145/3706890.3706997},
  abstract	= {Objective: To achieve the automated extraction of complex
		  medical history knowledge from Chinese electronic medical
		  records, a fine-tuned UIE extraction model is utilized to
		  automatically obtain medical history knowledge and
		  construct a knowledge graph (KG) of medical histories.
		  Method: Taking the current medical history as an example, a
		  fundamental knowledge base of Chinese medical history was
		  first built. Then, based on this knowledge base, a training
		  set was annotated, and the UIE model was fine-tuned using
		  this training set. The fine-tuned UIE model was then used
		  to extract medical history knowledge, which was processed
		  and stored to generate a KG of medical histories. Results:
		  The fine-tuned UIE model achieved entity, relationship, and
		  event extraction tasks. Subsequently, the extracted medical
		  history information was processed and stored, successfully
		  constructing a KG of the current medical history.
		  Conclusion: This method realizes the completion of entity,
		  relation, and event extraction tasks by training only one
		  model, efficiently achieving automatic extraction of
		  specified medical history knowledge from Chinese electronic
		  medical records and constructing a KG of medical histories.
		  It helps organize and analyze complex medical history
		  knowledge for clinical use, offering practical value.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {621–626},
  numpages	= {6},
  keywords	= {Constructing KG, Information extraction, Medical history
		  knowledge, Model fine-tuning},
  location	= { },
  series	= {ISAIMS '24}
}

@InProceedings{	  10.1145/3706890.3706892,
  author	= {Duan, Yidan and Lin, Shaofu and Liu, Xiliang and Huang,
		  Zhisheng and Su, Haoru and Bai, Yingfan},
  title		= {ElderQA-GPT: A Large Language Model for Online Q&amp;A on
		  Geriatric Diseases Based on BGE Semantic Vector Knowledge
		  Base and LangChain Architecture},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706892},
  doi		= {10.1145/3706890.3706892},
  abstract	= {Scientific guidance of disease Q&amp;A for the elderly is
		  of distinct significance for improving the health level of
		  the elderly, which is the immediate need of the current
		  aging society. The traditional question answering system
		  for geriatric diseases based on knowledge graph relies
		  heavily on manual annotation and has some problems such as
		  difficulty in knowledge fusion. The rapid development of
		  large models provides a new opportunity for geriatric
		  disease Q&amp;A system, but it also has problems such as
		  interpretability, applicability and multi-round Q&amp;A
		  illusion. In this paper, we propose a large language model
		  ElderQA-GPT for online Q&amp;A of geriatric diseases based
		  on BGE semantic vector knowledge base and LangChain
		  architecture. First, in order to improve the
		  interpretability of Q&amp;A dialogues, this paper adopts
		  the SELF-QA framework and combines the current professional
		  websites of geriatric diseases (including Dr. Ding Xiang,
		  Seeking Medical Care, and the Chinese government website)
		  to construct a vector database of geriatric diseases based
		  on the BGE semantic vector model; secondly, in order to
		  improve the accuracy and applicability of the system, the
		  lightweight open source ChatGLM3 model is privately
		  deployed based on the LangChain architecture, which
		  enhances the retrieval capability of the localized
		  inference knowledge base; lastly, in order to better
		  accumulate and comprehend the information in the complex
		  contexts and multiple rounds of dialogues, and to maintain
		  the semantic consistency, a LoRA fine-tuning technique is
		  used to enhance the ElderQA-GPT multi-round dialogue
		  capability. Two sets of quantitative experiments were
		  conducted to explore two dimensions: the interpretability
		  of model responses and the resolution of the illusion
		  problem in multi-round Q&amp;A sessions. Additionally, an
		  ablation study was carried out by omitting the knowledge
		  base to comparatively evaluate the impact on the system's
		  performance. The validation results demonstrate that
		  ElderQA-GPT exhibits improved answer interpretability and
		  applicability, effectively maintaining semantic consistency
		  across multi-round dialogues without introducing false
		  information. The findings of this study significantly
		  advance the field of online Q&amp;A for geriatric diseases
		  by addressing critical challenges such as interpretability,
		  accuracy, and multi-round dialogue. Our proposed model
		  ElderQA-GPT not only improves public knowledge and
		  understanding of geriatric diseases but also optimizes
		  medical resource allocation and enhances health management
		  for the elderly population. By providing interpretable and
		  accurate responses across multi-round dialogues,
		  ElderQA-GPT ensures that users receive reliable medical
		  information tailored to their needs. These advancements
		  have profound implications for healthcare delivery and
		  society, paving the way for more efficient and effective
		  healthcare services for elderly individuals.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {9–15},
  numpages	= {7},
  keywords	= {BGE, ElderQA-GPT, LangChain, SELF-QA},
  location	= { },
  series	= {ISAIMS '24}
}

@Article{	  10.1145/3704262,
  author	= {Cao, Yihan and Li, Siyu and Liu, Yixin and Yan, Zhiling
		  and Dai, Yutong and Yu, Philip and Sun, Lichao},
  title		= {A Survey of AI-Generated Content (AIGC)},
  year		= {2025},
  issue_date	= {May 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {5},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3704262},
  doi		= {10.1145/3704262},
  abstract	= {Recently, Artificial Intelligence Generated Content (AIGC)
		  has gained significant attention from society, especially
		  with the rise of Generative AI (GAI) techniques such as
		  ChatGPT, GPT-4 [165], DALL-E-3 [184], and Sora [137]. AIGC
		  involves using AI models to create digital content, such as
		  images, music, and natural language, with the goal of
		  making the content creation process more efficient and
		  accessible. Large-scale models have become increasingly
		  important in AIGC as they provide better intent extraction
		  and generation results. This survey provides a
		  comprehensive review of the history of generative models
		  and recent advances in AIGC, focusing on both unimodal and
		  multimodal interaction. From the perspective of
		  unimodality, we introduce the generation tasks and relative
		  models of text and image. From the perspective of
		  multimodality, we introduce the cross-application between
		  the modalities mentioned above. Finally, the survey
		  discusses the existing open problems and future challenges
		  in AIGC. Overall, this survey serves as a valuable resource
		  for individuals interested in understanding the background
		  and secrets behind the impressive performance of AIGC
		  techniques.},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  articleno	= {125},
  numpages	= {38},
  keywords	= {Generative AI, AI-generated content, multimodal machine
		  learning}
}

@Article{	  10.1145/3711857,
  author	= {Hu, Linmei and Zhang, Xinyu and Song, Dandan and Zhou,
		  Changzhi and He, Hongyu and Nie, Liqiang},
  title		= {Efficient and Effective Role Player: A Compact
		  Knowledge-grounded Persona-based Dialogue Model Enhanced by
		  LLM Distillation},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3711857},
  doi		= {10.1145/3711857},
  abstract	= {Incorporating explicit personas into dialogue models is
		  critical for generating responses that fulfill specific
		  user needs and preferences, creating a more personalized
		  and engaging interaction. Early works on persona-based
		  dialogue generation directly concatenate the persona
		  descriptions and dialogue history into relatively small
		  pre-trained language models (PLMs) for response generation,
		  which leads to uninformative and inferior results due to
		  the sparse persona information and the limited model
		  generation capabilities. Recently, large language models
		  (LLMs) have shown their surprising capabilities in language
		  generation. Prompting the LLMs with the persona
		  descriptions for role-playing dialogue generation has also
		  achieved promising results. However, deploying LLMs is
		  challenging for practical applications due to their large
		  scale, spurring efforts to distill the generation
		  capabilities into more concise and compact models through
		  teacher-student learning. In this paper, we propose an
		  efficient compact Knowledge-grounded Persona-based Dialogue
		  model enhanced by LLM Distillation (KPDD). Specifically,
		  first, we propose to enrich the annotated persona
		  descriptions by integrating external knowledge graphs (KGs)
		  with a mixed encoding network, coupled with a mixture of
		  experts (MoE) module for both informative and diverse
		  response generation. The mixed encoding network contains
		  multiple layers of modality interaction operations,
		  enabling information from both modalities propagates to the
		  other. Second, to fully exploit the generation capabilities
		  of LLMs, we turn to the distillation technique to improve
		  the generation capabilities of our model, facilitated by a
		  natural language inference (NLI) based filtering mechanism
		  to extract high-quality information from LLMs. In addition,
		  we employ a curriculum learning strategy to train our model
		  on the high-quality filtered distilled data and
		  progressively on the relatively noisy original data,
		  enhancing its adaptability and performance. Extensive
		  experiments show that KPDD outperforms state-of-the-art
		  baselines in terms of both automatic and human
		  evaluation.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  keywords	= {Persona-based Dialogue Generation, Knowledge Graph, MoE,
		  Large Language Model, Distillation, Curriculum Learning}
}

@Article{	  10.1145/3715909,
  author	= {Wu, Chunlian and Chen, Sen and Li, Jiaming and Chai,
		  Renchao and Fan, Lingling and Xie, Xiaofei and Feng,
		  Ruitao},
  title		= {Beyond Decision: Android Malware Description Generation
		  through Profiling Malicious Behavior Trajectory},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3715909},
  doi		= {10.1145/3715909},
  abstract	= {Malware family labels and key features used for the
		  decision-making of Android malware detection models fall
		  short of precise comprehension of malicious behaviors due
		  to their coarse granularity. To solve these problems, in
		  this paper, we first introduce the concept of the malicious
		  behavior trajectory (MBT) and propose an innovative
		  approach called ProMal. ProMal aims to automatically
		  generate malware descriptions with fine granularity through
		  extracted MBTs from malware for users. Specifically, a
		  labeled dataset of MBTs is constructed through substantial
		  human efforts to build a behavioral knowledge graph (BxKG).
		  The BxKG is scalable and can be automatically updated using
		  two strategies to ensure its completeness and timeliness:
		  1) taking into consideration the evolution of Android SDKs,
		  and 2) mining new MBTs by leveraging the widely-used
		  malware datasets. We highlight that the knowledge graph is
		  essential in ProMal, which can reason new MBTs based on
		  existing MBTs because of its structured data representation
		  and semantic relation modeling, and thus helps effectively
		  extract real MBTs in Android malware. We evaluated ProMal
		  on a recent malware dataset where researcher-crafted
		  malware descriptions are available, and the Precision,
		  Recall, and F1-Score of MBT identification based on BxKG
		  reached 96.97%, 91.43%, and 0.94, respectively,
		  outperforming the state-of-the-art approaches. Taking MBTs
		  identified from Android malware as inputs, precise,
		  fine-grained, and human-readable descriptions can be
		  generated using the large language model, whose readability
		  and usability are verified through a user study. The
		  generated descriptions play a significant role in
		  interpreting and comprehending malware behaviors.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jan,
  keywords	= {Android Malware, Malicious Behavior Analysis, Knowledge
		  Graph}
}

@InProceedings{	  10.1145/3707292.3707389,
  author	= {Li, Yanjun and Yang, Ruiting and Guo, Donghao and Song,
		  Yu},
  title		= {Research on the Construction of Digital Knowledge Graphs
		  Based on Resources of National First-Class Undergraduate
		  Programs},
  year		= {2025},
  isbn		= {9798400707308},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3707292.3707389},
  doi		= {10.1145/3707292.3707389},
  abstract	= {[Purpose/Significance]: The digitalization of education is
		  an essential path to advancing higher education. The
		  construction of knowledge graphs is a key approach to
		  achieving the digitalization and intelligence of education.
		  [Method/Process]: This paper leverages the rich video
		  resources of existing national first-class undergraduate
		  programs and, based on the teaching orientations of
		  different universities, independently designs customized
		  ontologies and extraction principles. These are then
		  integrated into the LLM knowledge graph builder to ensure
		  the hierarchical structure of the overall course framework.
		  The course video content is transformed into text form, and
		  large language models (LLMS) and word segmentation tools
		  are used for core content extraction, text cleaning, and
		  lexical analysis. The structured text is then converted
		  into SPO (Subject-Predicate-Object) triplets database.
		  [Results/Conclusions]: Finally, the database is imported
		  into the LLM knowledge graph builder, which is
		  pre-configured with extraction rules. It will automatically
		  generate the knowledge graph. After the text is imported
		  into the LLM knowledge graph builder, it will be manually
		  checked to ensure it better meets the actual needs of the
		  students. [Innovation/Limitations]: The research team plans
		  to apply the knowledge graph to train a specialized
		  knowledge-based Q&amp;A assistant. This will support
		  students' understanding and self-assessment of knowledge
		  points in an online learning community. Student feedback
		  will be used to improve and enrich the knowledge graph.
		  Compared to existing methods, this approach better aligns
		  with the constantly evolving digital teaching resources
		  available online, offering more comprehensive and
		  higher-level automation.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Artificial Intelligence and Intelligent Information
		  Processing},
  pages		= {353–359},
  numpages	= {7},
  keywords	= {Knowledge graph, course resources, intelligent Q&amp;A,
		  ontology construction, personalized learning},
  location	= { },
  series	= {AIIIP '24}
}

@InProceedings{	  10.1145/3704323.3704357,
  author	= {Zhang, Lu and Liu, Yu and Luo, Yitian and Gao, Feng and
		  Gu, Jinguang},
  title		= {Qwen-IG: A Qwen-based Instruction Generation Model for LLM
		  Fine-tuning},
  year		= {2025},
  isbn		= {9798400717482},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704323.3704357},
  doi		= {10.1145/3704323.3704357},
  abstract	= {The quality of instructions is crucial for LLM (large
		  language model) fine-tuning. The most compelling data for
		  instruction tuning exhibit not only high complexity, low
		  perplexity, high faithfulness, and high answer relevancy,
		  but also diversity, naturalness, coherence, and
		  understandability. However, traditional QAG (question and
		  answer generation) and LLMs face challenges regarding
		  instruction complexity, answer relevancy and diversity. To
		  overcome these issues, we have developed the Qwen-IG
		  instruction generation model for LLM fine-tuning. Firstly,
		  we collect high-quality instruction sets, such as alpaca,
		  GPT4-LLM, and TigerBot, and utilize them to build a train
		  set for LLM fine-tuning. Secondly, Qwen is trained with the
		  train set just generated through three approaches, namely
		  C2IO (context to instruction and output), C2I2O (context to
		  instruction to output), and C2O2I (context to output to
		  instruction). Finally, we employ the eight metrics
		  mentioned above to evaluate the effectiveness of Qwen-IG in
		  the task of generating instructions. The experimental
		  results demonstrate Qwen-IG has increased at least 9.6% on
		  complexity, 23% on answer relevancy and 17% on diversity.},
  booktitle	= {Proceedings of the 2024 13th International Conference on
		  Computing and Pattern Recognition},
  pages		= {295–302},
  numpages	= {8},
  keywords	= {Instruction Generation, Large Language Model, Instruction
		  tuning},
  location	= { },
  series	= {ICCPR '24}
}

@Article{	  10.1145/3708883,
  author	= {Qu, Zekai and Xie, Ruobing and Xiao, Chaojun and Yao, Yuan
		  and Liu, Zhiyuan and Lian, Fengzong and Kang, Zhanhui and
		  Zhou, Jie},
  title		= {Thoroughly Modeling Multi-domain Pre-trained
		  Recommendation as Language},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3708883},
  doi		= {10.1145/3708883},
  abstract	= {With the thriving of the pre-trained language model (PLM)
		  widely verified in various NLP tasks, pioneer efforts
		  attempt to explore the possible cooperation of the general
		  textual information in PLM with the personalized behavioral
		  information in user historical behavior sequences to
		  enhance sequential recommendation (SR). However, despite
		  the commonalities of input format and task goal, there are
		  huge gaps between the behavioral and textual information,
		  which obstruct thoroughly modeling SR as language modeling
		  via PLM. To bridge the gap, we propose a novel unified
		  pre-trained language model enhanced sequential
		  recommendation (UPSR) that thoroughly transfers the next
		  item prediction task to a text generation task, aiming to
		  build a unified pre-trained recommendation model for
		  multi-domain recommendation tasks. We formally design five
		  key indicators, namely naturalness, domain consistency,
		  informativeness, noise and ambiguity, and text length, to
		  guide the text (rightarrow) item adaptation (selecting
		  appropriate text to form the item textual representation)
		  and behavior sequence (rightarrow) text sequence adaptation
		  (transferring the sequence of item textual representations
		  into a text sequence) differently for pre-training and
		  fine-tuning stages, which are essential but under-explored
		  by previous works. In experiments, we conduct extensive
		  evaluations on seven datasets with both supervised and
		  zero-shot settings and achieve the overall best
		  performance. Comprehensive model analyses also provide
		  valuable insights for behavior modeling via PLM, shedding
		  light on large pre-trained recommendation models. The
		  source codes will be released in the future.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {54},
  numpages	= {28},
  keywords	= {Recommendation, Language model, Pre-training}
}

@Article{	  10.1145/3712600,
  author	= {Chen, Jiayue and Wang, Xiaomeng and Xu, Tong and Wu,
		  Shiwei},
  title		= {Towards Scene-Centric Multi-Level Interest Mining for
		  Video Recommendation},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1551-6857},
  url		= {https://doi.org/10.1145/3712600},
  doi		= {10.1145/3712600},
  abstract	= {Knowledge-aware video recommendation requires the ability
		  of associating external knowledge to capture high-order
		  connectivities between users and videos. One limitation of
		  existing methods is that they only extract user interests
		  at a granular level of relational paths by modeling
		  high-order connectivities, which are coarse-grained in user
		  interest modeling, failing to identify user-video relations
		  at a finer-grained level of semantics. In this paper, we
		  investigate the utility of semantic scene graphs in video
		  recommendation scenario, which provide detailed,
		  graph-based annotations of social situations depicted in
		  video clips. We propose a new method named Scene-Centric
		  multi-level Interest Miner (SCIMiner) which explicitly
		  models multi-level user interests. Specifically, we
		  construct user-video graph, knowledge graph and semantic
		  scene graphs as hierarchical heterogeneous graphs. On top
		  of the hierarchical graph representation, we propose two
		  information aggregation strategies to capture user
		  interests from different levels. Knowledge-aware
		  aggregation scheme extracts coarse-grained user interests
		  by aggregating relational paths in high-order
		  connectivities, while scene-aware aggregation scheme models
		  fine-grained user interests by capturing clip-level
		  semantic commonality of user favorite videos. Furthermore,
		  we adaptively distill complementary information about
		  multi-level user interests extracted by different
		  aggregation schemes and encode them into the
		  representations of users and videos. Empirical results show
		  that SCIMiner significantly outperforms the
		  state-of-the-art methods. Further studies verify the
		  complementary benefits of knowledge graph with semantic
		  scene graphs in video recommendation scenario and the
		  finer-grained explainability for predictions.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Multimedia Comput. Commun. Appl.},
  month		= jan,
  keywords	= {Video Recommendation, Graph Neural Networks, Knowledge
		  Graph, Scene Graph, Multi-Level Interests}
}

@Article{	  10.1145/3695995,
  author	= {Huang, Yuekai and Wang, Junjie and Wang, Song and Wei,
		  Moshi and Shi, Lin and Liu, Zhe and Wang, Qing},
  title		= {Deep API Sequence Generation via Golden Solution Samples
		  and API Seeds},
  year		= {2025},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {34},
  number	= {2},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3695995},
  doi		= {10.1145/3695995},
  abstract	= {Automatic API recommendation can accelerate developers’
		  programming and has been studied for years. There are two
		  orthogonal lines of approaches for this task, i.e.,
		  information retrieval-based (IR-based) approaches and
		  sequence to sequence (seq2seq) model-based approaches.
		  Although these approaches were reported to have remarkable
		  performance, our observation finds two major drawbacks,
		  i.e., IR-based approaches lack the consideration of
		  relations among the recommended APIs, and seq2seq models do
		  not model the API’s semantic meaning. To alleviate the
		  above two problems, we propose APIGens, which is a
		  retrieval-enhanced large language model (LLM)-based API
		  recommendation approach to recommend an API sequence for a
		  natural language query. The approach first retrieves
		  similar programming questions in history based on the input
		  natural language query, and then scores the results based
		  on API documents via a scorer model. Finally, these results
		  are used as samples for few-shot learning of LLM. To reduce
		  the risk of encountering local optima, we also extract API
		  seeds from the retrieved results to increase the search
		  scope during the LLM generation process. The results show
		  that our approach can achieve 48.41% ROUGE@10 on API
		  sequence recommendation and the 82.61% MAP on API set
		  recommendation, largely outperforming the state-of-the-art
		  baselines.},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jan,
  articleno	= {44},
  numpages	= {21},
  keywords	= {API recommendation, deep learning, information retrieval,
		  sequence generation, large language model}
}

@InProceedings{	  10.5555/3716662.3716801,
  author	= {Wolfe, Robert and Mitra, Tanushree},
  title		= {The Implications of Open Generative Models in
		  Human-Centered Data Science Work: A Case Study with
		  Fact-Checking Organizations},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {Calls to use open generative language models in academic
		  research have highlighted the need for reproducibility and
		  transparency in scientific research. However, the impact of
		  generative AI extends well beyond academia, as corporations
		  and public interest organizations have begun integrating
		  these models into their data science pipelines. We expand
		  this lens to include the impact of open models on
		  organizations, focusing specifically on fact-checking
		  organizations, which use AI to observe and analyze large
		  volumes of circulating misinformation, yet must also ensure
		  the reproducibility and impartiality of their work. We
		  wanted to understand where fact-checking organizations use
		  open models in their data science pipelines; what motivates
		  their use of open models or proprietary models; and how
		  their use of open or proprietary models can inform research
		  on the societal impact of generative AI. To answer these
		  questions, we conducted an interview study with N=24
		  professionals at 20 fact-checking organizations on six
		  continents. Based on these interviews, we offer a
		  five-component conceptual model of where fact-checking
		  organizations employ generative AI to support or automate
		  parts of their data science pipeline, including Data
		  Ingestion, Data Analysis, Data Retrieval, Data Delivery,
		  and Data Sharing. We then provide taxonomies of
		  fact-checking organizations' motivations for using open
		  models and the limitations that prevent them for further
		  adopting open models, finding that they prefer open models
		  for Organizational Autonomy, Data Privacy and Ownership,
		  Application Specificity, and Capability Transparency.
		  However, they nonetheless use proprietary models due to
		  perceived advantages in Performance, Usability, and Safety,
		  as well as Opportunity Costs related to participation in
		  emerging generative AI ecosystems. Finally, we propose a
		  research agenda to address limitations of both open and
		  proprietary models. Our research provides novel perspective
		  on open models in data-driven organizations.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {1595–1607},
  numpages	= {13},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@InProceedings{	  10.1145/3705754.3705790,
  author	= {Zhu, Sitong and Xia, Baobing and Duan, Fangwei and Zhao,
		  Zhenyang and Zhao, Zhenxia and Xiao, Teliang and Liu,
		  Zhicheng and Liu, Xia},
  title		= {Automated Framework for Constructing Knowledge Graphs
		  Oriented for Standard Analysis Using Large Language
		  Models},
  year		= {2025},
  isbn		= {9798400710193},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3705754.3705790},
  doi		= {10.1145/3705754.3705790},
  abstract	= {In an era characterized by rapid technological growth and
		  digital transformation, the necessity for efficient and
		  structured knowledge representation has grown more crucial.
		  Standards serve as fundamental cornerstones that offer
		  guidelines, specifications, and frameworks to ensure the
		  quality and interoperability of products, services, as well
		  as systems. Nonetheless, the complexity and extensive
		  nature of standard documents present significant challenges
		  in extraction, alignment, and organization. Traditional
		  manual processing methods frequently prove labor-intensive
		  and susceptible to errors, impeding the capturing of
		  intricate relationships and hierarchies within these
		  standards. Knowledge Graphs (KGs) provide a robust
		  methodology for organizing information, facilitating
		  improved functionalities for advanced search, reasoning,
		  and analytics. Despite their potential, structuring KGs
		  from standard documents continues to be challenging because
		  of unstructured text, domain-specific terminology, and the
		  intricacies in accurate information extraction. Recent
		  advancements in Natural Language Processing (NLP),
		  particularly the emergence of Large Language Models (LLMs)
		  like GPT-3, have opened new avenues for automating the
		  extraction and structuring of information from unstructured
		  content. These models exhibit exceptional proficiency in
		  comprehending and producing human-like text, positioning
		  them as feasible solutions for addressing the complexities
		  associated with standard documents. This paper presents an
		  automated framework named StandardKG Builder, which
		  utilizes LLMs to construct knowledge graphs tailored for
		  standard analysis from multiple perspectives for complex
		  information extraction. Our evaluation on a comprehensive
		  dataset of standard documents highlights the framework’s
		  effectiveness and scalability. By merging sophisticated
		  knowledge representation with advanced NLP techniques, our
		  work significantly enhances the accessibility and analysis
		  of standard documents, paving the way for more efficient
		  and intelligent standard management systems.},
  booktitle	= {Proceedings of the 2024 2nd International Conference on
		  Electronics, Computers and Communication Technology},
  pages		= {183–189},
  numpages	= {7},
  keywords	= {Knowledge Graph, Large Language Models, Standard
		  Analysis},
  location	= { },
  series	= {CECCT '24}
}

@Article{	  10.1145/3709727,
  author	= {Luoma, Kyle and Kumar, Arun},
  title		= {SNAILS: Schema Naming Assessments for Improved LLM-Based
		  SQL Inference},
  year		= {2025},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {1},
  url		= {https://doi.org/10.1145/3709727},
  doi		= {10.1145/3709727},
  abstract	= {Large Language Models (LLMs) have revolutionized Natural
		  Language to SQL (NL-to-SQL), dominating most NL-to-SQL
		  benchmarks. But LLMs still face limitations due to
		  hallucinations, semantic ambiguity, and lexical mismatches
		  between an NL query and the database schema. Naturally, a
		  lot of work in the ML+DB intersection aims to mitigate such
		  LLM limitations. In this work, we shine the light on a
		  complementary data-centric question: How should DB schemas
		  evolve in this era of LLMs to boost NL-to-SQL? The
		  intuition is that more NL-friendly schema identifiers can
		  help LLMs work better with DBs. We dive deeper into this
		  seemingly obvious, but hitherto underexplored and
		  important, connection between schema identifier
		  ''naturalness'' and the behavior of LLM-based NL-to-SQL by
		  creating a new integrated benchmark suite we call SNAILS.
		  SNAILS has 4 novel artifacts: (1) A collection of
		  real-world DB schemas not present in prior NL-to-SQL
		  benchmarks; (2) A set of labeled NL-SQL query pairs on our
		  collection not seen before by public LLMs; (3) A notion of
		  naturalness level for schema identifiers and a novel
		  labeled dataset of modified identifiers; and (4) AI
		  artifacts to automatically modify identifier naturalness.
		  Using SNAILS, we perform a comprehensive empirical
		  evaluation of the impact of schema naturalness on LLM-based
		  NL-to-SQL accuracy, and present a method for improving
		  LLM-based NL-to-SQL with natural views. Our results reveal
		  statistically significant correlations across multiple
		  public LLMs from OpenAI, Meta, and Google on multiple
		  databases using both zero-shot prompting as well as more
		  complex NL-to-SQL workflows: DIN SQL, and CodeS. We present
		  several fine-grained insights and discuss pathways for DB
		  practitioners to better exploit LLMs for NL-to-SQL.},
  journal	= {Proc. ACM Manag. Data},
  month		= feb,
  articleno	= {77},
  numpages	= {26},
  keywords	= {benchmark, database, llm, natural language to sql,
		  relational database schema, schema design, schema linking,
		  schema naturalness}
}

@InProceedings{	  10.1145/3708036.3708165,
  author	= {Huang, Ling and Deng, Wanqiu and Jiang, Yiling and Zhong,
		  Qinghua},
  title		= {Development trends of large language models and their
		  applications in green digital intelligence of supply
		  chains},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708165},
  doi		= {10.1145/3708036.3708165},
  abstract	= {In recent years, large language models (LLM) have
		  developed rapidly and been widely used in the field of
		  natural language processing, and have made significant
		  progress and been widely used in both academia and
		  industry. Large language models have also shown great
		  potential in promoting the green digital intelligence of
		  the supply chain. The development of language models
		  provides technical support for the green digital
		  intelligence of the supply chain. The model's text analysis
		  capabilities can help companies analyze information such as
		  suppliers' renewable energy usage, carbon footprint,
		  environmental and social responsibility reports, and thus
		  make more informed sourcing and supplier selection
		  decisions. At the same time, the large language model can
		  analyze supply chain data, provide suggestions and
		  optimization plans for energy conservation and emission
		  reduction, predict and manage environmental risks, and
		  promote enterprises to transform into a more sustainable
		  and environmentally friendly supply chain.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {770–774},
  numpages	= {5},
  keywords	= {Green digital intelligence of supply chain, Natural
		  Language Processing, large language model},
  location	= { },
  series	= {ICCSMT '24}
}

@Article{	  10.1145/3707650,
  author	= {Korn, Daniel and Hou, Pei-Yu and Schatz, Kara and Beasley,
		  Jon-Michael and Tropsha, Alexander and Chirkova, Rada},
  title		= {Towards Improving the Efficiency of Drug Repurposing by
		  Leveraging Node Promiscuity in Biomedical Knowledge
		  Graphs},
  year		= {2025},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {1},
  url		= {https://doi.org/10.1145/3707650},
  doi		= {10.1145/3707650},
  abstract	= {To accelerate the time- and labor-intensive processes of
		  drug discovery and repurposing, it is increasingly common
		  to mine knowledge sources for connections between diseases
		  and the drugs that can treat them. In this article we
		  address the scalability challenge in the connection mining,
		  by introducing algorithms that can be used to find
		  plausible mechanistic connections between drugs and the
		  potentially associated diseases in biomedical knowledge
		  graphs. These connections are then presented to biomedical
		  experts as candidate hypotheses for further studies of
		  whether the drugs can be repurposed to treat the
		  diseases.One challenge that has to be addressed in this
		  effort is the processing of promiscuous knowledge-graph
		  nodes, that is, nodes associated with numerous
		  relationships that may not be unique or indicative of the
		  node properties. As it turns out, the multiplicity of
		  relationships involving promiscuous graph nodes may prevent
		  the aforementioned path-finding algorithms from aiding in
		  drug repurposing. To address the promiscuous-node
		  challenge, we introduce promiscuity scores for nodes and
		  paths in knowledge graphs, and incorporate the scores in
		  the proposed path-finding algorithms. We report
		  experimental results that indicate that paths with
		  low-promiscuity scores could be meaningful and of interest
		  to biomedical experts in drug repurposing.},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= jan,
  articleno	= {8},
  numpages	= {32},
  keywords	= {Biomedical knowledge graphs, drug repurposing,
		  path-finding knowledge-graph algorithms}
}

@InProceedings{	  10.1145/3704522.3704537,
  author	= {Lee, Hyun and Islam, Maminur and Yi, Chris and
		  Chakraborttii, Chandranil (Nil)},
  title		= {Enhancing Graph Representation Learning with WalkLM for
		  Effective Community Detection},
  year		= {2025},
  isbn		= {9798400711589},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704522.3704537},
  doi		= {10.1145/3704522.3704537},
  abstract	= {Embeddings in deep neural networks are essential for
		  processing high-dimensional and categorical data by
		  converting it into compact, low-dimensional vectors. This
		  conversion enables the model to capture complex semantic
		  relationships and improves its generalization across
		  diverse tasks. Although embeddings have demonstrated
		  significant effectiveness in natural language processing
		  and applications involving large language models (LLMs),
		  their utility decreases when handling graph-based data. In
		  this study, we address this challenge by enhancing WalkLM,
		  a cutting-edge language model tailored for generating graph
		  embeddings. We refine the random walk algorithm to better
		  capture both local and global contexts within graphs.
		  Additionally, we implement a k-means algorithm for
		  community detection in these graphs. Our experiments across
		  various benchmark datasets confirm the efficacy of our
		  approach.},
  booktitle	= {Proceedings of the 11th International Conference on
		  Networking, Systems, and Security},
  pages		= {41–47},
  numpages	= {7},
  keywords	= {Graph Clustering, Graph Embedding, Community Detection,
		  Network, Machine Learning, LM(Language Model)},
  location	= { },
  series	= {NSysS '24}
}

@Article{	  10.1145/3652952,
  author	= {Degbelo, Auriol},
  title		= {Prolegomena to a Description Language for GenAI Tools in
		  Cities},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {6},
  number	= {1},
  url		= {https://doi.org/10.1145/3652952},
  doi		= {10.1145/3652952},
  abstract	= {The potential of generative AI has been recently
		  demonstrated through different applications. The open
		  government and smart city initiatives can leverage this
		  potential to produce innovations that improve government
		  workflows and the lives of citizens. This commentary makes
		  the case for a description language enabling the structured
		  documentation of these upcoming innovations. The
		  description language would facilitate the communication
		  between governments, citizens, and innovators. The key
		  elements of the description language are briefly sketched
		  and its usefulness is shown by the generation of ideas for
		  GenAI tools related to interactive maps in cities.},
  journal	= {Digit. Gov.: Res. Pract.},
  month		= feb,
  articleno	= {8},
  numpages	= {8},
  keywords	= {Smart cities, open government, GenAI tools, metadata
		  generation, interactive maps, human-computer interaction}
}

@InProceedings{	  10.1145/3706890.3706998,
  author	= {Wang, Qi and Li, Qiyuan and Gao, Chao and Liu, Haijiang
		  and Xu, Fangfang and Gu, Jinguang},
  title		= {A Medical Knowledge Management Mechanism with Knowledge
		  Hypergraph Theory},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706998},
  doi		= {10.1145/3706890.3706998},
  abstract	= {The construction of medical knowledge graphs(KGs)
		  structures complex medical data to support precise clinical
		  decision-making and knowledge discovery. A core challenge
		  in building medical KGs is effectively representing and
		  processing complex clinical data. Existing triple-based
		  medical knowledge representation methods often fail to
		  fully capture and express the complexity of the data,
		  particularly when dealing with higher-order relationships
		  involving multiple entities. Additionally, the current
		  "Attribute-Concept-Event" three-layer knowledge
		  organization framework shows limitations in representing
		  procedural knowledge in medical scenarios, falling short of
		  the hierarchical and dynamic requirements of evidence-based
		  medicine. To address these issues, we propose a four-layer
		  medical knowledge organization method based on hypergraph
		  theory. By incorporating hypergraph theory and a narrative
		  layer into the KG, we provide a more flexible and dynamic
		  framework for representing complex clinical information.
		  Based on this approach, we construct a four-layer KG,
		  RJUA-HKG, using real QA clinical data from urology as an
		  example. Experiments show that, compared to traditional
		  KGs, RJUA-HKG significantly reduces relational complexity
		  and retrieval time while accurately capturing changes in
		  diagnostic and treatment procedures.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {627–633},
  numpages	= {7},
  keywords	= {Knowledge graph, Knowledge hypergraph theory, Knowledge
		  organization, Medical knowledge management mechanism},
  location	= { },
  series	= {ISAIMS '24}
}

@InProceedings{	  10.1145/3706890.3706906,
  author	= {Zhang, Zhizheng and Yang, Qian and Du, Yan},
  title		= {A Question-and-Answer Scheme of Clinical Practice
		  Guidelines Based on SDA*},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706906},
  doi		= {10.1145/3706890.3706906},
  abstract	= {This paper proposes a medical diagnosis and treatment
		  knowledge question and answer scheme based on clinical
		  practice guidelines, aiming at the need of standardization
		  of medical process and popularization of medical knowledge.
		  The scheme realizes computer intelligent diagnosis and
		  treatment assistance through diagnosis and treatment
		  knowledge base construction, problem analysis, information
		  retrieval and answer generation. The
		  SDA*(State-Decision-Action) modeling method and question
		  formal definition for hypertension diagnose and treatment
		  are proposed, and the answers are generated based on the
		  answer set program (ASP). For the transformation of natural
		  language and ASP rules, large language model (LLM)
		  technology is adopted to automatically identify question
		  terms, generate ASP rules and process the generated
		  answers. To verify the scheme's ability to generate
		  reliable answers, accuracy for problem classification and
		  ASP rule generations are confirmed. The experimental
		  results show that the scheme could provide the ability to
		  solve medical problems based on the knowledge of clinical
		  practice guidelines.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {96–101},
  numpages	= {6},
  keywords	= {Clinical practice guideline, SDA* model, answer set
		  programming, hypertension, large language model, question
		  and answer},
  location	= { },
  series	= {ISAIMS '24}
}

@Article{	  10.1145/3715007,
  author	= {Chen, Xiang and Gao, Chaoyang and Chen, Chunyang and
		  Zhang, Guangbei and Liu, Yong},
  title		= {An Empirical Study on Challenges for LLM Application
		  Developers},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3715007},
  doi		= {10.1145/3715007},
  abstract	= {In recent years, large language models (LLMs) have seen
		  rapid advancements, significantly impacting various fields
		  such as computer vision, natural language processing, and
		  software engineering. These LLMs, exemplified by OpenAI’s
		  ChatGPT, have revolutionized the way we approach language
		  understanding and generation tasks. However, in contrast to
		  traditional software development practices, LLM development
		  introduces new challenges for AI developers in design,
		  implementation, and deployment. These challenges span
		  different areas (such as prompts, APIs, and plugins),
		  requiring developers to navigate unique methodologies and
		  considerations specific to LLM application
		  development.Despite the profound influence of LLMs, to the
		  best of our knowledge, these challenges have not been
		  thoroughly investigated in previous empirical studies. To
		  fill this gap, we present the first comprehensive study on
		  understanding the challenges faced by LLM developers.
		  Specifically, we crawl and analyze 29,057 relevant
		  questions from a popular OpenAI developer forum. We first
		  examine their popularity and difficulty. After manually
		  analyzing 2,364 sampled questions, we construct a taxonomy
		  of challenges faced by LLM developers. Based on this
		  taxonomy, we summarize a set of findings and actionable
		  implications for LLM-related stakeholders, including
		  developers and providers (especially the OpenAI
		  organization).},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jan,
  keywords	= {Mining Software Repository, Empirical Study, LLM
		  Developer, Development Challenges, Prompt Engineering}
}

@InProceedings{	  10.1145/3708036.3708061,
  author	= {Zhang, Chao and Li, Donghao and Dong, Bin},
  title		= {On-site smart maintenance based on collaboration between
		  network large model and AI models},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708061},
  doi		= {10.1145/3708036.3708061},
  abstract	= {On-site maintenance is the end of telecom operators'
		  network maintenance, and it is an important means of
		  ensuring the quality of telecom network operation and user
		  experience. Therefore, improving the quality and efficiency
		  of on-site maintenance is essential. This paper designs an
		  on-site smart maintenance scheme based on the network large
		  model, according to the actual maintenance processes of
		  telecom operators. It also proposes practical use cases
		  such as maintenance Q&amp;A assistant, on-site
		  troubleshooting and analysis assistant, and on-site
		  maintenance agent. By giving full play to generative AI and
		  orchestrating the ability of small models, the network
		  large model helps maintenance personnel solve practical
		  problems in production. It not only improves the
		  maintenance efficiency but also effectively guarantees the
		  safe operation of the network. While improving customer
		  satisfaction, it can also reduce the maintenance costs of
		  operators to a certain extent.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {145–150},
  numpages	= {6},
  keywords	= {Efficiency improvement, Network large model, On-site
		  maintenance},
  location	= { },
  series	= {ICCSMT '24}
}

@Article{	  10.1145/3715069,
  author	= {Varshney, Deeksha and Behera, Niranshu and Katari, Prajeet
		  and Ekbal, Asif},
  title		= {MedProm: Bridging Dialogue Gaps in Healthcare with
		  Knowledge-Enhanced Generative Models},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3715069},
  doi		= {10.1145/3715069},
  abstract	= {In medical dialogue systems, recent advancements
		  underscore the critical role of incorporating relevant
		  medical knowledge to enhance performance. However, existing
		  knowledge bases often lack completeness, posing a challenge
		  in sourcing pertinent information. We present MedProm, a
		  novel generative model tailored for medical dialogue
		  generation to address this gap. Motivated by the need for
		  comprehensive and contextually relevant responses, MedProm
		  leverages state-of-the-art language models such as BioGPT.
		  Our model is designed to integrate extensive medical
		  knowledge into conversations, facilitating effective
		  communication between patients and healthcare providers. At
		  the core of MedProm lies the MediConnect Graph, a
		  meticulously constructed knowledge graph capturing
		  intricate relationships among medical entities extracted
		  from dialogue contexts. By employing a KnowFusion encoder
		  with a pretraining objective and masked multi-head
		  self-attention, MedProm effectively processes the
		  MediConnect graph, enabling precise control over
		  information flow to capture its underlying structure.
		  Furthermore, MedProm incorporates a sophisticated
		  Curriculum Knowledge Decoder, leveraging transformer-based
		  decoding to generate response utterances conditioned on
		  input representations from the KnowFusion Encoder. The
		  training process is guided through curriculum learning,
		  gradually increasing optimization difficulty based on a
		  coherence-based criterion. Experimental results on two
		  datasets demonstrate the efficacy of MedProm in generating
		  accurate and contextually relevant responses compared to
		  state-of-the-art models.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= jan,
  keywords	= {Medical Dialogue Systems (MDS), Generative Neural Model,
		  MediConnect Graph, KnowFusion Encoder, Curriculum Knowledge
		  Decoder}
}

@Proceedings{	  10.1145/3681772,
  title		= {IWCTS'24: Proceedings of the 17th ACM SIGSPATIAL
		  International Workshop on Computational Transportation
		  Science GenAI and Smart Mobility Session},
  year		= {2024},
  isbn		= {9798400711510},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  abstract	= {The 17th International Workshop on Computational
		  Transportation Science (IWCTS 2024) will feature a Smart
		  Mobility track, emphasizing the growing relevance of human
		  mobility data from sources like cell phones, connected
		  vehicles, and volunteered geographic information. This data
		  integration is advancing smart city frameworks, intelligent
		  transportation systems, and urban planning. Managing and
		  analyzing large-scale datasets highlights the critical role
		  of advanced computational and AI techniques, including
		  Generative AI (GenAI), Large Language Models (LLMs), and
		  Retrieval-Augmented Generation (RAG). The workshop builds
		  on previous success, focusing on computational and
		  informatics approaches for optimized urban mobility. We
		  will build upon the success of previous workshops to
		  continue to focus on the computational and informatics
		  approaches for (not limited to):},
  location	= {Atlanta, GA, USA}
}

@InProceedings{	  10.5555/3716662.3716812,
  author	= {Zhang, Richard and van Liemt, Erin and Fischella, Tyler},
  title		= {Ontology of Belief Diversity: A Community-Based
		  Epistemological Approach},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {AI applications across classification, fairness, and human
		  interaction often implicitly require ontologies of social
		  concepts. Constructing these well, especially when there
		  are many relevant categories, is a controversial task but
		  is crucial for achieving meaningful inclusivity. Here, we
		  focus on developing a pragmatic ontology of belief systems,
		  which is a complex and often controversial space. By
		  iterating on our community-based design until mutual
		  agreement is reached, we found that epistemological methods
		  were best for categorizing the fundamental ways beliefs
		  differ, maximally respecting our principles of inclusivity
		  and brevity. We demonstrate our methodology's utility and
		  interpretability via user studies in term annotation and
		  sentiment analysis experiments for belief fairness in
		  language models.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {1735–1743},
  numpages	= {9},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@InProceedings{	  10.1145/3708036.3708114,
  author	= {Qiu, Han and Sun, Chuanqiang and Chen, Hongyun and Zou,
		  Baoyu and Dong, Zizheng},
  title		= {Design and Implementation of an Intelligent Document
		  Extraction and Review System Based on Multimodal Large
		  Language Models},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708114},
  doi		= {10.1145/3708036.3708114},
  abstract	= {This study aims to address the issues of low efficiency
		  and high manual involvement in existing government affairs
		  processes by proposing an automated system based on large
		  model technology. The system integrates modules such as
		  data collection, natural language processing, business rule
		  engine, process generation, and intelligent applications.
		  Innovations include using large models to transform
		  unstructured government information into executable rules,
		  automatically generating process models and documents, and
		  implementing intelligent approval and automated
		  verification at key nodes. Experimental results indicate
		  that the system significantly enhances the automation level
		  and execution efficiency of government affairs processes,
		  providing a new solution for the intelligent transformation
		  of government management.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {455–465},
  numpages	= {11},
  keywords	= {Government Affairs Process Automation, Intelligent
		  Governance, Large Language Model, Natural Language
		  Processing},
  location	= { },
  series	= {ICCSMT '24}
}

@InProceedings{	  10.1145/3708282.3708296,
  author	= {Qiu, Han and Chen, Hongyun and Zou, Baoyu and Dong,
		  Zizheng},
  title		= {Intelligent Design and Implementation of Government
		  Affairs Processes Driven by Large Language Models},
  year		= {2025},
  isbn		= {9798400709869},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708282.3708296},
  doi		= {10.1145/3708282.3708296},
  abstract	= {This study aims to address the issues of low efficiency
		  and high manual involvement in existing government affairs
		  processes by proposing an automated system based on large
		  model technology. The system integrates modules such as
		  data collection, natural language processing, business rule
		  engine, process generation, and intelligent applications.
		  Innovations include using large models to transform
		  unstructured government information into executable rules,
		  automatically generating process models and documents, and
		  implementing intelligent approval and automated
		  verification at key nodes. Experimental results indicate
		  that the system significantly enhances the automation level
		  and execution efficiency of government affairs processes,
		  providing a new solution for the intelligent transformation
		  of government management.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Artificial Intelligence of Things and Computing},
  pages		= {70–79},
  numpages	= {10},
  keywords	= {Government Affairs Process Automation, Intelligent
		  Governance, Large Language Model, Natural Language
		  Processing},
  location	= { },
  series	= {AITC '24}
}

@InProceedings{	  10.1145/3708394.3708450,
  author	= {Li, Zhuang and Chu, Zheng and Zhang, Qingwei},
  title		= {Artificial intelligence-based conversational exam model: a
		  perspective for higher education},
  year		= {2025},
  isbn		= {9798400710650},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708394.3708450},
  doi		= {10.1145/3708394.3708450},
  abstract	= {By analyzing the current situation of the application of
		  artificial intelligence in college education, based on the
		  educational assessment mode in college education at the
		  current stage, analyzing the characteristics of the
		  conversational examination mode, and proposing a
		  conversational examination mode based on artificial
		  intelligence. This model includes the examination system
		  architecture, the question bank design, and the
		  construction of the knowledge graph, as well as the
		  intelligent scoring mechanism. It shows great development
		  potential and application in the change of the examination
		  mode in college education. The model demonstrates
		  considerable potential for further development and
		  application in the context of evolving educational
		  assessment practices within the college and university
		  setting. It is particularly well-suited to the demands of
		  the digital age.},
  booktitle	= {Proceeding of the 2024 International Conference on
		  Artificial Intelligence and Future Education},
  pages		= {325–330},
  numpages	= {6},
  keywords	= {Educational innovations, Examination mode,
		  Multi-dimensional assessment, artificial intelligence
		  (AI)},
  location	= { },
  series	= {AIFE '24}
}

@Article{	  10.1145/3712003,
  author	= {He, Junda and Treude, Christoph and Lo, David},
  title		= {LLM-Based Multi-Agent Systems for Software Engineering:
		  Literature Review, Vision and the Road Ahead},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3712003},
  doi		= {10.1145/3712003},
  abstract	= {Integrating Large Language Models (LLMs) into autonomous
		  agents marks a significant shift in the research landscape
		  by offering cognitive abilities that are competitive with
		  human planning and reasoning. This paper explores the
		  transformative potential of integrating Large Language
		  Models into Multi-Agent (LMA) systems for addressing
		  complex challenges in software engineering (SE). By
		  leveraging the collaborative and specialized abilities of
		  multiple agents, LMA systems enable autonomous
		  problem-solving, improve robustness, and provide scalable
		  solutions for managing the complexity of real-world
		  software projects. In this paper, we conduct a systematic
		  review of recent primary studies to map the current
		  landscape of LMA applications across various stages of the
		  software development lifecycle (SDLC). To illustrate
		  current capabilities and limitations, we perform two case
		  studies to demonstrate the effectiveness of
		  state-of-the-art LMA frameworks. Additionally, we identify
		  critical research gaps and propose a comprehensive research
		  agenda focused on enhancing individual agent capabilities
		  and optimizing agent synergy. Our work outlines a
		  forward-looking vision for developing fully autonomous,
		  scalable, and trustworthy LMA systems, laying the
		  foundation for the evolution of Software Engineering 2.0.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jan,
  keywords	= {Large Language Models, Autonomous Agents, Multi-Agent
		  Systems, Software Engineering}
}

@Article{	  10.1613/jair.1.16777,
  author	= {Sundaresan, Divya and Watson, Akhira and Bardaka, Eleni
		  and Lee, Crystal Chen and Mayhorn, Christopher B. and
		  Singh, Munindar P.},
  title		= {Prosociality in Microtransit},
  year		= {2025},
  issue_date	= {Feb 2025},
  publisher	= {AI Access Foundation},
  address	= {El Segundo, CA, USA},
  volume	= {82},
  issn		= {1076-9757},
  url		= {https://doi.org/10.1613/jair.1.16777},
  doi		= {10.1613/jair.1.16777},
  abstract	= {We study (public) microtransit, a type of transportation
		  service wherein a municipality offers point-to-point rides
		  to residents, for a fixed, nominal fare. Microtransit
		  exemplifies practical resource allocation problems that are
		  often over-constrained in that not all ride requests
		  (pickup and dropoff locations at specified times) can be
		  satisfied or satisfied only by violating soft goals such as
		  sustainability, and where economic signals (e.g., surge
		  pricing) are not applicable—they would lead to unethical
		  outcomes by effectively coercing poor people. We posit that
		  instead of taking rider preferences as fixed, shaping them
		  prosocially will lead to improved societal outcomes.
		  Prosociality refers to an attitude or behavior that is
		  intended to benefit others. This paper demonstrates a
		  computational approach to prosociality in the context of a
		  (public) microtransit service for disadvantaged riders.
		  Prosociality appears as a willingness to adjust one’s
		  pickup and dropoff times and locations to accommodate the
		  schedules of others and to enable sharing rides (which
		  increases the number of riders served with the same
		  resources). This paper describes an interdisciplinary study
		  of prosociality in microtransit between a transportation
		  researcher, psychologists, a social scientist, and AI
		  researchers. Our contributions are these: (1) empirical
		  support for the viability of prosociality in microtransit
		  (and constraints on it) through interviews with drivers and
		  focus groups of riders; (2) a prototype mobile app
		  demonstrating how our prosocial intervention can be
		  combined with the transportation backend; (3) a
		  reinforcement learning approach to model a rider and
		  determine the best interventions to persuade that rider
		  toward prosociality; and (4) a cognitive model of rider
		  personas to enable evaluation of alternative
		  interventions.},
  journal	= {J. Artif. Int. Res.},
  month		= jan,
  numpages	= {34},
  keywords	= {Deep learning models, Artificial Intelligence, knowledge
		  graph, Natural Language processing, healthcare services.,
		  machine learning}
}

@InProceedings{	  10.1145/3708036.3708151,
  author	= {Yan, Erkai and Gao, Mengxiao and Tang, Mei},
  title		= {Analysis and Research on Generative Artificial
		  Intelligence in the Field of International Library and
		  Information Science},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708151},
  doi		= {10.1145/3708036.3708151},
  abstract	= {Generative artificial intelligence is an artificial
		  intelligence technology based on deep learning whose core
		  lies in leveraging computer algorithms and training data to
		  generate new, practically valuable content, encompassing
		  text, images, audio, videos, etc. This technology is poised
		  to exert profound impacts on the transformation and
		  development of libraries. Drawing on generative artificial
		  intelligence research publications in the field of
		  international library and information science included in
		  the Scopus database as the data source, this paper employs
		  CiteSpace software and SciVal tools to conduct a visual
		  analysis of literature outputs, core authors, journal
		  sources, and keywords. The results show that generative
		  artificial intelligence research in the international
		  library and information science field is applied primarily
		  in areas such as reference services, information literacy
		  education, and smart libraries. Recommendations are made to
		  promote the application and development of generative
		  artificial intelligence technology in libraries by
		  strengthening technological research and application,
		  boosting data analysis and data sharing, emphasizing
		  information security and privacy protection, promoting
		  cross-boundary integration and ecological development,
		  etc.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {679–686},
  numpages	= {8},
  keywords	= {ChatGPT, Generative artificial intelligence, library},
  location	= { },
  series	= {ICCSMT '24}
}

@InProceedings{	  10.5555/3716662.3716790,
  author	= {Varshney, Kush R.},
  title		= {Decolonial AI Alignment: Openness,
		  Vi\'{s}eundefineda-Dharma, and Including Excluded
		  Knowledges},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {Prior work has explicated the coloniality of artificial
		  intelligence (AI) development and deployment through
		  mechanisms such as extractivism, automation, sociological
		  essentialism, surveillance, and containment. However, that
		  work has not engaged much with alignment: teaching
		  behaviors to a large language model (LLM) in line with
		  desired values, and has not considered a mechanism that
		  arises within that process: moral absolutism---a part of
		  the coloniality of knowledge. Colonialism has a history of
		  altering the beliefs and values of colonized peoples; in
		  this paper, I argue that this history is recapitulated in
		  current LLM alignment practices and technologies.
		  Furthermore, I suggest that AI alignment be decolonialized
		  using three forms of openness: openness of models, openness
		  to society, and openness to excluded knowledges. This
		  suggested approach to decolonial AI alignment uses ideas
		  from the argumentative moral philosophical tradition of
		  Hinduism, which has been described as an open-source
		  religion. One concept used is vi\'{s}eundefineda-dharma, or
		  particular context-specific notions of right and wrong. At
		  the end of the paper, I provide a suggested reference
		  architecture to work toward the proposed framework.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {1467–1481},
  numpages	= {15},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@Proceedings{	  10.1145/3703619,
  title		= {VRCAI '24: Proceedings of the 19th ACM SIGGRAPH
		  International Conference on Virtual-Reality Continuum and
		  its Applications in Industry},
  year		= {2024},
  isbn		= {9798400713484},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Nanjing, Guangdong Province, China}
}

@Article{	  10.1145/3716131,
  author	= {Zhang, Haobo and Zhu, Qiannan and Dou, Zhicheng},
  title		= {A Unified Prompt-aware Framework for Personalized Search
		  and Explanation Generation},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3716131},
  doi		= {10.1145/3716131},
  abstract	= {Product search is crucial for users to find and purchase
		  products they need. Personalized product search, which
		  models users’ search intent and provides tailored
		  results, has become a prominent research problem in
		  industry and academia. Recent studies often leverage
		  knowledge graphs (KGs) to improve search performance and
		  generate explanations for search results. However, existing
		  KG-based methods treat search and explanation tasks
		  separately and explore paths in KGs as explanations,
		  creating a gap between search results and generated
		  explanations. Also, path-formed explanations in KGs are not
		  flexible enough to build correlations with the user’s
		  current query. To address these challenges, we propose
		  P-PEG, a unified prompt-aware framework for personalized
		  product search and explanation generation. P-PEG leverages
		  a pre-trained language model (PLM) and search signal to
		  enhance the generation of user-understandable explanations.
		  We introduce a prompt learning technique and design prompt
		  generators for search and explanation generation tasks
		  based on a fixed PLM. By incorporating search results in
		  explanation-based prompts, we bridge the gap between search
		  results and explanations, facilitating better interaction.
		  Additionally, we utilize the user’s current query,
		  historical search log, and KGs to personalize the
		  explanations and inject task knowledge into PLM.
		  Experimental results show that P-PEG outperforms existing
		  methods in the explanation generation task of the three
		  datasets and the search task of the Electronics dataset,
		  and achieves comparable performance in the search task of
		  the Cellphones &amp; Accessories and CD &amp; Vinyl
		  datasets.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= feb,
  keywords	= {Explanation generation, Product search, Prompt learning}
}

@Article{	  10.1145/3713032,
  author	= {Chau, Michael and Xu, Jennifer},
  title		= {An IS Research Agenda on Large Language Models:
		  Development, Applications, and Impacts on Business and
		  Management},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {1},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3713032},
  doi		= {10.1145/3713032},
  abstract	= {Large language models have been advancing very rapidly and
		  are making substantial impacts on all areas of business and
		  management. We review the development of large language
		  models and their applications in business and management,
		  and identify the major issues and challenges faced by both
		  practitioners and researchers. Based on our review, we
		  propose an agenda for information systems researchers on
		  large language models and discuss some of the potential
		  directions for future research. Lastly, we present the
		  articles in the special issue as exemplary research on
		  large language models and discuss their implications.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= feb,
  articleno	= {1},
  numpages	= {11},
  keywords	= {Large language models, artificial intelligence,
		  information systems research, business and management}
}

@Proceedings{	  10.1145/3704289,
  title		= {ICBDE '24: Proceedings of the 2024 7th International
		  Conference on Big Data and Education},
  year		= {2024},
  isbn		= {9798400716980},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3702386.3702400,
  author	= {Wang, Xianchuang and Fang, Haiguang and Shu, Lili and Li,
		  Zeyu},
  title		= {Creative Accessibility in the Era of Artificial
		  Intelligence and Its Applied Technology Research},
  year		= {2025},
  isbn		= {9798400710131},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3702386.3702400},
  doi		= {10.1145/3702386.3702400},
  abstract	= {In the era of artificial intelligence, an AI dual-teacher
		  classroom environment constructed using technologies such
		  as knowledge graphs, large models, and educational robots
		  will facilitate students in achieving the cognitive goals
		  related to creativity in Bloom's Taxonomy of educational
		  objectives. The use of artificial intelligence technology,
		  especially large model technology, can promote students'
		  inclination towards creativity and creative thinking.
		  However, it is essential to guide students to engage in
		  deep thinking before designing prompts based on their
		  ideas, thereby fostering their creative and critical
		  thinking through human-machine collaboration. This study
		  takes a certain experimental school as a case to explore
		  the effectiveness of creative accessibility in the AI
		  dual-teacher classroom environment and its application
		  technology in primary and secondary schools.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Artificial Intelligence and Teacher Education},
  pages		= {50–56},
  numpages	= {7},
  keywords	= {AI Dual-Teacher Classroom, Artificial Intelligence
		  Education, Creative Accessibility, Educational Prompt
		  Engineering, Large Models},
  location	= { },
  series	= {ICAITE '24}
}

@Article{	  10.1145/3678004,
  author	= {Lin, Jianghao and Dai, Xinyi and Xi, Yunjia and Liu,
		  Weiwen and Chen, Bo and Zhang, Hao and Liu, Yong and Wu,
		  Chuhan and Li, Xiangyang and Zhu, Chenxu and Guo, Huifeng
		  and Yu, Yong and Tang, Ruiming and Zhang, Weinan},
  title		= {How Can Recommender Systems Benefit from Large Language
		  Models: A Survey},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3678004},
  doi		= {10.1145/3678004},
  abstract	= {With the rapid development of online services and web
		  applications, recommender systems (RS) have become
		  increasingly indispensable for mitigating information
		  overload and matching users’ information needs by
		  providing personalized suggestions over items. Although the
		  RS research community has made remarkable progress over the
		  past decades, conventional recommendation models (CRM)
		  still have some limitations, e.g., lacking open-domain
		  world knowledge, and difficulties in comprehending users’
		  underlying preferences and motivations. Meanwhile, large
		  language models (LLM) have shown impressive general
		  intelligence and human-like capabilities for various
		  natural language processing (NLP) tasks, which mainly stem
		  from their extensive open-world knowledge, logical and
		  commonsense reasoning abilities, as well as their
		  comprehension of human culture and society. Consequently,
		  the emergence of LLM is inspiring the design of RS and
		  pointing out a promising research direction, i.e., whether
		  we can incorporate LLM and benefit from their common
		  knowledge and capabilities to compensate for the
		  limitations of CRM. In this article, we conduct a
		  comprehensive survey on this research direction, and draw a
		  bird’s-eye view from the perspective of the whole
		  pipeline in real-world RS. Specifically, we summarize
		  existing research works from two orthogonal aspects: where
		  and how to adapt LLM to RS. For the “WHERE” question,
		  we discuss the roles that LLM could play in different
		  stages of the recommendation pipeline, i.e., feature
		  engineering, feature encoder, scoring/ranking function,
		  user interaction, and pipeline controller. For the
		  “HOW” question, we investigate the training and
		  inference strategies, resulting in two fine-grained
		  taxonomy criteria, i.e., whether to tune LLM or not during
		  training, and whether to involve CRM for inference.
		  Detailed analysis and general development paths are
		  provided for both “WHERE” and “HOW” questions,
		  respectively. Then, we highlight the key challenges in
		  adapting LLM to RS from three aspects, i.e., efficiency,
		  effectiveness, and ethics. Finally, we summarize the survey
		  and discuss the future prospects.},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {28},
  numpages	= {47},
  keywords	= {Recommender systems, large language models}
}

@Article{	  10.1145/3706119,
  author	= {Fatemi, Sorouralsadat and Hu, Yuheng and Mousavi, Maryam},
  title		= {A Comparative Analysis of Instruction Fine-Tuning Large
		  Language Models for Financial Text Classification},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {1},
  issn		= {2158-656X},
  url		= {https://doi.org/10.1145/3706119},
  doi		= {10.1145/3706119},
  abstract	= {Large Language Models (LLMs) have demonstrated impressive
		  capabilities across diverse Natural Language Processing
		  (NLP) tasks, including language understanding, reasoning,
		  and generation. However, general-domain LLMs often struggle
		  with financial tasks due to the technical and specialized
		  nature of financial texts. This study investigates the
		  efficacy of instruction fine-tuning smaller-scale LLMs,
		  including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance
		  their performance in financial text classification tasks.
		  We fine-tuned both instruction-tuned and base models across
		  four financial classification tasks, achieving significant
		  improvements in task-specific performance. Furthermore, we
		  evaluated the zero-shot capabilities of these fine-tuned
		  models on three unseen complex financial tasks, including
		  argument classification, deal completeness classification,
		  and causal classification. Our results indicate while base
		  model fine-tuning led to greater degradation,
		  instruction-tuned models maintained more robust
		  performance. To address this degradation, we employed model
		  merging techniques, integrating single-task domain-specific
		  fine-tuned models with the base model. Using this merging
		  method resulted in significant enhancements in zero-shot
		  performance, even exceeding the original model’s accuracy
		  on certain datasets. Our findings underscore the
		  effectiveness of instruction fine-tuning and model merging
		  for adapting LLMs to specialized financial text
		  classification tasks.},
  journal	= {ACM Trans. Manage. Inf. Syst.},
  month		= feb,
  articleno	= {6},
  numpages	= {30},
  keywords	= {Large language models, parameter-efficient fine-tuning,
		  instruction fine-tuning, text classification, finance}
}

@Article{	  10.1145/3708344,
  author	= {Yuan, Wei and Yang, Chaoqun and Qu, Liang and Hung, Nguyen
		  Quoc Viet and Ye, Guanhua and Yin, Hongzhi},
  title		= {PTF-FSR: A Parameter Transmission-Free Federated
		  Sequential Recommender System},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3708344},
  doi		= {10.1145/3708344},
  abstract	= {Sequential recommender systems, as a specialized branch of
		  recommender systems that can capture users’ dynamic
		  preferences for more accurate and timely recommendations,
		  have made significant progress. Recently, due to increasing
		  concerns about user data privacy, some researchers have
		  implemented federated learning for sequential
		  recommendation, a.k.a., Federated Sequential Recommender
		  Systems (FedSeqRecs), in which a public sequential
		  recommender model is shared and frequently transmitted
		  between a central server and clients to achieve
		  collaborative learning. Although these solutions mitigate
		  user privacy to some extent, they present two significant
		  limitations that affect their practical usability: (1) They
		  require a globally shared sequential recommendation model.
		  However, in real-world scenarios, the recommendation model
		  constitutes a critical intellectual property for platform
		  and service providers. Therefore, service providers may be
		  reluctant to disclose their meticulously developed models.
		  (2) The communication costs are high as they correlate with
		  the number of model parameters. This becomes particularly
		  problematic as the current FedSeqRec will be inapplicable
		  when sequential recommendation marches into a large
		  language model era.To overcome the above challenges, this
		  article proposes a parameter transmission-free federated
		  sequential recommendation framework (PTF-FSR), which
		  ensures both model and data privacy protection to meet the
		  privacy needs of service providers and system users alike.
		  Furthermore, since PTF-FSR only transmits prediction
		  results under privacy protection, which are independent of
		  model sizes, this new federated learning architecture can
		  accommodate more complex and larger sequential
		  recommendation models. Extensive experiments conducted on
		  three widely used recommendation datasets, employing
		  various sequential recommendation models from both ID-based
		  and ID-free paradigms, demonstrate the effectiveness and
		  generalization capability of our proposed framework. To
		  facilitate future research in this direction, we release
		  our code at .},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {52},
  numpages	= {24},
  keywords	= {Sequential Recommendation, Federated Learning, Contrastive
		  Learning, Model Intellectual Property}
}

@Proceedings{	  10.1145/3702163,
  title		= {ICETC '24: Proceedings of the 2024 16th International
		  Conference on Education Technology and Computers},
  year		= {2024},
  isbn		= {9798400717819},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3712059,
  author	= {Qasim, Iqra and Horsch, Alexander and Prasad, Dilip},
  title		= {Dense Video Captioning: A Survey of Techniques, Datasets
		  and Evaluation Protocols},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3712059},
  doi		= {10.1145/3712059},
  abstract	= {Untrimmed videos have interrelated events, dependencies,
		  context, overlapping events, object-object interactions,
		  domain specificity, and other semantics that are worth
		  highlighting while describing a video in natural language.
		  Owing to such a vast diversity, a single sentence can only
		  correctly describe a portion of the video. Dense Video
		  Captioning (DVC) aims to detect and describe different
		  events in a given video. The term DVC originated in the
		  2017 ActivityNet challenge, after which considerable effort
		  has been made to address the challenge. DVC is divided into
		  three sub-tasks: (1) Video Feature Extraction, (2) Temporal
		  Event Localization, and (3) Dense Caption Generation. In
		  this survey, we discuss all of the studies that claim to
		  perform DVC along with its sub-tasks and summarize their
		  results. We also discuss all of the datasets that have been
		  used for DVC. Last, current challenges in the field are
		  highlighted along with observatory remarks and future
		  trends in the field.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {154},
  numpages	= {36},
  keywords	= {Dense video captioning models, video feature extraction,
		  event localization, ActivityNet challenge, deep learning,
		  artificial intelligence}
}

@Proceedings{	  10.1145/3702386,
  title		= {ICAITE '24: Proceedings of the 2024 International
		  Conference on Artificial Intelligence and Teacher
		  Education},
  year		= {2024},
  isbn		= {9798400710131},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3708597.3708606,
  author	= {Han, Xiaotian},
  title		= {Generative Artificial Intelligence for Future Education:
		  Current Research Status, Hot Spots, and Research Trends},
  year		= {2025},
  isbn		= {9798400718304},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708597.3708606},
  doi		= {10.1145/3708597.3708606},
  abstract	= {Generative artificial intelligence (GenAI), leveraging
		  deep learning techniques such as neural networks, is
		  revolutionizing education. This study explores the current
		  research status, key areas, and emerging trends by
		  analyzing 2,856 papers from 2014 to 2023 using CiteSpace
		  software. Key findings reveal that: 1. The focus of GenAI
		  research has shifted towards large language models, such as
		  ChatGPT, especially since 2022. 2. Major research
		  contributions come from China, the U.S., and South Korea,
		  with China leading in institutional involvement. 3.
		  Research trends indicate growing interest in AI
		  applications for immersive education, deep learning models,
		  and medical education. 4. Ethical considerations and data
		  processing methodologies, including anomaly detection and
		  data augmentation, are crucial emerging topics in the
		  field. This paper outlines the most active research
		  clusters and provides future directions for
		  interdisciplinary applications and ethical AI.},
  booktitle	= {Proceedings of the 2024 8th International Conference on
		  Algorithms, Computing and Systems},
  pages		= {56–61},
  numpages	= {6},
  keywords	= {Generative artificial intelligence 1, current research
		  status 3, future educationn2, hot spots 4, research trends
		  5},
  location	= { },
  series	= {ICACS '24}
}

@Article{	  10.1145/3705313,
  author	= {Dang, Xiaochao and Ding, Guozhen and Dong, Xiaohui and Li,
		  Fenfang and Gao, Shiwei and Wang, Yue},
  title		= {UIE-Based Relational Extraction Task for Mine Hoist Fault
		  Data},
  year		= {2025},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3705313},
  doi		= {10.1145/3705313},
  abstract	= {Information extraction is pivotal in natural language
		  processing, where the goal is to convert unstructured text
		  into structured information. A significant challenge in
		  this domain is the diversity and specific needs of various
		  processing tasks. Traditional approaches typically utilize
		  separate frameworks for different information extraction
		  tasks, such as named entity recognition and relationship
		  extraction, which hampers their uniformity and scalability.
		  In this study, this study introduce a Universal Information
		  Extraction (UIE) framework combined with a cue learning
		  strategy, significantly improving the efficiency and
		  accuracy of extracting mine hoist fault data. Initially,
		  domain-specific data is manually labeled to fine-tune the
		  model, and the accuracy is further enhanced by constructing
		  negative examples during this fine-tuning process. The
		  model then focuses on faults using the Structured
		  Extraction Language (SEL) and a schema-based prompt syntax,
		  the Structural Schema Instructor (SSI), which targets and
		  extracts key information from the fault data to meet
		  specific domain requirements. Experimental results show
		  that UIE substantially improves the processing efficiency
		  and the F1 accuracy of the extracted mine hoist fault data,
		  with the fine-tuned F1 score increasing from 23.59% to
		  92.51%.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  articleno	= {4},
  numpages	= {23},
  keywords	= {Joint extraction, mechanical problem, mining sector,
		  prompt learning}
}

@InBook{	  10.5555/3712729.3712835,
  author	= {Tolk, Andreas},
  title		= {Hybrid Modeling Integrating Artificial Intelligence and
		  Modeling &amp; Simulation Paradigms},
  year		= {2025},
  isbn		= {9798331534202},
  publisher	= {IEEE Press},
  abstract	= {This paper discusses the complementary relationship
		  between Modeling and Simulation (M&amp;S) and Artificial
		  Intelligence (AI) methods like machine learning. While
		  M&amp;S uses algorithms to model system behavior from input
		  parameters, AI learns patterns from correlation in data.
		  The paper argues that hybrid models combining M&amp;S and
		  AI can be more powerful than either alone. It provides a
		  conceptual framework showing how M&amp;S and AI can be
		  integrated in sequential, parallel, complementary or
		  competitive configurations. Several example applications
		  are given where AI enhances M&amp;S and vice versa, such as
		  using AI to optimize simulation parameters, generate
		  synthetic training data for AI from simulations, interpret
		  AI model behavior through simulation, and automate aspects
		  of simulation development with AI assistance. The potential
		  benefits of hybrid AI/M&amp;S modeling span improved
		  accuracy, efficiency, trustworthiness and
		  cross-disciplinary collaboration. The paper calls for
		  further research developing a solid theoretical foundation
		  for merging these complementary paradigms.},
  booktitle	= {Proceedings of the Winter Simulation Conference},
  pages		= {1271–1280},
  numpages	= {10}
}

@InProceedings{	  10.1145/3702386.3702397,
  author	= {Jian, Xiaoyan and Li, Feilong and Liu, Zhaohong},
  title		= {A Study on the Application of AIGC Context in the
		  Development of Digital Resources for Online Courses},
  year		= {2025},
  isbn		= {9798400710131},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3702386.3702397},
  doi		= {10.1145/3702386.3702397},
  abstract	= {This study explores the application of AIGC in the
		  development of digital resources for online programming
		  courses, pointing out that AI technology brings new
		  opportunities for educational resource development, but
		  faces problems such as technical limitations and uneven
		  resource quality. Cracking this dilemma can be started from
		  course research, PPT development, text and video resource
		  production, course portal design and auxiliary programming
		  software application, etc. It also puts forward the
		  deficiencies of specific AI in digital resource
		  construction. It is concluded that the complementary models
		  of AI and artificial intelligence need to be explored in
		  the context of teaching and learning in order to maximize
		  the potential of AI in education.},
  booktitle	= {Proceedings of the 2024 International Conference on
		  Artificial Intelligence and Teacher Education},
  pages		= {136–141},
  numpages	= {6},
  keywords	= {aigc, digital resource development, online course,
		  programming course},
  location	= { },
  series	= {ICAITE '24}
}

@Article{	  10.1145/3714464,
  author	= {Williams, Laurie and Benedetti, Giacomo and Hamer, Sivana
		  and Paramitha, Ranindya and Rahman, Imranur and Tamanna,
		  Mahzabin and Tystahl, Greg and Zahan, Nusrat and Morrison,
		  Patrick and Acar, Yasemin and Cukier, Michel and
		  K\"{a}stner, Christian and Kapravelos, Alexandros and
		  Wermke, Dominik and Enck, William},
  title		= {Research Directions in Software Supply Chain Security},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3714464},
  doi		= {10.1145/3714464},
  abstract	= {Reusable software libraries, frameworks, and components,
		  such as those provided by open-source ecosystems and
		  third-party suppliers, accelerate digital innovation.
		  However, recent years have shown almost exponential growth
		  in attackers leveraging these software artifacts to launch
		  software supply chain attacks. Past well-known software
		  supply chain attacks include the SolarWinds, log4j, and xz
		  utils incidents. Supply chain attacks are considered to
		  have three major attack vectors: through vulnerabilities
		  and malware accidentally or intentionally injected into
		  open-source and third-party
		  dependencies/components/containers; by infiltrating the
		  build infrastructure during the build and deployment
		  processes; and through targeted techniques aimed at the
		  humans involved in software development, such as through
		  social engineering. Plummeting trust in the software supply
		  chain could decelerate digital innovation if the software
		  industry reduces its use of open-source and third-party
		  artifacts to reduce risks. This paper contains perspectives
		  and knowledge obtained from intentional outreach with
		  practitioners to understand their practical challenges and
		  from extensive research efforts. We then provide an
		  overview of current research efforts to secure the software
		  supply chain. Finally, we propose a future research agenda
		  to close software supply chain attack vectors and support
		  the software industry.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jan
}

@Proceedings{	  10.1145/3708036,
  title		= {ICCSMT '24: Proceedings of the 2024 5th International
		  Conference on Computer Science and Management Technology},
  year		= {2024},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3715156,
  author	= {Varagnolo, Davide and Melo, Dora and Pimenta Rodrigues,
		  Irene},
  title		= {Translating Natural Language questions into CIDOC-CRM
		  SPARQL queries to access Cultural Heritage knowledge
		  bases},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1556-4673},
  url		= {https://doi.org/10.1145/3715156},
  doi		= {10.1145/3715156},
  abstract	= {To explore information on the Semantic Web, SPARQL queries
		  or DL-queries are suitable tools. However, users interested
		  in exploring the content of such knowledge bases often find
		  it challenging to employ formal query languages, as this
		  requires familiarity with the target domain’s
		  representation model. To address these challenges, a
		  Question-Answering System that automatically translates
		  natural language questions into SPARQL queries, over the
		  Smithsonian American Art Museum CIDOC-CRM representation is
		  presented. The proposed approach uses an ontology, named
		  Query Ontology, defined to represent the natural language
		  concepts and relations specific to the question’s domain.
		  This system’s architecture uses a traditional natural
		  language processing symbolic approach, with a pipeline of
		  modules for the syntactic, semantic, and pragmatic
		  analysis. An evaluation of the proposed system is presented
		  and shows very promising results.},
  note		= {Just Accepted},
  journal	= {J. Comput. Cult. Herit.},
  month		= jan,
  keywords	= {datasets, SPARQL queries, CIDOC-CRM representation,
		  SAAM’s knowledge base}
}

@Proceedings{	  10.1145/3688828,
  title		= {GROUP '25: Companion Proceedings of the 2025 ACM
		  International Conference on Supporting Group Work},
  year		= {2025},
  isbn		= {9798400711879},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= {Hilton Head, New Jersey, USA}
}

@Article{	  10.1145/3713079,
  author	= {Halder, Sajal and Lim, Kwan Hui and Chan, Jeffrey and
		  Zhang, Xiuzhen},
  title		= {Deep Learning of Dynamic POI Generation and Optimisation
		  for Itinerary Recommendation},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3713079},
  doi		= {10.1145/3713079},
  abstract	= {Itinerary recommendation involves suggesting a sequence of
		  Points of Interests (POIs) that users obtain maximum
		  satisfaction under a time budget. Existing models have
		  three challenges. First, they model user interest as
		  non-time dependent, which can not capture user interest
		  appropriately because user interest can be contextual on
		  time, e.g., interest in restaurants are likely higher
		  during typical meal times. Second, they model the distance
		  dependency of user interest as a linear one, which does not
		  always adequately capture this relationship, e.g., could be
		  a cubic decay relationship. Finally, existing studies treat
		  POI recommendation and itinerary optimisation as two
		  separate problems, which can result in sub-optimal
		  itinerary recommendations. In this paper, we propose a deep
		  learning model that recommend POIs and construct the
		  itinerary simultaneously and in an integrated manner. It
		  captures user dynamic interest and non-linear spatial
		  dependencies in itinerary recommendations. The proposed
		  model has two steps, where the candidate selection policy
		  generates a set of personalised candidate POIs based on
		  user interest and the itinerary construction step maximises
		  user interest within budget time. To recommend an
		  appropriate candidate set, we propose a multi-head,
		  attention-based transformer to leverage periodic trends and
		  recent activities to capture user dynamic preferences. We
		  also introduce a new co-visiting patterns-based graph
		  convolutional network (GCN) model to capture user
		  non-linear spatial dependencies. To construct the full
		  itinerary from the dynamic candidate sets, we apply greedy
		  policy that incrementally constructs itineraries within the
		  budget time which aims to maximise user interest and
		  minimise queuing time. Experimental results show that the
		  proposed deep learning model outperforms state-of-the-art
		  baselines in itinerary recommendation in four theme parks
		  and four cities datasets The proposed model outperforms the
		  baselines in itinerary recommendation from 7.79% to 26.28%
		  on various dataset in terms of F1-score value. We also show
		  that the proposed candidate generation approach outperforms
		  the state-of-the-art next POI recommendation models in
		  eight real datasets. The proposed model outperforms the
		  baselines on average by 11.29 % in terms of F1-score@5
		  values and 9.08% in terms of F1-score@10 values. We have
		  publicly shared our source code at GitHub for the
		  reproducibility of our proposed model.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Recomm. Syst.},
  month		= jan,
  keywords	= {Itinerary Recommendation, User Interest, Deep Learning,
		  Budget Time, Periodic Interest, Transformer}
}

@Article{	  10.1145/3715098,
  author	= {Wang, Hao and Guo, Bin and Zeng, Yating and Chen, Mengqi
		  and Ding, Yasan and Zhang, Ying and Yao, Lina and Yu,
		  Zhiwen},
  title		= {Enabling Harmonious Human-Machine Interaction with
		  Visual-Context Augmented Dialogue System: A Review},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3715098},
  doi		= {10.1145/3715098},
  abstract	= {The intelligent dialogue system, aiming at communicating
		  with humans harmoniously with natural language, is
		  brilliant for promoting the advancement of human-machine
		  interaction in the era of artificial intelligence. With the
		  gradually complex human-computer interaction requirements,
		  it is difficult for traditional text-based dialogue system
		  to meet the demands for more vivid and convenient
		  interaction. Consequently, Visual-Context Augmented
		  Dialogue System (VAD), which has the potential to
		  communicate with humans by perceiving and understanding
		  multimodal information (i.e., visual context in images or
		  videos, textual dialogue history), has become a predominant
		  research paradigm. Benefiting from the consistency and
		  complementarity between visual and textual context, VAD
		  possesses the potential to generate engaging and
		  context-aware responses. To depict the development of VAD,
		  we first characterize the concept model of VAD and then
		  present its generic system architecture to illustrate the
		  system workflow, followed by a summary of multimodal fusion
		  techniques. Subsequently, several research challenges and
		  representative works are investigated, followed by the
		  summary of authoritative benchmarks and real-world
		  application of VAD. We conclude this paper by putting
		  forward some open issues and promising research trends for
		  VAD, e.g., the cognitive mechanisms of human-machine
		  dialogue under cross-modal dialogue context, mobile and
		  lightweight deployment of VAD.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  keywords	= {Human-machine interaction, dialogue system, visual
		  context, vision and language, large language models}
}

@Article{	  10.1145/3708888,
  author	= {Wenbo, Zhang and Hongbo, Dang and Zhenshan, Bao and
		  Bingyan, Song},
  title		= {LAMGCN:Traditional Chinese Medicine Herb Recommendation
		  via LSTMs with Attention Mechanisms and Graph Convolutional
		  Networks},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3708888},
  doi		= {10.1145/3708888},
  abstract	= {Herb recommendation plays a crucial role in the
		  therapeutic process of Traditional Chinese Medicine (TCM),
		  which aims to recommend a set of herbs to treat patients
		  with different symptoms. Previous works used many methods
		  to discover regularities in prescriptions but rarely
		  considered the actual therapeutic process in TCM and the
		  information of herbs was ignored. In this work, we propose
		  LAMGCN(Herb Recommendation via LSTMs with Attention
		  Mechanisms and Graph Convolutional Networks), which takes
		  the syndrome induction process and the herb descriptions
		  into account. We utilize attention mechanisms and graph
		  neural networks to capture the correlation between symptoms
		  and herbs. Extensive experiments have been done and the
		  results demonstrate the effectiveness of our proposed
		  method.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  keywords	= {Traditional chinese medicine, Text classification, Herb
		  recommendation, Attention mechanism, Graph neural network}
}

@Article{	  10.1145/3711680,
  author	= {Kuang, Jiayi and Shen, Ying and Xie, Jingyou and Luo,
		  Haohao and Xu, Zhe and Li, Ronghao and Li, Yinghui and
		  Cheng, Xianfeng and Lin, Xika and Han, Yu},
  title		= {Natural Language Understanding and Inference with MLLM in
		  Visual Question Answering: A Survey},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3711680},
  doi		= {10.1145/3711680},
  abstract	= {Visual Question Answering (VQA) is a challenge task that
		  combines natural language processing and computer vision
		  techniques and gradually becomes a benchmark test task in
		  multimodal large language models (MLLMs). The goal of our
		  survey is to provide an overview of the development of VQA
		  and a detailed description of the latest models with high
		  timeliness. This survey gives an up-to-date synthesis of
		  natural language understanding of images and text, as well
		  as the knowledge reasoning module based on image-question
		  information on the core VQA tasks. In addition, we
		  elaborate on recent advances in extracting and fusing modal
		  information with vision-language pretraining models and
		  multimodal large language models in VQA. We also
		  exhaustively review the progress of knowledge reasoning in
		  VQA by detailing the extraction of internal knowledge and
		  the introduction of external knowledge. Finally, we present
		  the datasets of VQA and different evaluation metrics and
		  discuss possible directions for future work.},
  note		= {Just Accepted},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  keywords	= {visual question answering, multimodal representation and
		  reasoning, multimodal large language models}
}

@Proceedings{	  10.1145/3707292,
  title		= {AIIIP '24: Proceedings of the 2024 3rd International
		  Conference on Artificial Intelligence and Intelligent
		  Information Processing},
  year		= {2024},
  isbn		= {9798400707308},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3709134,
  author	= {Wang, Wenjie and Liu, Zheng and Feng, Fuli and Dou,
		  Zhicheng and Ai, Qingyao and Yang, Grace Hui and Lian, Defu
		  and Hou, Lu and Sun, Aixin and Zamani, Hamed and Metzler,
		  Donald and de Rijke, Maarten},
  title		= {Pre-Trained Models for Search and Recommendation:
		  Introduction to the Special Issue—Part 1},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3709134},
  doi		= {10.1145/3709134},
  journal	= {ACM Trans. Inf. Syst.},
  month		= feb,
  articleno	= {27},
  numpages	= {6},
  keywords	= {Pre-trained Models, Search, Recommendation, Large Language
		  Models, Trustworthiness}
}

@Article{	  10.1145/3715318,
  author	= {Zhang, Qiang and Ding, Keyan and Lv, Tianwen and Wang,
		  Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and
		  Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and Zhuang,
		  Xiang and Wang, Zeyuan and Qin, Ming and Zhang, Mengyao and
		  Zhang, Jinlu and Cui, Jiyu and Xu, Renjun and Chen,
		  Hongyang and Fan, Xiaohui and Xing, Huabin and Chen,
		  Huajun},
  title		= {Scientific Large Language Models: A Survey on Biological
		  &amp; Chemical Domains},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3715318},
  doi		= {10.1145/3715318},
  abstract	= {Large Language Models (LLMs) have emerged as a
		  transformative power in enhancing natural language
		  comprehension, representing a significant stride toward
		  artificial general intelligence. The application of LLMs
		  extends beyond conventional linguistic boundaries,
		  encompassing specialized linguistic systems developed
		  within various scientific disciplines. This growing
		  interest has led to the advent of scientific LLMs, a novel
		  subclass specifically engineered for facilitating
		  scientific discovery. As a burgeoning area in the community
		  of AI for Science, scientific LLMs warrant comprehensive
		  exploration. However, a systematic and up-to-date survey
		  introducing them is currently lacking. In this article, we
		  endeavor to methodically delineate the concept of
		  “scientific language,” whilst providing a thorough
		  review of the latest advancements in scientific LLMs. Given
		  the expansive realm of scientific disciplines, our analysis
		  adopts a focused lens, concentrating on the biological and
		  chemical domains. This includes an in-depth examination of
		  LLMs for textual knowledge, small molecules, macromolecular
		  proteins, genomic sequences, and their combinations,
		  analyzing them in terms of model architectures,
		  capabilities, datasets, and evaluation. Finally, we
		  critically examine the prevailing challenges and point out
		  promising research directions along with the advances of
		  LLMs. By offering a comprehensive overview of technical
		  developments in this field, this survey aspires to be an
		  invaluable resource for researchers navigating the
		  intricate landscape of scientific LLMs.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {161},
  numpages	= {38},
  keywords	= {Scientific domain, large language models, protein,
		  molecule, genome}
}

@InProceedings{	  10.1145/3712623.3712637,
  author	= {Tao, Jie and Cui, Peizhang and Wang, Xin},
  title		= {Natural Language Dialogue Model Based on Multi-Head Graph
		  Attention Neural Network},
  year		= {2025},
  isbn		= {9798400712883},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3712623.3712637},
  doi		= {10.1145/3712623.3712637},
  abstract	= {The practical application of artificial intelligence in
		  the field of natural language processing is becoming more
		  and more extensive, and the technology is changing day by
		  day. With the rapid update and development of artificial
		  intelligence technology, more and more researchers find
		  that the use of graph neural networks to construct a
		  natural language dialog model can obtain better results
		  than other artificial intelligence techniques, due to the
		  fact that the deep neural network model is too large and
		  complex, which hinders the practical application of
		  artificial intelligence technology in the field of natural
		  language dialog, in this paper, we propose a natural
		  language dialog model based on the multi-head graph
		  attention neural network This paper proposes a natural
		  language dialog model based on multi-head graph attention
		  neural network, which verifies the practicability and
		  robustness of the method after simulation experiments.},
  booktitle	= {Proceedings of the 2024 2nd International Conference on
		  Advances in Artificial Intelligence and Applications},
  pages		= {98–101},
  numpages	= {4},
  keywords	= {Artificial Intelligence, Multi-Head Graph Attention Neural
		  Network, Natural Language Dialogue Model},
  location	= { },
  series	= {AAIA '24}
}

@InProceedings{	  10.5555/3716662.3716788,
  author	= {Thais, Savannah},
  title		= {Misrepresented Technological Solutions in Imagined
		  Futures: The Origins and Dangers of AI Hype in the Research
		  Community},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {Technology does not exist in a vacuum; technological
		  development, media representation, public perception, and
		  governmental regulation cyclically influence each other to
		  produce the collective understanding of a technology's
		  capabilities, utilities, and risks. When these capabilities
		  are overestimated, there is an enhanced risk of subjecting
		  the public to dangerous or harmful technology, artificially
		  restricting research and development directions, and
		  enabling misguided or detrimental policy. The dangers of
		  technological hype are particularly relevant in the rapidly
		  evolving space of AI. Centering the research community as a
		  key player in the development and proliferation of hype, we
		  examine the origins and risks of AI hype to the research
		  community and society more broadly and propose a set of
		  measures that researchers, regulators, and the public can
		  take to mitigate these risks and reduce the prevalence of
		  unfounded claims about the technology.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {1455–1465},
  numpages	= {11},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@Article{	  10.1145/3712588,
  author	= {Liu, Yuanxing and Pei, Jiahuan and Zhang, Wei-Nan and Li,
		  Ming and Che, Wanxiang and de Rijke, Maarten},
  title		= {Augmentation with Neighboring Information for
		  Conversational Recommendation},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3712588},
  doi		= {10.1145/3712588},
  abstract	= {Conversational recommender systems (CRSs) suggest items to
		  users by understanding their needs and preferences from
		  natural language conversations. While users can freely
		  express preferences, modeling needs and preferences solely
		  from users’ conversations is challenging due to the
		  sparsity of the available information. Prior work
		  introduces external resources to enrich information
		  expressed in conversations. Obtaining such resources is
		  challenging and not always effective. Can learning
		  intrinsic relations among conversations and items enhance
		  information without the use of external resources? Inspired
		  by collaborative filtering, we propose to use so-called
		  neighboring relations within training data, i.e., relations
		  between conversations, items, and similar conversations and
		  items, to enhance our algorithmic understanding of CRSs.We
		  propose a neighboring relations enhanced conversational
		  recommender system (NR-CRS) and study how neighboring
		  relations improve CRSs from two angles: (i) We mine
		  preference information from neighboring conversations to
		  enhance the modeling of user representations and learning
		  of user preferences. (ii) We generate negative samples
		  based on neighboring items to extend the data available for
		  training CRSs. Experiments on the ReDial dataset show that
		  NR-CRS outperforms the state-of-the-art baseline by
		  11.3%–20.6% regarding recommendation performance while
		  generating informative and diverse responses. We also
		  assess the capabilities of large language models (LLMs)
		  (i.e., Llama 2, Llama 3, and Chinese-Alpaca2) for CRSs.
		  While the generated responses exhibit enhanced fluency and
		  informativeness, recommending target items with LLMs
		  remains challenging; we recommend that LLMs be used as a
		  decoding base for NR-CRS to generate relevant and
		  informative responses.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  keywords	= {Conversational recommendation, Neighboring relations}
}

@Proceedings{	  10.1145/3708394,
  title		= {AIFE '24: Proceeding of the 2024 International Conference
		  on Artificial Intelligence and Future Education},
  year		= {2024},
  isbn		= {9798400710650},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3715097,
  author	= {Deng, Yang and Liao, Lizi and Lei, Wenqiang and Yang,
		  Grace and Lam, Wai and Chua, Tat-Seng},
  title		= {Proactive Conversational AI: A Comprehensive Survey of
		  Advancements and Opportunities},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3715097},
  doi		= {10.1145/3715097},
  abstract	= {Dialogue systems are designed to offer human users social
		  support or functional services through natural language
		  interactions. Traditional conversation research has put
		  significant emphasis on a system’s response-ability,
		  including its capacity to understand dialogue context and
		  generate appropriate responses. However, the key element of
		  proactive behavior – a crucial aspect of intelligent
		  conversations – is often overlooked in these studies.
		  Proactivity empowers conversational agents to lead
		  conversations towards achieving pre-defined targets or
		  fulfilling specific goals on the system side. Proactive
		  dialogue systems are equipped with advanced techniques to
		  handle complex tasks, requiring strategic and motivational
		  interactions, thus representing a significant step towards
		  artificial general intelligence. Motivated by the necessity
		  and challenges of building proactive dialogue systems, we
		  provide a comprehensive review of various prominent
		  problems and advanced designs for implementing proactivity
		  into different types of dialogue systems, including
		  open-domain dialogues, task-oriented dialogues, and
		  information-seeking dialogues. We also discuss real-world
		  challenges that require further research attention to meet
		  application needs in the future, such as proactivity in
		  dialogue systems that are based on large language models,
		  proactivity in hybrid dialogues, evaluation protocols and
		  ethical considerations for proactive dialogue systems. By
		  providing a quick access and overall picture of the
		  proactive dialogue systems domain, we aim to inspire new
		  research directions and stimulate further advancements
		  towards achieving the next level of conversational AI
		  capabilities, paving the way for more dynamic and
		  intelligent interactions within various application
		  domains.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  keywords	= {Dialogue Systems, Proactivity, Open-domain Dialogue,
		  Task-oriented Dialogue, Conversational Information
		  Seeking}
}

@Article{	  10.1145/3711826,
  author	= {Taji, Mobina and Ghafouri, Arash and Naderi, Hasan and
		  Minaei-Bidgoli, Behrouz},
  title		= {PersianMHQA: A Dataset for Open Domain Persian Multi-hop
		  Question Answering Based on Wikipedia Encyclopedia},
  year		= {2025},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {2},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3711826},
  doi		= {10.1145/3711826},
  abstract	= {Today, one of the most important tasks in natural language
		  processing is answering user questions. Especially, users'
		  questions nowadays moved from simple questions to complex
		  questions. In recent years, several question answering
		  datasets have been produced for Persian language, but none
		  of them support complex open-domain and explainable
		  questions. In this article, the PersianMHQA dataset is
		  introduced which is the first open-domain question
		  answering dataset for complex questions based on the
		  unstructured Persian Wikipedia encyclopedia. This dataset
		  contains 7,000 complex questions and sentence-level
		  supporting facts are provided for each question that allows
		  question answering systems to explain the predictions. The
		  questions in this dataset are diverse and explainable and
		  are not limited to any previous knowledge base. Various
		  types of complexity are provided in this dataset, and the
		  questions are designed in such a way that answering them
		  requires reasoning over more than one paragraph.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= feb,
  articleno	= {18},
  numpages	= {17},
  keywords	= {Question Answering Systems, multi-hop question, dataset,
		  low resources languages, persian encyclopedia}
}

@Proceedings{	  10.1145/3712623,
  title		= {AAIA '24: Proceedings of the 2024 2nd International
		  Conference on Advances in Artificial Intelligence and
		  Applications},
  year		= {2024},
  isbn		= {9798400712883},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.5555/3716662.3716680,
  author	= {Bommasani, Rishi and Soylu, Dilara and Liao, Thomas I. and
		  Creel, Kathleen A. and Liang, Percy},
  title		= {Ecosystem Graphs: Documenting the Foundation Model Supply
		  Chain},
  year		= {2025},
  publisher	= {AAAI Press},
  abstract	= {Foundation models (e.g. GPT-4, Gemini, Llama 3)
		  pervasively influence society, warranting greater
		  understanding. While the models garner much attention,
		  accurately characterizing their impact requires considering
		  the broader sociotechnical ecosystem in which they are
		  created and deployed. We propose Ecosystem Graphs as a
		  documentation framework to centralize knowledge of this
		  ecosystem. Ecosystem Graphs is composed of assets
		  (datasets, models, applications) linked together by
		  dependencies that indicate technical and social
		  relationships. To supplement the graph structure, each
		  asset is further enriched with fine-grained metadata, such
		  as the model's estimated training emissions or licensing
		  guidelines. Since its release in March 2023, Ecosystem
		  Graphs represents an ongoing effort to document 568 assets
		  (112 datasets, 359 models, 97 applications) from 117
		  organizations. Ecosystem Graphs functions as a
		  multifunctional resource: we discuss two major uses by the
		  2024 AI Index and the UK's Competition and Markets
		  Authority that demonstrate the value of Ecosystem Graphs.},
  booktitle	= {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics,
		  and Society},
  pages		= {196–209},
  numpages	= {14},
  location	= {San Jose, California, USA},
  series	= {AIES '24}
}

@InProceedings{	  10.1145/3708394.3708455,
  author	= {Lv, Jiayan and Yao, Jinfang and Zhu, He},
  title		= {Research on the Cultivation of Teacher Candidates from the
		  Perspective of AI Empowerment with Sentiment analysis},
  year		= {2025},
  isbn		= {9798400710650},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708394.3708455},
  doi		= {10.1145/3708394.3708455},
  abstract	= {Over the past decade, with the advancement of technology,
		  chatbots have become a hotspot in the field of artificial
		  intelligence (AI) and are widely used in consumer services,
		  education, search engines, marketing, and other fields.
		  Among them, the Chat Generative Pre-Trained Transformer
		  (ChatGPT), composed of language models and optimization
		  techniques, is leading a transformation in human-computer
		  interaction methods. This study selects the social media
		  platforms "REDnote" and "Weibo" as the research field to
		  explore the role and impact of ChatGPT in education and
		  industry ecosystems. This work examines public sentiment
		  regarding the application of AI in educational ecosystems
		  and talent development by analyzing social media
		  discussions. The sentiment analysis conducted using
		  advanced machine learning models, highlights the prevalence
		  of positive emotions toward ChatGPT's role in enhancing
		  teaching and learning experiences. Furthermore, this study
		  introduces an AI-based dynamic talent cultivation model,
		  rooted in the "3H" (Head, Hand, Heart) framework, which
		  emphasizes cognitive skills, practical capabilities, and
		  emotional intelligence.},
  booktitle	= {Proceeding of the 2024 International Conference on
		  Artificial Intelligence and Future Education},
  pages		= {358–364},
  numpages	= {7},
  keywords	= {Artificial Intelligence, ChatGPT, Deep Learning, Machine
		  Learning, Normal Education, Social Media, Talent
		  Cultivation, User Experience},
  location	= { },
  series	= {AIFE '24}
}

@InProceedings{	  10.1145/3706890.3706951,
  author	= {Zhang, Tao and Zhao, Likun},
  title		= {MMR: Math Multi-step Reasoning in Medical Dialogue
		  Generation},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706951},
  doi		= {10.1145/3706890.3706951},
  abstract	= {With the advancements of large language models in text and
		  visual tasks, researchers are increasingly exploring their
		  applications in medical scenarios. While existing studies
		  have successfully applied these models to medical dialogue
		  systems, challenges remain in accurately calculating
		  medication dosages and handling multi-turn reasoning due to
		  the low tolerance for error in medical contexts. To address
		  this, we propose Math Multi-step Reasoning in Medical
		  Dialogue Generation (MMR), which enhances reasoning by
		  iteratively breaking down complex problems into simpler
		  questions using a “Least to Most Prompting”
		  (LMP)strategy. MMR integrates Chain of Thought, React
		  mechanisms, and Retrieval-Augmented Generation (RAG) with a
		  domain-specific knowledge base to improve reasoning
		  accuracy. Supported by the MedDGQA dataset, MMR outperforms
		  state-of-the-art methods in both objective and subjective
		  evaluations.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {348–351},
  numpages	= {4},
  keywords	= {Large Language Models, Medical Dialogue Generation,
		  Multi-step Reasoning},
  location	= { },
  series	= {ISAIMS '24}
}

@InProceedings{	  10.1145/3706890.3706930,
  author	= {Weng, Suxiang and Gong, Weibin and Chen, Qinyin and Yan,
		  Yiwei and Zheng, Yingbin and Zhuang, Jiaying and Liu,
		  Yishan and Guo, Xiaoyun and Zhao, Min},
  title		= {The Research of Disease Trend Early Warning Model Based on
		  Artificial Intelligence},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706930},
  doi		= {10.1145/3706890.3706930},
  abstract	= {By analyzing the word frequency and semantic parsing of
		  patient medical records, the system structures unstructured
		  data such as patient complaints, physical examination data
		  and reports, admission diagnoses, medical orders, and
		  medication plans to generate dynamic medical records. This
		  process automatically populates nursing system assessment
		  forms, identifies nursing problems, formulates nursing
		  tasks, and recommends nursing decisions. This disease trend
		  warning model leverages optimized algorithm to predict
		  disease occurrence and progression, providing a solid
		  foundation fobasis for personalized and precise nursing
		  care. This system have been implemented in the cardiology
		  department at the First Affiliated Hospital of Xiamen
		  University for over 2 years, resulting in a 22.34% increase
		  in nursing diagnosis accuracy and significant labor cost
		  savings. Its deployment demonstrates real-world
		  effectiveness in enhancing healthcare outcomes and
		  operational efficiencies.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {229–234},
  numpages	= {6},
  keywords	= {Dynamic medical records, disease trend warning models,
		  nursing assessment forms},
  location	= { },
  series	= {ISAIMS '24}
}

@InProceedings{	  10.1145/3704323.3704363,
  author	= {Li, Lulu and Li, Zhengtao and Hu, Peng and Zhang, Hao},
  title		= {A formal corpus of mathematical theorems: a bridge from
		  natural language to first-order logic},
  year		= {2025},
  isbn		= {9798400717482},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704323.3704363},
  doi		= {10.1145/3704323.3704363},
  abstract	= {Currently, there is a significant gap in the conversion of
		  mathematical theorems from natural language to logical
		  expressions, specifically in the form of first-order
		  predicate logic. To address this issue, this paper presents
		  a corpus of mathematical theorems expressed in first-order
		  predicate logic and explores its necessity and practical
		  applications. The primary objective of this work is to
		  introduce a new paradigm for the logical transformation of
		  mathematical theorems by detailing specific the
		  construction process. The paper aims to demonstrate the
		  potential functions and roles of this corpus, providing
		  solid data support for automated proofs, formal
		  verification, and other related fields. This corpus
		  facilitates the automated processing and understanding of
		  mathematical theorems. The contribution of this paper lies
		  in proposing an innovative solution to bridge the gap
		  between natural language processing and formal logic
		  research. This study aims to enhance the performance and
		  accuracy of algorithms and tools for processing
		  mathematical theorems by constructing a corpus of theorems
		  in first-order predicate logic.},
  booktitle	= {Proceedings of the 2024 13th International Conference on
		  Computing and Pattern Recognition},
  pages		= {394–398},
  numpages	= {5},
  keywords	= {First-order Logic, Mathematical Theorems, Corpus
		  Construction, Natural Language Processing},
  location	= { },
  series	= {ICCPR '24}
}

@Proceedings{	  10.1145/3706890,
  title		= {ISAIMS '24: Proceedings of the 2024 5th International
		  Symposium on Artificial Intelligence for Medicine Science},
  year		= {2024},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3706890.3707016,
  author	= {Wang, Juan and Hou, Li and Li, Yunhan and Sun, Yueping and
		  Li, Jiaming and Yang, Li},
  title		= {Classifying Public Health Questions Using Large Language
		  Models},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3707016},
  doi		= {10.1145/3706890.3707016},
  abstract	= {With the booming development of online medical
		  communities, a large amount of valuable medical and health
		  question-and-answer data emerges, providing the possibility
		  for the extraction of public health information and the
		  improvement of question and answer system efficiency. To
		  classify public health questions more effectively, this
		  paper proposes a model ensemble approach that utilizes BERT
		  and two large language models to accurately predict the
		  categories of public health questions. It integrates the
		  prediction results from each model through a voting
		  mechanism to derive the final classification outcome. To
		  verify the effectiveness of this method, this paper
		  conducts experiments based on the Chinese Medical Intent
		  Dataset (CMID) and compares it with various other methods.
		  The experimental results demonstrate that the method
		  proposed in this paper has achieved significant improvement
		  in accuracy, effectively proving the effectiveness of the
		  approach combining model integration and voting mechanism
		  in the classification task of public health questions.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {735–740},
  numpages	= {6},
  keywords	= {Large language models, Model integration, Public health
		  questions, Voting mechanism},
  location	= { },
  series	= {ISAIMS '24}
}

@Article{	  10.1145/3712184,
  author	= {Fan, Wenqi and Zhao, Shu and Tang, Jiliang},
  title		= {Introduction of the Special Issue on Trustworthy
		  Artificial Intelligence},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1556-4681},
  url		= {https://doi.org/10.1145/3712184},
  doi		= {10.1145/3712184},
  note		= {Just Accepted},
  journal	= {ACM Trans. Knowl. Discov. Data},
  month		= jan
}

@Article{	  10.1145/3708886,
  author	= {Wang, Yuanlong and Ma, Qiang and Li, Ru and Zhang, Hu},
  title		= {Visual Story Generation Model Guided by Multi Granularity
		  Image Information},
  year		= {2025},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {24},
  number	= {1},
  issn		= {2375-4699},
  url		= {https://doi.org/10.1145/3708886},
  doi		= {10.1145/3708886},
  abstract	= {Visual story generation, which involves generating short
		  stories from sequential images, has become a core task at
		  the intersection of computer vision and natural language
		  processing. However, existing methods suffer from a bias in
		  the concept predicates predicted, leading to a semantic gap
		  between the generated stories and the images. This article
		  proposes a novel visual story generation model that
		  utilizes multi granularity image information to guide the
		  generation process and correct the bias in concept
		  predicates, resulting in more image-consistent stories. The
		  proposed model consists of two stages: In the first stage,
		  a set of concepts predicates is predicted from the image
		  and enriched with external knowledge, and the most suitable
		  concepts for story generation are selected. In the second
		  stage, fine-grained image information are utilized to
		  integrate image information into the story generation
		  module, improving the bias in concept predicates. The image
		  theme information and the generated results of previous
		  moments are used as prompts to guide the story generation
		  module. Experimental results show that the proposed model
		  outperforms baseline models in all evaluation metrics.
		  Specifically, the Bilingual Evaluation Understudy 1
		  (BLEU-1), BLEU-2, BLEU-3, and BLEU-4 metrics are improved
		  by 4.0, 3.8, 3.02, and 1.98 percentage points,
		  respectively, and the METEOR metric is improved by 1.4
		  percentage points. The generated stories are more
		  consistent with the image content, maintain a consistent
		  theme, and enhance coherence between contexts.},
  journal	= {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month		= jan,
  articleno	= {8},
  numpages	= {13},
  keywords	= {Visual story generation, image information, concept
		  predicate bias, fine-grained visual features}
}

@InProceedings{	  10.1145/3701100.3701155,
  author	= {Jia, Quanye and Liang, Xiao and Zhang, Qiang and Zheng,
		  Huamingzhou and Fan, Xiaoxuan},
  title		= {Data Augmentation for Technical Standard Relation
		  Extraction},
  year		= {2025},
  isbn		= {9798400718120},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3701100.3701155},
  doi		= {10.1145/3701100.3701155},
  abstract	= {The paper introduces a method for fine-grained relation
		  extraction in grid technology standards, addressing
		  challenges in manual annotation due to complex guidelines
		  and large-scale dataset requirements. Data augmentation
		  techniques, specifically word-level perturbation and
		  sentence template methods, are applied to a limited
		  annotated dataset to improve model generalization and
		  efficiency. Experiments on Q/GDW 1168-2013 annotated data
		  show that the GPLinker model has demonstrated its higher
		  efficiency and accuracy in relation extraction, with the
		  integrated use of word-level perturbation and sentence
		  template augmentation significantly boosts the model's
		  F1-score to 0.582925, indicating improved precision and
		  recall in relation extraction from technical standards.},
  booktitle	= {Proceedings of the 2024 3rd International Conference on
		  Algorithms, Data Mining, and Information Technology},
  pages		= {265–269},
  numpages	= {5},
  keywords	= {Data Augmentation, Deep Learning, Relation Extraction,
		  Technical Standard},
  location	= { },
  series	= {ADMIT '24}
}

@Article{	  10.1145/3712005,
  author	= {Gao, Cuiyun and Hu, Xing and Gao, Shan and Xia, Xin and
		  Jin, Zhi},
  title		= {The Current Challenges of Software Engineering in the Era
		  of Large Language Models},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3712005},
  doi		= {10.1145/3712005},
  abstract	= {With the advent of large language models (LLMs) in the
		  artificial intelligence (AI) area, the field of software
		  engineering (SE) has also witnessed a paradigm shift. These
		  models, by leveraging the power of deep learning and
		  massive amounts of data, have demonstrated an unprecedented
		  capacity to understand, generate, and operate programming
		  languages. They can assist developers in completing a broad
		  spectrum of software development activities, encompassing
		  software design, automated programming, and maintenance,
		  which potentially reduces huge human efforts. Integrating
		  LLMs within the SE landscape (LLM4SE) has become a
		  burgeoning trend, necessitating exploring this emergent
		  landscape’s challenges and opportunities.The paper aims
		  at revisiting the software development life cycle (SDLC)
		  under LLMs, and highlighting challenges and opportunities
		  of the new paradigm. The paper first summarizes the overall
		  process of LLM4SE, and then elaborates on the current
		  challenges based on a through discussion. The discussion
		  was held among more than 20 participants from academia and
		  industry, specializing in fields such as software
		  engineering and artificial intelligence. Specifically, we
		  achieve 26 key challenges from seven aspects, including
		  software requirement &amp; design, coding assistance,
		  testing code generation, code review, code maintenance,
		  software vulnerability management, and data, training, and
		  evaluation. We hope the achieved challenges would benefit
		  future research in the LLM4SE field.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jan,
  keywords	= {Large Language Models, Challenges, LLM4SE}
}

@InProceedings{	  10.1145/3702163.3702174,
  author	= {Xu, Ye and Mao, Decheng and Wang, Chengliang},
  title		= {XR Technologies in vocational education and training
		  research (2000-2024): A Bibliometric Review},
  year		= {2025},
  isbn		= {9798400717819},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3702163.3702174},
  doi		= {10.1145/3702163.3702174},
  abstract	= {Extended reality (XR) technology plays a crucial role in
		  vocational education and training in the smart era, and it
		  has a significant impact on the cultivation of talents with
		  innovative and comprehensive skills. We can identify
		  current research hotspots and assess the depth and breadth
		  of existing research so that we can fill the gaps in
		  research in a timely manner. To achieve this goal, we used
		  two software tools, Vosviewer and Citespace, to conduct a
		  detailed multidimensional visualisation and analysis of 240
		  documents in the Web of Science (WoS) database since the
		  21st century. Our study reveals the following key findings:
		  1) The publication volume of the literature has shown a
		  steady growth trend since 2000 to 2019, and has witnessed a
		  significant increase between 2019 and 2024. Meanwhile,
		  author groups such as Cattaneo, Alberto and Hamalainen,
		  Raija are gradually becoming more representative scholars
		  producing more articles.2) The field has clearly fostered
		  three dominant research foci, which are: exploring VET
		  based on an educational psychology perspective, approaching
		  VET from a computer science and technology aspect, and
		  focusing on the use of XR technology devices. 3) XR
		  technology in vocational education and training shows a
		  clear stage-by-stage evolution and is moving towards a
		  deeper level of technological application. Some researchers
		  have begun to focus on the construction of student-oriented
		  educational models as well as teachers' acceptance and use
		  of the technology.},
  booktitle	= {Proceedings of the 2024 16th International Conference on
		  Education Technology and Computers},
  pages		= {76–83},
  numpages	= {8},
  keywords	= {Bibliometrics, Extended Reality, visual analysis,
		  vocational education and training},
  location	= { },
  series	= {ICETC '24}
}

@Proceedings{	  10.1145/3705754,
  title		= {CECCT '24: Proceedings of the 2024 2nd International
		  Conference on Electronics, Computers and Communication
		  Technology},
  year		= {2024},
  isbn		= {9798400710193},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3703594,
  author	= {Paschalides, Demetris and Pallis, George and Dikaiakos,
		  Marios},
  title		= {A Framework for the Unsupervised Modeling and Extraction
		  of Polarization Knowledge from News Media},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {8},
  number	= {1–2},
  url		= {https://doi.org/10.1145/3703594},
  doi		= {10.1145/3703594},
  abstract	= {Polarization poses global concerns for social cohesion and
		  stability, making its understanding crucial for effective
		  mitigation measures. In this paper, we introduce an
		  unsupervised, domain-agnostic framework for computationally
		  modeling, extracting, and measuring polarization in digital
		  media. By leveraging Natural Language Processing and Graph
		  Analysis techniques, the proposed framework creates a
		  Polarization Data Model (PDM) that encompasses key elements
		  of Polarization Knowledge (PK), such as entities,
		  fellowships, dipoles, and discussion topics. To evaluate
		  the effectiveness of the framework, we propose a
		  multi-level PK evaluation methodology that assesses its
		  ability to: (i) capture entities’ attitudes toward
		  various topics, (ii) align politically cohesive fellowships
		  with their respective party manifestos, and (iii) identify
		  domain-specific topics along with their degree of
		  polarization. We applied this evaluation methodology to the
		  use cases of Abortion, Immigration, and Gun Control. The
		  results demonstrate our framework’s robust performance
		  across these case studies, yielding promising outcomes
		  compared to state-of-the-art and baseline methods.},
  journal	= {Trans. Soc. Comput.},
  month		= jan,
  articleno	= {5},
  numpages	= {38},
  keywords	= {Polarization, Multi-level Polarization, Polarization
		  Modeling, Polarization Extraction, Polarization
		  Computational Evaluation}
}

@Article{	  10.1145/3701181,
  author	= {Muralikumar, Meena Devii and McDonald, David W.},
  title		= {An Emerging Design Space of How Tools Support
		  Collaborations in AI Design and Development},
  year		= {2025},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {1},
  url		= {https://doi.org/10.1145/3701181},
  doi		= {10.1145/3701181},
  abstract	= {Developing AI/ML models and incorporating them into a
		  product is complex work. While AI models are generally
		  non-deterministic and have high capability uncertainty, the
		  advent of foundation models further exacerbates the
		  complexity of working with AI and ensuring responsible
		  innovation. This complex work is achieved by not just AI
		  practitioners, but through coordination and collaboration
		  of different groups of practitioners, not all of whom might
		  be experts in AI. Our primary objective is to explore how
		  the tools and systems used by practitioners help achieve
		  this complex work. To that end, we conducted a design space
		  analysis of 18 relevant tools using corresponding research
		  publications and constructed a design space with the
		  dimensions - User, Axis of AI work, Semantics of Use, Tool
		  Architecture, Artifact Type and Availability, and
		  Collaboration Goals. Using these dimensions we derive four
		  spirits of the tool in supporting collaborations -
		  groupware, core practice &amp; communication, community of
		  practice, and visibility &amp; bridging. Through this work,
		  we contribute a conceptual design space of how tools can be
		  designed to support collaborations in AI development and
		  discuss how our design space can be leveraged by system
		  designers and researchers working at the intersection of
		  HCI, CSCW, and AI.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jan,
  articleno	= {GROUP2},
  numpages	= {28},
  keywords	= {ai, collaboration, design space, tools}
}

@Proceedings{	  10.1145/3705374,
  title		= {ICCDA '24: Proceedings of the 2024 8th International
		  Conference on Computing and Data Analysis},
  year		= {2024},
  isbn		= {9798400710445},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3712623.3712660,
  author	= {Shen, Meng and Luo, Ziheng and Liao, Yong},
  title		= {Comprehensive Out-of-context Misinformation Detection via
		  Global Information Enhancement},
  year		= {2025},
  isbn		= {9798400712883},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3712623.3712660},
  doi		= {10.1145/3712623.3712660},
  abstract	= {False information that spreads rapidly on the Internet may
		  have a negative impact on individuals, organizations, and
		  society. Cheapfake, which refers to out-of-context
		  misinformation, involves reusing the pairing of images and
		  captions to support false descriptions. Out-of-context
		  misinformation detection requires consideration of the
		  global semantic understanding of images and intertwined
		  contextual information. Currently, the baseline method
		  don’t extract enough global information. This paper uses
		  visual-semantic reasoning to extract the global features of
		  images and texts, and proposes a new detection framework
		  that employs fake statement checking and multimodal natural
		  language inference to determine whether the input triples
		  are cheapfake through multiple steps. The accuracy of our
		  method is 3.4% higher than that of the baseline method, and
		  it also has its own advantages compared with other improved
		  methods.},
  booktitle	= {Proceedings of the 2024 2nd International Conference on
		  Advances in Artificial Intelligence and Applications},
  pages		= {138–142},
  numpages	= {5},
  keywords	= {Content Security, Out-of-context, Misinformation
		  Detection, Multimodal Natural Language Inference},
  location	= { },
  series	= {AAIA '24}
}

@Article{	  10.1145/3716628,
  author	= {Deng, Zehang and Guo, Yongjian and Han, Changzhou and Ma,
		  Wanlun and Xiong, Junwu and Wen, Sheng and Xiang, Yang},
  title		= {AI Agents Under Threat: A Survey of Key Security
		  Challenges and Future Pathways},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3716628},
  doi		= {10.1145/3716628},
  abstract	= {An Artificial Intelligence (AI) agent is a software entity
		  that autonomously performs tasks or makes decisions based
		  on pre-defined objectives and data inputs. AI agents,
		  capable of perceiving user inputs, reasoning and planning
		  tasks, and executing actions, have seen remarkable
		  advancements in algorithm development and task performance.
		  However, the security challenges they pose remain
		  under-explored and unresolved. This survey delves into the
		  emerging security threats faced by AI agents, categorizing
		  them into four critical knowledge gaps: unpredictability of
		  multi-step user inputs, complexity in internal executions,
		  variability of operational environments, and interactions
		  with untrusted external entities. By systematically
		  reviewing these threats, this paper highlights both the
		  progress made and the existing limitations in safeguarding
		  AI agents. The insights provided aim to inspire further
		  research into addressing the security threats associated
		  with AI agents, thereby fostering the development of more
		  robust and secure AI agent applications.},
  note		= {Just Accepted},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  keywords	= {AI Agent, Trustworthiness, Security}
}

@Article{	  10.1145/3706631,
  author	= {Liu, Qidong and Qiu, Zhaopeng and Zhao, Xiangyu and Wu,
		  Xian and Zhang, Zijian and Xu, Tong and Tian, Feng},
  title		= {A Contrastive Pretrain Model with Prompt Tuning for
		  Multi-center Medication Recommendation},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3706631},
  doi		= {10.1145/3706631},
  abstract	= {Medication recommendation is one of the most critical
		  health-related applications, which has attracted extensive
		  research interest recently. Most existing works focus on a
		  single hospital with abundant medical data. However, many
		  small hospitals only have a few records, which hinders
		  applying existing medication recommendation works to the
		  real world. Thus, we seek to explore a more practical
		  setting, i.e., multi-center medication recommendation. In
		  this setting, most hospitals have few records, but the
		  total number of records is large. Though small hospitals
		  may benefit from total affluent records, it is also faced
		  with the challenge that the data distributions between
		  various hospitals are much different. In this work, we
		  introduce a novel conTrastive prEtrain Model with Prompt
		  Tuning (TEMPT) for multi-center medication recommendation,
		  which includes two stages of pretraining and finetuning. We
		  first design two self-supervised tasks for the pretraining
		  stage to learn general medical knowledge. They are mask
		  prediction and contrastive tasks, which extract the intra-
		  and inter-relationships of input diagnosis and procedures.
		  Furthermore, we devise a novel prompt tuning method to
		  capture the specific information of each hospital rather
		  than adopting the common finetuning. On the one hand, the
		  proposed prompt tuning can better learn the heterogeneity
		  of each hospital to fit various distributions. On the other
		  hand, it can also relieve the catastrophic forgetting
		  problem of finetuning. To validate the proposed model, we
		  conduct extensive experiments on the public eICU, a
		  multi-center medical dataset. The experimental results
		  illustrate the effectiveness of our model. The
		  implementation code is available to ease the
		  reproducibility1.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  keywords	= {Multi-Center, Medication Recommendation, Electronic Health
		  Record, Contrastive Learning, Prompt Tuning}
}

@Proceedings{	  10.1145/3704323,
  title		= {ICCPR '24: Proceedings of the 2024 13th International
		  Conference on Computing and Pattern Recognition},
  year		= {2024},
  isbn		= {9798400717482},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3717059,
  author	= {Liu, Huaping and Guo, Di and Cangelosi, Angelo},
  title		= {Embodied Intelligence: A Synergy of Morphology, Action,
		  Perception and Learning},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3717059},
  doi		= {10.1145/3717059},
  abstract	= {Embodied intelligence emphasizes that the intelligence is
		  affected by the tight coupling of brain, body and
		  environment. It is continuously and dynamically generated
		  through the process of information perception and physical
		  interaction with the environment. During the past years,
		  the research scope of embodied intelligence has also been
		  expanding and it has attracted great attentions from
		  various communities. At the same time, a huge number of
		  works relevant to embodied intelligence have been proposed,
		  especially in recent several years. In this paper, we
		  present a comprehensive survey of embodied intelligence
		  from the perspective that it is a synergy of morphology,
		  action, perception and learning, providing a thorough
		  summary and categorization of existing studies.
		  Specifically, as the embodied intelligence is a synergy of
		  all these components rather than themselves alone, we
		  mainly focus on the connections across these four
		  components (morphology, action, perception and learning)
		  and identify areas where future research can benefit from
		  their intrinsic connections.},
  note		= {Just Accepted},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  keywords	= {Embodied intelligence, morphology, action, perception,
		  learning.}
}

@Article{	  10.1145/3701211,
  author	= {Ankenbauer, Sam Addison and Brewer, Robin N.},
  title		= {Time's Sublimest Target: Practices of Forgetting in HCI
		  and CSCW},
  year		= {2025},
  issue_date	= {January 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {9},
  number	= {1},
  url		= {https://doi.org/10.1145/3701211},
  doi		= {10.1145/3701211},
  abstract	= {In our contemporary moment, there exists a hegemonic
		  design practice and a general social desire to retain
		  information. With the help of sociotechnical platforms and
		  other contemporary technologies, information has changed
		  its temporal and spatial boundaries, creating unbounded,
		  algorithmic, and emergent forms of retention. The
		  consequences of such retention are numerous, ranging from
		  an overabundance of autobiographical information that
		  cannot be fully understood by the individual to the
		  improper use and economization of such information by state
		  and corporation alike. Within this context, this paper
		  investigates a counter-hegemonic practice of forgetting,
		  specifically from the perspective of human-computer
		  interaction and computer-supported cooperative work
		  research, with additional insight drawn from adjacent
		  fields. In doing so, we present forgetting as a significant
		  area of research with HCI and CSCW, a burgeoning and
		  contradictory space that may offer solutions to issues we
		  face within a moment of persistence by default. This paper
		  also explores potential directions for future research and
		  design on forgetting in HCI and CSCW through an
		  investigation of an art piece by Chinese artist Song
		  Dong.},
  journal	= {Proc. ACM Hum.-Comput. Interact.},
  month		= jan,
  articleno	= {GROUP32},
  numpages	= {24},
  keywords	= {deletion, forgetting, intent, retention, spatial,
		  temporal}
}

@InProceedings{	  10.1145/3704814.3704819,
  author	= {Pang, Haijie and Li, Chengji},
  title		= {A BERT and TextCNN integration-based Method for Public
		  Complaints and Proposals Text Classification},
  year		= {2025},
  isbn		= {9798400718090},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3704814.3704819},
  doi		= {10.1145/3704814.3704819},
  abstract	= {With the wide use of Blockchain technology and online
		  public complaints and proposals (hereinafter referred to as
		  PCP) platforms, huge amounts of data are accordingly
		  produced, which poses a challenge on the government's
		  effectively choosing and classifying the key information.
		  In view of this, this paper endeavors to conduct research
		  on the PCP text classification for a smart government.
		  Firstly, PCP texts are collected automatically and
		  preprocessed, and then a hierarchical classification
		  structure involving targeted government departments and PCP
		  contents is proposed. Finally, a BERT and TextCNN model is
		  adopted for PCP text classification. The experimental
		  results show that the classification structure and model
		  proposed in this paper can effectively categorize the PCP
		  text, providing a scientific analysis and effective
		  technological method to improve the work efficiency and
		  service quality of PCP.},
  booktitle	= {Proceedings of the 8th International Conference on
		  Computer Science and Application Engineering},
  pages		= {15–18},
  numpages	= {4},
  keywords	= {BERT, Blockchain, Public complaints and proposals, Text
		  classification, TextCNN},
  location	= { },
  series	= {CSAE '24}
}

@Article{	  10.1145/3704922,
  author	= {Garcia, Cristiano Mesquita and Abilio, Ramon and Koerich,
		  Alessandro Lameiras and Britto, Alceu de Souza and Barddal,
		  Jean Paul},
  title		= {Concept Drift Adaptation in Text Stream Mining Settings: A
		  Systematic Review},
  year		= {2025},
  issue_date	= {April 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {2},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3704922},
  doi		= {10.1145/3704922},
  abstract	= {The society produces textual data online in several ways,
		  e.g., via reviews and social media posts. Therefore,
		  numerous researchers have been working on discovering
		  patterns in textual data that can indicate peoples’
		  opinions, interests, and so on. Most tasks regarding
		  natural language processing are addressed using traditional
		  machine learning methods and static datasets. This setting
		  can lead to several problems, e.g., outdated datasets and
		  models, which degrade in performance over time. This is
		  particularly true regarding concept drift, in which the
		  data distribution changes over time. Furthermore, text
		  streaming scenarios also exhibit further challenges, such
		  as the high speed at which data arrive over time. Models
		  for stream scenarios must adhere to the aforementioned
		  constraints while learning from the stream, thus storing
		  texts for limited periods and consuming low memory. This
		  study presents a systematic literature review regarding
		  concept drift adaptation in text stream scenarios.
		  Considering well-defined criteria, we selected 48 papers
		  published between 2018 and August 2024 to unravel aspects
		  such as text drift categories, detection types, model
		  update mechanisms, stream mining tasks addressed, and text
		  representation methods and their update mechanisms.
		  Furthermore, we discussed drift visualization and
		  simulation and listed real-world datasets used in the
		  selected papers. Finally, we brought forward a discussion
		  on existing works in the area, also highlighting open
		  challenges and future research directions for the
		  community.},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= feb,
  articleno	= {27},
  numpages	= {67},
  keywords	= {Concept drift, text stream mining, semantic shift,
		  representation shift, drift detection}
}

@Article{	  10.1145/3698193,
  author	= {Zhang, Dan and Zheng, Shaojie and Zhu, Yifan and Yuan,
		  Huihui and Gong, Jibing and Tang, Jie},
  title		= {MCAP: Low-Pass GNNs with Matrix Completion for Academic
		  Recommendations},
  year		= {2025},
  issue_date	= {March 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {43},
  number	= {2},
  issn		= {1046-8188},
  url		= {https://doi.org/10.1145/3698193},
  doi		= {10.1145/3698193},
  abstract	= {Graph neural networks (GNNs) are commonly used and have
		  shown promising performance in recommendation systems. A
		  major branch, heterogeneous GNNs, models heterogeneous
		  information by leveraging side information for academic
		  paper recommendations. These networks use message passing
		  and high-order propagation to learn representations for
		  users and items. However, existing recommendation methods
		  perform high-order propagation, leading to sub-optimal
		  representation learning. To address this issue, this
		  article proposes a framework called MCAP, which uses
		  relation-aware GNNs and executes low-pass propagation with
		  matrix completion to enhance academic paper
		  recommendations. The framework uses an attention mechanism
		  to learn top- (U) relationships by constructing a
		  user–user relation graph based on common authors and
		  venues from interacted items. To efficiently and
		  effectively capture semantic-aware similar items, MCAP
		  builds an item–item relation graph by fusing side
		  information of papers using text embedding models (e.g.,
		  Mistral) and large language models (e.g., GPT-3.5-Turbo,
		  GLM-4). Finally, the relation-aware user–user and
		  item–item graphs are incorporated into existing GNN-based
		  models to generate representations of users and papers to
		  enhance academic paper recommendations. The effectiveness
		  of the MCAP is validated using four academic datasets,
		  AMiner-PC, AMiner-WeChat, CiteULike, and DBLP, with
		  user–item interactions and side information of papers.
		  Comprehensive experiments show that the MCAP outperforms
		  state-of-the-art models in terms of Recall@5, NDCG@5, and
		  HR@5 with 69.2%, 70.5%, and 77.6% on the AMiner-WeChat
		  dataset. The code for MCAP is available at .},
  journal	= {ACM Trans. Inf. Syst.},
  month		= jan,
  articleno	= {33},
  numpages	= {29},
  keywords	= {Graph Neural Networks, Low-Pass Propagation, Relation
		  Graph, Matrix Completion, Academic Paper Recommendations}
}

@InProceedings{	  10.1145/3706890.3706971,
  author	= {Man, Jianping and Hu, Zhensheng and Liu, Hongze and Yang,
		  Rui and Liu, Jingjing and Chen, Ziyi and Zhou, Yi},
  title		= {A Model for Epilepsy Named Entity Recognition Based on
		  Chinese EEG Reports},
  year		= {2025},
  isbn		= {9798400717826},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3706890.3706971},
  doi		= {10.1145/3706890.3706971},
  abstract	= {It is critical for clinical diagnosis and research to
		  extract and utilize the medical information from
		  electroencephalogram (EEG) reports of epilepsy. With the
		  widespread application of deep learning techniques in
		  health care, particularly in natural language processing
		  tasks, named entity recognition (NER) has emerged as an
		  essential tool for information extraction from medical
		  text. This study proposed an epilepsy NER model for EEG
		  reports to improve the efficiency of epilepsy text
		  processing. 17,606 paragraphs from real Chinese epilepsy
		  EEG reports as data samples, were meticulously annotated
		  with 17 entity types by clinical experts. The model used
		  the BERT and the Global Pointer (GP) algorithm to identify
		  nested and non-nested entities, achieved outstanding
		  performance in the Precision, Recall and F1 score. The
		  experimental results demonstrate that our method
		  significantly enhances the effectiveness of epilepsy entity
		  recognition, providing robust support for the automated
		  extraction and analysis of medical information.},
  booktitle	= {Proceedings of the 2024 5th International Symposium on
		  Artificial Intelligence for Medicine Science},
  pages		= {467–472},
  numpages	= {6},
  keywords	= {BERT model, EEG reports, Epilepsy, Global Pointer
		  algorithm, Named entity recognition},
  location	= { },
  series	= {ISAIMS '24}
}

@Article{	  10.1145/3712296,
  author	= {Sun, Wei and Li, Mingxiao and Sileo, Damien and Davis,
		  Jesse and Moens, Marie-Francine},
  title		= {Generating Explanations in Medical Question-Answering by
		  Expectation Maximization Inference over Evidence},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3712296},
  doi		= {10.1145/3712296},
  abstract	= {Medical Question Answering (medical QA) systems play an
		  essential role in assisting healthcare workers in finding
		  answers to their questions. However, it is not sufficient
		  to merely provide answers by medical QA systems because
		  users might want explanations, that is, more analytic
		  statements in natural language that describe the elements
		  and context that support the answer. To do so, we propose a
		  novel approach for generating natural language explanations
		  for answers predicted by medical QA systems. As
		  high-quality medical explanations require additional
		  medical knowledge, so that our system extract knowledge
		  from medical textbooks to enhance the quality of
		  explanations during the explanation generation process.
		  Concretely, we designed an Expectation-Maximization
		  approach that makes inferences about the evidence found in
		  these texts, offering an efficient way to focus attention
		  on lengthy evidence passages. Experimental results,
		  conducted on two datasets MQAE-diag and MQAE, demonstrate
		  the effectiveness of our framework for reasoning with
		  textual evidence. Our approach outperforms state-of-the-art
		  models, achieving a significant improvement of 6.13 and
		  5.47 percentage points on the Rouge-L score; 6.49 and 5.28
		  percentage points on the Bleu-4 score on the MQAE-diag and
		  MQAE datasets.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Comput. Healthcare},
  month		= jan,
  keywords	= {Expectation Maximization, Medical Question Answering,
		  Explanation Generation}
}

@Article{	  10.1145/3714456,
  author	= {Haider Rizvi, Syed Mustafa and Imran, Ramsha and Mahmood,
		  Arif},
  title		= {Text Classification Using Graph Convolutional Networks: A
		  Comprehensive Survey},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3714456},
  doi		= {10.1145/3714456},
  abstract	= {Text classification is a quintessential and practical
		  problem in natural language processing with applications in
		  diverse domains such as sentiment analysis, fake news
		  detection, medical diagnosis, and document classification.
		  A sizable body of recent works exists where researchers
		  have studied and tackled text classification from different
		  angles with varying degrees of success. Graph convolution
		  network (GCN)-based approaches have gained a lot of
		  traction in this domain over the last decade with many
		  implementations achieving state-of-the-art performance in
		  more recent literature and thus, warranting the need for an
		  updated survey. This work aims to summarize and categorize
		  various GCN-based Text Classification approaches with
		  regard to the architecture and mode of supervision. It
		  identifies their strengths and limitations and compares
		  their performance on various benchmark datasets. We also
		  discuss future research directions and the challenges that
		  exist in this domain.},
  note		= {Just Accepted},
  journal	= {ACM Comput. Surv.},
  month		= jan,
  keywords	= {GCN, Text Classification, Text Analysis, Text
		  Categorization}
}

@Article{	  10.1145/3715005,
  author	= {Zhang, Shenglin and Xia, Sibo and Fan, Wenzhao and Shi,
		  Binpeng and Xiong, Xiao and Zhong, Zhenyu and Ma, Minghua
		  and Sun, Yongqian and Pei, Dan},
  title		= {Failure Diagnosis in Microservice Systems: A Comprehensive
		  Survey and Analysis},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1049-331X},
  url		= {https://doi.org/10.1145/3715005},
  doi		= {10.1145/3715005},
  abstract	= {Widely adopted for their scalability and flexibility,
		  modern microservice systems present unique failure
		  diagnosis challenges due to their independent deployment
		  and dynamic interactions. This complexity can lead to
		  cascading failures that negatively impact operational
		  efficiency and user experience. Recognizing the critical
		  role of fault diagnosis in improving the stability and
		  reliability of microservice systems, researchers have
		  conducted extensive studies and achieved a number of
		  significant results. This survey provides an exhaustive
		  review of 98 scientific papers from 2003 to the present,
		  including a thorough examination and elucidation of the
		  fundamental concepts, system architecture, and problem
		  statement. It also includes a qualitative analysis of the
		  dimensions, providing an in-depth discussion of current
		  best practices and future directions, aiming to further its
		  development and application. In addition, this survey
		  compiles publicly available datasets, toolkits, and
		  evaluation metrics to facilitate the selection and
		  validation of techniques for practitioners.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Softw. Eng. Methodol.},
  month		= jan,
  keywords	= {Microservice, failure diagnosis, root cause localization,
		  failure classification, multimodal data}
}

@Article{	  10.1145/3714430,
  author	= {Xu, Shuyuan and Ji, Jianchao and Li, Yunqi and Ge,
		  Yingqiang and Tan, Juntao and Zhang, Yongfeng},
  title		= {Causal Inference for Recommendation: Foundations, Methods
		  and Applications},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3714430},
  doi		= {10.1145/3714430},
  abstract	= {Recommender systems are important and powerful tools for
		  various personalized services. Traditionally, these systems
		  use data mining and machine learning techniques to make
		  recommendations based on correlations found in the data.
		  However, relying solely on correlation without considering
		  the underlying causal mechanism may lead to various
		  practical issues such as fairness, explainability,
		  robustness, bias, echo chamber and controllability
		  problems. Therefore, researchers in related area have begun
		  incorporating causality into recommendation systems to
		  address these issues. In this survey, we review the
		  existing literature on causal inference in recommender
		  systems. We discuss the fundamental concepts of both
		  recommender systems and causal inference as well as their
		  relationship, and review the existing work on causal
		  methods for different problems in recommender systems.
		  Finally, we discuss open problems and future directions in
		  the field of causal inference for recommendations.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= jan,
  keywords	= {Recommender Systems, Causal Inference}
}

@Article{	  10.1145/3673233,
  author	= {M\"{o}ller, Lucas and Pad\'{o}, Sebastian},
  title		= {Explaining Neural News Recommendation with Attributions
		  onto Reading Histories},
  year		= {2025},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {16},
  number	= {1},
  issn		= {2157-6904},
  url		= {https://doi.org/10.1145/3673233},
  doi		= {10.1145/3673233},
  abstract	= {An important aspect of responsible recommendation systems
		  is the transparency of the prediction mechanisms. This is a
		  general challenge for deep-learning-based systems such as
		  the currently predominant neural news recommender
		  architectures, which are optimized to predict clicks by
		  matching candidate news items against users’ reading
		  histories. Such systems achieve state-of-the-art
		  click-prediction performance, but the rationale for their
		  decisions is difficult to assess. At the same time, the
		  economic and societal impact of these systems makes such
		  insights very much desirable.In this article, we ask the
		  question to what extent the recommendations of current news
		  recommender systems are actually based on content-related
		  evidence from reading histories. We approach this question
		  from an explainability perspective. Building on the concept
		  of integrated gradients, we present a neural news
		  recommender that can accurately attribute individual
		  recommendations to news items and words in input reading
		  histories while maintaining a top scoring click-prediction
		  performance.Using our method as a diagnostic tool, we find
		  that: (a), a substantial number of users’ clicks on news
		  are not explainable from reading histories, and many
		  history-explainable items are actually skipped; (b), while
		  many recommendations are based on content-related evidence
		  in histories, for others the model does not attend to
		  reasonable evidence, and recommendations stem from a
		  spurious bias in user representations. Our code is publicly
		  available at .},
  journal	= {ACM Trans. Intell. Syst. Technol.},
  month		= jan,
  articleno	= {7},
  numpages	= {25},
  keywords	= {News recommendation, explainability, attribution,
		  interpretability, diagnosis, neural recommender}
}

@Proceedings{	  10.1145/3704522,
  title		= {NSysS '24: Proceedings of the 11th International
		  Conference on Networking, Systems, and Security},
  year		= {2024},
  isbn		= {9798400711589},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3701100,
  title		= {ADMIT '24: Proceedings of the 2024 3rd International
		  Conference on Algorithms, Data Mining, and Information
		  Technology},
  year		= {2024},
  isbn		= {9798400718120},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@InProceedings{	  10.1145/3708036.3708077,
  author	= {Yang, Guangyuan and Bai, Meicheng and Xie, Quanying and
		  Chen, Lu},
  title		= {Review on ChatGPT in Library of China},
  year		= {2025},
  isbn		= {9798400709999},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3708036.3708077},
  doi		= {10.1145/3708036.3708077},
  abstract	= {As an important application field of new technology, the
		  field of library has conducted in-depth discussions on the
		  application of ChatGPT, and have been achieved a certain
		  number of research results. For providing some reference
		  for the future application of ChatGPT in libraries, this
		  paper analyzes the current research status of the
		  application of ChatGPT in libraries from the aspects of
		  Academic Attention, Research Subjects and Main Research
		  Topics, and puts forward that it should strengthen the
		  discussion of the application of ChatGPT in libraries from
		  the aspects of “Improve the influence of the application
		  of ChatGPT in Chinese libraries to help build the core
		  research group” and “Deepen the exchange and
		  cooperation of interdisciplinary talents to empower the
		  practical of the application of ChatGPT in Chinese
		  libraries”.},
  booktitle	= {Proceedings of the 2024 5th International Conference on
		  Computer Science and Management Technology},
  pages		= {238–241},
  numpages	= {4},
  keywords	= {AI chatbot, ChatGPT, data visualization, intelligent
		  service, research progress},
  location	= { },
  series	= {ICCSMT '24}
}

@InProceedings{	  10.1109/scw63240.2024.00193,
  author	= {Ta\c{s}yaran, Fatih and Yasal, Osman and Morgado, Jos\'{e}
		  A. and Ilic, Aleksandar and Unat, Didem and Kaya, Kamer},
  title		= {P-MoVE: Performance Monitoring and Visualization with
		  Encoded Knowledge},
  year		= {2025},
  isbn		= {9798350355543},
  publisher	= {IEEE Press},
  url		= {https://doi.org/10.1109/SCW63240.2024.00193},
  doi		= {10.1109/SCW63240.2024.00193},
  abstract	= {P-MoVE is a modern, open-source framework designed to
		  monitor and visualize live and/or recorded performance data
		  with the ultimate goal of being a digital twin for HPC
		  systems. Leveraging a Knowledge Base (KB), built upon an
		  HPC-specific ontology with an intuitive encoding for
		  comprehending the performance, it rigorously manages
		  telemetry samplers, databases, and visualization
		  frameworks. The KB is generated through an in-depth probing
		  of the system. It enables the configuration and monitoring
		  of performance metric samplers, the generation of real-time
		  visualizations, the establishment of linked-data
		  connections, and the generation of queries for advanced
		  analysis. Furthermore, with an Abstraction Layer, P-MoVE
		  can be used for low-level profiling even on components from
		  different vendors. It is equipped with modern profiling
		  capabilities, including live cache-aware roofline modeling,
		  crafted to provide realtime insights without impeding
		  system performance. P-MoVE's capabilities have been
		  demonstrated on various architectures using microbenchmarks
		  and a common kernel, sparse-matrix vector multiplication.},
  booktitle	= {Proceedings of the SC '24 Workshops of the International
		  Conference on High Performance Computing, Network, Storage,
		  and Analysis},
  pages		= {1531–1542},
  numpages	= {12},
  keywords	= {HPC, digital twins for HPC, optimization, performance
		  visualization, profiling},
  location	= {Atlanta, GA, USA},
  series	= {SC-W '24}
}

@Proceedings{	  10.1145/3702191,
  title		= {ICDIS '24: Proceedings of the 2024 International Symposium
		  on Integrated Circuit Design and Integrated Systems},
  year		= {2024},
  isbn		= {9798400718229},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3708778,
  title		= {CIIS '24: Proceedings of the 2024 7th International
		  Conference on Computational Intelligence and Intelligent
		  Systems},
  year		= {2024},
  isbn		= {9798400717437},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3709681,
  author	= {Omar, Reham and Mangukiya, Omij and Mansour, Essam},
  title		= {Dialogue Benchmark Generation from Knowledge Graphs with
		  Cost-Effective Retrieval-Augmented LLMs},
  year		= {2025},
  issue_date	= {February 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {3},
  number	= {1},
  url		= {https://doi.org/10.1145/3709681},
  doi		= {10.1145/3709681},
  abstract	= {Dialogue benchmarks are crucial in training and evaluating
		  chatbots engaging in domain-specific conversations.
		  Knowledge graphs (KGs) represent semantically rich and
		  well-organized data spanning various domains, such as DBLP,
		  DBpedia, and YAGO. Traditionally, dialogue benchmarks have
		  been manually created from documents, neglecting the
		  potential of KGs in automating this process. Some
		  question-answering benchmarks are automatically generated
		  using extensive preprocessing from KGs, but they do not
		  support dialogue generation. This paper introduces
		  Chatty-Gen, a novel multi-stage retrieval-augmented
		  generation platform for automatically generating
		  high-quality dialogue benchmarks tailored to a specific
		  domain using a KG. Chatty-Gen decomposes the generation
		  process into manageable stages and uses assertion rules for
		  automatic validation between stages. Our approach enables
		  control over intermediate results to prevent time-consuming
		  restarts due to hallucinations. It also reduces reliance on
		  costly and more powerful commercial LLMs. Chatty-Gen
		  eliminates upfront processing of the entire KG using
		  efficient query-based retrieval to find representative
		  subgraphs based on the dialogue context. Our experiments
		  with several real and large KGs demonstrate that Chatty-Gen
		  significantly outperforms state-of-the-art systems and
		  ensures consistent model and system performance across
		  multiple LLMs of diverse capabilities, such as GPT-4o,
		  Gemini 1.5, Llama 3, and Mistral.},
  journal	= {Proc. ACM Manag. Data},
  month		= feb,
  articleno	= {31},
  numpages	= {26},
  keywords	= {assertion-based validation, benchmarking, conversational
		  question answering, cost-effecive inference, graph
		  serialization, knowledge graphs (kgs), large language
		  models (llms), retrieval-augumented generation (rag)}
}

@Proceedings{	  10.5555/3715674,
  title		= {SC-W '24: Proceedings of the SC '24 Workshops of the
		  International Conference on High Performance Computing,
		  Network, Storage, and Analysis},
  year		= {2024},
  isbn		= {9798350355543},
  publisher	= {IEEE Press},
  location	= {Atlanta, GA, USA}
}

@Article{	  10.1145/3711908,
  author	= {Garg, Piyush and Chakraborty, Roshni and Dandapat,
		  Sourav},
  title		= {PORTRAIT: A Hybrid Approach to Create Extractive
		  Ground-truth Summary for Disaster Event},
  year		= {2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  issn		= {1559-1131},
  url		= {https://doi.org/10.1145/3711908},
  doi		= {10.1145/3711908},
  abstract	= {Nowadays, Twitter is an important source of information
		  and latest updates during ongoing events, such as disaster
		  events. However, the huge number of tweets posted during a
		  disaster makes identification of relevant information
		  highly challenging. Therefore, a summary of the tweets can
		  help the decision-makers to ensure efficient allocation of
		  resources among the affected population. There exist
		  several automated summarization approaches which can
		  generate a summary given the tweets related to a disaster.
		  Development of these automated summarization approaches
		  require availability of ground-truth summary of the dataset
		  for verification. However, the number of publicly available
		  datasets along with the ground-truth summary for disaster
		  events are still inadequate. To improve this situation, we
		  need to create more number of ground-truth summaries.
		  Existing approaches for ground-truth summary generation
		  rely on the annotators’ wisdom and intuition. This
		  process requires immense human effort and significant time.
		  Moreover, the selection of the important tweets from the
		  humongous set of input tweets often results in sub-optimal
		  choice of tweets in the final summary. Therefore, to handle
		  these challenges, we propose a hybrid approach (PORTRAIT)
		  for ground-truth summary generation, where we partly
		  automate the procedure to improve the quality of
		  ground-truth summary and reduce human effort and time. We
		  validate the effectiveness of PORTRAIT on 9 disaster events
		  through quantitative and qualitative analysis. We prepare
		  and release the ground-truth summaries for 9 disaster
		  events, which consist of both natural and man-made disaster
		  events belonging to 5 different continents.},
  note		= {Just Accepted},
  journal	= {ACM Trans. Web},
  month		= jan,
  keywords	= {Disaster tweet summarization, Ground-truth summary, Social
		  media, Hybrid approach}
}

@Proceedings{	  10.1145/3708282,
  title		= {AITC '24: Proceedings of the 2024 International Conference
		  on Artificial Intelligence of Things and Computing},
  year		= {2024},
  isbn		= {9798400709869},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Article{	  10.1145/3637487,
  author	= {Hossain, Md Imran and Zamzmi, Ghada and Mouton, Peter R.
		  and Salekin, Md Sirajus and Sun, Yu and Goldgof, Dmitry},
  title		= {Explainable AI for Medical Data: Current Methods,
		  Limitations, and Future Directions},
  year		= {2025},
  issue_date	= {June 2025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {57},
  number	= {6},
  issn		= {0360-0300},
  url		= {https://doi.org/10.1145/3637487},
  doi		= {10.1145/3637487},
  abstract	= {With the power of parallel processing, large datasets, and
		  fast computational resources, deep neural networks (DNNs)
		  have outperformed highly trained and experienced human
		  experts in medical applications. However, the large global
		  community of healthcare professionals, many of whom
		  routinely face potentially life-or-death outcomes with
		  complex medicolegal consequences, have yet to embrace this
		  powerful technology. The major problem is that most current
		  AI solutions function as a metaphorical black-box
		  positioned between input data and output decisions without
		  a rigorous explanation for their internal processes. With
		  the goal of enhancing trust and improving acceptance of
		  artificial intelligence– (AI) based technology in
		  clinical medicine, there is a large and growing effort to
		  address this challenge using eXplainable AI (XAI), a set of
		  techniques, strategies, and algorithms with an explicit
		  focus on explaining the “hows and whys” of DNNs. Here,
		  we provide a comprehensive review of the state-of-the-art
		  XAI techniques concerning healthcare applications and
		  discuss current challenges and future directions. We
		  emphasize the strengths and limitations of each category,
		  including image, tabular, and textual explanations, and
		  explore a range of evaluation metrics for assessing the
		  effectiveness of XAI solutions. Finally, we highlight
		  promising opportunities for XAI research to enhance the
		  acceptance of DNNs by the healthcare community.},
  journal	= {ACM Comput. Surv.},
  month		= feb,
  articleno	= {148},
  numpages	= {46},
  keywords	= {Explainability, medical data, responsible AI, deep neural
		  networks, interpretable AI}
}

@Proceedings{	  10.1145/3708597,
  title		= {ICACS '24: Proceedings of the 2024 8th International
		  Conference on Algorithms, Computing and Systems},
  year		= {2024},
  isbn		= {9798400718304},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3707127,
  title		= {ICBBE '24: Proceedings of the 2024 11th International
		  Conference on Biomedical and Bioinformatics Engineering},
  year		= {2024},
  isbn		= {9798400718274},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}

@Proceedings{	  10.1145/3700666,
  title		= {ICBRA '24: Proceedings of the 11th International
		  Conference on Bioinformatics Research and Applications},
  year		= {2024},
  isbn		= {9798400717536},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  location	= { }
}
